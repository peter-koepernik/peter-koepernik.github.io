[
  {
    "front": "PGWS:<div></div><div>Definition Nullschema und erste Charakterisierung</div>",
    "back": "Seite 1"
  },
  {
    "front": "PGWS:<div></div><div>Definition und Eigenschaften erzeugende Funktion</div>",
    "back": "<div></div><div>[$$]\\begin{align*}&amp;(1) \\ g_X^{(r)}(0) = r! \\mathbb{P}(X = r)\\\\&amp;(2) \\ X, Y \\text{ unabhaengig } \\implies g_{X+Y} = g_X \\cdot g_Y\\\\&amp;(3) \\ X \\sim \\textrm{Po}(\\lambda) \\implies g_X(s) = \\mathrm{e}^{\\lambda (s-1)}\\end{align*}[/$$]</div><div></div><div></div>[Seite 1]"
  },
  {
    "front": "PGWS:<div></div><div>Charakterisierung Vereilungskonvergenz [$]\\mathbb{N}_0[/$]-wertige ZVen</div>",
    "back": "Seite 1"
  },
  {
    "front": "PGWS:<div></div><div>Charakterisierung Nullschema mit EF/CF</div>",
    "back": "Seite 1"
  },
  {
    "front": "PGWS:<div></div><div>Poissonscher GWS</div>",
    "back": "Seite 1"
  },
  {
    "front": "PGWS:<div></div><div>Uebersicht</div>",
    "back": "-Nullschema<div>-EF (Defintion, Charakterisierung Vert-Konv, Charakterisierung Nullschema)</div><div>-PGWS</div>"
  },
  {
    "front": "MM:<div></div><div>Fatou fuer Verteilungskonvergenz<div></div><div>Beispiel &nbsp;[$]X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} X \\text{ aber } \\mathbb{E} X_n[/$] konv nicht</div></div>",
    "back": "Seite 1"
  },
  {
    "front": "MM:<div></div><div>Definition gleichgradig integrierbar und Zusammenhang mit</div><div><br><div><div>[$$]\\sup_n \\mathbb{E}|X_n|^\\star[/$$]</div></div></div>",
    "back": "Seite 1"
  },
  {
    "front": "MM:<div>[$$](X_n) \\text{ gleichgr. ib und } X_n\\mathrel{\\stackrel{d}{\\longrightarrow}} X[/$$]</div>",
    "back": "Seite 1"
  },
  {
    "front": "MM:<div></div><div>Bedingung Konvergenz der Momente (aus GI)</div>",
    "back": "Seite 1"
  },
  {
    "front": "MM:<div></div><div>Momentenmethode</div>",
    "back": "Seite 2"
  },
  {
    "front": "MM:<div></div><div>Kriterium fuer Bestimmtheit der Verteilung durch Momente</div>",
    "back": "Seite 2"
  },
  {
    "front": "MM:<div></div><div>(4) Beispiele</div>",
    "back": "Seite 2"
  },
  {
    "front": "MM:<div></div><div>Ueberblick</div>",
    "back": "-Fatou fuer Verteilungskonvergenz<div>-GI (Definition, [$]\\sup \\mathbb{E} |X_n|, \\mathbb{E} X_n \\longrightarrow \\mathbb{E} X[/$])</div><div>-[$]\\mathbb{E} X_n^r \\longrightarrow &nbsp;\\mathbb{E} X^r [/$]</div><div>-Momentenmthode (Satz, Bedingung, Beispiele)</div>"
  },
  {
    "front": "m-ZGWS:<div></div><div>Definition stationaer und [$]m[/$]-abhaengig</div>",
    "back": "Seite 2"
  },
  {
    "front": "m-ZGWS:<div></div><div>Beispiel: Funktion einer uiv-Folge</div>",
    "back": "Seite 2"
  },
  {
    "front": "m-ZGWS:<div></div><div>Langzeitvarianz [$] \\sigma^2 [/$]</div>",
    "back": "Seite 2"
  },
  {
    "front": "m-ZGWS:<div></div><div>ZGWS (Voraussetzungen, Aussage, Beweis)</div><div></div><div>Was wenn [$]\\sigma^2 = 0[/$]?</div>",
    "back": "[$] (Y_j)_j \\ m \\text{-abhaengig und stationaer, } \\mathbb{E} Y_1^2 &lt; \\infty, \\sigma^2 &gt; 0. \\text{ Dann gilt mit } S_n := \\sum_{j=1}^n Y_j\\colon[/$]<div></div><div>[$$] \\frac{S_n - \\mathbb{E} S_n}{\\sqrt{n}} \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}(0, \\sigma^2). [/$$]</div><div></div><div>[Seite 2]</div>"
  },
  {
    "front": "m-ZGWS:<div></div><div>Lemma fuer m-ZGWS.</div>",
    "back": "Seite 2"
  },
  {
    "front": "Nd:<div></div><div>Definition [$]\\Sigma (X)[/$] und [$] \\Sigma(AX + b) = ? [/$]</div>",
    "back": "[$] \\Sigma(AX + b) = A \\Sigma(X) A^\\top [/$]<br><div></div><div>[Seite 2]</div>"
  },
  {
    "front": "m-ZGWS:<div></div><div>Ueberblick</div>",
    "back": "-Definition und Beispiel (Funktion von uiv Folge)<div>-Langzeitvarianz</div><div>-Lemma</div><div>-Satz</div>"
  },
  {
    "front": "Nd:<div></div><div>[$]\\Sigma X \\ge 0[/$] und Charakterisierung von [$] \\Sigma X [/$] invertierbar.</div>",
    "back": "Trick in beiden Faellen:<div><br><div>[$$] (0 \\le) \\ \\mathbb{V}(c^\\top X) = c^\\top \\Sigma c \\ ( = 0 \\text{ gdw } c \\in \\ker \\Sigma)[/$$]</div></div>"
  },
  {
    "front": "Nd:<div></div><div>Definition [$]d[/$]-dimensionale Normalverteilung</div>",
    "back": "Bemerkung:<div>-[$]\\mathbb{E} X[/$] und [$] \\Sigma X[/$] exisiteren und legen die Verteilung von [$]X[/$] fest.</div><div>-Jeder Subvektor ist auch normalverteilt.<br><div></div><div>[Seite 2]</div></div>"
  },
  {
    "front": "Nd:<div></div><div>[$$] X \\sim \\mathcal{N}_d(a, \\Sigma) \\implies AX + b \\sim ?[/$$]</div>",
    "back": "[$$] AX + b \\sim \\mathcal{N}_s(Aa + b, A \\Sigma A^\\top) [/$$]<div></div><div>[Seite 2]</div>"
  },
  {
    "front": "Nd:<div></div><div>[$$] \\text{Fuer } \\Sigma \\ge 0\\colon \\Sigma = AA^\\top [/$$]</div>",
    "back": "Seite 2"
  },
  {
    "front": "Nd:<div></div><div>Existenz Normalverteilung [$] \\mathcal{N}_d(a, \\Sigma)[/$] fuer [$] a \\in \\mathbb{R}^d, \\Sigma \\ge 0[/$].</div>",
    "back": "[$$] Y := (Y_1, \\ldots, Y_d) \\sim \\mathcal{N}_d(0, I_d) \\implies AY + b \\sim \\mathcal{N}_d(a, \\Sigma) \\text{ falls } \\Sigma = AA^\\top [/$$]<div></div><div>[Seite 2]</div>"
  },
  {
    "front": "Nd:<div></div><div>gemeinsam normalverteilte Zufallsvektoren sind unabhaengig gdw unkorreliert.</div><div></div><div>Insbesondere:</div><div>[$$] X \\sim \\mathcal{N}_d(a, \\Sigma)\\colon X_1,\\ldots,X_d \\text{ unabhaengig } \\iff \\Sigma \\text{ diagonal}.[/$$]</div>",
    "back": "Seite 2/3"
  },
  {
    "front": "Nd:<div></div><div>Additionssatz [$]d[/$]-dimensionale Normalverteilung</div>",
    "back": "Seite 3"
  },
  {
    "front": "Nd:<div></div><div>Dichte von [$] X \\sim \\mathcal{N}_d(a, \\Sigma)[/$] fuer [$]\\Sigma &gt; 0[/$]</div>",
    "back": "[$$] f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^d &nbsp;\\det \\Sigma}} \\exp \\left( - \\frac 12 (x - a)^\\top \\Sigma^{-1} (x-a) \\right) [/$$]<div></div><div>Beweis via Transformationsformel</div><div></div><div>[Seite 3]</div>"
  },
  {
    "front": "Nd:<div></div><div>Hauptkomponentenzerlegung</div>",
    "back": "Falls [$] X \\sim \\mathcal{N}_d(a, \\Sigma)\\colon[/$]:<div></div><div>[$$] X = \\sum_{j=1}^d \\sqrt{\\lambda_j} Z_j v_j + a \\text{ mit } Z_j \\sim \\mathcal{N}(0,1) \\text{ i.i.d.} [/$$]</div><div></div><div>[Seite 3]</div>"
  },
  {
    "front": "Nd:<div></div><div>Erzeugung [$] \\chi_d^2 [/$]-Verteilung aus Normalverteilung</div>",
    "back": "[$$] X \\sim \\mathcal{N}_d(a, \\Sigma), \\Sigma &gt; 0 \\implies (X-a)^\\top \\Sigma^{-1} (X-a) \\sim \\chi_d^2 [/$$]<div></div><div>[Seite 3]</div>"
  },
  {
    "front": "<div>VKRd:</div><div></div>Definition Verteilungsfunktion und charakterisierende Eigenschaften",
    "back": "Seite 3"
  },
  {
    "front": "VKRd:<div></div><div>Portmanteau-Theorem</div>",
    "back": "Seite 3"
  },
  {
    "front": "VKRd:<div></div><div>CMT</div>",
    "back": "Seite 3"
  },
  {
    "front": "VKRd:<div></div><div>Slutsky</div>",
    "back": "Seite 3"
  },
  {
    "front": "VKRd:<div></div><div>Definition Straffheit und relative Kompaktheit.</div>",
    "back": "Seite 3"
  },
  {
    "front": "VKRd:<div></div><div>Zusammenhang zwischen [$] X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} X [/$] und [$](X_n)_n[/$] straff.</div>",
    "back": "Seite 4"
  },
  {
    "front": "VKRd:<div></div><div>Stetigkeitssatz von Lévy-Cramer im [$]\\mathbb{R}^d[/$].</div>",
    "back": "Seite 4"
  },
  {
    "front": "VKRd:<div></div><div>Cramér-Wold-Technik fuer Verteilungskonvergenz.</div>",
    "back": "[$$] X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} X \\Longleftrightarrow \\forall c \\in \\mathbb{R}^d\\colon c^\\top X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} c^\\top X[/$$]<div></div><div>[Seite 4]</div>"
  },
  {
    "front": "VKRd:<div></div><div>Multivariater ZGWS.</div>",
    "back": "Seite 4"
  },
  {
    "front": "VKRd:<div></div><div>[$]\\chi^2[/$]-Test fuer Multinomialverteilung.</div>",
    "back": "Ergebnis:<div><br><div>[$$]\\sum_{j=1}^s \\frac{(N_{nj} - n p_j)^2}{n p_j} \\mathrel{\\stackrel{d}{\\longrightarrow}} \\chi^2_{s-1}.[/$$]</div><div></div><div>[Seite 4]</div></div>"
  },
  {
    "front": "VKRd:<div></div><div>Delta-Methode.</div>",
    "back": "Seite 4"
  },
  {
    "front": "EVF:<div></div><div>Definition EVF + 2 Bemerkungen</div>",
    "back": "Bemerkungen:<div><br><div>(1) Wovon ist [$]F_n(\\omega,\\cdot)[/$] die VF?</div><div>(2) SGGZ</div><div></div><div>[Seite 4]</div></div>"
  },
  {
    "front": "EVF:<div></div><div>Glivenko-Cantelli (zunaechst ohne Beweis)</div>",
    "back": "Seite 4"
  },
  {
    "front": "EVF:<div></div><div>Bemerkung zu Glivenko-Cantelli (mit konvexen Mengen)</div>",
    "back": "Seite 4"
  },
  {
    "front": "EVF:<div></div><div>DKW-Ungleichung (+2 Bemerkungen)</div>",
    "back": "(1) Impliziert Glivenko-Cantelli<div>(2) Genuegt fuer GV zu zeigen</div><div></div><div>[Seite 4]</div>"
  },
  {
    "front": "EVF:<div></div><div>fidi-Konvergenz BB</div>",
    "back": "Seite 4"
  },
  {
    "front": "<div>U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):</div><div></div><div>Definition U-Statistik und zwei Beispiele</div>",
    "back": "Seite 5"
  },
  {
    "front": "<div>U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):</div><div></div><div>Definition und Zusammenhang [$]\\sigma_c, h_c[/$].</div>",
    "back": "Seite 5"
  },
  {
    "front": "<div>U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):</div><div></div><div>Varianz U-Statistik und Beispiel mit Stichprobenvarianz.</div>",
    "back": "Seite 5"
  },
  {
    "front": "<div>U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):</div><div></div><div>Hajek-Projektion (Definition und Eigenschaften)</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):<div></div><div>ZGWS und Beispiel</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 &gt; 0[/$]):<br><div></div><div>Uebersicht</div>",
    "back": "-Definition (mit Beispiel)<div>-[$]h_c, \\sigma_c^2, \\mathbb{V}(U_n)[/$] (mit Beispiel)</div><div>-Hajek-Projektion (Definition und Eigenschaften)</div><div>-GWS (mit Beispiel)</div><div></div><div>[Seite 5]</div>"
  },
  {
    "front": "U-Statistiken (2-Stichproben):<div></div><div>Definition [$]U_{mn}[/$].</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken (2-Stichproben):<div></div><div>[$] \\sigma_{cd}, h_{cd}, \\mathbb{V}(U_{mn}) [/$].</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken (2-Stichproben):<div></div><div>Hajek-Projektion Definition und Darstellung.</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken (2-Stichproben):<div></div><div>GWS</div>",
    "back": "Seite 5"
  },
  {
    "front": "U-Statistiken (2-Stichproben):<div></div><div>Mann-Whitney-Statistik.</div>",
    "back": "Seite 6"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<div></div><div>Motivation</div>",
    "back": "[$]n(U_n - \\vartheta)[/$] ist straff [$]\\rightarrow[/$] Grenzverteilung?<div></div><div>Seite 6</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Einleitendes Beispiel</div>",
    "back": "Kern [$]h[/$] mit [$]nU_n \\mathrel{\\stackrel{d}{\\longrightarrow}} \\sum_{\\nu=1}^s \\lambda_\\nu (N_\\nu^2 - 1)[/$].<div></div><div>[Seite 6]</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Hajek-Projektion (Definition und Eigenschaften)</div>",
    "back": "Seite 6"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Zwischenergebnis</div>",
    "back": "[$$] n(\\hat{U}_n - \\vartheta) = \\binom{k}{2} \\frac{n}{n-1} T_n, \\text{ mit } T_n := \\frac 1n \\sum_{j\\neq l} \\tilde{h}_2(X_j,X_l)[/$$]<div></div><div>[Seite 6]</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Definition Kern [$]K[/$], passender [$]L^2[/$], darauf Integraloperator [$]A[/$] und Eigenschaften [$]\\leadsto K_s[/$].</div>",
    "back": "[$]A[/$] ist linearer beschraenkter symmetrischer Hilbert-Schmidt-Operator [$]\\leadsto \\lambda_j, \\varphi_j[/$]. Dann<div></div><div>[$] K_s(x, y) := \\sum_{j=1}^s \\lambda_j \\varphi_j(x) \\varphi_j(y) [/$].</div><div></div><div>[Seite 6]</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Approximation [$]K[/$] durch [$]K_s[/$].</div>",
    "back": "Seite 6 (Lemma 8.25)"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Definition [$]T_{ns}[/$] und Approximation von [$]T_n[/$].</div>",
    "back": "[$$]\\mathbb{E} (T_n - T_{ns})^2 \\le 2 \\sum_{j = s+1}^\\infty \\lambda_j^2.[/$$]<div></div><div>Zusatz Seite 1</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>GWS</div>",
    "back": "[$]U_n[/$] U-Statistik mit [$]0 = \\sigma_1^2 &lt; \\sigma_2^2[/$]. Dann<div></div><div>[$$] n(U_n - \\vartheta) \\mathrel{\\stackrel{d}{\\longrightarrow}} \\binom{k}{2} \\sum_{j=1}^\\infty \\lambda_j (N_j^2 -1). [/$$]</div><div></div><div>[Seite 6]</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<br><div></div><div>Uebersicht</div>",
    "back": "-Motivation und einfuehrendes Beispiel<div>-Idee und Hajek-Projektion [$]\\leadsto[/$] Zwischenergebnis</div><div>-Definition [$]K, A, K_s[/$]</div><div>-Approximation [$]K[/$] durch [$]K_s[/$] und [$]T_n[/$] durch [$]T_{ns}[/$]</div><div>-GWS</div><div>-Cramer-von-Mises Statistik</div>"
  },
  {
    "front": "U-Statistiken ([$] \\sigma_1^2 = 0[/$]):<div></div><div>Cramer-von-Mises</div>",
    "back": "Seite 6/7"
  },
  {
    "front": "Schaetztheorie:<div></div><div>Setting, parametrisches Modell, kanonisches Modell</div>",
    "back": "Seite 7"
  },
  {
    "front": "Schaetztheorie:<div></div><div>Definition Schaetzer, Kriterium fuer schwache Konsistenz.</div>",
    "back": "Kriterium:<div></div><div>[$$] (S_n) \\text{ asymptotisch erwartungstreu und } \\forall j\\colon \\mathbb{V}(S_{nj}) \\stackrel{  }{\\longrightarrow} 0 \\implies (S_n) \\text{ konsistent.}[/$$]</div><div><div></div><div>[Seite 7]</div></div>"
  },
  {
    "front": "Schaetztheorie:<div></div><div>Beispiel Schaetzer fuer [$]\\sigma^2[/$] bei [$]\\mathcal{N}(\\mu,\\sigma^2)[/$].</div>",
    "back": "Wegen §8:<div></div><div>[$$]\\frac{1}{n-1} \\sum_{j=1}^n (X_j - \\overline{X}_n)^2 \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}(0,2\\sigma^4),[/$$]</div><div></div><div>also [$]\\sqrt{n}[/$]-konsistent.</div><div></div><div>[Seite 7]</div>"
  },
  {
    "front": "Schaetztheorie:<div></div><div>Asymptotischer Konfidenzbereich und Beispiel.</div>",
    "back": "Seite 7"
  },
  {
    "front": "MLL:<div></div><div>Setting, ML-Schaetzer ohne Asymptotik und Beispiel (Binomialfall).</div>",
    "back": "Seite 7"
  },
  {
    "front": "MLL:<div></div><div>Definition asymptotischer ML-Schaetzer</div>",
    "back": "Seite 7"
  },
  {
    "front": "MLL:<div></div><div>Ziel und Idee [$]\\leadsto[/$] Regularitaetsvoraussetzungen.</div>",
    "back": "Ziel: [$] \\sqrt{n} (\\hat{\\vartheta}_n-\\vartheta) \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}_d(0, ?)[/$].<div><br><div>Idee:</div><div>&nbsp;- ZGWS -&gt; Summe? -&gt; Log-likelihood-Funktion ([$]\\leadsto[/$] Score-Vektor).</div><div>&nbsp;- [$]\\hat{\\vartheta}_n[/$] konsistent -&gt; Taylor? -&gt; Glattheitsbedingungen! (1)</div><div>&nbsp;- Vertauschen von ableiten, integrieren und [$]\\mathbb{E}[/$] fuehrt zu (2), (3)</div><div>&nbsp;- Restglied Taylor vernachlaessigen -&gt; (4)</div><div>&nbsp;- (5) Fisher-Matrix invertierbar</div><div></div><div>[Seite 7]</div></div>"
  },
  {
    "front": "MLL:<div></div><div>Hauptsatz</div>",
    "back": "Falls Regularitaetsbedingungen gelten und [$](\\hat{\\vartheta}_n)[/$] konsistent:<div></div><div>[$$] \\forall \\vartheta \\in \\Theta^\\circ\\colon \\sqrt{n} (\\hat{\\vartheta}_n-\\vartheta) \\stackrel{ d_\\vartheta }{\\longrightarrow} \\mathcal{N}(0,I_1(\\vartheta)^{-1}). [/$$]</div><div></div><div>[Seite 8]</div>"
  },
  {
    "front": "MLL:<div></div><div>Score-Vektor</div>",
    "back": "Seite 7/8"
  },
  {
    "front": "MLL:<div></div><div>Uebersicht</div>",
    "back": "-Definition ML-Schaetzer<div>-Idee und Reg-Bedingungen</div><div>-Score-Vektor</div><div>-Hauptsatz</div>"
  },
  {
    "front": "ARE:<br><br>Motivation und Bahadur",
    "back": "Seite 8"
  },
  {
    "front": "ARE:<br><br>Definition BAN-Schätzer",
    "back": "Seite 8"
  },
  {
    "front": "ARE:<br><br>Momentenschätzer",
    "back": "Seite 8"
  },
  {
    "front": "ARE:<br><br>Definition und Interpretation zu ARE",
    "back": "Seite 8"
  },
  {
    "front": "ARE:<br><br>Beispiel ARE",
    "back": "Seite 8"
  },
  {
    "front": "MR:<br><br>Charakterisierung Separabilität und Erzeuger von [$]\\mathcal{B}(S)[/$].",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>Definition [$]\\varepsilon[/$]-Netz und total beschränkt.",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>Charakterisierung relativ kompakt",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>Satz von Baire",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>Definition trennendes System und Charakterisierung [$]P=Q[/$]",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>C: Definition, separabel und vollstandig",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>C: nicht sigma kompakt",
    "back": "Seite 9"
  },
  {
    "front": "MR:<br><br>C: Cf (endlichdim mengen)",
    "back": "Seite 9"
  },
  {
    "front": "MR:<div></div><div>Raum [$]\\mathbb{R}^\\mathbb{N}[/$] (Metrik, [$]\\mathcal{R}_f^\\infty[/$], Charakterisierung relativ kompakt)</div>",
    "back": "Seite 10"
  },
  {
    "front": "MR:<div></div><div>Ueberblick</div>",
    "back": "- Charakterisierung separabel, relativ kompakt<div>- Baire</div><div>- Definition trennendes System</div><div>- [$]C[0,1], \\mathbb{R}^\\mathbb{N}[/$]</div>"
  },
  {
    "front": "14 VKMR:<div></div><div>Definition schwache Konvergenz und Beispiel</div>",
    "back": "Beispiel mit Dirac-Massen<div></div><div>[Seite 10]</div>"
  },
  {
    "front": "14 VKMR:<div></div><div>Portmanteau-Theorem</div>",
    "back": "<div>Let $S$ be a metric space and $P_n,P,\\,n\\in \\mathbb{N},$ be probability measures on $S$. Then the following are equivalent.<br><ol>  <li>$P_n \\stackrel{  }{\\longrightarrow} P$ weakly,&nbsp;&nbsp;&nbsp;</li>  <li>$\\int_S f \\mathop{}\\!\\mathrm{d} P_n \\stackrel{  }{\\longrightarrow} \\int_S f\\mathop{}\\!\\mathrm{d} P$ for all $f\\in C_b^0(S)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\limsup_{n\\to \\infty} P_n(A) \\le P(A)$ for all closed $A\\subset S$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\liminf_{n\\to \\infty} P_n(O) \\ge P(O)$ for all open $O\\subset S$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\lim_{n\\to \\infty} P_n(B) = P(B)$ for all $B\\in \\mathcal{C}(P)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\int_S f \\mathop{}\\!\\mathrm{d} P_n \\stackrel{  }{\\longrightarrow} \\int_S f\\mathop{}\\!\\mathrm{d} P$ for all $f\\in B(S)$ with $P(\\mathcal{C}(f)) = 1$,</li></ol></div><div></div><div></div><div></div><div>[Asymptotische Stochastic, Seite 10 (und PDEs Page 6)]</div>"
  },
  {
    "front": "14 VKMR:<div></div><div>Kriterien I und II</div>",
    "back": "Seite 10"
  },
  {
    "front": "14 VKMR:<div></div><div>Definition und Kriterium KBS.</div>",
    "back": "Seite 10"
  },
  {
    "front": "14 VKMR:<div></div><div>Beispiele KBS.</div>",
    "back": "Seite 10"
  },
  {
    "front": "14 VKMR:<div></div><div>Charakterisierung Verteilungskonvergenz in [$]\\mathbb{R}^\\mathbb{N}[/$].</div>",
    "back": "[$$] P_n \\mathrel{\\stackrel{d}{\\longrightarrow}} P \\iff \\forall k \\in \\mathbb{N}\\colon P_n \\circ \\pi_k^{-1} \\mathrel{\\stackrel{d}{\\longrightarrow}} P \\circ \\pi_k^{-1}.[/$$]<div></div><div>[Seite 10]</div>"
  },
  {
    "front": "14 VKMR:<div></div><div>Uebersicht</div>",
    "back": "- Definition und Beispiel schwache Konvergenz, Portmanteau<div>- Kriterien I und II</div><div>- KBS Definition, Charakterisierung, Beispiel</div><div>- CMT und TF-Kriterium</div>"
  },
  {
    "front": "14 VKMR:<div></div><div>Teilfolgenkriterium</div>",
    "back": "Seite 10"
  },
  {
    "front": "14 VKMR:<div></div><div>CMT</div>",
    "back": "Seite 10"
  },
  {
    "front": "15 VK:<div></div><div>PSP</div>",
    "back": "Seite 10"
  },
  {
    "front": "15 VK:<div></div><div>Stochastische Konvergenz</div>",
    "back": "Seite 11"
  },
  {
    "front": "15 VK:<div></div><div>Slutsky und Korollar</div>",
    "back": "Seite 11"
  },
  {
    "front": "15 VK:<div></div><div>Uebersicht</div>",
    "back": "- PSP<div>- Stochastische Konvergenz, Slutsky</div>"
  },
  {
    "front": "16 Straffheit in MR:<div></div><div>[$]S = C[0,1][/$]: Fidi-Konvergenz und Zusammenhang mit [$]\\mathrel{\\stackrel{d}{\\longrightarrow}}[/$].</div>",
    "back": "Seite 11"
  },
  {
    "front": "16 Straffheit in MR:<div></div><div>[$]S = C[0,1][/$]: Kriterium [$]P_n \\mathrel{\\stackrel{d}{\\longrightarrow}} P[/$] (mit Existenz von [$]P[/$]).</div>",
    "back": "Seite 11"
  },
  {
    "front": "<div>16 Straffheit in MR:</div><div></div>Uebersicht",
    "back": "- [$]S = C[0,1][/$]:Fidi-Konvergenz und zwei Zusammenhaenge mit [$]\\mathrel{\\stackrel{d}{\\longrightarrow}}[/$]<div>- Prokhorov und Korollar</div>"
  },
  {
    "front": "17 VK in [$]C[/$]:<div></div><div>Stetigkeitsmodul [$]w_x(\\delta)[/$].</div><div></div><div>(Zshg. gleichmaessige Stetigkeit, [$]w_\\cdot(\\delta)[/$] ist stetig).</div>",
    "back": "<div>Definition: [$$]w_x(\\delta) := \\sup_{|t-s|\\le \\delta} |x(t) - x(s)|.[/$$]</div><div></div><div>[$] x\\colon [0,1] \\to \\mathbb{R} \\text{ gleichmaessig stetig } \\iff w_x(\\delta) \\stackrel{ \\delta\\to 0 }{\\longrightarrow} 0.[/$]</div><div></div><div>[$]\\forall x, y \\in C[0,1]\\colon |w_x(\\delta) - w_y(\\delta)| \\le 2 \\|x - y\\|_\\infty.[/$]</div><div></div><div></div>[Seite 11]"
  },
  {
    "front": "17 VK in [$]C[/$]:<div></div><div>Arzèla-Ascoli</div>",
    "back": "<div>[$]A\\subset C[0,1] \\text{ ist relativ kompakt genau dann wenn}[/$]</div><div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\sup_{x\\in A} |x(0)| &lt; \\infty, \\\\&amp;(2) \\ \\sup_{x\\in A} w_x(\\delta) \\stackrel{ \\delta\\to 0 }{\\longrightarrow} 0 \\ (\\textit{gleichgradig stetig}).\\end{align*}[/$$]</div><div></div><div></div><div></div>[Seite 11]"
  },
  {
    "front": "17 VK in [$]C[/$]:<div></div><div>Charakterisierung Straffheit in [$]C[/$].</div>",
    "back": "<div>Seite 12</div>"
  },
  {
    "front": "17 VK in [$]C[/$]:<div></div><div>Hinreichende Bedingung fuer [$]X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} X[/$] in [$]C[/$].</div>",
    "back": "<div>[$$] X_n \\stackrel{ \\text{fidi} }{\\longrightarrow} X \\text{ und } \\limsup_{n\\to\\infty} \\mathbb{P}\\left(w(X_n,\\delta) \\ge \\varepsilon\\right) \\stackrel{ \\delta\\to 0 }{\\longrightarrow} 0 \\ \\forall \\varepsilon &gt; 0[/$$].</div><div></div><div></div>[Seite 12]"
  },
  {
    "front": "17 VK in [$]C[/$]:<div></div><div>Uebersicht und Idee</div>",
    "back": "<div>Idee: Uebersetzung von §16 auf [$]C[0,1][/$], hinreichende Bedingung fuer Straffheit.</div><div></div>- Stetigkeitsmodul, Arzèla-Ascoli<div>- Charakterisierung Straffheit in [$]C[/$], hinreichende Bedinung fuer VK&nbsp;</div>"
  },
  {
    "front": "18 Donsker:<div><br><div>kanonische Konstruktion und Definition + Eigenschaften Wiener-Mass</div></div>",
    "back": "Eigenschaften: stationaere Zuwaechse, Kovarianz, Gauss-Prozess<div></div><div>[Seite 12]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>[$](X_n)[/$] PSP, dann [$] X_n \\stackrel{ \\text{fidi} }{\\longrightarrow} W[/$].</div>",
    "back": "Seite 12"
  },
  {
    "front": "18 Donsker:<div></div><div>Kriterium Straffheit eines PSP (zunaechst ohne Beweis).</div>",
    "back": "<div>Ist [$](X_n)[/$] der PSP einer stationaeren Folge [$](Z_n)[/$] mit [$]\\mathbb{E} Z_1 = 0[/$] und [$]\\mathbb{V} (Z_1) =: \\sigma^2 \\in (0, \\infty) [/$] und gilt</div><div></div><div>[$$] \\limsup_{n\\to\\infty}\\left[ \\lambda^2 \\mathbb{P}\\left(\\max_{k\\le n} |S_k| \\ge \\lambda \\sigma \\sqrt{n} \\right) \\right] \\stackrel{ \\lambda\\to\\infty }{\\longrightarrow} 0, [/$$]</div><div></div><div>so ist [$](X_n)[/$] straff.</div><div></div><div></div>[Seite 12]"
  },
  {
    "front": "18 Donsker:<div></div><div>Ungleichung von Etemadi</div>",
    "back": "[$] Z_1,\\ldots,Z_n[/$] unabhaengig, reell, [$]\\alpha &gt;0[/$]:<div><div></div><div>[$$]\\mathbb{P} \\left( \\max_{k\\le n} |S_k| \\ge 3\\alpha \\right) \\le 3 \\max_{k\\le n} \\mathbb{P}\\left( |S_k| \\ge \\alpha\\right).[/$$]</div><div><div><div></div><div></div><div>[Seite 12]</div></div></div></div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Straffheit eines einfachen PSP ([$]\\leadsto[/$] Existenz von [$]W[/$]).</div>",
    "back": "Waehle [$]Z_i \\sim \\mathcal{N}(0,\\sigma^2)[/$] i.i.d.<div></div><div>[Seite 12]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>[$]\\operatorname{\\mathbb{P}-a.s.}[/$] Eigenschaften des Brown-Wiener-Prozesses</div>",
    "back": "- nirgends db<div>- nirgends lokal wachsend oder fallend</div><div>- von unbeschraenkter Variation auf jedem Intervall [$][s,t][/$]</div><div></div><div>[Seite 12]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Der Satz von Donsker (1951)</div>",
    "back": "Sei [$](X_n)[/$] der PSP einer i.i.d. Folge&nbsp; [$](Z_i)[/$] mit [$]\\mathbb{E} Z_1 = 0, \\sigma^2 := \\mathbb{E} Z_1^2 \\in (0,\\infty)[/$]. Dann gilt<div><br><div><div>[$$] X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} W. [/$$]</div><div></div><div></div><div>[Seite 12]</div></div></div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Invarianzprinzip und funktionaler ZGWS.</div>",
    "back": "<b>Invarianzprinzip</b>: GW des PSP haengt nicht von spezieller Verteilung von [$]Z_1[/$] ab.<div><br><div><b>funktionaler ZGWS</b>: Ist [$]h:C \\to \\mathbb{R}^k[/$] messbar mit [$]W(\\mathcal{C}(h)) = 1[/$] und [$](X_n)[/$] ein PSP, so gilt</div><div></div><div>[$$] h(X_n) \\mathrel{\\stackrel{d}{\\longrightarrow}} h(W).[/$$]</div></div><div></div><div>Finde also [$]h(X_n)[/$] fuer einen einfachen PSP [$]\\leadsto h(W)[/$].</div><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Verteilung von [$$]\\max_{0\\le t \\le 1} W(t)[/$$] und Korollar.</div>",
    "back": "[$$] \\max_{0\\le t \\le 1} W(t) \\sim |\\mathcal{N}(0,1)|.[/$$]<div></div><div><br><div><b>Korollar</b>: Fuer [$](Z_i)[/$] i.i.d. mit [$]\\mathbb{E} Z_1 = 0, \\sigma^2 := \\mathbb{E} Z_1^2 \\in (0,\\infty)[/$] gilt</div></div><div></div><div>[$$] \\frac{1}{\\sigma \\sqrt{n}} \\max_{k=0\\ldots n} S_k \\mathrel{\\stackrel{d}{\\longrightarrow}} |\\mathcal{N}(0,1)|.[/$$]</div><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Satz von Mercer (1909) und Spezialfall [$]K(t,s) = t \\wedge s[/$].</div>",
    "back": "<div>Spezialfall:</div><div></div>[$$]t \\wedge s = \\sum_{j=1}^\\infty \\lambda_j \\varphi_j(t) \\varphi_j(s), \\ \\lambda_j = \\frac{1}{\\pi^2 (j- \\frac 12 )^2}, \\ \\varphi_j(t) = \\sqrt{2} \\sin\\left( (j- \\frac 12 ) \\pi t\\right).[/$$]<br><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Fourierdarstellung von [$]W[/$].</div>",
    "back": "[$$] \\tilde{W}(t) := \\sum_{j=1}^\\infty \\frac{\\sqrt{2}\\sin\\left( (j-\\frac 12 ) \\pi t\\right)}{(j - \\frac 12 ) \\pi} N_j,[/$$]<div></div><div>wobei [$] N_j \\sim \\mathcal{N}(0,1)[/$] i.i.d. und Limes in [$]\\mathrm{L}^2[/$]. Dann [$] \\tilde{W} \\stackrel{d}{=} W[/$].</div><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Uebersicht</div>",
    "back": "- Definition [$]W[/$]<div>- Existenz von [$]W[/$] (Fidi-Konvergenz von bel. PSP, Straffheit eines speziellen)</div><div>- Donsker und funkt. ZGWS ([$]\\leadsto[/$] Verteilung von [$]\\max W[/$] und Arcussinus-Gesetz)</div><div>- Fourier-Darstellung von [$] W[/$]</div>"
  },
  {
    "front": "18 Donsker:<div></div><div>Arcussinus-Gesetz</div>",
    "back": "[$$] h_+(x) := \\lambda^1\\left( \\{ t\\in [0,1] \\colon x(t) &gt; 0\\}\\right),[/$$]<div>[$$] h_0(x) := \\sup \\{ t\\in [0,1] \\colon x(t) = 0 \\}.[/$$]</div><div></div><div>Dann [$] \\mathbb{P}(h_0(W) \\le u) = \\mathbb{P}(h_+(W) \\le u) = \\frac 2\\pi \\arcsin \\sqrt{u}, \\ 0\\le u\\le 1[/$].</div><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Definition und Existenz Brownsche Bruecke</div>",
    "back": "Seite 13"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Der Raum [$]C[0,\\infty)[/$] (Metrik, Charakterisierung (Verteilungs-)Konvergenz).</div>",
    "back": "Seite 13"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Konstruktion BW auf [$][0,\\infty)[/$].</div>",
    "back": "Seite 13"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>[$]\\mathbb{P}^B = \\mathbb{P}^{W | W(1) = 0}[/$].</div>",
    "back": "Fuer [$]\\varepsilon &gt; 0[/$] definiere [$]P_\\varepsilon (A) := \\mathbb{P}\\left( W\\in A | 0 \\le W(1) \\le \\varepsilon \\right).[/$] Dann gilt<div></div><div>[$$] P_\\varepsilon \\mathrel{\\stackrel{d}{\\longrightarrow}} B. [/$$]</div><div></div><div></div><div>[Seite 13]</div>"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Beziehungen zwischen BWPs und BBs.</div>",
    "back": "- Zeitskalierung, Zeitshift, Zeitumkehr<div>- BWP [$]\\to[/$] BB</div><div>- BB [$]\\to[/$] BWP</div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Reproduktionssatz fuer [$]B[/$].</div>",
    "back": "Sind [$]B_1, B_2[/$] unabhaengige BBs und [$]a_1, a_2 \\in \\mathbb{R}[/$] mit [$]a_1^2 + a_2^2 = 1[/$], so ist auch<div></div><div>[$$] B := a_1 B_1 + a_2 B_2 [/$$]</div><div></div><div>eine BB.</div><div></div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "19 BB und [$][0,\\infty)[/$]:<div></div><div>Uebersicht</div>",
    "back": "- Definition und Konstruktion BB<div>- Konstruktion BWP auf [$][0,\\infty)[/$]</div><div>- [$]\\mathbb{P}^B = \\mathbb{P}^{W | W(1) = 0}[/$]</div><div>- Beziehungen / Transformationen</div><div>- Reproduktionssatz</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Motivation und Definition</div>",
    "back": "Motivation: §7 [$]\\leadsto[/$] Gilt<div></div><div>[$$] \\left( \\sqrt{n} (\\hat{F}_n(t) - t) \\right)_{0\\le t\\le 1} \\mathrel{\\stackrel{d}{\\longrightarrow}} B[/$$]</div><div></div><div>in einem geeigneten Raum?</div><div></div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Zerlegungslemma. Was folgt fuer Stetigkeit, Messbarkeit, und [$]\\|x\\|_\\infty[/$]?</div>",
    "back": "<b>Lemma</b>: [$]\\forall x\\in D,\\varepsilon &gt; 0 \\exists 0 = t_0 &lt; t_1 &lt; \\ldots &lt; t_k = 1\\colon[/$]<div></div><div>[$$]w_x\\left( [t_{j-1},t_j)\\right) &lt; \\varepsilon.[/$$]<br><div></div><div></div><div><b>Korollar</b>:</div><div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\forall x \\in D, \\varepsilon &gt; 0\\colon | \\{t\\in (0,1] \\colon |x(t) - x(t-)| \\ge \\varepsilon \\}| &lt; \\infty.\\\\&amp;(2) \\ \\|x\\|_\\infty &lt; \\infty. \\\\&amp;(3) \\ x \\text{ ist messbar.}\\end{align*}[/$$]</div><div></div><div></div><div>Insbesondere (aus (1)) hat [$]x[/$] hoechstens abz. viele Unstetigkeitsstellen.</div><div></div><div></div><div>[Seite 14]</div></div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>[$](D,\\rho_\\infty)[/$] nicht separabel.</div>",
    "back": "[$$] \\forall u \\neq v\\colon \\| \\boldsymbol{1}_{[0,u)} - \\boldsymbol{1}_{[0,v)} \\|_\\infty = 1.[/$$]<div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Definition und (zwei) Eigenschaften von [$]d_S[/$].</div>",
    "back": "[$$] d_S(x,y) := \\inf_{\\lambda\\in\\Lambda} \\max \\left( \\|x \\circ \\lambda - y\\|_\\infty, \\|\\lambda - \\operatorname{id}\\|_\\infty \\right) \\stackrel{!}{\\le} \\|x - y\\|_\\infty.[/$$]<div></div><div>Charakterisierung Konvergenz.</div><div></div><div>Eigenschaften: Falls [$]d_S(x_n,x) \\stackrel{  }{\\longrightarrow} 0[/$]:</div><div>(1) [$]x_n(t) \\stackrel{  }{\\longrightarrow} x(t) \\forall t \\in \\mathcal{C}(x)[/$], insb. fuer alle bis auf abz. viele [$]t[/$].</div><div>(2) [$]\\|x_n - x\\|_\\infty \\stackrel{  }{\\longrightarrow} 0[/$] falls [$]x[/$] stetig.</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>[$](D,d_S)[/$] separabel (oB), aber nicht vollstaendig.</div>",
    "back": "Seite 14"
  },
  {
    "front": "20 càdlàg:<div></div><div>Definition [$]d_S^\\circ[/$] und Verhaeltnis zu [$]d_S[/$].</div>",
    "back": "\"[$$] d_S^\\circ(x, y) := \\inf_{\\lambda\\in\\Lambda} \\max \\left( \\|x\\circ \\lambda - y\\|_\\infty, \\|\\lambda\\|^\\circ\\right), [/$$]<div></div><div>wobei [$] \\|\\lambda\\|^\\circ := \\sup_{s &lt; t} \\left| \\log \\frac{\\lambda(t)-\\lambda(s)}{t-s}\\right|.[/$]</div><div></div><div>Dann sind [$]d_S[/$] und [$]d_S^\\circ[/$] aequivalent und [$](D,d_S^\\circ)[/$] ist separabel und vollstaendig.</div><div>(Es gibt bzgl. [$]d_S^\\circ[/$] \"\"weniger\"\" Cauchy-Folgen).</div><div></div><div>[Seite 14]</div>\""
  },
  {
    "front": "20 càdlàg:<div><br><div>Zusammenhang Skorokhod-Topologie mit [$]C[0,1][/$].</div></div>",
    "back": "(1) Skorokhod-Topologie auf [$]C[/$] eingeschraenkt ist [$]\\|\\cdot\\|_\\infty[/$]-Topologie.<div>(2) [$]\\mathcal{B}(D) \\cap C = \\mathcal{B}(C)[/$] (Spur-[$]\\sigma[/$]-Algebra).</div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Definition càdlàg-Modul [$]w_x'[/$] und Zusammenhang zu [$]w_x[/$].</div>",
    "back": "[$$] w_x'(\\delta) := \\inf_{(t_i) \\delta\\text{-duenn}} \\max_{j=1\\ldots k} w_x([t_{j-1},t_j)) \\left( \\stackrel{ \\delta\\to 0 }{\\longrightarrow} 0 \\ \\forall x\\in D\\right).[/$$]<div></div><div>(1) Fuer [$] \\delta &lt; \\frac 12\\colon w_x'(\\delta) \\le 2 w_x(\\delta)[/$].</div><div>(2) [$]w_x(\\delta) \\le 2w_x'(\\delta) + j(x)[/$] mit [$] j(x) := \\sup_t |x(t-) - x(t)|[/$].</div><div></div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Arzèla-Ascoli in [$]D[/$].</div><div></div><div>Warum reicht [$]\\sup_x |x(0)| &lt; \\infty[/$] nicht?</div>",
    "back": "Gegenbeispiel: [$]x_n := n \\boldsymbol{1}_{[\\frac 12,1)}, \\ A := \\{x_n \\colon n\\in\\mathbb{N}\\}[/$].<div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Satz ueber Messbarkeit und Stetigkeit der Projektionsabbildungen.</div>",
    "back": "<div>[$$]\\begin{align*}&amp;(1) \\ \\pi_0 \\text{ und } \\pi_1 \\text{ sind stetig}. \\\\&amp;(2) \\ \\forall t\\in (0,1)\\colon&nbsp; \\pi_t \\text{ stetig in } x \\in D \\iff t \\in \\mathcal{C}(x) \\\\&amp;(3) \\ \\forall k\\in\\mathbb{N}, t_1\\ldots t_k\\colon \\pi_{t_1\\ldots t_k}\\colon D \\to \\mathbb{R}^k \\text{ ist messbar.} \\\\&amp;(4) \\ \\text{Ist } T \\subset [0,1], \\overline{T} = [0,1], 1 \\in T\\colon \\mathcal{B}(D) = \\sigma\\left( \\pi_t \\colon t\\in T\\right).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 14]</div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Kriterium fuer [$]P_n\\mathrel{\\stackrel{d}{\\longrightarrow}} P[/$] in [$]D[/$] (dafuer Definition von [$]T_P\\subset [0,1][/$]).</div>",
    "back": "Seite 15"
  },
  {
    "front": "20 càdlàg:<div></div><div>Wiener-Mass und Satz von Donsker in [$]D[0,1][/$].</div>",
    "back": "[$] W := W \\circ \\imath^{-1}[/$].<div></div><div>[Seite 15]</div>"
  },
  {
    "front": "20 càdlàg:<div><br><div>Kriterium fuer [$]&nbsp; X_n \\mathrel{\\stackrel{d}{\\longrightarrow}} X[/$] in [$]D[/$] (Satz 20.10).</div></div>",
    "back": "Seite 15<div></div>"
  },
  {
    "front": "20 càdlàg:<div></div><div>Uebersicht</div>",
    "back": "- Motivation, Definition und Zerlegungslemma<div>- [$]d_S[/$] und [$]d_S^\\circ[/$]</div><div>- Arzèla-Ascoli und Satz ueber Projektionen</div><div>- Kriterium fuer [$]P_n\\mathrel{\\stackrel{d}{\\longrightarrow}} P[/$] und [$]X_n\\mathrel{\\stackrel{d}{\\longrightarrow}} X[/$] (20.10)</div><div>- Satz von Donsker</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>Setting und Definition empirischer (Standard-)Prozess.</div>",
    "back": "[$](X_i)[/$] i.i.d. mit [$]\\mathbb{P}\\left( 0\\le X_1 \\le 1\\right) = 1[/$] und VF [$]F[/$]. Dann<div></div><div>[$$]Y_n(t) := \\sqrt{n} \\left( \\hat{F}_n(t) - F(t)\\right), \\ 0\\le t \\le 1.[/$$]</div><div></div><div>Dann ist [$]Y_n[/$] ein Zufallselement in [$]D[0,1][/$], der <i>empirische Prozess</i> zu [$]X_1,\\ldots,X_n[/$].</div><div></div><div></div><div>[Seite 15]</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>[$]Y_n \\mathrel{\\stackrel{d}{\\longrightarrow}} Y[/$] mit Gauss-Prozess [$]Y[/$] in [$]D[/$] mit [$]\\mathbb{E} Y = 0[/$] und [$] \\mathbb{E} \\left[ Y_t Y_s \\right] = F(t) \\wedge F(s) - F(t) F(s).[/$]</div>",
    "back": "Seite 15"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Setting and Topology on [$]\\mathcal{F} = \\mathcal{F}(E)[/$] (<i>Fell-topology</i> or <i>hit-and-miss-topology</i>).</div><div></div><div>Subbasis?</div>",
    "back": "<div>[$$]\\begin{align*}&amp;(1) \\ \\mathcal{F}^A := \\{F\\in\\mathcal{F}\\colon F\\cap A = \\varnothing\\} ,\\mathcal{F}_{A_1\\ldots A_k} := \\{F\\in\\mathcal{F}\\colon \\forall i\\colon F\\cap A_i \\neq \\varnothing\\}.\\\\&amp;(2) \\ \\tau := \\{\\mathcal{F}_{G_1\\ldots G_k}^C\\colon C\\in \\mathcal{C}, G_i \\in \\mathcal{G}\\} \\text{ is basis of topology.}\\\\&amp;(3) \\ \\{\\mathcal{F}^C\\colon C\\in\\mathcal{C}\\} \\cup \\{F_G \\colon G\\in\\mathcal{G}\\} \\text{ forms a subbasis.}\\\\&amp;(4) \\ \\varnothing \\in \\mathcal{F}^A, \\varnothing \\notin \\mathcal{F}_A, \\left(\\mathcal{F}_A\\right)^c = \\mathcal{F}^A, \\mathcal{F}_\\varnothing = \\varnothing.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Basic properties of [$](\\mathcal{F}, \\mathcal{O}_\\mathcal{F})[/$].</div>",
    "back": "<div>[$$]\\begin{align*}&amp;(1) \\ (\\mathcal{F},\\mathcal{O}_\\mathcal{F}) \\text{ is a compact Hausdorff space with countable base.}\\\\&amp;(2) \\ \\text{If } C\\in\\mathcal{C}\\colon \\mathcal{F}_C \\text{ is compact.}\\\\&amp;(3) \\ \\{\\mathcal{F}^C\\colon C\\in\\mathcal{C}\\} \\text{ is a neighbourhood-basis of } \\varnothing.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Lemma: countable base [$]\\mathcal{D}[/$] of [$]\\mathcal{O}_E[/$].</div>",
    "back": "There is a countable base [$]\\mathcal{D}[/$] such that all [$]D\\in\\mathcal{D}[/$] are relatively compact and<div></div><div>[$$] \\forall G\\in\\mathcal{G}\\colon G = \\bigcup_{D\\in\\mathcal{D}, \\overline{D}\\subset G} D.[/$$]</div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Characterisation of [$]F_n \\stackrel{  }{\\longrightarrow} F[/$].</div>",
    "back": "<div>The following are equivalent:</div><div>[$$]\\begin{align*}(1)&amp; \\ F_j \\stackrel{  }{\\longrightarrow} F.\\\\(2)&amp; \\ \\forall G\\in\\mathcal{G}\\colon F\\cap G = \\varnothing \\implies F_j \\cap G = \\varnothing \\text{ f.a.a. } j\\in\\mathbb{N},\\\\&amp; \\ \\forall C\\in\\mathcal{C}\\colon F\\cap C \\neq \\varnothing \\implies F_j\\cap C \\neq \\varnothing&nbsp; \\text{ f.a.a. } j \\in\\mathbb{N}.\\\\(3)&amp; \\ \\forall x\\in F \\exists x_j\\in F_j \\text{ s.t. } x_j \\stackrel{  }{\\longrightarrow} x,\\\\&amp; \\ \\forall (F_{k(n)}) \\text{ and } x_{k(n)} \\in F_{k(n)} \\text{ with } x_{k(n)} \\stackrel{  }{\\longrightarrow} x \\text{ we have } x \\in F.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>[$]\\cup\\colon \\mathcal{F}\\times\\mathcal{F} \\to \\mathcal{F}; (F_1,F_2)\\mapsto F_1\\cup F_2[/$] is continuous.</div>",
    "back": "Page 1"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Definition and characterisation upper- and lower semi-continuity.</div>",
    "back": "<div><div></div><div></div><div>Let $f\\colon T\\to \\mathcal{F}$ be a map, where $(T, \\mathcal{O}_T)$ is a topological space. Then $f$ is called<br><div>[$$]\\begin{align*}(1)&amp; \\ \\textbf{upper semi-continuous }\\mathrel{:\\Longleftrightarrow} \\forall C\\in\\mathcal{C}\\colon f^{-1}\\left(\\mathcal{F}^C\\right) \\in \\mathcal{O}_T\\iff \\forall t_i \\to t\\colon \\overline{\\lim} \\  f(t_i) \\subset f(t). \\\\(2)&amp; \\ \\textbf{lower semi-continuous } \\mathrel{:\\Longleftrightarrow} \\forall G\\in\\mathcal{G}\\colon f^{-1}\\left(\\mathcal{F}_G\\right) \\in \\mathcal{O}_T \\iff \\forall t_i\\to t\\colon \\underline{\\lim} \\  f(t_i) \\supset f(t).\\\\\\end{align*}[/$$]</div><div>Then $f$ is continuous if and only if it is upper- and lower semi-continuous.</div><div></div></div><div></div><div></div><div>[Page 1]</div></div>"
  },
  {
    "front": "I.1 Space of closed sets:<div><div></div><div>[$]\\cap\\colon \\mathcal{F}\\times\\mathcal{F}\\to\\mathcal{F}[/$] is upper-, but not lower semi-continuous.</div></div>",
    "back": "Page 1"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Accumulation point of a sequence [$](F_n)[/$].</div>",
    "back": "[$]F\\in\\mathcal{F} \\text{ s.t. } F_{k(n)} \\stackrel{  }{\\longrightarrow} F[/$] for some subsequence.<div>Then every sequence has at least one accumulation point and<br><div></div><div></div><div><div>[$$]\\begin{align*}&amp; (F_n) \\text{ converges}\\\\\\iff &amp; (F_n) \\text{ has exactly one accumulation point}\\\\\\iff &amp; \\overline{\\lim} \\  F_i = \\underline{\\lim} \\  F_i.\\end{align*}[/$$]</div></div><div></div><div></div><div></div><div>[Page 1]</div></div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Definition and characterisation of [$]\\overline{\\lim} \\  F_i, \\underline{\\lim} \\  F_i[/$].</div>",
    "back": "<div>[$$]\\begin{align*}&amp; \\overline{\\lim} \\  F_i := \\bigcup_{F \\text{ ac}} F = \\{x\\in E\\colon \\exists x_{k(n)}\\in F_{k(n)}\\text{ s.t. } x_{k(n)} \\stackrel{  }{\\longrightarrow} x\\}\\\\&amp; \\underline{\\lim} \\  F_i := \\bigcap_{F \\text{ ac}} F = \\{x\\in E \\colon \\exists x_n\\in F_n \\text{ s.t. } x_n \\stackrel{  }{\\longrightarrow} x\\}\\end{align*}[/$$]</div><div></div><div></div><div>In particular, both sets are closed.</div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.1 Space of closed sets:<div></div><div>Overview</div>",
    "back": "- Definition and properties of [$]\\mathcal{O}_\\mathcal{F}[/$]<div>- Convergence w.r.t. [$]\\mathcal{O}_\\mathcal{F}[/$]</div><div>- Continuity</div><div>- Accumulation points</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>Kolmogorov-Statistik</div>",
    "back": "Warum muss [$]F[/$] stetig sein?<div></div><div>Pruefgroesse [$]K_n := \\| \\hat{F}_n - F_0\\|_\\infty[/$].</div><div>Unter [$]H_0\\colon F = F_0[/$] gilt [$]\\sqrt{n} K_n \\mathrel{\\stackrel{d}{\\longrightarrow}} \\| B \\|_\\infty =: K[/$], wobei</div><div></div><div>[$$] \\mathbb{P}\\left( K \\le x\\right) = 1 - 2 \\sum_{j=1}^\\infty (-1)^{j-1} \\mathrm{e}^{-2j^2x^2}, \\ 0 &lt; x &lt; \\infty.[/$$]<br><div></div><div></div><div>[Seite 15]</div></div>"
  },
  {
    "front": "21 Empirische Prozesse:<div><br><div>Verteilung von [$$] \\int_0^1 B(t)^2 \\mathop{}\\!\\mathrm{d} t[/$$].</div></div>",
    "back": "[$$] \\int_0^1 B(t)^2 \\mathop{}\\!\\mathrm{d} t \\sim \\omega^2 = \\sum_{k=1}^\\infty \\frac{N_k^2 - 1}{\\pi^2 k^2} + \\frac 16.[/$$]<div></div><div></div><div>[Seite 15]</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>Kolmogorov-Smirnov-Statistik (nichtparametrisches 2-Stichprobenproblem).</div>",
    "back": "[$](X_n), (Y_n)[/$] je i.i.d. und unabhaengig mit stetigen VFen [$]F, G[/$]. Testgroesse [$] K_{mn} := \\| \\hat{F}_m - \\hat{G}_n\\|_\\infty[/$] haengt unter [$] H_0\\colon F=G[/$] nicht von Verteilung ab. Dann gilt unter [$]H_0[/$]:<div></div><div>[$$] \\sqrt{\\frac{mn}{m+n}} K_{mn} \\mathrel{\\stackrel{d}{\\longrightarrow}} \\| B \\|_\\infty, m,n\\to\\infty.[/$$]</div><div></div><div></div><div>[Seite 15/16]</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>Verteilung von [$$]\\sup_{0 \\le t \\le 1} B(t) [/$$] (Brown'sche Bruecke).</div>",
    "back": "[$$] \\mathbb{P}\\left(\\sup_{0 \\le t \\le 1} B(t)\\le x\\right) = 1 - \\mathrm{e}^{-2x^2}, \\ x \\ge 0.[/$$]<div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "21 Empirische Prozesse:<div></div><div>Ueberblick</div>",
    "back": "- Definition und Grenzverteilung empirischer Prozess<div>- Kolmogorov- und Kolmogorov-Smirnov-Statistik</div><div>- Verteilung von [$] \\sup_t B(t)[/$]</div>"
  },
  {
    "front": "I.2 Random closed sets:<div></div><div>Generators of [$]\\mathcal{B}(\\mathcal{F})[/$] and Corollary.</div>",
    "back": "[$] \\mathcal{B}(\\mathcal{F}) = \\sigma\\left( \\mathcal{F}^C\\colon C\\in\\mathcal{C}\\right) = \\sigma \\left(\\mathcal{F}_G\\colon G\\in\\mathcal{G}\\right).[/$]<div></div><div><b>Corollary</b>: If [$]f\\colon T \\to \\mathcal{F}[/$] is semi-continuous, then it is measurable.</div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.2 Random closed sets:<div></div><div>Definition and (four) examples of random closed sets (racs).</div>",
    "back": "A <i>random closed set (racs)</i> is a mb map [$]Z\\colon (\\Omega,\\mathcal{A},\\mathbb{P}) \\to (\\mathcal{F},\\mathcal{B})[/$].<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ Z \\equiv F\\in\\mathcal{F} \\text{ constant racs.}\\\\&amp;(2) \\ Z_1, Z_2 \\text{ racs} \\implies Z_1 \\cap Z_2 \\text{ and } Z_1 \\cup Z_2 \\text{ are racs.}\\\\&amp;(3) \\ \\xi\\colon \\Omega \\to E \\text{ mb} \\implies Z := \\{\\xi\\} \\text{ is racs.}\\\\&amp;(4) \\ Z \\text{ racs in } \\mathbb{R}^n, t\\in\\mathbb{R}^n, \\nu\\in \\textrm{SO}(n)\\implies Z+t \\text{ and } \\nu Z \\text{ are racs.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 1]</div>"
  },
  {
    "front": "I.2 Random closed sets:<div></div><div>Definition stationarity and isotropy.</div>",
    "back": "A racs [$]Z[/$] in [$]\\mathbb{R}^n[/$] is called<div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\textbf{stationary} \\mathrel{:\\Longleftrightarrow} \\forall t\\in\\mathbb{R}^n\\colon Z + t \\stackrel{d}{=} Z.\\\\&amp;(2) \\ \\textbf{isotropic} \\mathrel{:\\Longleftrightarrow} \\forall \\rho \\in \\text{SO}(n)\\colon \\rho Z \\stackrel{d}{=} Z.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "I.2 Random closed sets:<div></div><div>If [$]Z[/$] is a racs in [$]\\mathbb{R}^n[/$], when can we conclude [$]Z = \\varnothing \\operatorname{\\mathbb{P}-a.s.}[/$] ?</div><div></div><div>Why are [$]\\{Z = \\varnothing\\}[/$] and [$]\\{Z \\text{ compact}\\}[/$] measurable?</div>",
    "back": "<div><b></b></div><div>If [$]Z[/$] is a stationary, compact racs in [$]\\mathbb{R}^n[/$], then [$]Z = \\varnothing \\operatorname{\\mathbb{P}-a.s.}[/$].<b></b></div><b><div><b></b></div>Corollary</b>: If [$]Z[/$] is stationary and [$]\\mathbb{P}\\left( Z \\text{ compact}\\right) &gt; 0[/$], then [$]\\mathbb{P}\\left(Z = \\varnothing | Z \\text{ compact}\\right) = 1[/$].<div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "I.3 Capacity functional:<div></div><div>Motivation and Definition.</div>",
    "back": "Motivation: [$]X\\colon \\Omega \\to \\mathbb{R}[/$] has [$]F_X(t) = \\mathbb{P}(X\\le t) = \\mathbb{P}\\left(\\{X\\} \\cap (-\\infty,t] \\neq \\varnothing\\right)[/$].<div></div><div>Definition: Let [$]Z[/$] be a racs in [$]\\mathcal{F}[/$]. Then its capacity functional is</div><div></div><div>[$$] T_Z\\colon \\mathcal{C} \\to [0,1]; \\ C \\mapsto \\mathbb{P}\\left(Z\\cap C \\neq \\varnothing\\right) = \\mathbb{P}\\left(Z\\notin \\mathcal{F}^C\\right) = \\mathbb{P}^Z\\left(\\mathcal{F}_C\\right).[/$$]</div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "I.3 Capacity functional:<div></div><div>Properties of [$]T_Z[/$] (including uniqueness).</div>",
    "back": "<div>[$$]\\begin{align*}&amp;(1) \\ 0 \\le T_Z \\le 1, \\ T_Z(\\varnothing) = 0.\\\\&amp;(2) \\ C_i \\downarrow C \\implies T_Z(C_i) \\to T_Z(C).\\\\&amp;(3) \\ S_k \\ge 0, \\text{ where } S_0(C_0) := 1 - T_Z(C_0), S_k(C_0;C_1\\ldots C_k) := \\\\ &amp; S_{k-1}(C_0;C_1\\ldots C_{k-1}) - S_{k-1}(C_0\\cup C_k; C_1\\ldots C_{k-1}).\\\\\\end{align*}[/$$]</div><div></div><div></div><div>If a map [$]T\\colon \\mathcal{C}&nbsp; \\to [0,1][/$] satisfies (1) to (3), then there exists a unique probability measure [$]Q[/$] on [$]\\mathcal{F}[/$] such that [$] T(C) = Q(\\mathcal{F}_C)\\forall C\\in\\mathcal{C}[/$].</div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "I.3 Capacity functional:<div></div><div>Characterisation of stationarity (isotropy) with [$]T_Z[/$].</div>",
    "back": "[$]Z[/$] is stationary (isotropic) iff [$]T_Z[/$] is translation (rotation) invariant.<div></div><div>[Page 2]</div>"
  },
  {
    "front": "Overview I.2 and I.3",
    "back": "<div>- Generator [$]\\mathcal{B}(\\mathcal{F})[/$]</div>- Definition and examples for racs<div>- Stationarity / Isotropy (definition and thm)</div><div></div><div>- Motivation and Definition</div><div>- Properties and Uniqueness</div><div>- Characteriasation stationarity / isotropy</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Definition of [$]\\mathbb{M}(E)[/$] and (two) examples.</div>",
    "back": "[$] \\mathbb{M} := \\mathbb{M}(E) := \\{\\eta\\colon \\eta \\text{ is a locally finite Borel measure on } E\\}.[/$]<div></div><div>Examples:</div><div></div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ E = \\mathbb{R}^n, \\eta = f \\lambda^n, f \\text{ bounded or continuous.}\\\\(2)&amp; \\ \\eta = \\sum_{i = 1}^k m_i \\delta_{x_i}, m_i \\ge 0. \\text{ If } k = \\infty, \\text{ then } (x_i) \\text{ may not have an ac.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Definition and generator of [$]\\mathcal{M}(E)[/$].</div>",
    "back": "[$]\\mathcal{M}(E) := \\sigma\\left(\\pi_A\\colon A\\in\\mathcal{B}\\right) = \\sigma(\\mathcal{E}),[/$]<div></div><div>where [$]\\mathcal{E}= \\{M_{G,r} = \\pi_G^{-1}\\left((-\\infty,r]\\right) \\colon G \\in \\mathcal{G}_c, r\\ge 0\\}[/$].</div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Definition [$]\\mathbb{N}(E), \\mathcal{N}(E) = \\sigma(?)[/$].</div>",
    "back": "<div>[$$]\\begin{align*}&amp;\\mathbb{N}(E) := \\{\\eta\\in\\mathbb{M}\\colon \\eta \\text{ is a counting measure}\\} \\stackrel{!}{\\in} \\mathcal{M}(E).\\\\&amp;\\mathcal{N}(E):= \\mathcal{M}(E)\\cap\\mathbb{N}(E) = \\sigma(\\mathcal{E}'),\\\\&amp;\\text{ where } \\mathcal{E}' := \\{N_{G,k} = \\pi_G^{-1}(\\{k\\})\\colon G \\in \\mathcal{G}_c, k\\in\\mathbb{N}_0\\}.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Measurable enumeration of the atoms of [$]\\eta\\in\\mathbb{N}(E)[/$].</div>",
    "back": "There are mb maps [$]\\zeta_i\\colon \\mathbb{N}(E) \\to E, i\\in\\mathbb{N},[/$] such that for [$]\\eta\\in\\mathbb{N}(E)[/$] we have<div></div><div>[$$] \\eta = \\sum_{i = 1}^{\\eta(E)} \\delta_{\\zeta_i(\\eta)}. [/$$]</div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Definition [$]\\mathbb{N}_s(E), \\mathcal{N}_s(E) = \\sigma(?)[/$].</div>",
    "back": "<div>[$$]\\begin{align*}&amp; \\mathbb{N}_s(E) := \\{ \\eta\\in\\mathbb{N}\\colon\\eta \\text{ is simple}\\} \\stackrel{!}{\\in}\\mathcal{N}(E).\\\\&amp; \\mathcal{N}_s(E) := \\mathcal{N}(E) \\cap \\mathbb{N}_s(E) = \\sigma(\\mathcal{E}_0'),\\\\&amp; \\text{where } \\mathcal{E}_0':=\\{\\mathbb{N}_{C,0}^s = \\pi_C^{-1}(\\{0\\})\\colon C\\in\\mathcal{C}\\}.\\end{align*}[/$$]</div><div></div><div></div><div><b>[$]\\mathcal{E}_0'[/$] is [$]\\cap[/$]-stable!</b></div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Definition [$]\\operatorname{supp} \\eta[/$] for [$]\\eta\\in\\mathbb{M}(E)[/$].</div>",
    "back": "<div></div>[$$]\\operatorname{supp} \\eta := E \\setminus \\bigcup_{\\substack{B\\text{ open}\\\\ \\eta(B) = 0}} B.[/$$]<div><br><div>If [$]\\eta\\in\\mathbb{N}(E)\\colon \\operatorname{supp} \\eta = \\{x\\in E\\colon \\eta(\\{x\\}) \\ge 1\\}[/$], the set of [$]\\eta[/$] 's atoms.</div></div><div></div><div></div><div>[Page 2]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Grundlegende Eigenschaften (drei/vier Stueck) und zwei Beispiele.</div>",
    "back": "Eigenschaften:<div>- Entwicklung [$]x = \\sum_k \\langle x, e_k\\rangle e_k[/$].</div><div>- CSU</div><div>- (verallgemeinerte) Parsevalsche Gleichung</div><div></div><div>Beispiele: [$]\\ell^2(\\mathbb{N})[/$] und [$]L^2(\\Omega, \\mathcal{A}, \\mu),[/$] falls [$]\\sigma[/$]-endlich und [$]\\mathcal{A}[/$] abzaehlbar erzeugt.</div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Definition Operator und vier Eigenschaften.</div>",
    "back": "[$]\\text{Ein linearer, beschraenkter Operator } T\\colon \\mathcal{H} \\to\\mathcal{H} \\text{ heisst}[/$]<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\textbf{kompakt} \\mathrel{:\\Longleftrightarrow} \\forall M\\subset \\mathcal{H} \\text{ beschraenkt} \\colon T(M) \\text{ relativ kompakt.}\\\\&amp;(2) \\ \\textbf{symmetrisch} \\mathrel{:\\Longleftrightarrow} \\forall x,y\\in\\mathcal{H}\\colon \\langle x, Ty\\rangle = \\langle T x, y\\rangle.\\\\&amp;(3) \\ \\textbf{positiv}\\mathrel{:\\Longleftrightarrow} \\forall x\\in\\mathcal{H}\\colon \\langle x, Tx\\rangle \\ge 0.\\\\&amp;(4) \\ \\textbf{Spurklasse-Operator (SKO)}\\mathrel{:\\Longleftrightarrow} \\forall \\text{ ONS } (e_k)\\colon \\sum_k \\left| \\langle e_k, T e_k \\rangle \\right| &lt; \\infty.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Darstellungssatz von Riesz.</div>",
    "back": "Fuer jedes beschraenkte lineare Funktional [$]l\\colon \\mathcal{H}\\to\\mathbb{R}[/$] exisitert genau ein [$]z\\in\\mathcal{H}[/$] mit<div></div><div>[$$] l(\\cdot) = \\langle z, \\cdot \\rangle.[/$$]</div><div></div><div>Es gilt [$]\\| l \\| = \\|z\\|.[/$]</div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Erzeuger von [$]\\mathcal{B}(\\mathcal{H})[/$] und Charakterisierung Messbarkeit von [$]X\\colon (\\Omega,\\mathcal{A})\\to (\\mathcal{H},\\mathcal{B})[/$].</div>",
    "back": "[$]\\mathcal{B} = \\sigma\\left(\\langle \\cdot, y \\rangle \\colon y \\in \\mathcal{H}\\right) = \\{ \\langle \\cdot, y\\rangle^{-1}(H) \\colon H\\in \\mathcal{B}(\\mathbb{R})\\}.[/$]<div></div><div>Folglich ist [$]X[/$] messbar genau dann wenn [$]\\langle X, y\\rangle \\colon \\Omega \\to \\mathbb{R}[/$] messbar ist fuer alle [$]y\\in \\mathcal{H}[/$].</div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Definition [$]\\mathbb{E} X[/$].</div>",
    "back": "Falls [$]\\mathbb{E} \\left| \\langle X, y\\rangle\\right| &lt; \\infty \\forall y\\in\\mathcal{H}[/$], so heisst [$]m\\in\\mathcal{H}[/$] <b>Erwartungswert von [$]X[/$] </b>falls<div></div><div>[$$] \\forall y\\in\\mathcal{H}\\colon \\langle m, y\\rangle = \\mathbb{E} \\langle X, y\\rangle.[/$$]</div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Kriterium fuer Existenz von [$]\\mathbb{E} X[/$].</div>",
    "back": "Hinreichend: [$] \\mathbb{E} \\| X \\| &lt; \\infty[/$].<div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Dreiecksungleichung fuer [$]\\mathbb{E}[/$].</div>",
    "back": "Falls existent: [$] \\| \\mathbb{E} X \\| \\le \\mathbb{E} \\| X \\|[/$].<div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Fuer [$](\\Omega, \\mathcal{A}, \\mathbb{P})[/$] fest: [$]L^1 := \\{X\\colon \\Omega \\to \\mathcal{H} \\text{ messbar}\\colon \\mathbb{E} \\| X \\| &lt; \\infty\\}[/$] ist Vektorraum.</div><div></div><div>[$]\\mathbb{E}[/$] ist linear (was genau heisst das?).</div>",
    "back": "Linearitaet: Falls [$]X,Y[/$] Erwartungswerte besitzen und [$]a, b\\in\\mathbb{R}[/$], so auch [$]aX+bY[/$] und<div></div><div>[$$]\\mathbb{E} \\left[ aX + bY\\right] = a\\mathbb{E} X + b \\mathbb{E} Y.[/$$]</div><div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>[$]\\mathcal{F}_\\textit{lf}[/$] and [$]\\imath_*[/$] -maps.</div>",
    "back": "[$]\\mathcal{F}_\\textit{lf}:= \\{F\\in\\mathcal{F}\\colon F \\text{ locally finite}\\}[/$]. Then the maps<div></div><div>[$$]\\begin{cases}\\imath_s \\colon \\mathbb{N}_s(E) \\to \\mathcal{F}_\\textit{lf} &amp; ,\\\\\\imath_\\mathbb{N} \\colon \\mathbb{N}(E) \\to \\mathcal{F}_\\textit{lf}&nbsp;&amp; ,\\\\\\imath_\\mathbb{M} \\colon \\mathbb{M}(E) \\to \\mathcal{F} &amp; ,\\end{cases}\\ \\eta \\mapsto \\operatorname{supp} \\eta[/$$]</div><div></div><div>are well-defined, mb and [$]\\imath_s[/$] is bijective and bi-measurable.</div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Definition and two characterisations random measure and (simple) point process.</div>",
    "back": "A mb map [$]\\Phi\\colon (\\Omega,\\mathcal{A},\\mathbb{P})\\to\\begin{cases} \\mathbb{M}(E)\\\\ \\mathbb{N}(E) \\\\ \\mathbb{N}_s(E)\\end{cases}[/$] is called a [$]\\begin{cases}\\textbf{random measure}\\\\ \\textbf{point process} \\\\ \\textbf{simple point process}\\end{cases}[/$].<br><div></div><div>This is the case iff [$]\\Phi(G), G \\in\\mathcal{G}_c[/$] (or [$]\\Phi(C),C\\in\\mathcal{C}[/$] for spp) are mb.</div><div></div><div>Third characterisation see generators of the [$]\\sigma[/$] -algebras.</div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Characterisation of [$] \\Phi \\stackrel{d}{=} \\Phi'[/$] for spps.</div>",
    "back": "[$]\\Phi \\stackrel{d}{=} \\Phi' \\iff \\forall C \\in \\mathcal{C} \\colon \\mathbb{P}\\left(\\Phi(C) = 0\\right) = \\mathbb{P}\\left(\\Phi'(C) = 0\\right).[/$]<div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Three remarks on / examples for rms (pps, spps) (Sum of rm's, Restriction of rm, proper maps)</div>",
    "back": "<div>[$$]\\begin{align*}&amp;(1) \\ \\Phi, \\Phi' \\text{ rm/pp} \\implies \\Phi + \\Phi' \\text{ rm/pp}.\\\\&amp;(2) \\ \\Phi \\text{ rm/pp/spp}, A\\in\\mathcal{B}(E)\\implies \\Phi_A:= \\Phi(\\cdot\\cap A) \\text{ is rm/pp/spp}.\\\\&amp;(3) \\ \\text{If } T\\colon E\\to E' \\text{ is }&nbsp;\\textbf{proper}^\\star \\text{, then } \\tilde{T}\\colon \\mathbb{M}(E)\\to\\mathbb{M}(E'); \\eta \\mapsto \\eta \\circ T^{-1} \\text{ is mb.}\\end{align*}[/$$]</div><div></div><div></div><div>[$](\\star) \\&nbsp; \\forall C \\in\\mathcal{C}\\colon T^{-1}(C) \\text{ is relatively compact.}[/$]</div><div></div><div>Example for (3): translation on [$]\\mathbb{R}^d[/$].</div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Definition of intensity measure (why is it a measure?).</div><div></div><div>What does it look like for (simple) point processes and for stationary random measures?</div>",
    "back": "[$]\\Phi[/$] rm on [$]E[/$]. Its <b>intensity measure</b> is [$] \\Theta\\colon\\mathcal{B}(E)\\to [0,\\infty], A\\to\\mathbb{E}\\left[ \\Phi(A)\\right].[/$]<div></div><div></div><div>Remark:</div><div></div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\Phi \\text{ (s)pp} \\implies \\Theta(A) = \\text{number of points of }\\Phi\\text{ in } A.\\\\&amp; \\ \\Phi \\text{ simple}\\colon \\Theta(A) =: \\mathbb{E} \\left| \\Phi\\cap A\\right|.\\\\(2)&amp; \\ \\Phi \\text{ stationary in } \\mathbb{R}^n \\implies \\Theta \\text{ is stationary Borel-measure.}\\\\&amp; \\ \\Theta \\text{ locally finite}\\implies \\exists c \\ge 0\\text{ s.t. }\\Theta = c \\lambda^n.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Campbell's theorem.</div>",
    "back": "[$] \\text{Let } \\Phi \\text{ be a random measure on } E \\text{ and } f\\colon E\\to [0,\\infty) \\text{ measurable. Then}[/$]<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\int_E f \\mathop{}\\!\\mathrm{d} \\Phi \\text{ is measurable.}\\\\&amp;(2) \\ \\mathbb{E} \\left[ \\int_E f\\mathop{}\\!\\mathrm{d} \\Phi\\right] = \\int_E f \\mathop{}\\!\\mathrm{d} \\Theta.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Binomial process (definition, distribution of [$]\\Phi(A)[/$], im).</div>",
    "back": "[$$]\\Phi := \\sum_{i=1}^m \\delta_{\\xi_i}, \\ \\Phi(A) \\sim \\text{Bin}(m, \\mathbb{P}^{\\xi_1}(A)), \\ \\Theta = m\\mathbb{P}^{\\xi_1}.[/$$]<div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.1 Random measures and point processes.<div></div><div>Prototype of a Poisson process.</div>",
    "back": "Let [$]\\Theta[/$] finite, positive measure on [$]E[/$], [$]\\xi_i \\sim \\frac{\\Theta}{\\Theta(E)}[/$] i.i.d. and [$]\\tau \\sim \\text{Po}(\\Theta(E))[/$] independent. Then [$]\\Phi := \\sum_{i=1}^\\tau \\delta_{\\xi_i}[/$] is a point process such that<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\forall B\\in\\mathcal{B}(E)\\colon \\Phi(B) \\sim \\text{Po}(\\Theta(E)).\\\\&amp;(2) \\ \\forall k\\in\\mathbb{N}, B_1,\\ldots,B_k\\in\\mathcal{B}(E) \\text{ p.d.}\\colon \\Phi(B_1),\\ldots,\\Phi(B_k) \\text{ are independent.}\\\\&amp;(3) \\ \\mathbb{P}^\\Phi = \\mathrm{e}^{-\\Theta(E)}\\left( \\delta_0 + \\sum_{k=1}^\\infty \\frac{\\Theta^k\\left(\\Gamma_k^{-1}(\\cdot)\\right)}{k!}\\right).\\end{align*}[/$$]</div><div></div><div></div><div>[$]\\text{Here, } \\Gamma_k\\colon E^k\\to \\mathbb{N}(E), (x_1,\\ldots,x_k)\\mapsto \\sum_{j=1}^k \\delta_{x_j}.[/$]</div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Bochner-Integral: Setup und Definition.</div>",
    "back": "Ziel: Definiere [$]\\int_\\Omega f \\mathop{}\\!\\mathrm{d} \\mu \\in B[/$] fuer [$]f\\colon (\\Omega,\\mathcal{A},\\mu)\\to (B,\\|\\cdot\\|)[/$].<div></div><div></div><div>[$$]\\begin{align*}&amp;(\\text{Schritt 1}) \\ \\int_\\Omega f \\mathop{}\\!\\mathrm{d} \\mu := \\sum_{i=1}^k g_i \\mu(A_i) \\text{ fuer } f \\text{ einfach.}\\\\&amp;(\\text{Schritt 2}) \\ \\text{Falls } (f_n) \\text{ einfach mit } f_n \\stackrel{ \\|\\cdot\\|_1 }{\\longrightarrow} f\\colon \\int_\\Omega f \\mathop{}\\!\\mathrm{d} \\mu := \\lim_{n\\to\\infty} \\int_\\Omega f_n \\mathop{}\\!\\mathrm{d} \\mu.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Bochner-Integral: Hinreichende Bedingung fuer Bochner-integrierbarkeit allgemein und speziell falls [$]B[/$] separabler Hilbertraum.</div>",
    "back": "Hinreichend: [$]\\int_\\Omega \\|f\\|\\mathop{}\\!\\mathrm{d} \\mu &lt; \\infty[/$] und es existiert Folge [$]g_n\\colon\\Omega\\to B_n[/$] mb, [$]\\dim B_n &lt; \\infty[/$] mit [$]g_n \\stackrel{ \\|\\cdot\\|_1 }{\\longrightarrow} f[/$].<div></div><div>Falls [$]B = \\mathcal{H}[/$] separabler Hilbertraum: [$]\\int_\\Omega \\|f\\|\\mathop{}\\!\\mathrm{d}\\mu&lt;\\infty[/$] hinreichend.</div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Falls [$]B[/$] Hilbertraum: Zusammenhang Bochner-Integral mit [$]\\langle \\cdot,\\cdot\\rangle[/$].</div><div></div><div>Folge fuer [$]\\mathbb{E} X[/$]?</div>",
    "back": "Fuer [$]f\\colon \\Omega\\to B, \\ y\\in B[/$]:<div></div><div>[$$] \\langle \\int_\\Omega f\\mathop{}\\!\\mathrm{d}\\mu, y\\rangle = \\int_\\Omega \\langle f, y\\rangle \\mathop{}\\!\\mathrm{d}\\mu.[/$$]</div><div></div><div>Insbesondere ist [$]\\mathbb{E} X = \\int_\\Omega X \\mathop{}\\!\\mathrm{d} \\mathbb{P}[/$].</div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Existenz eines Operators [$]T[/$] mit</div><div></div><div>[$$] \\langle Tx, y\\rangle = \\mathbb{E}\\left[ \\langle X, x\\rangle \\langle X, y\\rangle\\right], \\ x, y\\in\\mathcal{H}.[/$$]</div><div></div><div>(mit genauen Voraussetzungen etc).</div>",
    "back": "Ist [$]X[/$] ZE in [$]\\mathcal{H}[/$] mit [$]\\mathbb{E} \\|X\\|^2 &lt; \\infty[/$], dann existiert genau ein linearer, beschraenkter, symmetrischer, positiver SKO mit obiger Eigenschaft. Es gilt [$]\\text{Sp} (T) = \\mathbb{E}\\|X\\|^2[/$].<div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Definition und Existenz Kovarianzoperator [$]\\Sigma =: \\Sigma(X)[/$].</div>",
    "back": "Ist [$]X[/$] ZE in [$]\\mathcal{H}[/$] mit [$]\\mathbb{E} \\|X\\|^2 &lt; \\infty[/$], dann existiert genau ein linearer, beschraenkter, positiver SKO [$]\\Sigma[/$] mit<div></div><div>[$$] \\langle \\Sigma x, y\\rangle = \\mathbb{E}\\left[ \\langle X-\\mathbb{E} X, x\\rangle \\langle X-\\mathbb{E} X, y\\rangle\\right], \\ x, y\\in\\mathcal{H}.[/$$]</div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Kovarianzoperator fuer [$]\\mathcal{H} = \\mathbb{R}^d[/$].</div>",
    "back": "Dann ist [$]\\langle \\Sigma e_i, e_j\\rangle = \\sigma_{ij}[/$].<div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>[$]X, Y[/$] unabhaengig mit [$]\\mathbb{E} \\|X\\|^2, \\mathbb{E}\\|Y\\|^2 &lt; \\infty[/$].</div><div></div><div>[$]\\Sigma(X + Y)= ?[/$].</div>",
    "back": "[$]\\Sigma(X+Y) = \\Sigma(X)+\\Sigma(Y).[/$]<div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>[$]L^2[/$]-wertige Prozesse: Setup</div>",
    "back": "[$](E,\\rho)[/$] kompakt, [$]\\mu[/$] endliches Borel-Mass. Dann ist [$]\\mathcal{H}:= L^2(E,\\mathcal{B},\\mu)[/$] separabler HR.<div></div><div>Weiter sei [$]X\\colon \\Omega \\times E\\to\\mathbb{R}[/$] produkt-messbar mit [$]\\mathbb{E} X_t^2 &lt; \\infty, \\ t\\in E[/$].</div><div></div><div></div><div>[$$]\\begin{align*}&amp; m(t) := \\mathbb{E} X_t \\text{ heisst} \\textbf{ Erwartungswertfunktion von }X.\\\\&amp; C(t,s):= \\text{Cov}(X_t,X_s) \\text{ heisst } \\textbf{Kovarianzfunktion von }X.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>[$]L^2[/$]-wertige Prozesse: Falls [$]X(\\omega,\\cdot)\\in\\mathcal{H}, \\ \\omega\\in\\Omega[/$], so ist [$]X\\colon \\Omega\\to\\mathcal{H}[/$] messbar.</div>",
    "back": "Cramér-Wold in [$]\\mathcal{H}[/$] und Fubini.<div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>[$]L^2[/$]-wertige Prozesse: Hinreichende Bedingung fuer Produktmessbarkeit.</div>",
    "back": "Falls [$]X_t\\colon \\Omega\\to\\mathbb{R}[/$] messbar und [$]X(\\omega,\\cdot)\\colon E\\to\\mathbb{R}[/$] stetig (je fuer alle [$]t\\in E[/$] bzw. [$]\\omega\\in\\Omega[/$]).<div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>[$]L^2[/$]-wertige Prozesse: Wie sehen [$]\\mathbb{E} X, \\ \\Sigma(X)[/$] aus?</div>",
    "back": "Falls [$]X[/$] produktmessbar und [$]\\mathcal{H}[/$]-wertig, und [$]m, C[/$] stetig, so gilt<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ m = \\mathbb{E} X.\\\\&amp;(2) \\ K = \\Sigma(X).\\end{align*}[/$$]</div><div></div><div></div><div>Dabei ist [$$] K\\colon \\mathcal{H}\\to\\mathcal{H}, \\ f \\mapsto \\left[ t \\mapsto \\int_E C(t,s) f(s)\\mu(\\mathop{}\\!\\mathrm{d} s)\\right].[/$$]</div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Uebersicht</div>",
    "back": "(- Grundlagen ONS, Riesz)<div>- Erzeuger von [$]\\mathcal{B}(\\mathcal{H})[/$], Cramér-Wold</div><div>- [$]\\mathbb{E} X[/$]</div><div>- Bochner-Integral</div><div>- [$]\\Sigma(X)[/$]</div><div>- [$]L^2[/$]-wertige Prozesse</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Definition poisson process (ppp).</div><div></div><div>Why is [$]\\Theta[/$] locally finite and the im of [$]\\Phi[/$]?</div><div>What is [$]\\Phi_A, \\ A\\in\\mathcal{B}(E)[/$]?</div>",
    "back": "A <b>poisson process</b>&nbsp;is a pp [$]\\Phi[/$] for which there exists a measure [$]\\Theta[/$] such that<div></div><div></div><div>[$$]\\begin{align*}&amp;(1) \\ \\forall B_1,\\ldots,B_k\\in\\mathcal{B}(E) \\text{ p.d.}\\colon \\Phi(B_1),\\ldots,\\Phi(B_k) \\text{ are independent.}\\\\&amp;(2) \\ \\forall A\\in\\mathcal{B}(E)\\colon \\Phi(A) \\sim \\text{Po}\\left(\\Theta(A)\\right) \\ (\\text{where Po}(\\infty):= \\delta_\\infty).\\end{align*}[/$$]</div><div></div><div></div><div>Then [$]\\Theta[/$] is the locally finite intensity measure of [$]\\Phi[/$] (in particular unique).</div><div></div><div>[$]\\Phi_A[/$] is a ppp with im [$]\\Theta_A[/$].</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Characterisation of [$]\\Phi \\stackrel{d}{=} \\Phi'[/$] for pps.</div>",
    "back": "The following are equivalent ((i) and (ii) also for rms):<div></div><div></div><div>[$$]\\begin{align*}&amp;(i) \\ \\Phi \\stackrel{d}{=} \\Phi'.\\\\&amp;(ii) \\ \\forall A_1,\\ldots,A_k\\in\\mathcal{G}_c\\colon (\\Phi(A_1),\\ldots,\\Phi(A_k))\\stackrel{d}{=} (\\Phi'(A_1),\\ldots,\\Phi'(A_k)).\\\\&amp;(iii) \\ \\text{see } (ii) \\text{, but with } A_i \\text{ p.d.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 3]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Existence and uniqueness of the poisson process.</div>",
    "back": "If [$]\\Theta[/$] is a locally finite measure on [$]E[/$], then there exists a ppp (unique in distribution) on [$]E[/$] with im [$]\\Theta[/$].<div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Four basic properties of a ppp.</div>",
    "back": "<div>Let [$]\\Phi[/$] be a ppp on [$]E[/$] with im [$]\\Theta[/$]. Then:</div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{If } B_1,B_2,\\ldots \\in\\mathcal{B}(E) \\text{ p.d., then } \\Phi_{B_1},\\Phi_{B_2},\\ldots\\text{ are independent.}\\\\(2)&amp; \\ \\text{If } A\\in\\mathcal{B}(E), 0&lt;\\Theta(A)&lt;\\infty\\colon \\mathbb{P}\\left(\\Phi_A\\in\\cdot | \\Phi(A) = k\\right) = \\mathbb{P}\\left(\\sum_{j=1}^k \\delta_{\\xi_j} \\in\\cdot\\right), \\text{ where } \\xi_j \\sim \\frac{\\Theta_A}{\\Theta(A)} \\text{ i.i.d.}\\\\(3)&amp; \\ \\text{If } A\\in\\mathcal{B}(E), 0&lt;\\Theta(A)&lt;\\infty, A = \\bigcup_{i=1}^k A_i \\text{ p.d., then } \\left(\\Phi(A_1),\\ldots,\\Phi(A_k)\\right) \\text{ follows}\\\\ &amp; \\text{a multinomial distribution given } \\Phi(A) = k.\\\\(4)&amp; \\ \\text{If } A\\in\\mathcal{B}(E),\\Theta(A) &lt; \\infty, f\\colon \\mathbb{N}(E)\\to [0,\\infty] \\text{ mb, then}\\\\&amp;\\mathbb{E}\\left[f(\\Phi_A)\\right] = \\mathrm{e}^{-\\Theta(A)} \\left( f(0) + \\sum_{k=1}^\\infty \\frac{1}{k!} \\int_{A^k} f\\left(\\sum_{j=1}^k \\delta_{x_j}\\right) \\Theta^k\\left(\\mathop{}\\!\\mathrm{d} (x_1,\\ldots,x_k)\\right)\\right)\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Simulation of a poisson process.</div>",
    "back": "<div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{Generate } k \\sim \\text{Po}(\\Theta(A)).\\\\(2)&amp; \\ \\text{Generate } k \\text{ independet points } \\xi_i \\sim \\frac{\\Theta_A}{\\Theta(A)}.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Laplace transform of [$]\\text{Po}(a)[/$], Laplace functional (why does it determine [$]\\mathbb{P}_\\Phi[/$]?).</div>",
    "back": "If [$] X \\sim \\text{Po}(a)\\colon L_X(t) = \\mathbb{E}\\left[\\mathrm{e}^{-tX}\\right] = \\mathrm{e}^{-a(1-\\mathrm{e}^{-t})}.[/$]<div><br><div></div><div>The <b>Laplace functional</b>&nbsp;of an rm [$]\\Phi[/$] is the map</div><div></div><div>[$$] L_\\Phi \\colon \\{h\\colon E\\to [0,\\infty] \\text{ mb}\\} \\to \\mathbb{R}, h\\mapsto \\mathbb{E}\\left[\\mathrm{e}^{-\\int_E h \\mathop{}\\!\\mathrm{d} \\Phi}\\right].[/$$]</div><div></div><div>If [$]\\Phi,\\Psi[/$] are two rms, then [$]\\Phi\\stackrel{d}{=}\\Psi \\iff L_\\Phi = L_\\Psi[/$].</div><div></div><div></div><div>[Page 4]</div></div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>Definition und (fuenf) Eigenschaften von [$]\\varphi_X[/$] (<b>charakteristisches Funktional von [$]X[/$]</b>).</div>",
    "back": "Mit [$]\\varphi_X\\colon \\mathcal{H}\\to\\mathbb{C}, h\\mapsto \\mathbb{E}\\left[ \\mathrm{e}^{\\mathrm{i}\\langle X, h\\rangle}\\right][/$] gilt:<div></div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\varphi_X(0) = 1, \\ \\varphi_X \\text{ ist stetig.}\\\\(2)&amp; \\ \\varphi_X \\text{ ist positiv semidefinit, also }\\forall n\\in\\mathbb{N}, h_i\\in\\mathcal{H},c_i\\in\\mathbb{C}\\colon \\sum_{k,l=1}^n c_k \\overline{c_l} \\varphi_X(h_k-h_l) \\ge 0.\\\\(3)&amp; \\ X,Y \\text{ unabhaengig}\\implies \\varphi_{X+Y} = \\varphi_X \\cdot \\varphi_Y.\\\\(4)&amp; \\ X \\stackrel{d}{=} Y \\iff \\varphi_X = \\varphi_Y.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 17]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>[$]\\varphi_X\\colon \\mathbb{R}^d\\to\\mathbb{C}[/$] fuer [$]X\\sim \\mathcal{N}_d(m,\\Sigma)[/$].</div>",
    "back": "<div></div>[$]\\varphi_X(t) = \\exp\\left( \\mathrm{i} t^\\top m - \\frac 12 t^\\top \\Sigma t\\right).[/$]<div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>Definition [$]X\\sim \\mathcal{N}(m, \\Sigma)[/$] fuer [$]m\\in\\mathcal{H}, \\Sigma \\in \\mathcal{L}_\\textit{tr}^1(\\mathcal{H})[/$].</div>",
    "back": "[$$]X\\sim \\mathcal{N}(m, \\Sigma)\\mathrel{:\\Longleftrightarrow} \\varphi_X(h) = \\exp \\left(\\mathrm{i} \\langle m, h\\rangle - \\frac 12 \\langle \\Sigma h, h\\rangle\\right), h\\in\\mathcal{H}.[/$$]<div></div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{Falls } \\ker \\Sigma = \\{0\\} \\text{ heisst }X\\textbf{ nicht ausgeartet.}\\\\(2)&amp; \\ \\text{Falls } m = 0\\text{ heisst }X\\textbf{ zentriert.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>[$]\\forall m\\in\\mathcal{H}, \\Sigma\\in\\mathcal{L}_\\textit{tr}^1(\\mathcal{H}) \\text{ existiert } \\mathcal{N}(m, \\Sigma).[/$]</div><div></div><div>Warum ist [$]\\ell^2\\in\\mathcal{B}^\\infty[/$]?</div>",
    "back": "Seite 18 / Zusatz"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>(Vier) Elementare Eigenschaften von [$]\\mathcal{N}(m, \\Sigma)[/$].</div>",
    "back": "<div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\forall h\\in\\mathcal{H}\\colon \\langle X, h\\rangle \\sim \\mathcal{N}_1(\\langle m, h\\rangle, \\langle \\Sigma h, h\\rangle).\\\\(2) &amp; \\ \\forall k\\in\\mathbb{N}, h_i\\in\\mathcal{H}\\colon \\left( \\langle X, h_1\\rangle, \\ldots,\\langle X, h_k \\rangle \\right)\\sim \\mathcal{N}_k.\\\\(3) &amp; \\ \\mathbb{E}\\|X\\|^2 &lt; \\infty.\\\\(4) &amp; \\ \\mathbb{E} X = m, \\ \\Sigma(X) = \\Sigma.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>Charakterisierung [$]X[/$] normalverteilt mit [$]\\langle X, h\\rangle, h\\in\\mathcal{H}[/$].</div>",
    "back": "Es gilt<div></div><div>[$$]X \\sim \\mathcal{N} \\iff \\forall h\\in\\mathcal{H}\\colon \\langle X, h\\rangle \\sim \\mathcal{N}_1[/$$]</div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>Verteilung von [$]\\|X - m\\|^2[/$] falls [$]X\\sim\\mathcal{N}(m,\\Sigma)[/$]</div>",
    "back": "<br><div>[$$]\\|X-m\\|^2 = \\sum_{j=1}^\\infty \\lambda_j N_j^2,[/$$]</div><div></div><div>wobei [$]N_j \\left( = \\frac{\\langle X-m, e_j\\rangle}{\\sqrt{\\lambda_j}}\\right) \\sim \\mathcal{N}_1(0,1)[/$] i.i.d.</div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$]:<div></div><div>Kriterium fuer [$]X_n\\mathrel{\\stackrel{d}{\\longrightarrow}} X[/$].</div>",
    "back": "<div></div><div></div><div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\forall l\\in\\mathbb{N}\\colon \\Pi_l(X_n) \\mathrel{\\stackrel{d}{\\longrightarrow}} \\Pi_l(X).\\\\(2) &amp; \\ \\forall \\delta &gt; 0\\colon \\limsup_{n\\to\\infty}\\mathbb{P}\\left(\\|X_n - \\Pi_l(X_n)\\| \\ge \\delta\\right) \\stackrel{ l\\to\\infty }{\\longrightarrow} 0.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>Dabei bezeichnet [$$]\\Pi_l\\colon \\mathcal{H}\\to\\mathcal{H}, h\\to \\sum_{k=1}^l \\langle h, e_k\\rangle e_k.[/$$]</div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "23 Normalverteilung in [$]\\mathcal{H}[/$].<div></div><div>ZGWS in [$]\\mathcal{H}[/$].</div>",
    "back": "Sind [$](Z_i)[/$] i.i.d. in [$]\\mathcal{H}[/$] mit [$]\\mathbb{E} \\| Z_1 \\|^2 &lt; \\infty[/$], so gilt<div></div><div>[$$]\\frac{1}{\\sqrt{n}} \\sum_{j=1}^n \\left(Z_j - m\\right) \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}(0,C),[/$$]</div><div></div><div>wobei [$]m := \\mathbb{E} Z_1, C := \\Sigma(Z_1)[/$].</div><div></div><div></div><div>[Seite 18]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Definition Likelihood-Quotient ([$]\\leadsto[/$] Neyman-Pearson-Test) und verallgemeinerter Likelihood-Quotient.</div>",
    "back": "Neyman-Pearson-Test: Optimaler Test fuer [$]H_0\\colon \\theta = \\theta_0[/$] gegen [$]H_1\\colon \\theta = \\theta_1[/$]. Verwendet<div></div><div>[$$]T_n(X_1,\\ldots,X_n):= \\prod_{j=1}^n\\frac{f(X_j,\\theta_1)}{f(X_j,\\theta_0)}.[/$$]</div><div></div><div></div><div>Fuer [$]H_0\\colon \\theta \\in\\Theta_0[/$] gegen [$]H_1\\colon \\theta\\in \\Theta\\setminus\\Theta_0[/$]: <b>verallgemeinerter LQ (GLR)</b>:</div><div></div><div>[$$]\\Lambda_n(X_1,\\ldots,X_n) := \\frac{\\sup_{t\\in\\Theta_0} \\prod_{j=1}^n f(X_j,t)}{\\sup_{t\\in\\Theta} \\prod_{j=1}^n f(X_j,t)} (\\le 1).[/$$]</div><div></div><div></div><div>[Seite 9]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>GWS fuer einfache Hypothese [$]\\Theta_0 = \\{\\vartheta_0\\}[/$].</div>",
    "back": "Mit [$]M_n := -2\\log \\Lambda_n[/$] gilt dann (unter (10.5))<div></div><div>[$$]M_n \\stackrel{ d_{\\vartheta_0} }{\\longrightarrow} \\chi_k^2.[/$$]</div><div></div><div></div><div>[Seite 9]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Beispiel: [$]M_n[/$] bei einfacher Hypothese im Multinomialfall.</div>",
    "back": "Ergebnis:<div></div><div>[$$]M_n = 2\\sum_{j=1}^s N_{nj} \\log\\left( \\frac{N_{nj}}{np_j} \\right)\\mathrel{\\stackrel{d}{\\longrightarrow}} \\chi_{s-1}^2.[/$$]</div><div></div><div></div><div>[Seite 9]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Zusammengesetzte Hypothese und Beweisidee.</div>",
    "back": "<div></div><div>Falls $1\\le l &lt; k$, $U\\subset\\mathbb{R}^l$ offen, $h\\colon U\\to\\mathbb{R}^k$ zweimal stetig db und injektiv, $\\Theta_0 = h(U)$, dann gilt unter weiteren Regularitaetsvoraussetzungen</div><div>\\[ M_n = -2 \\log \\Lambda_n \\stackrel{ d_\\vartheta }{\\longrightarrow} \\chi_{k-l}^2 \\ \\forall \\vartheta \\in \\Theta_0.\\]</div><div></div><div></div><div></div><div>[Seite 9 / Extraseite]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Verallgemeinerter Likelihood-Quotiententest.</div><div></div><div>Kullback-Leibler-Information.</div>",
    "back": "<div>Sei $c_\\alpha$ das $(1-\\alpha)$-Quantil der $\\chi_{k-l}^2$-Verteilung (also $\\mathbb{P}(\\chi_{k-l}^2 \\ge c_\\alpha) = \\alpha$). Dann setze</div><div>\\[ \\varphi_n := \\boldsymbol{1}\\{M_n \\ge c_\\alpha\\}.\\]</div><div>Dann hat $(\\varphi_n)$ asymptotisch das Niveau $\\alpha$, das heisst $\\lim_{n\\to\\infty} \\mathbb{E}_\\vartheta \\varphi_n = \\alpha \\forall \\vartheta \\in \\Theta_0$ und es gilt</div><div>\\[\\forall \\vartheta\\notin\\Theta_0\\colon\\lim_{n\\to\\infty} \\mathbb{E}_\\vartheta \\varphi_n = 1.\\]</div><div></div><div>Der im Beweis auftretende Term $I_{KL}(Q_{\\vartheta_1}, Q_{\\vartheta_0}) :=\\mathbb{E}_{\\vartheta_1} \\left[ \\log \\frac{f(X_1,\\vartheta_1)}{f(X_1, \\vartheta_0)}\\right]$ heisst <b>Kullback-Leibler-Information</b> von $Q_{\\vartheta_1}$ bezueglich $Q_{\\vartheta_0}$.</div><div></div><div></div><div>[Seite 9 / Extraseite]</div>"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Kontingenztafeln (Idee)</div>",
    "back": "Extraseite"
  },
  {
    "front": "12 Asymptotische Tests:<div></div><div>Parametrischer Bootstrap.</div>",
    "back": "Extraseite."
  },
  {
    "front": "22 ZE in [$]\\mathcal{H}[/$]:<div></div><div>Cramér-Wold in [$]\\mathcal{H}[/$].</div>",
    "back": "Sind [$]X, Y[/$] ZE in [$]\\mathcal{H}[/$] mit [$]\\langle X, h\\rangle \\stackrel{d}{=} \\langle Y, h\\rangle \\forall h\\in\\mathcal{H}[/$], so gilt [$]X \\stackrel{d}{=} Y[/$].<div></div><div></div><div>[Seite 16]</div>"
  },
  {
    "front": "Charakterisierung [$]X_n \\mathrel{\\stackrel{\\operatorname{f.s.}}{\\longrightarrow}} X[/$].",
    "back": "[$$]X_n\\mathrel{\\stackrel{\\operatorname{f.s.}}{\\longrightarrow}} X \\iff \\forall \\varepsilon &gt; 0\\colon \\mathbb{P}\\left( \\sup_{k \\ge n} \\|X_k - X\\| \\ge \\varepsilon\\right) \\stackrel{ n\\to\\infty }{\\longrightarrow}0[/$$].<div></div><div></div><div>[Extraseite]</div>"
  },
  {
    "front": "Charakterisierung [$]X_n\\mathrel{\\stackrel{\\mathbb{P}}{\\longrightarrow}} X[/$].",
    "back": "[$$] X_n\\mathrel{\\stackrel{\\mathbb{P}}{\\longrightarrow}} X \\iff \\text{ Jede Teilfolge } (k(n)) \\text{ besitzt weitere Teilfolge } (l(n)) \\text{ mit } X_{l(n)} \\mathrel{\\stackrel{\\operatorname{f.s.}}{\\longrightarrow}} X.[/$$]<div></div><div></div><div>[Extraseite]</div>"
  },
  {
    "front": "ZGWS von Lindeberg-Feller und Ljapunov-Bedingung.",
    "back": "<div>Sei $\\left(X_{nj}\\colon 1\\le j\\le k_n\\right)$ Schema zeilenweise unabhaengiger reeller ZVen mit $\\sigma_{nj}^2:= \\mathbb{V}(X_{nj}) \\in (0,\\infty)$. Setze $S_n := \\sum_{j=1}^{k_n}X_{nj}$ und $\\sigma_n^2:= \\mathbb{V}(S_n)$. Gilt dann</div><div>\\[ \\forall \\varepsilon &gt; 0\\colon L_n(\\varepsilon) := \\frac{1}{\\sigma_n^2}\\sum_{j=1}^{k_n} \\mathbb{E} \\left[ |\\tilde{X}_{nj}|^2\\boldsymbol{1}\\{|\\tilde{X}_{nj}| \\ge \\sigma_n \\varepsilon\\} \\right] \\stackrel{ n\\to\\infty }{\\longrightarrow} 0\\]</div><div>oder</div><div>\\[ \\exists \\delta &gt; 0\\colon \\frac{1}{\\sigma_n^{2+\\delta}} \\sum_{j=1}^{k_n} \\mathbb{E}\\left[ |\\tilde{X}_{nj}|^{2+\\delta} \\right] \\stackrel{ n\\to\\infty }{\\longrightarrow}0,\\]</div><div>so gilt $\\frac{S_n-\\mathbb{E} S_n}{\\sigma_n} \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}(0,1)$.</div><div></div><div></div><div></div><div>[Extraseite]</div>"
  },
  {
    "front": "Quantiltransformation.",
    "back": "<div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\forall u\\in(0,1), x\\in\\mathbb{R}\\colon F^{-1}(u) \\le x \\iff u \\le F(x).\\\\(2)&amp; \\ U \\sim \\mathcal{U}(0,1)\\implies F^{-1}(U) \\sim F.\\\\(3)&amp; \\ \\text{Falls } F \\text{ stetig}\\colon X\\sim F \\implies F(X) \\sim U(0,1).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Extraseite]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Characterisation of a ppp with [$]L_\\Phi[/$].</div>",
    "back": "<div>Let $\\Theta$ be a locally finite measure on $E$. Then a random measure $\\Phi$ is &nbsp;a ppp with i.m. $\\Theta$ if and only if for $h\\colon E\\to[0,\\infty]$ mb,</div><div>\\[L_\\Phi (h) = \\exp \\left( - \\int_E \\left(1 - \\mathrm{e}^{-h(x)}\\right) \\Theta(\\mathop{}\\!\\mathrm{d} x)\\right).\\]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Definition generating functional and relation to Laplace functional.</div>",
    "back": "<div></div><div>Let $\\Phi$ be a pp. Then the <b>generating functional</b> of $\\Phi$ is the map</div><div>\\[ G_\\Phi\\colon \\{f\\colon E\\to [0,1] \\text{ mb}\\} \\to \\mathbb{R}, f \\mapsto \\mathbb{E} \\left[ \\prod_{x\\in \\operatorname{supp} \\Phi} f(x)^{\\Phi(\\{x\\})} \\right] = \\mathbb{E} \\left[ \\mathrm{e}^{\\int_E \\log f \\mathop{}\\!\\mathrm{d} \\Phi}\\right].\\]</div><div>Then for $f\\colon E\\to [0,1]$ and $h\\colon E \\to [0,\\infty]$ mb we have</div><div>\\[ G_\\Phi(f) = L_\\Phi(-\\log f) \\text{ and } L_\\Phi(h) = G_\\Phi(\\mathrm{e}^{-h}).\\]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Characterisation of a ppp with generating functionals.</div>",
    "back": "<div>Let $\\Theta$ be locally finite and $\\Phi$ a pp on $E$. Then $\\Phi$ is a ppp with i.m. $\\Theta$ if and only if for $f\\colon E \\to [0,1]$ mb,</div><div>\\[G_\\Phi(f) = \\exp \\left( - \\int_E \\left( 1 - f(x)\\right) \\Theta(\\mathop{}\\!\\mathrm{d} x)\\right).\\]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Measurability argument in preparation for Mecke's equation.</div>",
    "back": "<div></div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{If } f\\colon \\mathbb{N}(E)\\times E \\to [0,\\infty] \\text{ is mb, then } T\\colon \\mathbb{N}(E)\\to [0,\\infty], \\eta \\mapsto \\int_E f(\\eta,x)\\eta(\\mathop{}\\!\\mathrm{d} x) \\text{ is mb.}\\\\(2)&amp; \\ \\text{The map } \\mathbb{N}(E) \\times E \\ni (\\eta, x) \\mapsto \\eta + \\delta_x\\in \\mathbb{N}(E) \\text{ is mb.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "\"II.2 Poisson processes:<div></div><div>Mecke characterisation of a ppp (\"\"[$]\\implies[/$]\"\").</div>\"",
    "back": "<div></div><div>Let $\\Phi$ be a point process in $E$ and $\\Theta$ a locally finite measure on $E$. Then $\\Phi$ is a ppp with i.m. $\\Theta$ if and only if for any $ f\\colon \\mathbb{N}(E)\\times E\\to [0,\\infty]$ mb,<div></div><div>\\[\\mathbb{E} \\left[ \\int_E f\\left(\\Phi,x\\right) \\Phi(\\mathop{}\\!\\mathrm{d} x)\\right] = \\int_E \\mathbb{E} \\left[ f\\left( \\Phi + \\delta_x, x\\right) \\right] \\Theta(\\mathop{}\\!\\mathrm{d} x).\\]</div><div></div><div></div><div></div><div>[Page 4]</div></div>"
  },
  {
    "front": "\"II.2 Poisson processes:<div></div><div>Mecke characterisation of a ppp (\"\"[$]\\impliedby[/$]\"\").</div>\"",
    "back": "Let [$]\\Phi[/$] be a pp and [$]\\Theta[/$] a locally finite measure on [$]E[/$]. Then [$]\\Phi[/$] is a ppp with i.m. [$]\\Theta[/$] if and only if<div></div><div>[$$]\\forall f\\colon \\mathbb{N}(E)\\times E\\to [0,\\infty] \\text{ mb}\\colon \\mathbb{E} \\left[ \\int_E f\\left(\\Phi,x\\right) \\Phi(\\mathop{}\\!\\mathrm{d} x)\\right] = \\int_E \\mathbb{E} \\left[ f\\left( \\Phi + \\delta_x, x\\right) \\right] \\Theta(\\mathop{}\\!\\mathrm{d} x).[/$$]</div><div></div><div></div><div>[Page 4]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Definition [$]n[/$]-fold product measure and [$]n[/$]-th moment measure of a pp [$]\\Phi[/$].</div><div></div>",
    "back": "<div></div><div>Let $\\Phi$ be a point process in $E$ and $n\\in\\mathbb{N}$. Then $\\Phi^n:=\\bigotimes_{j=1}^n \\Phi$ is a point process in $E^n$, the \\textbf{$n$-fold product measure of $\\Phi$}. Then $\\Gamma^n(\\cdot):= \\mathbb{E} \\Phi^n(\\cdot)$ is called the \\textbf{$n$-th moment measure of $\\Phi$}.</div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{If } \\Phi = \\sum_{i\\in I} \\delta_{\\xi_i}, \\text{ then } \\Phi^n = \\sum_{i\\in I^n} \\delta_{(\\xi_{i_1}\\ldots \\xi_{i_n})}.\\\\(2)&amp; \\ \\text{If } A\\in\\mathcal{B}(E)\\colon \\Gamma^n(A^n) = \\mathbb{E} \\left[\\Phi(A)^n\\right], \\text{ hence the name.}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Definition of factorial product and factorial moment measure of a pp [$]\\Phi[/$].</div><div></div><div>What does [$]\\Phi^{(n)}(A)[/$] look like?</div><div></div>",
    "back": "<br><div></div><div>Let $\\Phi = \\sum_{i\\in I} \\delta_{\\xi_i}$ be a pp in $E$ and $n\\in \\mathbb{N}$. Set $I_n^\\star := \\{i\\in I^n\\colon i_k\\neq i_l \\forall l\\neq k\\}$. Then \\[\\Phi^{(n)}:= \\sum_{i\\in I_n^\\star} \\delta_{(\\xi_{i_1}\\ldots \\xi_{i_n})}\\] is called \\textbf{factorial product measure of $\\Phi$} and $\\Gamma^{(n)} (\\cdot):= \\mathbb{E} \\left[\\Phi^{(n)}(\\cdot)\\right]$ is called \\textbf{factorial moment measure of $\\Phi$}. The following properties hold ((2) and (3) follow from (4)):</div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{If } \\Phi &nbsp;\\text{ is simple, then }\\Phi^{(n)} = \\Phi^n\\restriction_{E^n_\\star}, \\text{ where } E^n_\\star := \\{x\\in E^n\\colon x_k\\neq x_l \\forall k\\neq l\\}.\\\\(2)&amp; \\ \\text{If $A \\in \\mathcal{B}(E)\\colon$} \\Phi^{(n)}(A^n) = \\Phi(A) \\left(\\Phi(A) - 1\\right) \\ldots \\left(\\Phi(A) - n + 1\\right).\\\\(3) &amp; \\ \\text{If $A_1,\\ldots,A_n\\in\\mathcal{B}(E)$ p.d.$\\colon$} \\Phi^{(n)}(A_1\\times \\ldots \\times A_n) = \\Phi(A_1) \\cdot \\ldots \\cdot \\Phi(A_n).\\\\(4)&amp; \\ \\text{If } A\\in\\mathcal{B}(E^n)\\colon \\Phi^{(n)}(A) = \\int_E\\ldots\\int_E\\boldsymbol{1}_A(x_1,\\ldots,x_n) \\left( \\Phi\\setminus \\delta_{x_1}\\setminus \\ldots \\setminus \\delta_{x_{n-1}}\\right)(\\mathop{}\\!\\mathrm{d} x_n)\\ldots \\Phi(\\mathop{}\\!\\mathrm{d} x_1).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Multivariate Mecke formula and corollary.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a ppp in $E$ with i.m. $\\Theta$. Then for any $n\\in\\mathbb{N}$ and $f\\colon \\mathbb{N}(E)\\times E^n \\to [0,\\infty]$ measurable we have</div><div>\\[ \\mathbb{E} \\left[ \\int_{E^n} f(\\Phi,x) \\Phi^{(n)} (\\mathop{}\\!\\mathrm{d} x) \\right] = \\int_{E^n} \\mathbb{E} \\left[ f\\left(\\Phi + \\sum_{i=1}^n \\delta_{x_i}, x\\right)\\right] \\Theta^n(\\mathop{}\\!\\mathrm{d} x).\\]</div><div></div><div><b>Corollary:</b> A pp $\\Phi$ with i.m. $\\Theta$ is a ppp if and only if $\\Gamma^{(n)} = \\Theta^n$ for all $n\\in\\mathbb{N}$.</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Definition of [$]\\mathbb{P}_x[/$] for a pp [$]\\Phi[/$]. Why is it a kernel? Relation to Mecke equation?</div><div></div>",
    "back": "<div></div><div>Let $\\Phi$ be a point process in $E$. Then set</div><div>\\[ \\mathbb{P}_x(\\cdot) := \\mathbb{P}^{\\Phi+\\delta_x}(\\cdot) = \\mathbb{P}\\left( \\Phi+\\delta_x \\in \\cdot\\right).\\]</div><div>Then $E\\times\\mathcal{N}(E)\\to [0,\\infty], (x, A) \\mapsto \\mathbb{P}_x(A)$ is a stochastic kernel. The Mecke equation now reads, for a ppp $\\Phi$ and $f\\colon \\mathbb{N}(E)\\times E\\to [0,\\infty]$ measurable,</div><div>\\[ \\int_{\\mathbb{N}(E)} \\int_E f(\\eta, x) \\eta(\\mathop{}\\!\\mathrm{d} x) \\mathbb{P}^\\Phi(\\mathop{}\\!\\mathrm{d} \\eta) = \\int_E \\int_{\\mathbb{N}(E)} f(\\eta, x) \\mathbb{P}_x(\\mathop{}\\!\\mathrm{d} \\eta) \\Theta(\\mathop{}\\!\\mathrm{d} x).\\]</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Definition Borel and Polish spaces.</div><div></div><div>Definition vague convergence and why [$]\\mathbb{M}(E)[/$] and [$]\\mathbb{N}(E)[/$] are Polish spaces.</div>",
    "back": "<br><div></div><div>A topological space $X$ is called <b>Polish</b> iff $X$ is separable and completely metrizable.</div><div></div><div>A measurable space $(E, \\mathcal{E})$ is called a <b>Borel space</b> iff $\\exists B\\in\\mathcal{B}(\\mathbb{R})$ such that $(E, \\mathcal{E}) \\cong (B, \\mathcal{B}(\\mathbb{R})\\cap B),$ i.e. there is a bijective, bimeasurable map $\\varphi\\colon E\\to B$. All Polish spaces are Borel spaces.\\\\</div><div></div><div>A sequence $(\\mu_n)$ in $\\mathbb{M}(E)$ is said to converge <b>vaguely</b> to $\\mu\\in\\mathbb{M}(E)$ iff</div><div>\\[ \\int_E f \\mathop{}\\!\\mathrm{d} \\mu_n \\stackrel{ n\\to\\infty }{\\longrightarrow} \\int_E f \\mathop{}\\!\\mathrm{d} \\mu\\]</div><div>for all $f\\colon E \\to \\mathbb{R}$ continuous with compact support. Vague convergence in $\\mathbb{M}(E)$ can be completely metriced such that $\\mathcal{M}(E) = \\mathcal{B}(\\mathbb{M}(E))$, hence $(\\mathbb{M}(E), \\mathcal{M}(E))$ is Polish. $\\mathbb{N}(E)\\subset \\mathbb{M}(E)$ is closed, hence also Polish.</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Definition Campbell measure. What do [$]C(A\\times B)[/$] and in particular [$]\\pi_2 C[/$] look like?</div>",
    "back": "<div></div><div>Let $\\Phi$ be a random measure in $E$ (with locally finite intensity measure $\\Theta$). Then the <b>Campbell measure</b> of $\\Phi$ is</div><div>\\[ C\\colon \\mathcal{N}(E)\\times \\mathcal{B}(E)\\to [0,\\infty], F\\mapsto \\int_{\\mathbb{N}(E)} \\int_E \\boldsymbol{1}_F(\\eta, x) \\eta(\\mathop{}\\!\\mathrm{d} x) \\mathbb{P}^\\Phi(\\mathop{}\\!\\mathrm{d} \\eta) = \\mathbb{E} \\left[ \\int_E \\boldsymbol{1}_F(\\Phi, x) \\Phi(\\mathop{}\\!\\mathrm{d} x) \\right].\\]</div><div>Then, in particular, we have</div><div>[$$]\\begin{align*}(1)&amp; \\ \\text{If } A\\in\\mathcal{N}(E), B\\in \\mathcal{B}(E)\\colon C(A\\times B) = \\mathbb{E} \\left[ \\boldsymbol{1}\\{\\Phi\\in A\\} \\Phi(B)\\right] \\le \\Theta(B).\\\\(2)&amp; \\ \\text{In particular, } \\pi_2 C = \\Theta \\text{ is locally finite and hence $\\sigma$-finite}.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 5]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Overview.</div>",
    "back": "- Definition, Existence, Uniqueness, Properties.<div>- Characterisation via [$]L_\\Phi[/$] and [$]G_\\Phi[/$].</div><div>- Mecke Characterisation.</div><div>- Factorial product, factorial moment measure and multivariate Mecke equation.</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Definition Palm distributions and Palm pair.</div><div></div><div>Palm family for a poisson process?</div>",
    "back": "<br><div></div><div>By disintegration of the Campbell measure, for any random measure $\\Phi$ with i.m. $\\Theta$ there exists a probability kernel $(\\mathbb{P}_x)_{x\\in E}$ such that for any $f\\colon \\mathbb{N}(E)\\times E\\to [0,\\infty]$ measurable we have</div><div>\\[\\int_{\\mathbb{N}(E)} \\int_E f(\\eta, x) \\eta(\\mathop{}\\!\\mathrm{d} x) \\mathbb{P}^\\Phi(\\mathop{}\\!\\mathrm{d} \\eta) = \\int_E \\int_{\\mathbb{N}(E)} f(\\eta, x) \\mathbb{P}_x(\\mathop{}\\!\\mathrm{d} \\eta) \\Theta (\\mathop{}\\!\\mathrm{d} x).\\]</div><div>The family $(\\mathbb{P}_x)$ is called a family of <b>Palm distributions</b> of $\\Phi$. The pair $(\\mathbb{P}_x, \\Phi)$ is called a <b>Palm pair</b>. For two such kernels $\\mathbb{P}_x, \\tilde{\\mathbb{P}}_x$, we have $\\mathbb{P}_x = \\tilde{\\mathbb{P}}_x$ for $\\Theta$-a.a $x\\in E$.\\\\</div><div></div><div>If $\\Phi$ is a ppp, then $\\mathbb{P}_x := \\mathbb{P}^{\\Phi+\\delta_x}$ is a Palm family of $\\Phi$ by Mecke.</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>(1) Property of [$]\\mathbb{P}_x[/$] that justifies the definition of [$]\\mathbb{P}_x^![/$].</div><div>(2), (3) [$]\\Theta[/$] and [$](\\mathbb{P}_x)[/$] determine [$]\\mathbb{P}^\\Phi[/$].</div>",
    "back": "<div></div><div>Let $\\Phi$ be a point process in $E$ and $(\\mathbb{P}_x)$ a family of Palm distributions of $\\Phi$ . Then<br>[$$]\\begin{align*} (1)&amp; \\ \\text{For }\\Theta\\text{-a.a. } x\\in E\\colon \\mathbb{P}_x \\left( \\{\\eta\\colon \\eta(\\{x\\} ) \\ge 1\\} \\right) =1.\\\\ (2)&amp; \\ \\exists h \\colon \\mathbb{N}(E)\\times E \\to [0,\\infty) \\text{ mb s.t. }\\forall \\eta\\in \\mathbb{N}(E)\\setminus \\{0\\} \\colon \\int_E h(\\eta, x) \\eta(\\mathop{}\\!\\mathrm{d} x) = 1.\\\\ (3)&amp; \\ \\text{For } A\\in \\mathcal{N}(E), 0 \\not\\in A\\colon \\mathbb{P}\\left( \\Phi \\in A \\right) = \\int_E \\int_{\\mathbb{N}(E)}\\boldsymbol{1}_A(\\eta)h(\\eta, x) \\mathbb{P}_x(\\mathop{}\\!\\mathrm{d} \\eta) \\Theta(\\mathop{}\\!\\mathrm{d} x).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Definition reduced Palm measures. What does [$]\\mathbb{P}_x^![/$] look like for a poisson process?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a point process in $E$ with Palm distributions $(\\mathbb{P}_x)$. Then for $x\\in E$ we set<br>\\[ \\mathbb{P}_x^!(\\cdot ) := \\mathbb{P}_x\\left( \\eta\\setminus \\delta_x\\in \\cdot \\right) = \\int_{\\mathbb{N}(E)} \\boldsymbol{1}\\{\\eta\\setminus \\delta_x\\in \\cdot \\}\\mathbb{P}_x(\\mathop{}\\!\\mathrm{d} \\eta)\\] and call $(\\mathbb{P}_x^!)$ the <b>family of reduced Palm distributions of</b> $\\Phi$.<br><br>If $\\Phi$ is a poisson process, then \\[ \\mathbb{P}_x^!(\\cdot ) = \\mathbb{P}\\left( (\\Phi + \\delta_x)\\setminus \\delta_x\\in \\cdot \\right) = \\mathbb{P}^\\Phi(\\cdot ).\\]</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Representation of [$]\\int_E h(\\eta) \\mathbb{P}_{x_0}(\\mathop{}\\!\\mathrm{d} \\eta)[/$] for [$]\\Theta(\\{x_0\\}) &gt; 0[/$]. What does this imply in the case that [$]\\Phi[/$] is simple?</div><div></div>",
    "back": "<div></div><div>Let $\\Phi$ be a pp with i.m. $\\Theta$ and Palm families $(\\mathbb{P}_x)$. Then for $x_0\\in E$ with $\\Theta(\\{x_0\\} )&gt; 0$ (in particular, $\\mathbb{P}_{x_0}$ is uniquely determined), we have<br>\\[ \\int_E h(\\eta)\\mathbb{P}_{x_0}(\\mathop{}\\!\\mathrm{d} \\eta) = \\frac{\\mathbb{E} \\left[ h(\\Phi) \\Phi(\\{x_0\\} ) \\right] }{\\mathbb{E}\\left[ \\Phi(\\{x_0\\} ) \\right] }.\\] <br>for any $h \\colon \\mathbb{N}(E) \\to [0,\\infty]$. In particular, if $\\Phi$ is simple, we have \\[ \\mathbb{P}_{x_0}(\\cdot ) = \\mathbb{P}\\left( \\Phi\\in\\cdot \\ | \\ x_0\\in \\Phi \\right).\\]</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Mixtures of point processes.</div>",
    "back": "<br><div></div><div>Let $(\\Sigma, \\mathcal{F}, \\mathbb{Q})$ be a probability space and $\\mathbb{P}^{(\\sigma)}(\\cdot )$ a stochastic kernel from $\\Sigma$ to $\\mathbb{N}(E)$. Then \\[ \\mathbb{P}(\\cdot ) := \\int_\\Sigma \\mathbb{P}^{(\\sigma)}(\\cdot ) \\mathbb{Q}(\\mathop{}\\!\\mathrm{d} \\sigma)\\] defines a probability measure on $\\mathbb{N}(E)$ and hence a point process, called the <b>mixture of point processes with distributions</b> $\\mathbb{P}^{(\\sigma)}$.</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.3 Palm measures:<div></div><div>Overview</div>",
    "back": "- Disintegration theorem, Campbell measure<div>- Palm distributions + properties</div><div>- reduced Palm distributions</div><div>- mixtures of point processes</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Definition of a Cox process with directing measure [$]\\mathbb{Q}[/$]. Why is [$]\\Pi_\\Theta(\\cdot)[/$] a stochastic kernel?</div>",
    "back": "<br><div></div><div>Let $\\mathbb{Q}$ be a probability measure on $\\mathcal{M}(E)$. Then a point process with distribution \\[ \\mathbb{P}^\\Phi(\\cdot ) = \\int_{\\mathbb{M}(E)} \\Pi_\\Theta(\\cdot ) \\mathbb{Q}(\\mathop{}\\!\\mathrm{d} \\Theta)\\] is called a <b>Cox process with directing measure</b> $\\mathbb{Q}$, where $\\Pi_\\Theta$ denotes the distribution of a poisson process with intensity measure $\\Theta$.</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>What does [$]\\mathbb{P}(\\Phi(A_i) = k_i)[/$] look like for a Cox process?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\mathbb{Q}$. Then if $A_1,\\ldots , A_m\\in \\mathcal{B}(E), k_1,\\ldots ,k_m\\in \\mathbb{N}_0,$ \\[ \\mathbb{P}\\left( \\forall i\\colon \\Phi(A_i) = k_i \\right) = \\sum_{m\\in M} \\int_{\\mathbb{M}(E)} \\prod_{I\\subset [m]} \\mathrm{e}^{-\\Theta(B_I)} \\frac{\\Theta(B_I)^{m_I}}{m_I!} \\mathbb{Q}(\\mathop{}\\!\\mathrm{d} \\Theta)\\] If the $A_i$ are pairwise disjoint, then \\[ \\mathbb{P}\\left( \\forall i\\colon \\Phi(A_i) = k_i \\right) = \\int_{\\mathbb{M}(E)} \\prod_{j=1}^m \\mathrm{e}^{-\\Theta(A_j)} \\frac{\\Theta(A_j)^{k_j}}{k_j!} \\mathbb{Q}(\\mathop{}\\!\\mathrm{d} \\Theta)\\]</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Intensity measure of a Cox process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\mathbb{Q}$. Then for $B\\in \\mathcal{B}(E),$\\[ \\mathbb{E}\\left[ \\Phi(B) \\right] = \\int_{\\mathbb{M}(E)}\\Theta(B) \\mathbb{Q}(\\mathop{}\\!\\mathrm{d} \\Theta) = \\Lambda_\\mathbb{Q}(B)\\] is the intensity measure of $\\mathbb{Q}$.</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>When is a Cox process simple.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\mathbb{Q}$. Then $\\Phi$ is simple iff $\\Theta$ is $\\mathbb{Q}$-almost surely diffusive (i.e., $\\chi$ is almost surely diffusive).</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Definition of Cox process with directing measure [$]\\chi[/$]. Relation to definition via [$]\\mathbb{Q}[/$]?</div>",
    "back": "<br><div></div><div>Let $\\chi$ be a random measure in $E$. Then $\\Phi$ is called a <b>Cox process with directing measure</b> $\\chi$ if \\[ \\mathbb{P} \\left( \\Phi \\in \\cdot \\ | \\ \\chi \\right) = \\Pi_\\chi(\\cdot) \\operatorname{\\mathbb{P}-a.s.}\\] In other words, \\(<br>\\Omega \\times \\mathcal{N}(E) \\ni (\\omega, A) \\mapsto \\Pi_{\\chi(\\omega)}(A) \\in [0,1]<br>\\) is a version of $P_{\\Phi | \\chi}$. This means that \\[ \\mathbb{P}^{(\\Phi, \\chi)}(C) = \\int_{\\mathbb{M}(E)}\\int_{\\mathbb{N}(E)} \\boldsymbol{1}_{C}(\\eta, \\Theta) \\Pi_\\Theta(\\mathop{}\\!\\mathrm{d} \\eta) \\mathbb{P}^\\chi(\\mathop{}\\!\\mathrm{d} \\Theta)\\] for any $C\\in \\mathcal{M}(E) \\otimes \\mathcal{N}(E)$. In particular, $\\mathbb{P}(\\Phi\\in \\cdot ) = \\int_{\\mathbb{M}(E)}\\Pi_\\Theta(\\cdot) \\mathbb{P}^\\chi(\\mathop{}\\!\\mathrm{d} \\Theta)$, so the previous definition is retrieved via $\\mathbb{Q} = \\mathbb{P}^\\chi$.</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Example of a directing measure via intensity field [$]Y[/$].</div><div></div><div>Why / when is it a random measure (measurable, locally finite)?</div>",
    "back": "<br><div></div><div>Let $\\Theta\\in \\mathbb{M}(E)$ be fixed and $Y \\colon \\Omega\\times E \\to [0,\\infty)$ measurable. Then set \\[ \\chi(B) := \\int_B Y \\mathop{}\\!\\mathrm{d} \\Theta, \\ B\\in \\mathcal{B}(E).\\] Special case: $Y$ depends only on $\\omega\\in \\Omega$, i.e. $\\chi = Y \\Theta_0$. Then $\\mathbb{P}_\\chi(\\cdot) = \\int_0^\\infty \\boldsymbol{1}\\{t\\Theta_0\\in \\cdot \\}\\mathbb{P}^Y(\\mathop{}\\!\\mathrm{d} t)$ and hence \\[P^\\Phi(\\cdot ) = \\int_0^\\infty \\Pi_{t\\Theta_0}(\\cdot ) \\mathbb{P}^Y(\\mathop{}\\!\\mathrm{d} t).\\]</div><div></div><div></div><div></div><div>[Page 6]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>[$]\\mathbb{E} \\left[ \\Phi(B) \\right][/$] and [$]\\mathbb{V}(\\Phi(B))[/$] for a Cox process. Can a Cox process be a poisson process?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\chi$. Then $\\mathbb{E}\\left[ \\Phi(B) \\right] = \\mathbb{E} \\left[ \\chi(B) \\right] $ and \\[ \\mathbb{V}(\\Phi(B)) = \\mathbb{V}(\\chi(B)) + \\mathbb{E}\\left[ \\Phi(B) \\right].\\] In particular, if $\\tilde{\\Phi}$ is a poisson process with the same intensity measure, then \\[\\mathbb{V}(\\Phi(B)) = \\mathbb{V}(\\tilde{\\Phi}(B)) + \\mathbb{V}(\\chi(B)).\\] Hence for non-constant $\\chi$, a Cox process cannot be a poisson process.</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Mecke characterisation of a Cox process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a point process in $E$ and $\\chi$ a random measure in $E$. Then $\\Phi$ is a Cox process with directing measure $\\chi$ if and only if \\[ \\mathbb{E} \\left[ \\int_E f(\\Phi,\\chi,x)\\Phi(\\mathop{}\\!\\mathrm{d} x) \\right] =\\mathbb{E}\\left[ \\int_E f(\\Phi+\\delta_x, \\chi, x)\\chi(\\mathop{}\\!\\mathrm{d} x) \\right] \\] for all $f\\colon \\mathbb{N}(E)\\times \\mathbb{M}(E)\\times E\\to[0,\\infty]$ measurable.</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Multivariate Mecke formula.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process in $E$ with directing measure $\\chi$. Then for $n\\in \\mathbb{N}$ and $f\\colon \\mathbb{N}(E)\\times \\mathbb{M}(E)\\times E^n\\to[0,\\infty]$ measurable we have \\[ \\mathbb{E} \\left[ \\int_{E^n}&nbsp;f\\left(\\Phi, \\chi, x\\right) \\Phi^{(n)}(\\mathop{}\\!\\mathrm{d} x) \\right] = \\mathbb{E} \\left[ \\int_{E^n}&nbsp;f\\left( \\Phi+\\sum_{i=1}^n \\delta_{x_i}, \\chi, x \\right) \\chi^n(\\mathop{}\\!\\mathrm{d} x) \\right] .\\] In particular, $\\mathbb{E}\\left[ \\Phi^{(n)}(\\cdot ) \\right] = \\mathbb{E} \\left[ \\chi^n(\\cdot ) \\right] $.</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Reduced Palm measures of a Cox process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\chi$ and <b>locally finite</b> intensity measure $\\Theta$. Let $K(x,\\cdot )$ be a Palm family of $\\chi$, i.e. \\[ \\int_{\\mathbb{M}}\\int_E f(\\mu, x)\\mu(\\mathop{}\\!\\mathrm{d} x) \\mathbb{P}_\\chi(\\mathop{}\\!\\mathrm{d} \\mu) = \\int_E \\int_\\mathbb{M} f(\\mu, x) K(x, \\mathop{}\\!\\mathrm{d} \\mu) \\Theta(\\mathop{}\\!\\mathrm{d} x).\\] Then for $\\Theta$-a.a. $x\\in E$, we have \\[\\mathbb{P}_x^!(\\cdot ) = \\int_\\mathbb{M} \\Pi_\\mu(\\cdot ) K(x, \\mathop{}\\!\\mathrm{d} \\mu).\\]</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Generating functional of a Cox process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Cox process with directing measure $\\chi$. Then for $f\\colon E\\to[0,1]$ measurable,\\[ G_\\Phi(f) = L_\\chi(1-f).\\]</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Let [$]\\chi[/$] be a random measure on [$]\\mathbb{R}^n[/$] derived from an intensity field [$]Y[/$]. What is meant by <b>stationarity</b>&nbsp;of [$]Y[/$] and what does it imply for [$]\\chi[/$]?</div><div></div><div>What does the intensity measure of [$]\\chi[/$] look like in that case?</div>",
    "back": "<br><div></div><div>Let $Y\\colon \\Omega\\times \\mathbb{R}^n\\to [0,\\infty)$ be measurable and $\\chi(A):= \\int_A Y \\mathop{}\\!\\mathrm{d} \\lambda^n, A\\in \\mathcal{B}^n$. Then $Y$ is called <b>stationary</b> iff $T_t Y \\stackrel{d}{=} Y$ as random fields, or equivalently if \\[ \\forall x_1,\\ldots x_k, t\\in \\mathbb{R}^n\\colon (Y(x_1),\\ldots Y(x_k)) \\stackrel{d}{=} (Y(x_1+t),\\ldots ,Y(x_k+t)). \\] Now if for $\\mathbb{P}$-a.a. $\\omega\\in \\Omega$, $Y(\\omega,\\cdot )$ is Riemann-integrable on compact sets, then stationarity of $Y$ implies stationarity of $\\chi$ (which implies stationarity of $\\Phi\\sim \\text{Cox}(\\chi)$). In that case, \\[\\mathbb{E}\\left[ \\chi(\\cdot ) \\right] = \\Theta \\lambda^n,\\\\\\] where $\\Theta := \\mathbb{E} \\left[ Y(0) \\right] \\in [0,\\infty)$ is called the <b>intensity</b> of $\\Phi$.</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>If [$]\\Phi\\sim \\text{Cox}(\\chi)[/$] and [$]\\chi[/$] is stationary (in [$]\\mathbb{R}^n[/$]), then [$]\\Phi[/$] is stationary.</div>",
    "back": "[Page 7]"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Let [$]\\Phi \\sim \\text{Cox}(Z \\lambda^n)[/$]. What can you say about the estimator</div><div></div><div>[$$]\\tilde{\\Theta}_W := \\frac{\\Phi(W)}{\\lambda^n(W)}[/$$]?</div>",
    "back": "<br><div></div><div>It is unbiased, but \\[\\mathbb{V}(\\tilde{\\Theta}_W) = \\frac{\\Theta\\lambda^n(W) + \\mathbb{V}(Z) \\lambda^n(W)^2}{\\lambda^n(W)^2}\\stackrel{ \\lambda^n(W)\\to \\infty }{\\longrightarrow}\\mathbb{V}(Z),\\] so we cannot conclude $\\tilde{\\Theta}_W \\stackrel{ \\mathbb{P} }{\\longrightarrow}\\Theta.$</div><div></div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Factorial moment measure of a Cox process.</div>",
    "back": "<br><div>[$$] \\Gamma^{(n)} (B) = \\mathbb{E} \\left[ \\Phi^{(n)}(B)\\right] = \\mathbb{E} \\left[\\chi^n(B)\\right], \\ B \\in \\mathcal{B}(E^n)[/$$].</div><div></div><div>Follows directly from multivariate Mecke formula for Cox processes.</div><div></div><div></div><div>[Page 7]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Neyman-Scott processes.</div>",
    "back": "<br><div></div><div><b>Definition:</b> Let $E=\\mathbb{R}^n$. Let $\\Theta_0 = \\vartheta_0 \\lambda^n$ be locally finite and $\\Phi_0 = \\sum_{i=1}^\\tau \\delta_{\\xi_i}\\sim \\text{Po} (\\Theta_0)$. Let $\\Theta_1 = \\vartheta_1 \\lambda^n$ be finite such that \\[ \\int_B \\int_E \\vartheta_1(x-y)\\vartheta_0(y) \\mathop{}\\!\\mathrm{d} y\\mathop{}\\!\\mathrm{d} x &lt; \\infty \\ \\forall B\\in \\mathcal{C}\\] and let $\\Phi^{(i)}\\sim \\text{Po} (\\Theta_1)$ i.i.d. Then set $\\Phi := \\sum_{i=1}^\\tau \\left( \\Phi^{(i)}+\\xi_i \\right) $.<br><br><b>Cox process:</b> $\\Phi$ is a Cox process with intensity field $Y(x) = \\int_E \\vartheta_1(x-y)\\Phi_0(\\mathop{}\\!\\mathrm{d} y)$ (w.r.t. $\\lambda^n)$, i.e. with directing measure \\[ \\chi(B) = \\int_E \\int_E \\boldsymbol{1}_{B}(x) \\vartheta_1(x-y) \\mathop{}\\!\\mathrm{d} x \\Phi_0(\\mathop{}\\!\\mathrm{d} y), B\\in\\mathcal{B}(E).\\]<br><br><b>Stationarity:</b> Furthermore, if $\\Phi_0$ is stationary (i.e. $\\vartheta_0 \\equiv \\overline{\\vartheta}_0 $), then $\\Phi$ is stationary with intensity \\[\\Theta = \\overline{\\vartheta}_0\\int_E \\vartheta_1(x) \\mathop{}\\!\\mathrm{d} x\\].</div><div></div><div></div><div></div><div>[Page 7/8]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Matern-Cluster processes.</div>",
    "back": "<br><div></div><div>Let $\\overline{\\vartheta}_1, r&gt;0$. Then a Matern-Cluster process is a Neyman-Scott process with \\[ \\vartheta_1(x) := \\overline{\\vartheta}_1 \\boldsymbol{1}\\{\\|x\\|\\le r\\}.\\].</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Estimator [$]\\tilde{\\Theta}_W[/$] for the intensity of a stationary Neyman-Scott process.</div><div>(Definition, Variance, weak and [$]\\sqrt{n}[/$]-consistency).</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary Neyman-Scott process (i.e. $\\vartheta_0 \\equiv \\overline{\\vartheta}_0$ ). Then $\\tilde{\\Theta}_W := \\frac{\\Phi(W)}{|W|}$ is an unbiased estimator for its intensity $\\Theta = \\overline{\\vartheta}_0 \\int \\vartheta_1 \\mathop{}\\!\\mathrm{d} \\lambda^n$. The following properties hold:<br><br>[$$]\\begin{align*} &amp;(1) \\ \\mathbb{V}(\\tilde{\\Theta}_W) = \\frac{\\overline{\\vartheta}_0}{|W|}\\left( \\int_E \\vartheta_1(x) \\mathop{}\\!\\mathrm{d} x + \\int_E \\int_E \\frac{|W \\cap \\left( W - (x_2-x_1) \\right) |}{|W|}\\vartheta_1(x_1)\\vartheta_1(x_2) \\mathop{}\\!\\mathrm{d} x_1\\mathop{}\\!\\mathrm{d} x_2\\right) .\\\\ &amp;(2) \\ \\tilde{\\Theta}_W \\text{ is weakly consistent, i.e. } \\tilde{\\Theta}_{W_k}\\stackrel{ \\mathbb{P} }{\\longrightarrow} \\Theta \\text{ if } W_k \\in \\mathcal{B}^n \\text{ bounded and } |W_k| \\to \\infty.\\\\ &amp;(3) \\ \\text{If } \\forall x\\in \\mathbb{R}^n, \\frac{|W_k\\cap (W_k-x)|}{|W_k|} \\to 1, \\text{ then } \\sqrt{|W_k|} \\left( \\tilde{\\Theta}_{W_k}-\\Theta\\right) \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mathcal{N}(0,\\sigma^2) \\text{ where}\\\\ &amp; \\ \\ \\ \\ \\sigma^2 := \\lim_{k\\to\\infty}\\left( |W_k|\\cdot \\mathbb{V}(\\tilde{\\Theta}_{W_k})\\right) = \\overline{\\vartheta}_0 \\left( \\int \\vartheta_1 \\mathop{}\\!\\mathrm{d} \\lambda^n + \\left( \\int \\vartheta_1 \\mathop{}\\!\\mathrm{d} \\lambda^n \\right)^2 \\right) .\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>Overview</div>",
    "back": "- Definition (via [$]\\mathbb{Q}[/$] and [$]\\chi[/$]).<div>- [$]\\mathbb{E}[/$] and [$]\\mathbb{V}[/$] of a Cox process.</div><div>- Mecke characterisation / equation for Cox processes.</div><div>- [$]\\mathbb{P}_x^![/$] and [$]G_\\Phi[/$]&nbsp;for a Cox process.</div><div>- Intensity fields and Neyman-Scott processes.</div>"
  },
  {
    "front": "Charakterisierung von bedingter Unabhaengigkeit von [$](X_i)[/$] gegeben [$]\\mathcal{F}[/$] mit bedingten Verteilungen.",
    "back": "<div></div><div></div>Seien $(X_i\\colon \\Omega \\to E_i)$ Zufallsvariablen in polnischen $(E_i, \\mathcal{E}_i)$ mit abzählbar erzeugten $\\mathcal{E}_i$. Seien $P_i\\colon \\Omega \\times \\mathcal{E}_i\\to [0,1]$ stochastische Kerne und $\\mathcal{F}\\subset \\mathcal{A}$ eine sub-$\\sigma$-Algebra. Dann sind äquivalent:<br>[$$]\\begin{align*} (1)&amp; \\ (X_i) \\text{ sind bedingt unabhängig gegeben } \\mathcal{F} \\text{ und $P_i$ ist eine bedingte Verteilung } P_{X_i|\\mathcal{F}}.\\\\ (2)&amp; \\ \\text{Für alle } n\\in \\mathbb{N}\\text{ ist } P^{(n)}(\\omega, \\cdot ):=\\bigotimes_{i=1}^n P_i(\\omega,\\cdot ) \\text{ bedingte Verteilung von $(X_i)_{i=1}^n$ unter } \\mathcal{F}.\\\\ (3)&amp; \\ P(\\omega, \\cdot ):= \\bigotimes_{i\\in \\mathbb{N}}P_i(\\omega,\\cdot ) \\text{ ist bedingte Verteilung von $(X_i)$ unter } \\mathcal{F}.\\end{align*}[/$$]<br><div></div><div></div><div></div><div></div><div>[GTK-Stuff Seite 1]</div>"
  },
  {
    "front": "Let [$]X, Y[/$] be random variables. Connection between conditional distribution [$]K(Y,\\cdot) = P_{X|Y}[/$] and [$]\\mathbb{P}^{(X,Y)}[/$].",
    "back": "<br><div></div><div>Let $X\\colon \\Omega\\to E$ and $Y\\colon \\Omega\\to E'$ be measurable and $K\\colon E'\\times \\mathcal{E}\\to [0,1]$ a stochastic kernel. Then the following are equivalent:<br>[$$]\\begin{align*} (1)&amp; \\ K(Y,\\cdot )\\colon \\Omega\\times \\mathcal{E}\\to[0,1] \\text{ is a conditional distribution } P_{X|Y}.\\\\ (2)&amp; \\ \\mathbb{P}^{(X,Y)} = \\mathbb{P}^Y \\otimes K, \\text{ i.e. } \\mathbb{P}^{(X,Y)}(\\cdot ) = \\int_{E'}\\int_E \\boldsymbol{1}\\{(x,y)\\in \\cdot \\}K(y,\\mathop{}\\!\\mathrm{d} x) \\mathbb{P}^Y(\\mathop{}\\!\\mathrm{d} y).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[GTK-Stuff Page 1]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Definition [$]K[/$]-marking.</div><div></div><div>Does the definition depend on the concrete choice of the [$](\\xi_i)[/$]?</div>",
    "back": "<br><div></div><div>Let $\\Phi_0 = \\sum_{i=1}^\\tau \\delta_{\\xi_i}$ be a point process in $E$ and $K\\colon E\\times \\mathcal{N}(E)\\to[0,1]$ a stochastic kernel. Then a point process $\\Psi$ on $E\\times \\mathbb{N}(E)$ with distribution \\[ \\mathbb{P}^\\Psi(\\cdot ) = \\int \\int \\boldsymbol{1}\\left\\lbrace\\sum_{i=1}^k \\delta_{(x_i,y_i)}\\in \\cdot \\right\\rbrace\\left( \\bigotimes_{i=1}^k K(x_i,\\cdot ) \\right) (\\mathop{}\\!\\mathrm{d} (y_1,\\ldots )) \\mathbb{P}^{(\\tau,\\xi_1,\\ldots )}(\\mathop{}\\!\\mathrm{d} (k, x_1,\\ldots ))\\] is called a \\textbf{$K$-marking of $\\Phi_0$}. If $K(x,\\cdot ) = \\mathbb{Q}(\\cdot )$, then $\\Psi$ is called an \\textbf{independent $\\mathbb{Q}$-marking of $\\Phi_0$}.</div><div></div><div>Since the Laplace-functional of $\\Psi$ only depends on the distribution of $\\Phi_0$, this is well-defined.</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Representation of a [$]K[/$]-marking as [$]\\Psi \\stackrel{d}{=} \\sum_{n=1}^\\tau \\delta_{(\\xi_n, \\chi_n)}[/$].</div>",
    "back": "<br><div></div><div>Let $\\Phi_0 = \\sum_{i=1}^\\tau \\delta_{\\xi_i}$ be a point process in $E$ and $\\Psi$ a $K$-marking of $\\Phi_0$. Let $(\\chi_i)$ be point processes in $E$ (on the same probability space as $\\Phi_0$) such that<br>[$$]\\begin{align*} (1)&amp; \\ (\\chi_i) \\text{ are conditionally independent given } \\tau, (\\xi_i).\\\\ (2)&amp; \\ \\mathbb{P}\\left( \\chi_n\\in \\cdot \\ | \\ \\tau, (\\xi_i) \\right) = K(\\xi_n,\\cdot ) \\ \\mathbb{P}\\text{-a.s.} \\end{align*}[/$$]<br>Then $\\Psi \\stackrel{d}{=} \\sum_{n=1}^\\tau \\delta_{(\\xi_n, \\chi_n)}$.</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Let [$]\\Psi \\stackrel{d}{=} \\sum_{n=1}^\\tau \\delta_{(\\xi_n, \\chi_n)}[/$] be a [$]K[/$]-marking of [$]\\Phi_0[/$]. If [$]K(x,\\cdot) = \\mathbb{Q}(\\cdot)[/$], what do we know about [$]\\Phi_0, \\chi_1, \\chi_2,\\ldots[/$]?</div>",
    "back": "They are independent.<div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Laplace functional of a [$]K[/$]-marking.</div>",
    "back": "<br><div></div><div>Let $\\Psi$ be a $K$-marking of $\\Phi_0$. Then for $f\\colon E\\times \\mathbb{N}(E)\\to [0,\\infty]$ measurable, \\[ L_\\Psi(f) = L_{\\Phi_0}(f^\\star),\\\\\\] where $f^\\star\\colon E\\to [0,\\infty], \\ x\\mapsto -\\log \\left( \\int_{\\mathbb{N}(E)} \\mathrm{e}^{-f(x,\\eta)}K(x,\\mathop{}\\!\\mathrm{d} \\eta) \\right) $. In particular, $\\Psi$ does not depend on the concrete choice of the $(\\xi_i)$.</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Definition Cluster process.</div>",
    "back": "<br><div></div><div>Let $\\Psi = \\sum_{i=1}^\\tau \\delta_{(\\xi_i, \\chi_i)}$ be a $K$-marking of $\\Phi_0$. Define \\[\\Phi := \\sum_{i=1}^\\tau \\chi_i.\\] If for any $B\\in \\mathcal{B}(E)$ relatively compact, $\\mathbb{P}(\\Phi(B) &lt; \\infty) = 1$ (by properties of $E$, this implies $\\mathbb{P}(\\Phi(B) &lt; \\infty \\ \\forall B\\in \\mathcal{B}(E) \\text{ r.c.} ) = 1$), then $\\Phi$ is called a \\textbf{Cluster process with Kernel (cluster field) $K$ (and mother process $\\Phi_0$)}.</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and Cluster processes:<div></div><div>Neyman-Scott process as Cluster process.</div>",
    "back": "<br><div></div><div>A Neyman-Scott process with parent $\\Phi_0$ and child intensity $\\Theta_1$ is a Cluster process with parent $\\Phi_0$ and cluster field \\[ K(x,\\cdot ) = \\Pi_{\\Theta_1}(\\cdot -x) = \\int_{\\mathbb{N}(E)} \\boldsymbol{1}\\{\\mu + x\\in \\cdot \\}\\Pi_{\\Theta_1}(\\mathop{}\\!\\mathrm{d} \\mu).\\]</div><div></div><div></div><div></div><div>[Page 8]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Intensity measure of a cluster process.</div><div></div><div>Don't use the conditional independence of the [$](\\chi_i)[/$] in the proof!</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a cluster process with parent $\\Phi_0$ and kernel $K$. Then the intensity measure of $\\Phi$ is given by \\[ \\mathbb{E} \\left[ \\Phi(B)\\right] = \\int_E \\int_{\\mathbb{N}(E)} \\mu(B) K(x, \\mathop{}\\!\\mathrm{d} \\mu) \\Theta_0(\\mathop{}\\!\\mathrm{d} x) = \\int_E \\Theta_x(B) \\Theta_0(\\mathop{}\\!\\mathrm{d} x),\\\\\\] where $\\Theta_x(B) := \\int_{\\mathbb{N}(E)} \\mu(B) K(x, \\mathop{}\\!\\mathrm{d} \\mu)$ is the intensity measure of $K(x, \\cdot )$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Three examples of cluster processes.</div><div></div><div>(location dependent shift (or deletion), [$]p[/$]-thinning)</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a cluster process with kernel $K$. Then $\\Phi$ is called a<br>[$$]\\begin{align*} (1)&amp; \\ \\textbf{location dependent shift (of $\\Phi_0$) if } K(x, \\{\\mu(E) = 1\\} ) = 1 \\ \\forall x\\in E.\\\\ (2)&amp; \\ \\textbf{location dependent shift or deletion (of $\\Phi_0$) if } K(x, \\{\\mu(E) \\in \\{0,1\\}\\}) = 1 \\ \\forall x\\in E.\\\\ (3)&amp; \\ \\textbf{$p$-thinning (of $\\Phi_0$) if } K(x, \\cdot ) = (1-p(x)) \\delta_0 + p(x) \\delta_{\\delta_x} \\ \\forall x\\in E \\text{ for some mb } p\\colon E\\to [0,1].\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "Bedingte Version von Borel-Cantelli.",
    "back": "<br><div></div><div>Seien $(X_i\\colon \\Omega \\to E)$ Zufallsvariablen in polnischem $(E, \\mathcal{E})$ mit abzählbar erzeugtem $\\mathcal{E}$. Seien $P_i:= P_{X_i|\\mathcal{F}}\\colon \\Omega \\times \\mathcal{E}\\to [0,1]$ und $\\mathcal{F}\\subset \\mathcal{A}$ eine sub-$\\sigma$-Algebra. Seien $A_i\\in \\mathcal{E}, i\\in \\mathbb{N}$. Dann gelten:<br>[$$]\\begin{align*} (1)&amp; \\ \\sum_{n=1}^\\infty \\mathbb{P}(X_n\\in A_n \\ | \\ \\mathcal{F}) &lt; \\infty \\operatorname{\\mathbb{P}-a.s.} \\implies \\mathbb{P}\\left( X_n \\in A_n \\text{ für $\\infty$ viele $n\\in \\mathbb{N}$} \\ | \\ \\mathcal{F} \\right) =0 \\operatorname{\\mathbb{P}-a.s.}.\\\\ (2)&amp; \\ \\sum_{n=1}^\\infty \\mathbb{P}(X_n\\in A_n \\ | \\ \\mathcal{F}) = \\infty \\operatorname{\\mathbb{P}-a.s.} \\text{ und $(X_n)$ bedingt unabhängig gegeben $\\mathcal{F}$} \\\\ &amp; \\implies \\mathbb{P}\\left( X_n \\in A_n \\text{ für $\\infty$ viele $n\\in \\mathbb{N}$} \\ | \\ \\mathcal{F} \\right) =1 \\operatorname{\\mathbb{P}-a.s.}.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[GTK-Stuff Seite 2]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Definition Poisson cluster process and KLM-measure. By what is [$]\\Phi[/$] already determined?</div>",
    "back": "<br><div></div><div>Let $\\Phi_0$ be a poisson process with intensity measure $\\Theta_0$ and $K$ a kernel from $E$ to $\\mathbb{N}(E)$. Then $\\Phi_0$ and $K$ give a cluster process $\\Phi$ if and only if \\[ \\forall B\\in \\mathcal{C}(E)\\colon \\tilde{\\Theta}\\left( \\{\\mu(B) &gt; 0\\} \\right) &lt; \\infty,\\\\\\] where \\[\\tilde{\\Theta}\\colon \\mathcal{N}(E)\\to [0,1], A \\mapsto \\int_{E} K(x, A) \\Theta_0(\\mathop{}\\!\\mathrm{d} x).\\] In that case, $\\Phi$ is called <b>Poisson cluster process</b> with <b>KLM-measure</b> $\\tilde{\\Theta}$. Then \\[\\mathbb{E} \\left[ \\Phi(B) \\right] = \\int_{\\mathbb{N}(E)} \\mu(B) \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu).\\] The distribution of $\\Phi$ is already determined by $\\tilde{\\Theta}$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Laplace functional of a poisson cluster process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Poisson cluster process with KLM-measure $\\tilde{\\Theta}.$ Then for any $h\\colon E\\to [0,\\infty]$ we have \\[ L_\\Phi(h) = \\exp\\left( - \\int_{\\mathbb{N}(E)}\\left( 1 - \\mathrm{e}^{-\\int_E h \\mathop{}\\!\\mathrm{d} \\mu} \\right) \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu) \\right).\\] In particular, the distribution of $\\Phi$ only depends on $\\tilde{\\Theta}$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>[$]\\mathbb{E}[/$] and [$]\\mathbb{V}[/$] of [$]\\int_E h \\mathop{}\\!\\mathrm{d} \\Phi[/$] for a Poisson cluster process [$]\\Phi[/$].</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Poisson cluster process with KLM-measure $\\tilde{\\Theta}$ and intensity measure $\\Theta$. Then for any $h\\colon E\\to [0,\\infty]$ satisfying $\\int_{\\mathbb{N}} \\mu(h) \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu), \\int_{\\mathbb{N}} \\mu(h)^2 \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu) &lt; \\infty$ we have \\[ \\mathbb{E} \\left[ \\Phi(h)^2 \\right] = \\Theta(h)^2 + \\int_{\\mathbb{N}} \\mu(h)^2 \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu) (&lt;\\infty),\\\\\\] or, equivalently, \\[\\mathbb{V}(\\Phi(h)) = \\int_{\\mathbb{N}} \\mu(h)^2 \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu).\\] <br><br>Here, we have denoted, for $\\mu\\in \\mathbb{N}(E),$ \\[ \\mu(h) := \\int_E h \\mathop{}\\!\\mathrm{d} \\mu.\\]</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Poisson cluster processes have higher variances than comparable Poisson processes (when are they equal?)</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a Poisson cluster process with KLM-measure $\\tilde{\\Theta}$ and intensity measure $\\Theta$. Let $\\Phi'$ be a poisson process with intensity measure $\\Theta$. Then, for $B\\in \\mathcal{C}(E),$ \\[ \\mathbb{V}(\\Phi(B)) = \\int_{\\mathbb{N}}\\mu(B)^2 \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu) \\ge \\int_{\\mathbb{N}} \\mu(B) \\tilde{\\Theta}(\\mathop{}\\!\\mathrm{d} \\mu) = \\Theta(B) = \\mathbb{V}(\\Phi'(B)).\\] Equality holds iff $\\mu(B) \\in \\{0,1\\} $ for $\\tilde{\\Theta}$-a.a. $\\mu\\in \\mathbb{N}(E)$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.5 Markings and cluster processes:<div></div><div>Overview</div>",
    "back": "- [$]K[/$]-marking: Definition, Laplace-functional.<div>- Cluster processes: Definition, intensity measure</div><div>- Poisson cluster processes: Definition + KLM-measure, Laplace-functional, [$]\\mathbb{V}(\\Phi(h))[/$]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Definition <b>intensity</b>&nbsp;of a stationary point process [$]\\Phi[/$] in [$]\\mathbb{R}^n[/$] with locally finite i.m. [$]\\Theta[/$]</div>",
    "back": "<br><div></div><div>Since $\\Phi$ is stationary, $\\Theta$ is a locally finite, translation-invariant Borel-measure on $\\mathbb{R}^n$. Hence there exists some $\\gamma \\ge 0$ such that \\[\\Theta = \\gamma \\lambda^n.\\] $\\gamma$ is called the <b>intensity</b> of $\\Phi$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>For each [$]\\gamma \\ge 0[/$] there exists a (distributionally) unique stationary Poisson process in [$]\\mathbb{R}^n[/$] with intensity [$]\\gamma[/$]. [$]\\Phi[/$] is simple and isotropic.</div>",
    "back": "<div></div>Why is [$]\\Phi[/$] simple, stationary and isotropic?<div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Definition <b>(spherical) contact-distribution function</b>.</div>",
    "back": "<br><div></div><div>Let $Z$ be a random closed set in $\\mathbb{R}^n$. Then \\[ H_s\\colon (0,\\infty) \\to [0,1], r \\mapsto \\mathbb{P}\\left( Z \\cap B(0,r) \\neq \\varnothing \\ | \\ 0 \\not\\in Z \\right) \\] is called the \\textbf{(spherical) contact distribution function of $Z$}.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Contact distribution function of [$] \\operatorname{supp} \\Phi[/$] for a Poisson process.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary Poisson process in $\\mathbb{R}^n$ with intensity $\\gamma$. Then, with $Z := \\operatorname{supp} \\Phi$, we have $\\mathbb{P}(0\\not\\in Z) = 1$ and \\[ H_s(r) = 1 - \\mathrm{e}^{-\\gamma \\kappa_n r^n},\\\\\\] where $\\kappa_n := \\lambda^n(B(0,1)) = \\frac{\\pi^{n / 2}}{\\Gamma\\left( \\frac{n}{2} + 1 \\right) }$.</div><div></div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Estimator for the intensity of a stationary Poisson process.</div>",
    "back": "<div>[$$]&nbsp;\\gamma = - \\frac{\\log\\left( 1-H_s(r) \\right) }{\\kappa_n r^n}. [/$$]</div><div></div><div></div><div>[Page 9]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Palm distribution [$]\\mathbb{P}^0[/$] of a stationary point process [$]\\Phi[/$] in [$]\\mathbb{R}^n[/$].</div><div></div><div>Connection with palm family?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary point process in $\\mathbb{R}^n$ with intensity $\\gamma &gt; 0$ and Campbell measure $C$. Then there exists a probability measure $\\mathbb{P}^0$ on $\\mathcal{N}(E)$, the \\textbf{Palm distribution of $\\Phi$}, such that for $A\\in \\mathcal{N}(E), B\\in \\mathcal{B}^n$, \\[ C(A\\times B) = \\gamma \\int_E \\mathbb{P}^0 (A-x) \\lambda^n(\\mathop{}\\!\\mathrm{d} x). \\] or equivalently, \\[\\int_{E\\times \\mathbb{N}(E)}f(\\eta, x) C\\left( \\mathop{}\\!\\mathrm{d} (\\eta, x) \\right) = \\gamma \\int_E \\int_{\\mathbb{N}(E)} f(\\eta+x, x)\\mathbb{P}^0(\\mathop{}\\!\\mathrm{d} \\eta) \\lambda^n(\\mathop{}\\!\\mathrm{d} x).\\]<br> If $B\\in \\mathcal{B}^n$ with $\\lambda^n(B) &gt; 0,$ \\[ \\mathbb{P}^0(A) = \\frac{1}{\\gamma \\lambda^n(B)}\\mathbb{E} \\left[ \\int_B \\boldsymbol{1}\\{\\Phi-x\\in A\\}\\Phi(\\mathop{}\\!\\mathrm{d} x) \\right]. \\] <br><br>Then if $(\\mathbb{P}_x)$ is a Palm family of $\\Phi$, \\[ \\mathbb{P}_x (\\cdot ) = \\mathbb{P}^0(\\cdot -x) \\text{ for $\\lambda^n$-almost all } x\\in \\mathbb{R}^n.\\]</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Let [$]\\Phi[/$] be a stationary point process in [$]\\mathbb{R}^n[/$]. Characterisation of [$]\\Phi[/$] being Poisson using [$]\\mathbb{P}^0[/$].</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary point process in $\\mathbb{R}^n$. Then $\\Phi$ is a poisson process if and only if \\[ \\mathbb{P}^0(\\cdot ) = \\mathbb{P}(\\Phi + \\delta_0 \\in \\cdot ).\\]</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.6 Point processes in [$]\\mathbb{R}^n[/$]:<div></div><div>Overview.</div>",
    "back": "- Intensity, [$]H_s(r)[/$].<div>- [$]\\mathbb{P}^0[/$] and characterisation poisson process</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Definition marked and unmarked process.</div>",
    "back": "<br><div></div><div>Let $E_0$ and $M$ be lcHcb. Then a \\underline{simple} point process $\\Phi$ in $E_0\\times M$ that satisfies \\[\\Theta(C\\times M) :=\\mathbb{E} \\left[ \\Phi(C\\times M) \\right] &lt;\\infty \\ \\forall C\\in \\mathcal{C}(E_0)\\]is called a <b>marked process (with mark space)</b> $M$. The <b>unmarked point process</b> associated with $\\Phi$ is $\\Phi_0(\\cdot ) := \\Phi(\\cdot \\times M)$.</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Definition stationary marked pp (in [$]\\mathbb{R}^n[/$]).</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a marked point process in $\\mathbb{R}^n\\times M$. Then $\\Phi$ is called <b>stationary</b> if for any $x\\in \\mathbb{R}^n$, \\[t_x (\\Phi) := \\Phi \\circ t_x^{-1} \\stackrel{d}{=} \\Phi,\\\\\\] where $t_x \\colon E\\to E, (y,m)\\mapsto (y+x, m).$</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked processes:<div></div><div>Example of a marked process [$]\\Phi[/$] obtained from a point process [$]\\eta[/$] in [$]E_0[/$] (with locally finite i.m.).</div><div></div><div>If [$]E_0 = \\mathbb{R}^n[/$] and [$]\\eta[/$] is stationary, what about [$]\\Phi[/$]?</div>",
    "back": "<br><div></div><div>Let $\\eta$ be a point process in $E_0$. Then \\[ \\Phi := \\sum_{x\\in \\eta}\\delta_{(x,\\eta(x))} = \\int_{E_0} \\boldsymbol{1}\\{(x, \\eta(x))\\in \\cdot \\} \\eta(x)^{-1}\\eta(\\mathop{}\\!\\mathrm{d} x)\\] is a marked process in $E_0\\times \\mathbb{N}$. If $\\eta$ is stationary in $\\mathbb{R}^n$, then so is $\\Phi$. (We have abbreviated $\\eta(x) := \\eta(\\{x\\} ).$</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Intensity measure of a stationary marked process in [$]\\mathbb{R}^n[/$].</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary marked process in $\\mathbb{R}^n$ with i.m. $\\Theta \\neq 0 $. Then there is a probability measure $\\mu$ on $M$ and $\\gamma &gt; 0$ such that \\[\\Theta = \\gamma \\lambda^n \\otimes \\mu.\\] $\\gamma$ is called the <b>intensity</b> of $\\Phi$ and $\\mu$ its <b>marked distribution</b>.</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>If [$]\\Phi[/$] is a stationary marked process in [$]\\mathbb{R}^n[/$], then so is [$]\\Phi_0[/$]. What is the intensity of [$]\\Phi_0[/$]?</div>",
    "back": "<br><div>Intensity is the same.</div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Definition independent [$]\\mathbb{Q}[/$]-marking.</div>",
    "back": "<br><div></div><div>Let $\\Phi_0 = \\sum_{i=1}^\\tau \\delta_{\\xi_i}$ be a point process in $E_0$ and let $(\\zeta_i)$ be i.i.d. random variables in $M$ with distribution $\\mathbb{Q}$, indepedent of $\\Phi_0$. Then \\[ \\Phi := \\sum_{i=1}^\\tau \\delta_{(\\xi_i, \\zeta_i)}\\] is called \\textbf{independent $\\mathbb{Q}$-marking of $\\Phi_0$}.</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Defintion [$]K[/$]-marking.</div>",
    "back": "<br><div></div><div>Let $\\Phi_0 = \\sum_{i=1}^\\tau \\delta_{\\xi_i}$ be a point process in $E_0$ and let $K$ be a stochastic kernel from $E_0$ to $M$. Let $(\\zeta_i)$ be random variables in $M$ such that $(\\zeta_i)$ are independent given $\\Phi_0$ and $\\forall i\\in \\mathbb{N}\\colon \\mathbb{P}\\left( \\zeta_i\\in \\cdot \\ | \\ \\Phi_0 \\right) = K(\\xi_i,\\cdot ) \\ \\mathbb{P}\\text{-a.s.}$.\\\\</div><div></div><div>Then $\\Phi := \\sum_{i=1}^\\tau \\delta_{(\\xi_i,\\zeta_i)}$ is called a $K$-marking of $\\Phi_0$. The distribution of $\\Phi$ is given by \\[\\mathbb{P}^\\Phi(\\cdot ) = \\int \\int \\boldsymbol{1}\\left\\lbrace\\sum_{i=1}^k \\delta_{(x_i,y_i)}\\in \\cdot \\right\\rbrace\\left( \\bigotimes_{i=1}^k K(x_i,\\cdot ) \\right) (\\mathop{}\\!\\mathrm{d} (y_1,\\ldots )) \\mathbb{P}^{(\\tau,\\xi_1,\\ldots )}(\\mathop{}\\!\\mathrm{d} (k, x_1,\\ldots )).\\]</div><div></div><div></div><div></div><div>[Page 10]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Intensity measure of a [$]K[/$]-marking.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a $K$-marking of $\\Phi_0$ in $E_0\\times M$ with intensity measures $\\Theta$ and $\\Theta_0$ respectively. Then $\\Theta = \\Theta_0\\otimes K$, i.e. \\[ \\Theta(\\cdot ) = \\int_{E_0} \\int_M \\boldsymbol{1}\\{(x,m)\\in \\cdot \\}K(x, \\mathop{}\\!\\mathrm{d} m) \\Theta_0(\\mathop{}\\!\\mathrm{d} x).\\] If $K(x,\\cdot ) \\equiv \\mathbb{Q}(\\cdot )$, then $\\Theta = \\Theta_0 \\otimes \\mathbb{Q}$ is the product measure.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Laplace functional of a [$]K[/$]-marking.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a $K$-marking of $\\Phi_0$ in $E_0\\times M$. Then, if $f\\colon E_0\\times M\\to [0,\\infty]$ is measurable, \\[ L_{\\Phi}(f) = L_{\\Phi_0}(f^\\star),\\\\\\] where \\[f^\\star(x) := - \\log \\left( \\int_M \\mathrm{e}^{-f(x,m)}K(x,\\mathop{}\\!\\mathrm{d} m) \\right) .\\]</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>A [$]K[/$]-marking of a ppp is a ppp.</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a $K$-marking of $\\Phi_0$ in $E_0\\times M$. Then if $\\Phi_0$ is a poisson process in $E_0$ with intensity measure $\\Theta_0$, $\\Phi$ is a poisson process in $E_0\\times M$ with intensity measure $\\Theta_0\\otimes K$.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Is a ppp in [$]E_0\\times M[/$] a [$]K[/$]-marking of a ppp in [$]E_0[/$]?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a poisson process in $E_0\\times M$ with intensity measure $\\Theta = \\Theta_0\\otimes K$, where $\\Theta_0$ is a locally finite measure on $E_0$ and $K$ is a stochastic kernel from $E_0$ to $M$. Then $\\Phi$ is the $K$-marking of $\\Phi_0:= \\Phi(\\cdot \\times M)$.<br><br>Sufficient for the disintegration of $\\Theta$ is that $\\Theta_0 := \\Theta(\\cdot \\times M)$ is locally finite and $M$ is a Polish space.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Can a stationary poisson process in [$]\\mathbb{R}^n\\times M[/$] be obtained as a marking?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary poisson process in $\\mathbb{R}^n\\times M$. Then $\\Phi$ is the independent $\\mathbb{Q}$-marking of $\\Phi_0:= \\Phi(\\cdot \\times M)$.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "II.7 Marked point processes:<div></div><div>Overview.</div>",
    "back": "- marked point processes and stationarity<div>- K-markings: Definition, intensity measure, Laplace-functional</div><div>- Markings and unmarkings of ppp's are ppp's (and a stat. ppp is an independent [$]\\mathbb{Q}[/$]-marking)</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition random field and fidis.</div>",
    "back": "<div></div><div></div><div>Let $T\\neq \\varnothing$, $E_t, \\mathcal{E}_t$ measurable spaces. Set $S:= \\{f\\colon T\\to \\bigcup_{t \\in T} E_t \\colon f(t)\\in E_t \\forall t\\in T\\} $ and $\\mathcal{B}_T := \\bigotimes_{t\\in T}\\mathcal{E}_t = \\sigma \\left( \\pi_t\\colon t\\in T \\right) $. Then a measurable map \\[ X\\colon (\\Omega, \\mathcal{A}, \\mathbb{P}) \\to (S, \\mathcal{B}_T)\\] is called a random field. The measures \\[\\mathbb{P}_{t_1,\\ldots t_k}:= \\mathbb{P}^{(X_{t_1},\\ldots ,X_{t_k})} \\colon \\bigotimes_{i=1}^k \\mathcal{E}_{t_i} \\to [0,1]\\] are called <b>finite dimensional marginal distributions (fidis)</b> of $X$ and determine its distribution.</div><div></div><div></div><div></div>[Page 11]"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Projective families and connection with probability measures on [$]S[/$].</div>",
    "back": "<br><div></div><div>Let $(\\mathbb{P}_I), \\varnothing \\neq I \\subset T, |I| &lt; \\infty$ be a family of probability measures on $ \\bigotimes_{t\\in I}\\mathcal{E}_t$. It is called a <b>projective family</b> iff for any $I\\subset J\\subset T, 0 &lt; |I|, |J| &lt; \\infty$, \\[ \\pi_I^J (\\mathbb{P}_J) = \\mathbb{P}_I.\\] If $\\mathbb{P}$ is a probablity measure on $(S, \\mathcal{B}_T)$, then $\\mathbb{P}_I := \\pi_I (\\mathbb{P})$ defines a projective family. If $E_t$ is Borel for any $t\\in T$, then for any projective family $(\\mathbb{P}_I)$ there exists such a probability measure $\\mathbb{P}$ on $(S,\\mathcal{B}_T)$.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition and basic properties of characteristic function.</div><div></div><div>For what kind of measures is it defined?</div>",
    "back": "<div>Defined for finite Borel-measures [$]\\mu[/$] on [$]\\mathbb{R}^k[/$].</div><div></div>Properties: [$]\\varphi_\\mu(0)[/$], continuity, positive semi-definite.<div></div><div>If [$]f\\colon \\mathbb{R}^k \\to \\mathbb{C}[/$] is positive semi-definite and continuous at [$]0[/$], then [$]f = \\varphi_\\mu[/$] for some [$]\\mu[/$].<br><div></div><div></div><div>[Page 11]</div></div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Levy's continuity theorem.</div>",
    "back": "<div></div><div></div><div>Let $P_i, i\\in \\mathbb{N}, P$ be probability measures on $\\mathbb{R}^d$ with characteristic functions $\\varphi_i, i\\in \\mathbb{N}$ and $\\varphi$. Then the following holds:<br>[$$]\\begin{align*} (1)&amp; \\ \\text{If } P_i \\mathrel{\\stackrel{d}{\\longrightarrow}} P, \\text{ then } \\varphi_i \\stackrel{  }{\\longrightarrow}\\varphi \\text{ pointwise and uniformly on compact sets.} \\\\ (2)&amp; \\ \\text{If } \\varphi_i \\stackrel{  }{\\longrightarrow}f \\text{ ptw. and $f$ continuous at $0$, then }f = \\varphi_\\mu \\text{ for some} \\\\ &amp; \\text{probability measure $\\mu$ on $\\mathbb{R}^d$ and } P_i \\mathrel{\\stackrel{d}{\\longrightarrow}} \\mu.\\end{align*}[/$$]</div><div></div><div></div><div></div>[Page 11]"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition and construction of Gaussian distributions / Gaussian random fields.</div>",
    "back": "<br><div></div><div>Set $E_t:= \\mathbb{R}, t \\in T$. A \\textbf{($\\infty$-dim.) Gaussian distribution} on $(\\mathbb{R}^T, \\mathcal{B}_T)$ is a probability measure $\\mathbb{Q}$ such that all marginals $\\mathbb{Q}_I$ are multivariate Gaussians. A <b>Gaussian random field</b> is a random field with a Gaussian distribution.<br><br>Let $\\mathbb{Q}_I \\sim \\mathcal{N}(a_I, \\Sigma_I)$. Then $(\\mathbb{Q}_I)$ forms a projective family iff there is a map $a\\colon T\\to\\mathbb{R}$ and a symmetric, positive semi-definite map $K\\colon T\\times T\\to \\mathbb{R}$ such that for all $\\varnothing \\subset I\\subset T, |I| &lt; \\infty$, \\[a_I = a\\restriction_I \\text{ and } \\Sigma_I = K\\restriction_{I\\times I}.\\] $a$ is called <b>mean value map</b> and $K$ is called <b>covariance function</b>.</div><div></div><div></div><div></div><div>[Page 11]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Examples for Gaussian random fields: Wiener process and Brownian bridge.</div>",
    "back": "<br><div></div><div><b>Wiener process:</b> $T = [0,\\infty)$, $a\\equiv 0$, $K(t,s) = \\sigma^2 \\left( t\\wedge s\\right) $.<br><br><b>Brownian bridge:</b> $T=[0,1]$, $a\\equiv 0$, $K(t,s) = \\sigma^2 \\left( t\\wedge s - ts \\right) $.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Examples for Gaussian random fields:&nbsp;Ohrnstein-Uhlenbeck process.</div>",
    "back": "<br><div></div><div>$T=[0,\\infty), a\\equiv 0, K(s,t) = \\mathrm{e}^{-|t-s|}$. Can be obtained via \\[U_t := \\mathrm{e}^{-t}B_{e^{2t}},\\\\\\] if $B$ is a Wiener process.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Shot-noise field (Definition, mean-value, covariance function).</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a stationary poisson process in $\\mathbb{R}^n$ with intensity $\\gamma &gt; 0$. Let $g\\colon \\mathbb{R}^n\\to \\mathbb{R}$ mb and $g\\in L^1\\cap L^2$. Define \\[ X_t := \\sum_{x\\in \\Phi} g(t-x) = \\int_{\\mathbb{R}^n}g(t-x) \\Phi(\\mathop{}\\!\\mathrm{d} x).\\] Then $(X_t)$ is a random field with<br>[$$]\\begin{align*} (1)&amp; \\ \\mathbb{E}\\left[ X_t \\right] = \\gamma \\int g(t-x) \\mathop{}\\!\\mathrm{d} x.\\\\ (2)&amp; \\ \\text{Cov} (X_s, X_t) = \\gamma \\int g(t-x) g(s-x) \\mathop{}\\!\\mathrm{d} x.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition &nbsp;[$]m, C, D, K[/$] for a complex random field.</div>",
    "back": "<div></div><div></div><div>Let $Z\\colon \\Omega \\to \\mathbb{C}^T$ be a complex random field with $\\mathbb{E}\\left[ |Z_t|^2 \\right] &lt; \\infty \\forall t\\in T$. Then we define<br>[$$]\\begin{align*} (1)&amp; \\ m(t) := \\mathbb{E}\\left[ Z_t \\right], \\text{ the } \\textbf{mean value function.} \\\\ (2)&amp; \\ C(t, s) := \\mathbb{E} \\left[ Z_t \\overline{Z_s} \\right] , \\text{ the } \\textbf{correlation function.} \\\\ (3)&amp; \\ D(t, s) := \\mathbb{E} \\left[ Z_t Z_s \\right] , \\text{ the } \\textbf{pseudo-correlation function} .\\\\ (4)&amp; \\ K(t, s) := \\text{Cov} (Z_t,Z_s) := \\mathbb{E} \\left[ (Z_t - m_t) \\overline{(Z_s - m_s)} \\right] = C(t, s) - m(t)\\overline{m(s)},\\\\ &amp; \\text{ the } \\textbf{covariance function.} \\end{align*}[/$$]</div><div></div><div></div><div></div>[Page 12]"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition of a positive semi-definite map [$]\\alpha\\colon T\\times T \\to \\mathbb{C}[/$] (and [$]f\\colon T\\to \\mathbb{C}[/$]).</div><div></div><div>What do you know about [$]\\alpha(s,t), \\alpha(t,s)[/$] and [$]\\alpha(t,t)[/$]?</div>",
    "back": "<br><div></div><div>A map $\\alpha\\colon T\\times T\\to \\mathbb{C}$ is called <b>positive semi-definite</b> iff \\[ \\forall n\\in \\mathbb{N}, c_i\\in \\mathbb{C}, t_i\\in T\\colon \\sum_{i,j=1}^n c_i \\overline{c_j}\\alpha(t_i,t_j) \\ge 0.\\] In that case, if $s,t\\in T$, we have \\[\\alpha(t,s) = \\overline{\\alpha(s,t)}.\\] In particular, $\\alpha(t,t) \\ge 0 \\ \\forall t\\in T$.</div><div></div><div>If $f\\colon T\\to\\mathbb{C}$ and $T$ is an additive group, then $f$ is called positive semi-definite if $(s,t) \\mapsto f(s-t)$ is positive semi-definite.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition complex Gaussian field.</div><div></div><div>By what is the distribution of [$]Z[/$] already determined?</div>",
    "back": "<br><div></div><div>A random field $Z\\colon \\Omega \\to \\mathbb{C}^T$ is called a <b>complex Gaussian field</b> iff for any $n\\in \\mathbb{N}, t_1,\\ldots ,t_n\\in T$, \\[ (\\Re Z_{t_1},\\Im Z_{t_1},\\ldots ,\\Re Z_{t_n}, \\Im Z_{t_n}) \\sim \\mathcal{N}_{2n}.\\] Hence, the distribution of $Z$ is already determined by $m, C, D$.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Definition and characterisation of spherical symmetry of a complex Gaussian rf.</div>",
    "back": "<br><div></div><div>Let $Z$ be a complex random Gaussian field. It is called <b>spherically symmetric</b> iff for any $\\varphi \\in \\mathbb{R}$, \\[\\mathrm{e}^{\\mathrm{i}\\varphi}Z\\stackrel{d}{=} Z.\\] Equivalently, for any $s,t\\in T,$ \\[\\mathbb{E} \\left[ Z_t \\right] = 0 \\text{ and } \\mathbb{E} \\left[ Z_s Z_t \\right] = 0.\\]</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Existence of complex Gaussian rf's with prescribed mean and covariance.</div>",
    "back": "<br><div></div><div>Let $m\\colon T\\to\\mathbb{C}$ and $K\\colon T\\times T\\to \\mathbb{C}$ positive semi-definite. Then there exists a unique (in distribution) Gaussian random field with mean $m$ and covariance function $K$ such that $(Z-m)$ is spherically symmetric (i.e. $\\mathbb{E}\\left[ Z_s Z_t \\right] = m_s m_t \\ \\forall s, t\\in T$).</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.2 Stationarity and isotropy ([$]T=\\mathbb{R}^k[/$]):<div></div><div>Definition (weak) stationarity (isotropy). What is the relationship between weak and not weak?</div>",
    "back": "<br><div></div><div>Let $(X_t)_{t\\in T}$ be a real or complex random field. Then $X$ is called<br>[$$]\\begin{align*} (1)&amp; \\ \\textbf{stationary} \\mathrel{:\\Longleftrightarrow} \\forall u\\in T\\colon (X_{t+u})_t \\stackrel{d}{=} (X_t)_t.\\\\ (2)&amp; \\ \\textbf{weakly stationary} \\mathrel{:\\Longleftrightarrow} \\forall t\\in T\\colon \\mathbb{E} \\left[ |X_t|^2 \\right] &lt; \\infty \\text{ and } \\forall s, t, u\\in T\\colon \\\\ &amp; \\mathbb{E}\\left[ X_t \\right] = \\mathbb{E}\\left[ X_s \\right],\\\\ &amp; \\text{Cov} (X_{t+u}, X_{s+u}) = \\text{Cov} (X_t, X_s),\\\\ &amp; \\mathbb{E}\\left[ (X_t - m_t)(X_s - m_s) \\right] = \\mathbb{E}\\left[ (X_{t+u} - m_{t+u})(X_{s+u} - m_{s+u}) \\right].\\end{align*}[/$$]<br>For isotropy, replace translations by rotations. $(1)$ together with $\\mathbb{E}\\left[ |X_t|^2 \\right] &lt; \\infty\\forall t\\in T$ implies $(2)$. The converse holds if $X$ is Gaussian.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "<div>III.2 Stationarity and isotropy ([$]T=\\mathbb{R}^k[/$]):</div><div></div><div>Definition [$]L^2[/$]-continuity of a real or complex rf and connection with continuity of [$]C[/$].</div>",
    "back": "<br><div></div><div>Let $Z$ be a real or complex random field. Then $Z$ is called \\textbf{$L^2$-continuous} iff for any $t_0\\in T$ we have \\[ \\mathbb{E} \\left[|Z_t - Z_{t_0}|^2\\right] \\stackrel{ t\\to t_0 }{\\longrightarrow} 0.\\] Then the following holds:<br>[$$]\\begin{align*} (1)&amp; \\ Z \\text{ is $L^2$-continuous at $t_0\\in T$ iff $C$ is continuous at $(t_0,t_0)$}.\\\\ (2)&amp; \\ \\text{If $C$ is continuous at $(t,t)$ and $(s,s)$, $C$ is also continuous at $(t,s)$ and $(s,t)$.} \\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.2 Stationarity and isotropy ([$]T=\\mathbb{R}^k[/$]):<br><div></div><div>Theorem (Bochner).</div>",
    "back": "<br><div></div><div>Let $f\\colon \\mathbb{R}^d\\to\\mathbb{C}$ be continuous at $0$. Then <br> $f$ is positive semi-definite iff there exists a (unique) finite measure $\\mu$ on $\\mathbb{R}^d$ such that $f = \\varphi_\\mu$.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.2 Stationarity and isotropy ([$]T=\\mathbb{R}^k[/$]):<br><div></div><div>Theorem (Polya, Askey).</div>",
    "back": "<br><div></div><div>A function $f\\colon \\mathbb{R}^d\\to\\mathbb{R}$ with $f(t) = g(\\|t\\|), t\\in \\mathbb{R}^d$, for some $g\\colon \\mathbb{R}\\to\\mathbb{R}$ is positive semi-definite if<br>[$$]\\begin{align*} (1)&amp; \\ g \\text{ is continuous on } [0,\\infty),\\\\ (2)&amp; \\ g(0) = 1,\\\\ (3)&amp; \\ g(t) \\stackrel{ t\\to \\infty }{\\longrightarrow} 0,\\\\ (4)&amp; \\ (-1)^k g^{(k)} \\text{ is convex, where } k = \\left\\lfloor \\frac{d}{2} \\right\\rfloor.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Definition (semi-)ring, measure on a (semi-)ring, Caratheodory.</div>",
    "back": "<br><div></div><div>Let $E$ be a set. A family $\\mathcal{K}(E)\\subset \\mathcal{P}(E)$ is called a <b>semi-ring</b> if<br>[$$]\\begin{align*} &amp;(1) \\ \\varnothing \\in \\mathcal{K}(E)\\\\ &amp;(2) \\ \\forall A,B\\in \\mathcal{K}(E)\\colon A\\cap B\\in \\mathcal{K}(E)\\\\ &amp;(3) \\ \\forall A,B\\in \\mathcal{K}(E) \\exists n\\in \\mathbb{N}, B_1,\\ldots ,B_n\\in \\mathcal{K}(E) \\text{ p.d.} \\colon A\\setminus B = \\bigcup_{i = 1}^n B_i\\\\\\end{align*}[/$$]<br>It is called a <b>ring</b> if it contains $\\varnothing$ and is stable under $\\cup$ and $\\setminus $. A <b>measure</b> on a (semi-)ring is a $\\sigma$-additive set-function (here: $\\mu(E) &lt; \\infty$!).<br><br><b>Caratheodory:</b> A measure on a semi-ring $\\mathcal{K}(E)$ can be extended to a (unique and $\\sigma$-finite) measure on $\\sigma\\left( \\mathcal{K}(E) \\right) $.</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Definition complex [$]\\nu[/$]-noise.</div>",
    "back": "<br><div></div><div>Let $\\mathcal{K}(E)$ be a semi-ring and $\\nu$ a measure on $\\mathcal{K}(E)$. A collection $W = (W(A))_{A\\in \\mathcal{K}(E)}$ of complex random variables on $(\\Omega, \\mathcal{F},\\mathbb{P})$ is called a \\textbf{complex $ \\nu$-noise} if<br>[$$]\\begin{align*} (1)&amp; \\ \\forall A\\in \\mathcal{K}(E)\\colon W(A) \\in L^2(\\Omega, \\mathcal{F},\\mathbb{P}).\\\\ (2)&amp; \\ \\forall A,B\\in \\mathcal{K}(E)\\colon \\mathbb{E} \\left[ W(A) \\overline{W(B)} \\right] = \\nu(A\\cap B).\\end{align*}[/$$]<br>In particular, $\\nu(A) = \\mathbb{E} \\left[ |W(A)|^2\\right] $ and $W(\\varnothing) = 0 \\ \\mathbb{P}\\text{-a.s.} $</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Characterisation of a complex [$]\\nu[/$]-noise.</div>",
    "back": "<br><div></div><div>Let $\\mathcal{K}(E)$ be a semi-ring and $(W(A))_{A\\in \\mathcal{K}(E)}$ a collection of complex random variables. Then $(W(A))$ is a complex $\\nu$-noise (for some measure $\\nu$ on $\\mathcal{K}(E)$) iff<br>[$$]\\begin{align*} (1)&amp; \\ \\forall A\\in \\mathcal{K}(E)\\colon W(A) \\in L^2(\\Omega, \\mathcal{F},\\mathbb{P}).\\\\ (2)&amp; \\ \\forall A, B\\in \\mathcal{K}(E) \\text{ disjoint} \\colon \\mathbb{E} \\left[ W(A) \\overline{W(B)} \\right] = 0.\\\\ (3)&amp; \\ \\forall (A_i)\\in \\mathcal{K}(E)^\\mathbb{N} \\text{ p.d. such that } A:= \\bigcup_{i \\in \\mathbb{N}} A_i\\in \\mathcal{K}(E)\\colon W(A) = \\sum_{i\\in \\mathbb{N}}W(A_i).\\end{align*}[/$$]<br>In $(3)$, convergence is meant in $L^2(\\Omega, \\mathcal{F},\\mathbb{P})$. For finite sums, the equality holds $\\mathbb{P}$-a.s.</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>(Real and complex) Gaussian noise.</div>",
    "back": "<br><div></div><div>Let $E=\\mathbb{R}^n$ and $\\nu$ be a measure on $\\mathcal{B}^n$. Then $\\mathcal{K}(E) := \\{B\\in \\mathcal{B}^n\\colon \\nu(B)&lt;\\infty\\} $ is a ring. Let $W$ be the centred Gaussian process with index set $\\mathcal{K}(E)$ and $K(A,B) := \\nu(A\\cap B)$. Then $W$ is a (real) $\\nu$-noise, called <b>Gaussian noise</b>.<br><br>Let $W_1, W_2$ be independent (real) $\\nu$-noises. Then $W(A) := W_1(A) + \\mathrm{i} W_2(A)$ defines a complex $(2\\nu)$-noise.</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Let [$]\\nu[/$] be a measure on a semi-ring [$]\\mathcal{K}(E)[/$]. Explicit construction of a centred complex [$]\\nu[/$]-noise on [$]\\{B\\in \\sigma(\\mathcal{K}(E))\\colon \\nu(B) &lt; \\infty\\}[/$].</div><div></div><div>([$]\\nu[/$] also denotes the unique [$]\\sigma[/$]-finite extension of [$]\\nu[/$] to [$]\\sigma(\\mathcal{K}(E))[/$]).</div>",
    "back": "[Page 13]"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Setup and aim.</div>",
    "back": "<br><div></div><div><b>Setup:</b> Let $E\\neq \\varnothing$, $\\mathcal{K}(E)$ a semi-ring with measure $\\nu$ and $W$ a complex $\\nu$-noise. Set $\\tilde{\\mathcal{A}}:= \\mathcal{R}(E), \\mathcal{A}:= \\sigma(\\mathcal{K}(E))$. Then $\\nu$ can be uniquely extended to a $\\sigma$-finite measure on $\\mathcal{A}$.<br><br><b>Aim:</b> Define a linear isometry $I_W\\colon L^2(E, \\mathcal{A}, \\nu) \\to L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ and set \\[ \\int_E f \\mathop{}\\!\\mathrm{d} W := I_W(f).\\] <br><br>$W$ can be extended to $\\tilde{\\mathcal{A}}$ via $W\\left(\\bigcup_{i=1}^k A_i\\right) := \\sum_{i=1}^k W(A_i).$</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Definition of [$]I_W(f)[/$] for simple functions. (well-defined)?</div>",
    "back": "<br><div></div><div>A function $f\\colon E\\to\\mathbb{C}$ is called <b>simple</b> if there are $n\\in \\mathbb{N}, a_i\\in \\mathbb{C}, A_i\\in \\mathcal{K}(E)$ p.d. such that $f = \\sum_{i=1}^k a_i \\boldsymbol{1}_{A_i}$. In that case, we define \\[ I_W(f) := \\sum_{i=1}^k a_i W(A_i).\\] This is well-defined (as an element of $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$).</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>[$]I_W[/$] for simple functions is linear and isometric.</div>",
    "back": "<br><div></div><div>Let $f, g\\colon E\\to\\mathbb{C}$ be simple and $a, b\\in \\mathbb{C}$. Then \\[ I_W(af+bg) = aI_W(f) + b I_W(g)\\] and \\[\\left&lt;I_W(f), I_W(g) \\right&gt;_{L^2(\\mathbb{P})} = \\left&lt;f, g \\right&gt;_{L^2(\\nu)}, \\text{ i.e. } \\mathbb{E} \\left[ I_W(f) \\overline{I_W(g)} \\right] = \\int_E f\\overline{g} \\mathop{}\\!\\mathrm{d} \\nu.\\]</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Definition of [$]I_W(f)[/$] for general [$]f\\colon E\\to\\mathbb{C}[/$].</div>",
    "back": "<br><div></div><div><b>Lemma:</b> Simple functions are dense in $L^2(E, \\mathcal{A}, \\nu)$.<br><br>Let $f\\colon E\\to\\mathbb{C}$ and $f_n \\stackrel{ L^2 }{\\longrightarrow}f $ where $f_n$ are simple. Then set $I_W(f) := \\lim_{L^2}I_W(f_n)$.</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.3 Stochastic integration:<div></div><div>Final result: Properties of the map [$] I_W\\colon L^2(E, \\mathcal{A}, \\nu) \\to L^2(\\Omega, \\mathcal{F}, \\mathbb{P})[/$].</div>",
    "back": "<br><div></div><div>The map $I_W\\colon L^2(E, \\mathcal{A}, \\nu) \\to L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ has the following properties:<br>[$$]\\begin{align*} (1)&amp; \\ I_W \\text{ is a linear isometry.} \\\\ (2)&amp; \\ W \\text{ can be extended from $\\tilde{\\mathcal{A}}$ to $\\{A\\in \\mathcal{A}\\colon \\nu(A) &lt; \\infty\\} $ via } W(A) := I_W( \\boldsymbol{1}_{A}).\\\\ (3)&amp; \\ \\text{If $W$ is centred, then } \\mathbb{E}\\left[ I_W(f) \\right] = 0 \\ \\forall f\\in L^2(E, \\mathcal{A},\\nu).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.4 Spectral representations:<div></div><div>Motivation.</div>",
    "back": "<br><div></div><div>Let $\\nu$ be a $\\sigma$-finite measure on $(E,\\mathcal{A})$ and $W$ a complex $\\nu$-noise on $\\{A\\in \\mathcal{A}\\colon \\nu(A) &lt; \\infty\\} $. Let $f_t\\in L^2(E,\\mathcal{A},\\nu), t\\in T$. Then \\[ X_t := I_W(f_t) = \\int_E f_t \\mathop{}\\!\\mathrm{d} W \\in L^2(\\Omega, \\mathcal{F}, \\mathbb{P})\\] defines a square-integrable complex random field that satisfies \\[C(t, s) = \\mathbb{E}\\left[ X_t \\overline{X_s} \\right] = \\int_E f_t \\overline{f_s}\\mathop{}\\!\\mathrm{d} \\nu, \\ s, t, \\in T.\\]</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.4 Spectral representations:<div></div><div>Theorem (Karhunen).</div><div></div><div>When does [$]\\Omega[/$] not have to be extended? Is [$]W[/$] unique?</div>",
    "back": "<br><div></div><div>Let $(X_t)$ be a (centred) square-integrable complex random field on $(\\Omega, \\mathcal{F}, \\mathbb{P})$. If there exists a $\\sigma$-finite measure $\\nu$ on some $(E,\\mathcal{A})$ and $f_t\\in L^2(E,\\mathcal{A},\\nu)$ such that for $t, s \\in T$, \\[ C(t, s) = \\int_E f_t \\overline{f_s}\\mathop{}\\!\\mathrm{d} \\nu,\\\\\\] then there exists a (centred) complex $\\nu$-noise $W$ on an extension of $(\\Omega, \\mathcal{F}, \\mathbb{P})$ such that $X_t = I_W(f_t)$ for $t\\in T$.<br><br>If $\\{f_t\\colon t\\in T\\} $ is a complete system in $L^2(E,\\mathcal{A},\\nu)$ (or if $\\dim L(f)^\\perp \\le \\dim L(X)^\\perp$), then $W$ exists on $(\\Omega, \\mathcal{F}, \\mathbb{P})$. In that case, $W$ is uniquely determined.</div><div></div><div>$(W(A))_{A}$ is a Gaussian field if and only if $(X_t)$ is a Gaussian field.</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "III.4 Spectral representations:<div></div><div>Spectral representation of a square-integrable, weakly-stationary and [$]L^2[/$]-continuous complex random field.</div>",
    "back": "<br><div></div><div>Let $(X_t)_{t\\in \\mathbb{R}^d}$ be a (centred) square-integrable, weakly stationary, $L^2$-continuous complex random field. Then there is a finite measure $\\nu$ on $\\mathbb{R}^d$ and a (centred) complex $\\nu$-noise $W$ such that \\[ X_t = \\int_{\\mathbb{R}^d}\\mathrm{e}^{\\mathrm{i} \\left&lt; t, x\\right&gt;} W(\\mathop{}\\!\\mathrm{d} x),\\ t\\in \\mathbb{R}^d.\\] $\\nu$ is the spectral measure of $C_0\\colon \\mathbb{R}^d\\to \\mathbb{C}, h \\mapsto C(h, 0) = \\mathbb{E} \\left[ X_h \\overline{X_0} \\right] $.</div><div></div><div></div><div></div><div>[Page 14]</div>"
  },
  {
    "front": "If [$]f, g\\colon \\mathbb{R}^d \\to \\mathbb{C}[/$] are positive semi-definite, so is [$]f \\cdot g[/$].",
    "back": "[Extra Page 1]"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Situation and goal.</div>",
    "back": "<br><div></div><div><b>Situation:</b> Let $T$ be a compact metric space, $\\nu$ a finite Borel-measure on $T$ with $\\operatorname{supp} \\nu = T$. Then $\\mathcal{H} := L^2(T, \\mathcal{B}(T), \\nu)$ is a separable Hilbert space.<br><br><b>Goal:</b> For a random field $(X_t)$, find pairwise orthogonal $\\xi_i\\in L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ and an orthonormal system $\\{\\psi_i\\} \\subset \\mathcal{H}$ such that for some $\\lambda_i \\ge 0,$\\[ X_t = \\sum_{i=1}^\\infty \\sqrt{\\lambda_i} \\psi_i(t) \\xi_i.\\]</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Definition [$]A_K\\colon \\mathcal{H}\\to\\mathcal{H}[/$] for [$]K\\colon T\\times T\\to \\mathbb{K}[/$].</div><div></div><div>What do you know about eigenvalues and eigenfunctions of [$]A_K[/$]?</div>",
    "back": "<br><div></div><div>Let $K\\colon T\\times T\\to \\mathbb{C} \\ (\\mathbb{R})$ be continuous, positive semi-definite (and symmetric). Then define \\[ A_K\\colon \\mathcal{H}\\to\\mathcal{H}, \\ (A_K g)(t) := \\int_T K(t,s)g(s) \\nu(\\mathop{}\\!\\mathrm{d} s).\\] Then all eigenvalues of $A_K$ must be non-negative and since $A_K g$ is continuous for any $g$, eigenfunctions for positive eigenvalues must be continuous.</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Theorem (Mercer).</div>",
    "back": "<br><div></div><div>Let $K\\colon T\\times T\\to \\mathbb{C} \\ (\\mathbb{R})$ be continuous, positive semi-definite (and symmetric). Then there exists an orthonormal basis $(\\psi_i)_{i\\in S\\cup B}$ (with countable $S, B$) s.t. $A_K \\psi_i = \\lambda_i \\psi_i$ and $\\lambda_i = 0 \\iff i\\in B$ and for $s,t\\in T,$ \\[ K(t, s) = \\sum_{i\\in S}\\lambda_i \\psi_i(t) \\overline{\\psi_i(s)},\\\\\\] where convergence is absolute and uniform in $t, s$.</div><div></div><div></div><div></div><div>[Page 15, Idea of proof on some extra page]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Connection between [$]K[/$] and [$] \\text{tr} \\ A_K[/$].</div>",
    "back": "<div></div><div>By Mercer,</div><br><div>[$$] \\int_T K(t, t) \\nu(\\mathop{}\\!\\mathrm{d} t) = \\sum_{i\\in S} \\lambda_i = \\text{tr} \\ A_K.[/$$]</div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Theorem (Karhunen and Loeve).</div>",
    "back": "<br><div></div><div>Let $(X_t)$ be a square-integrable, $L^2$-continuous complex random field with covariance function $C$. Then \\[ X_t = \\sum_{i\\in S}\\sqrt{\\lambda_i} \\psi_i(t) \\xi_i,\\\\\\] where convergence is in $L^2$ and uniform in $t$. $S, \\lambda_i, \\psi_i$ are obtained from Mercers theorem applied to $C$. The $\\xi_i$ are pairwise orthogonal and centred if $X$ is centred.<br><br>Explicitly, \\[ \\xi_n = \\frac{1}{\\sqrt{\\lambda_n}} I_{\\nu_n}(X)\\] where $\\nu_n(B) := \\int_B \\overline{\\psi_n(t)}\\nu(\\mathop{}\\!\\mathrm{d} t), \\ B\\in \\mathcal{B}(T), $ defines a complex measure on $T$.</div><div></div><div></div><div></div><div>[Page 15, Proof on Page 16]</div>"
  },
  {
    "front": "Complex measures:<div></div><div>Definition complex measure [$]\\nu[/$] and [$]\\int_T f \\mathop{}\\!\\mathrm{d} \\nu[/$].</div>",
    "back": "<br><div></div><div>Let $T$ be a lcHcb. Then a $\\sigma$-additive map $\\nu\\colon \\mathcal{B}(T) \\to \\mathbb{C}$ is called a <b>complex measure</b>.<br><br><b>Jordan-decomposition:</b> There are real, finite measures $\\nu_i\\colon \\mathcal{B}(T) \\to [0,\\infty), \\ i=1,2,3, 4$ such that $\\nu = \\nu_1 - \\nu_2 + \\mathrm{i} (\\nu_3 - \\nu_4)$.<br><br>A measurable map $f\\colon T \\to \\mathbb{C}$ is called \\textbf{$\\nu$-integrable} if $\\int_T |f| \\mathop{}\\!\\mathrm{d} \\nu_i &lt; \\infty\\forall i$. In that case, set \\[ \\int_T f \\mathop{}\\!\\mathrm{d} \\nu := \\int_T f\\mathop{}\\!\\mathrm{d} \\nu_1 - \\int_T f \\mathop{}\\!\\mathrm{d} \\nu_2 + \\mathrm{i} \\left( \\int_T f \\mathop{}\\!\\mathrm{d} \\nu_3 - \\int_T f \\mathop{}\\!\\mathrm{d} \\nu_4 \\right).\\] This is well-defined.</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "Complex measures:<div></div><div>Total variation measure and two properties.</div>",
    "back": "<br><div></div><div>Let $T$ be a lcHcb and $\\nu$ a complex measure. The <b>total variation measure</b> $|\\nu|$ of $\\nu$ is a real, finite measure on $\\mathcal{B}(T)$ defined by \\[ |\\nu(A)| := \\sup \\left\\{ \\sum_{j=1}^k \\left| \\nu(A_j) \\right| \\colon k\\in \\mathbb{N}, A = \\sum_{j=1}^k A_j\\right\\}.\\] Then $|\\nu| = | \\overline{\\nu}|$ and $|\\nu_1 \\otimes \\nu_2| = |\\nu_1| \\otimes |\\nu_2|$.</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "Complex measures:<div></div><div>Representation of [$]\\int_T f \\mathop{}\\!\\mathrm{d} \\nu[/$] via [$]|\\nu|[/$] and triangle-inequality.</div>",
    "back": "<br><div></div><div>Let $T$ be a lcHcb and $\\nu$ a complex measure. Then there exists a real, finite measure $|\\nu|$ on $\\mathcal{B}(T)$ and $g\\colon T\\to \\mathbb{C}$ measurable, $|g|\\equiv 1$, such that for any integrable $f\\colon T\\to\\mathbb{C}$, \\[\\int_T f \\mathop{}\\!\\mathrm{d} \\nu = \\int_T f g \\mathop{}\\!\\mathrm{d} |\\nu|.\\] In particular, \\[\\left| \\int_T f \\mathop{}\\!\\mathrm{d} \\nu \\right| \\le \\int_T |f| \\mathop{}\\!\\mathrm{d} |\\nu|.\\]</div><div>$|\\nu|$ is actually the total variation measure of $\\nu$.</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Integration of a random field against a complex measure:</div><div></div><div>Setup, Goal.</div>",
    "back": "<br><div></div><div>Let $T$ be a lcH (locally compact Hausdorff) space and $\\nu$ a complex measure on $\\mathcal{B}(T)$. For a square-integrable, $L^2$-continuous complex random field $(Z_t)$ we wish to define \\[ \\int_T Z(t) \\nu(\\mathop{}\\!\\mathrm{d} t) := I_\\nu(Z) \\in L^2(\\Omega, \\mathcal{F},\\mathbb{P}).\\]</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Integration of a random field against a complex measure:</div><div></div><div>Definition [$]\\nu[/$]-integrability and basic properties (uniqueness, linearity, constant integrand).</div>",
    "back": "<br><div></div>Let $T$ be a lcH space, $\\nu$ a complex measure on $\\mathcal{B}(T)$ and $(Z_t)$ a square-integrable, $L^2$-continuous complex random field.<br><br>$Z$ is called \\textbf{$\\nu$-integrable} iff<br>[$$]\\begin{align*} (1)&amp; \\ \\forall h\\in L^2(\\Omega)\\colon \\left&lt;Z(\\cdot ), h \\right&gt;\\colon T\\to\\mathbb{C} \\text{ is $\\nu$-integrable.} \\\\ (2)&amp; \\ \\exists I_\\nu(Z)\\in L^2(\\Omega)\\colon \\left&lt;I_\\nu(Z), h \\right&gt; = \\int_T \\left&lt;Z(t), h \\right&gt;\\nu(\\mathop{}\\!\\mathrm{d} t), \\ h\\in L^2(\\Omega).\\end{align*}[/$$]<br><div>Then, if it exists, $I_\\nu(Z) =: \\int_T Z \\mathop{}\\!\\mathrm{d} \\nu$ is uniquely determined. Furthermore, $(\\nu,Z)\\mapsto I_\\nu(Z)$ is linear in both arguments (in the obvious sense) and if $Z_t = h\\in L^2(\\Omega)\\forall t\\in T$, then $I_\\nu(Z) = \\nu(T) h.$</div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Integration of a random field against a complex measure:</div><div></div><div>Characterisation of [$]\\nu[/$]-integrability.</div>",
    "back": "<br><div></div><div>Let $T$ be a lcH space, $\\nu$ a complex measure on $\\mathcal{B}(T)$ and $(Z_t)$ a square-integrable, $L^2$-continuous complex random field.<br><br>Let $\\left&lt;Z(\\cdot ), h \\right&gt;$ be $\\nu$-integrable for all $h\\in L^2(\\Omega, \\mathcal{F},\\mathbb{P})$. Then the following are equivalent:<br>[$$]\\begin{align*} (1)&amp; \\ \\text{$Z$ is $\\nu$-integrable.} \\\\ (2)&amp; \\ \\exists K\\in (0,\\infty)\\colon \\left| \\int_T \\left&lt;Z(t), h \\right&gt;\\nu(\\mathop{}\\!\\mathrm{d} t)\\right| \\le K \\|h\\| \\ \\forall h\\in L^2(\\Omega).\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Integration of a random field against a complex measure:</div><div></div><div>Sufficient condition for [$]\\nu[/$]-integrability and [$]\\vartriangle[/$]-inequality for [$]I_\\nu(Z) = \\int_T Z \\mathop{}\\!\\mathrm{d} \\nu[/$].</div>",
    "back": "<br><div></div><div>Let $T$ be a lcH space, $\\nu$ a complex measure on $\\mathcal{B}(T)$ and $(Z_t)$ a square-integrable, $L^2$-continuous complex random field.<br><br>If \\[ K := \\int_T \\|Z(t)\\| |\\nu| (\\mathop{}\\!\\mathrm{d} t) &lt; \\infty,\\\\\\] then $Z$ is $\\nu$-integrable and \\[\\|I_\\nu(Z)\\|= \\left\\|\\int_T Z \\mathop{}\\!\\mathrm{d} \\nu\\right\\|\\le K.\\]</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "III.5 Orthogonal expansion:<div></div><div>Integration of a random field against a complex measure:</div><div></div><div>Connection between [$]\\left&lt; I_\\mu(Z), I_\\nu(Z) \\right&gt;[/$] and [$]C(t, s)[/$].</div><div></div><div>What is [$]\\| I_\\nu(Z)\\|^2[/$] ?</div>",
    "back": "<br><div></div><div>Let $T$ be a lcH space, $\\mu, \\nu$ complex measures on $\\mathcal{B}(T)$ and $(Z_t)$ a square-integrable, $L^2$-continuous complex random field.<br><br>If $\\int \\|Z(t)\\| |\\mu|(\\mathop{}\\!\\mathrm{d} t) &lt; \\infty$ and $\\int \\|Z(t)\\| |\\nu|(\\mathop{}\\!\\mathrm{d} t)&lt; \\infty$, then $C\\colon T\\times T\\to \\mathbb{C}$ is $(\\mu\\otimes \\overline{\\nu})$-integrable and \\[ \\left&lt;I_\\mu(Z), I_\\nu(Z) \\right&gt; = \\int_{T\\times T}C(t, s) \\left( \\mu\\otimes \\overline{\\nu} \\right) (\\mathop{}\\!\\mathrm{d} (t, s)).\\] In particular, \\[\\|I_\\nu(Z)\\|^2 = \\int_{T\\times T}C(t, s) \\left( \\nu\\otimes \\overline{\\nu} \\right) (\\mathop{}\\!\\mathrm{d} (t, s)).\\]</div><div></div><div></div><div></div><div>[Page 15]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Setup and motivation.</div>",
    "back": "<br><div></div><div><b>Setup:</b> $\\xi\\colon (\\Omega, \\mathcal{A}, \\mathbb{P}) \\to (\\Sigma, \\mathcal{F})$, $T\\colon \\Sigma \\to \\Sigma$ measurable and $f\\colon \\Sigma \\to \\tilde{\\Sigma}$ measurable. $\\xi$ is called \\textbf{$T$-measure preserving} if $T\\circ \\xi \\stackrel{d}{=} \\xi$.<br><br><b>Motivation:</b> If $T\\circ \\xi \\stackrel{d}{=}\\xi$, can we conclude \\[ \\frac{1}{n}\\sum_{k=1}^n f\\left( T^k(\\xi) \\right) \\stackrel{ n\\to \\infty }{\\longrightarrow} \\ ?\\]</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Definition shift-operator and stationarity.</div>",
    "back": "<br><div></div><div>The <b>shift operator</b> is defined by \\[ S\\colon \\left(\\Omega^{\\mathbb{N}_0}, \\mathcal{F}^{\\mathbb{N}_0}\\right)\\to \\left( \\Omega^{\\mathbb{N}_0}, \\mathcal{F}^{\\mathbb{N}_0} \\right) , \\ (x_n)_n \\mapsto (x_{n+1})_n.\\] A random variable $\\xi = (\\xi_n)_{n\\in \\mathbb{N}_0}$ is called <b>stationary</b> if $S\\circ \\xi \\stackrel{d}{=} \\xi$.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Characterisation of [$]T\\circ \\xi \\stackrel{d}{=} \\xi[/$].</div><div></div><div>What do you know about [$]\\left( f\\circ T^n\\circ \\xi \\right)_{n\\in \\mathbb{N}_0}[/$] ?</div>",
    "back": "<br><div>[$$] T\\circ \\xi \\stackrel{d}{=} \\xi \\iff \\left(T^n \\circ \\xi\\right)_{n\\in\\mathbb{N}_0} \\text{ is stationary}.[/$$]</div><div></div><div>In that case, also [$]\\left( f\\circ T^n\\circ \\xi \\right)_{n\\in \\mathbb{N}_0}[/$] is stationary.</div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Definition [$]T[/$]-invariance and [$]J_T[/$].</div>",
    "back": "<br><div></div><div>$f$ is called \\textbf{$T$-invariant} if $f\\circ T = f$.<br><br>A set $I\\in \\mathcal{F}$ is called \\textbf{$T$-invariant} if $T^{-1}(I) = I$. Then $J_T := \\{I\\in \\mathcal{F}\\colon T^{-1}(I) = I\\} $ is a $\\sigma$-algebra.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Characterisation of [$]T[/$]-invariance of [$]f[/$].</div>",
    "back": "<br><div></div><div>Let $(\\tilde{\\Sigma}, \\tilde{\\mathcal{F}})$ be a Borel space. Then $f$ is $T$-invariant iff $f$ is $(J_T, \\tilde{\\mathcal{F}})$-measurable.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Definition [$]T[/$]-ergodicity.</div>",
    "back": "<br><div></div><div>$\\xi$ is called \\textbf{$T$-ergodic} if $\\mathbb{P}\\left( \\xi \\in I \\right) \\in \\{0,1\\} $ for any $I\\in J_T$. That is, $J_{\\xi}:= \\xi^{-1}(J_T)$ is $\\mathbb{P}$-trivial.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic Theory:<div></div><div>Characterisation of [$]T[/$]-ergodicity of [$]\\xi[/$] (with [$]T\\circ\\xi\\stackrel{d}{=} \\xi[/$]).</div><div></div><div>What do you know about [$]\\left( f\\circ T^n\\circ \\xi \\right)_{n\\in \\mathbb{N}_0}[/$] ?</div>",
    "back": "<br><div></div><div>Let $T\\circ \\xi \\stackrel{d}{=} \\xi$. Then: \\[ \\xi \\text{ is $T$-ergodic} \\iff \\left( T^n\\circ \\xi \\right)_{n\\in \\mathbb{N}_0} \\text{is $S$-ergodic.} \\] In that case, $\\left( f\\circ T^n\\circ \\xi \\right)_{n\\in \\mathbb{N}_0}$ is $S$-ergodic.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "Let [$](X_n)[/$] be a stationary sequence of real random variables.<div></div><div>What do you know about [$]\\mathbb{E} \\left[ X_1 \\boldsymbol{1}\\{M_n &gt; 0\\} \\right][/$] (or [$]\\ge[/$]) ?</div><div></div>",
    "back": "<br><div></div><div>Let $(X_n)$ be a stationary sequence of real random variables, $S_n := \\sum_{k=1}^n X_k$, $M_n := \\max_{k\\le n}S_k$, $M_\\infty:= \\sup_{k\\in \\mathbb{N}}S_k$. Then for any $n\\in \\mathbb{N},$ \\[ \\mathbb{E} \\left[ X_1 \\boldsymbol{1}\\{M_n &gt; 0\\} \\right] \\ge 0.\\] If $\\mathbb{E} |X_1| &lt; \\infty$, then \\[\\mathbb{E} \\left[ X_1 \\boldsymbol{1}\\left\\{M_\\infty &gt; 0\\right\\} \\right] \\ge 0.\\] <b><b>The first</b></b>&nbsp;assertion remains true when $&gt;$ is replaced by $\\ge$.</div><div></div><div></div><div></div><div>[GTK Stuff Page 2]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Birkhoff's ergodic theorem.</div><div></div><div>Proof of the case [$]f\\ge 0, f\\in L^1[/$].</div>",
    "back": "<br><div></div><div>Let $\\xi\\colon (\\Omega, \\mathcal{A}, \\mathbb{P})\\to (\\Sigma, \\mathcal{F})$, $T\\colon \\Sigma \\to \\Sigma$ measurable such that $T\\circ \\xi \\stackrel{d}{=} \\xi$. Then if $f\\colon \\Sigma \\to \\mathbb{R}$ is measurable and non-negative or in $L^p$, then \\[ \\frac{1}{n} \\sum_{k=0}^{n-1} f\\left( T^k(\\xi) \\right) \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] \\ \\mathbb{P}\\text{-a.s.} \\] If $f\\in L^p$, then convergence holds in $L^p$. Note that if $\\xi$ is $T$-ergodic, then $\\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] = \\mathbb{E} \\left[ f\\circ \\xi \\right] $.</div><div></div><div></div><div></div><div>[Page 16]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Birkhoff's ergodic theorem.</div><div></div><div>Proof of the case [$]f\\in L^p[/$].</div>",
    "back": "<br><div><div></div><div>Let $\\xi\\colon (\\Omega, \\mathcal{A}, \\mathbb{P})\\to (\\Sigma, \\mathcal{F})$, $T\\colon \\Sigma \\to \\Sigma$ measurable such that $T\\circ \\xi \\stackrel{d}{=} \\xi$. Then if $f\\colon \\Sigma \\to \\mathbb{R}$ is measurable and non-negative or in $L^p$, then \\[\\frac{1}{n} \\sum_{k=0}^{n-1} f\\left( T^k(\\xi) \\right) \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] \\ \\mathbb{P}\\text{-a.s.}&nbsp;\\] If $f\\in L^p$, then convergence holds in $L^p$. Note that if $\\xi$ is $T$-ergodic, then $\\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] = \\mathbb{E} \\left[ f\\circ \\xi \\right] $.</div><div></div><div></div><div></div><div>[Page 16]</div></div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Birkhoff's ergodic theorem.</div><div></div><div>Proof of the case [$]f\\ge 0[/$].</div>",
    "back": "<br><div><div></div><div>Let $\\xi\\colon (\\Omega, \\mathcal{A}, \\mathbb{P})\\to (\\Sigma, \\mathcal{F})$, $T\\colon \\Sigma \\to \\Sigma$ measurable such that $T\\circ \\xi \\stackrel{d}{=} \\xi$. Then if $f\\colon \\Sigma \\to \\mathbb{R}$ is measurable and non-negative or in $L^p$, then \\[\\frac{1}{n} \\sum_{k=0}^{n-1} f\\left( T^k(\\xi) \\right) \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] \\ \\mathbb{P}\\text{-a.s.}&nbsp;\\] If $f\\in L^p$, then convergence holds in $L^p$. Note that if $\\xi$ is $T$-ergodic, then $\\mathbb{E} \\left[ f\\circ \\xi \\ | \\ J_\\xi \\right] = \\mathbb{E} \\left[ f\\circ \\xi \\right] $.</div><div></div><div></div><div></div><div>[Page 16]</div></div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Definition flow.</div>",
    "back": "<br><div></div><div>A family $(T_t)_{t\\ge 0}$ of measurable maps $\\Sigma \\to \\Sigma$ is called a <b>flow</b> if<br>[$$]\\begin{align*} (1)&amp; \\ T_0 = \\id_\\Sigma.\\\\ (2)&amp; \\ T_{t+s} = T_t \\circ T_s \\ \\forall t, s\\ge 0.\\\\ (3)&amp; \\ \\Sigma \\times [0,\\infty) \\ni (\\sigma, t) \\mapsto T_t(\\sigma) \\in \\Sigma \\text{ is measurable.} \\end{align*}[/$$]<br>In that case, set $J_T:= \\bigcap_{t\\ge 0}J_{T_i} = \\{I\\in \\Sigma\\colon \\forall t\\ge 0\\colon T_t^{-1}(I) = I\\}\\subset \\mathcal{F} $. Then $\\xi$ is called \\textbf{$T$-stationary} if for all $t\\ge 0$ we have $T_t \\circ \\xi \\stackrel{d}{=} \\xi$.</div><div></div><div></div><div></div><div>[Page 16]</div>"
  },
  {
    "front": "IV. Ergodic theory:<div></div><div>Continuous version of Birkhoff's theorem.</div>",
    "back": "<br><div></div><div>Let $T$ be a flow and $\\xi$ be $T$-stationary. Then if $f\\colon \\Sigma \\to [0,\\infty)$ is measurable, \\[ \\frac{1}{t} \\int_0^t f\\left( T_s(\\xi) \\right) \\mathop{}\\!\\mathrm{d} s \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ f\\circ \\xi \\ | J_\\xi \\right] \\operatorname{\\mathbb{P}-a.s.}\\] If $f\\in L^p$, then the convergence holds in $L^p$.</div><div></div><div></div><div></div><div>[Page 16]</div>"
  },
  {
    "front": "IV. Ergodicity:<div></div><div>Proof of Kolmogorov's law of large number's with Birkhoff's theorem.</div>",
    "back": "<br><div></div><div>Let $(\\xi)_i$ be i.i.d. real random variables with $\\mathbb{E} |\\xi_1| &lt; \\infty$. Then \\[ \\frac{1}{n}\\sum_{j=1}^n \\xi_j \\stackrel{ n\\to \\infty }{\\longrightarrow} \\mathbb{E}\\left[ \\xi_1 \\right] \\operatorname{\\mathbb{P}-a.s.}.\\] This follows from Birkhoff's ergodic theorem using $\\Sigma = \\mathbb{R}^\\mathbb{N}, T = S, \\xi = (\\xi_i)_i, f = \\pi_1$ and Kolmogorov's zero-one law.</div><div></div><div></div><div></div><div>[Page 16]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Multivariate Gaussian processes.</div>",
    "back": "<br><div></div><div>A random field $X\\colon \\Omega \\to \\mathbb{R}^m$ is called <b>multivariate Gaussian process</b> if for $t_1,\\ldots ,t_n\\in T, I:= \\{t_1,\\ldots ,t_n\\}, $ <br>\\begin{equation}\\label{eq}<br> (X_{t_1},\\ldots ,X_{t_n}) \\sim \\mathcal{N}_{n\\cdot m} (m_I, B_I).<br>\\end{equation} Set $m(t) := \\mathbb{E}\\left[ X_t \\right] $ and $B(t,s) := \\left[(X_t - m_t)(X_s-m_s)^\\top \\right] $. Then $m_I = (m(t_i))_{i=1}^n$ and $B_I = \\left( B(t_i, t_j) \\right)_{i,j=1}^n$.<br><br>Conversely, if $m\\colon T\\to \\mathbb{R}^m$ and $B\\colon T\\times T\\to \\mathbb{R}^{m,m}$ symmetric and positive semidefinite, i.e. $B(t, s) = B(s, t)^\\top$ and \\[ \\forall n\\in \\mathbb{N}, t_i\\in T, u_i\\in \\mathbb{R}^n\\colon \\sum_{i,j=1}^n \\left&lt; B(t_i, t_j) u_i, u_j\\right&gt;\\ge 0,\\\\\\] then equation (1) defines a projective family.</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Hoelder-continuity criterion.</div>",
    "back": "<br><div></div><div>Let $X\\colon \\Omega \\to \\mathbb{R}^{[a,b]}$ be a real-valued stochastic process. If there exist constants $c_1,c_2,c_3&gt;0$ such that for all $t \\in T$ and $h\\neq 0$ sufficiently small, \\[\\mathbb{E} \\left[ \\left| X_{t+h}-X_t \\right|^{c_1} \\right] \\le c_2 |h|^{1+c_3},\\\\\\] then $X$ has a version with$\\operatorname{\\mathbb{P}-a.s.}$ $\\gamma$-Hölder-continuous paths for all $\\gamma &lt; \\frac{c_3}{c_1}.$</div><div></div><div></div><div></div><div>[Page 12]</div>"
  },
  {
    "front": "III.1 Foundations:<div></div><div>Fractional Brownian motion.</div>",
    "back": "<br><div></div><div>$T=[0,\\infty), E=\\mathbb{R}$. Let $H\\in (0,1]$ and set $a\\equiv 0$ and \\[K_H(t, s) := \\frac{1}{2}\\left( |t|^{2H}+ |s|^{2H}-|t-s|^{2H} \\right). \\] Then $B_H$ is almost surely Hölder-continuous for exponents $\\gamma &lt; H$. For $H &gt; \\frac{1}{2}$, increments are positivley, for $H &lt; \\frac{1}{2}$ negativley correlated.<br><br>Special cases: $K_1(t, s) = ts$ and $K_{1 / 2} (t, s) = t\\wedge s$ (Brownian motion).</div><div></div><div></div><div></div><div>[Page 13]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Let [$]\\Phi[/$] be a poisson process with intensity measure [$]\\Theta[/$]. Then [$]\\Phi[/$] is simple iff [$]\\Theta[/$] is diffusive.</div>",
    "back": "<br><div></div><div><b>Lemma:</b> Let $E$ be lcHcb, $(X_i)$ i.i.d. in $E$, $N$ independent $\\mathbb{N}_0$-valued with $\\mathbb{P}(N&gt;1) &gt; 0$. Then $\\Phi := \\sum_{i=1}^N \\delta_{X_i}$ is a point process such that \\[ \\Phi \\text{ is simple} \\iff \\mathbb{P}^{X_1}(\\{x\\} ) = 0 \\ \\forall x\\in E.\\] <br><br>From this we can conclude that if $\\Phi$ is a poisson process in $E$ with intensity measure $\\Theta$, $\\Phi$ is simple iff $\\Theta$ is diffusive (i.e. $\\Theta(\\{x\\} ) = 0 \\ \\forall x\\in E$).</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.1 Random measures and point processes:<div></div><div>Measurability of the maps [$]\\zeta_i \\colon \\mathbb{N}(E) \\to E[/$]</div>",
    "back": "<div>[Some extra page]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div></div><div>Connection between&nbsp;</div><div></div><div>[$]\\int_E f \\wedge 1 \\mathop{}\\!\\mathrm{d} \\Theta &lt;(=)\\infty[/$] &nbsp;and &nbsp;[$]\\int_E f \\mathop{}\\!\\mathrm{d} \\Phi &lt; (=) \\infty[/$]</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a point process in $E$ with intensity measure $\\Theta$. Let $f\\colon E\\to [0,\\infty)$ be measurable. Then<br>[$$]\\begin{align*} (1)&amp; \\ \\int_E f \\wedge 1 \\mathop{}\\!\\mathrm{d} \\Theta &lt; \\infty \\implies \\int_E f\\mathop{}\\!\\mathrm{d} \\Phi &lt; \\infty \\operatorname{\\mathbb{P}-a.s.}.\\\\ (2)&amp; \\ \\text{$\\Phi$ Poisson: } \\int_E f \\wedge 1 \\mathop{}\\!\\mathrm{d} \\Theta = \\infty \\implies \\int_E f \\mathop{}\\!\\mathrm{d} \\Phi = \\infty \\operatorname{\\mathbb{P}-a.s.}.\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>In what sense does the first defining property of a ppp ([$]\\Phi(A)\\sim \\text{Po}(\\Theta(A))[/$]) imply the second for simple point processes?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a simple point process in $E$. If there exists a measure $\\Theta$ on $\\mathcal{B}(E)$ such that for any $A\\in \\mathcal{B}(E),$ <br>\\[\\Phi(A) \\sim \\text{Po} (\\Theta(A)),\\]<br>then ($\\Theta$ is locally finite and diffusive and) $\\Phi\\sim \\text{Po} (\\Theta)$.</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>In what sense does the second defining property of a ppp (independence) imply the first for simple point processes?</div>",
    "back": "<br><div></div><div>Let $\\Phi$ be a simple point process in $E$ with locally finite and diffusive intensity measure $\\Theta$. Then if $\\Phi(A)$ and $\\Phi(B)$ are independent for all disjoint $A, B\\in \\mathcal{B}(E)$, then $\\Phi \\sim \\text{Po} (\\Theta)$.</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>[$]\\text{Cov}(\\Phi(A),\\Phi(B))[/$] for a poisson process.</div>",
    "back": "<br><div>[$$] \\text{Cov}(\\Phi(A),\\Phi(B)) = \\Theta (A\\cap B)[/$$]</div><div></div><div></div><div>[Do it yourself!]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>[$]\\Phi \\sim \\text{Cox}(\\chi), B\\in\\mathcal{B}(E) \\implies \\Phi_B \\sim ?[/$]</div>",
    "back": "<br><div>[$]\\Phi_B \\sim \\text{Cox}(\\chi_B).[/$]</div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.4 Cox processes:<div></div><div>If [$]\\Phi_{1,2} \\sim \\text{Cox}(\\chi_{1,2})[/$], then [$]\\Phi_1\\stackrel{d}{=}\\Phi_2 \\stackrel{?}{\\leftrightsquigarrow} \\chi_1\\stackrel{d}{=}\\chi_2[/$].</div>",
    "back": "<br><div>[$$] \\Phi_1\\stackrel{d}{=}\\Phi_2 \\iff \\chi_1\\stackrel{d}{=}\\chi_2[/$$]</div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "II.2 Poisson processes:<div></div><div>Mapping theorem.</div>",
    "back": "<br><div></div><div>If $\\Phi\\sim\\text{Po} (\\Theta)$ and $T\\colon E\\to E'$ is proper, then $T(\\Phi) \\sim \\text{Po} (T(\\Theta))$.</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "III.4 Spectral representations:<div></div><div>Example with [$]W = \\sum_{j=1}^\\infty X_j \\delta_{t_j}[/$].</div>",
    "back": "<br><div></div><div>Let $(E,\\mathcal{A})$ be a measurable space, $(t_i)\\subset E$ p.d. and $(X_i)\\subset L^2(\\Omega, \\mathcal{F},\\mathbb{P})$ pairwise orthogonal with $\\sum_{i=1}^\\infty \\|X_i\\|^2 &lt; \\infty$. Then $\\nu := \\sum_{i=1}^\\infty \\|X_i\\|^2 \\delta_{t_j}$ is a finite measure on $E$ with \\[ f_n \\stackrel{ L^2 }{\\longrightarrow} f \\iff \\sum_{i=1}^\\infty \\|X_i\\|^2 |f(t_i) - f_n(t_i)|^2 \\stackrel{  }{\\longrightarrow} 0.\\] In this setting, \\(<br>W:= \\sum_{i=1}^{\\infty} X_i \\delta_{t_i}\\in L^2(\\Omega, \\mathcal{F}, \\mathbb{P})<br>\\) defines a complex $\\nu$-noise, where the limit is taken pointwise in $L^2$. Then if $f\\in L^2(E,\\mathcal{A},\\nu),$ \\[ \\int_E f \\mathop{}\\!\\mathrm{d} W = \\sum_{i=1}^{\\infty} X_i f(t_i),\\\\\\] where the limit is again taken in $L^2$.<br><br>Now if $E=\\mathbb{R}^d, \\mathcal{A} = \\mathcal{B}(E)$ and $\\mathbb{E} \\left[ X_i \\right] = 0 \\ \\forall i\\in \\mathbb{N}$, then <br>\\[Z(t) := \\sum_{i=1}^{\\infty} X_i \\mathrm{e}^{\\mathrm{i}\\left&lt;t, t_i \\right&gt;}\\] defines a centred, square-integrable, $L^2$-continuous and weakly stationary random field with $C_0 = \\varphi_\\nu$ and $Z(t) = \\int \\mathrm{e}^{\\mathrm{i}\\left&lt;t, x \\right&gt;}W(\\mathop{}\\!\\mathrm{d} x)$.</div><div></div><div></div><div></div><div>[Some extra page]</div>"
  },
  {
    "front": "Martingale transform (discrete stochastic integral).",
    "back": "<div>Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, $(Y_n)$ an $(\\mathcal{F}_n)$ (sub/super-)martingale and $(V_n)$ predictable (and non-negative). Then, subject to integrability, \\[&nbsp;&nbsp;&nbsp; (V\\circ Y)_n := \\sum_{k=1}^n V_k (Y_k - Y_{k-1})\\] defines a (sub/super-)martingale.</div><div></div><div></div><div>[Probability B1, Page 1]</div>"
  },
  {
    "front": "Doob decomposition of integrable sequence [$](X_n)[/$].",
    "back": "<div>Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, $(X_n)$ integrable and $(\\mathcal{F}_n)$-adapted. Then there exists a martingale $(M_n)$ and a predictable sequence $(A_n)$ with $M_0 = A_0 = 0$ such that \\[X_n = X_0 + M_n + A_n\\\\\\] for all $n\\in \\mathbb{N}$. $(M_n)$ and $(A_n)$ are a.s. unique. $(X_n)$ is a sub-/supermartingale if and only if $(A_n)$ is a.s. increasing/decreasing.</div><div></div><div></div><div>[Probability B1, Page 1]</div>"
  },
  {
    "front": "Stopped (discrete) *martingales are *martingales.",
    "back": "<div>Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, $(M_n)$ a *martingale and $\\tau$ a stopping time. Then $(M_{\\tau \\wedge n})$ is a *martingale.</div><div></div><div>Furthermore, if $(M_n)$ is a (super-)submartingale, then, for all $n\\in\\mathbb{N}$, \\[\\mathbb{E} \\left[ M_{n\\wedge \\tau}\\right] (\\ge) \\le \\mathbb{E} \\left[ M_n\\right].\\]</div><div></div><div></div><div>[Probability B1, Page 1]</div>"
  },
  {
    "front": "Doob's Optional Stopping for discrete martingales (conditions under which $\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]$).",
    "back": "<div>Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, $(M_n)$ a *martingale and $\\tau$ a stopping time. Suppose that one of the following hold.<br><ol>  <li>$\\tau$ is bounded.&nbsp;&nbsp;&nbsp;</li>  <li>$\\tau &lt; \\infty$ a.s. and, for some $K &gt; 0$, $|M_n| \\le K$ a.s. for all $n\\in \\mathbb{N}$.&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ \\tau \\right] &lt;\\infty$ and there exists $L &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[\\left| M_{n+1}-M_n \\right|&nbsp; \\,\\middle\\vert\\, \\mathcal{F}_n\\right] \\le L \\quad \\text{a.s.} &nbsp;&nbsp;&nbsp; \\] for all $n\\in \\mathbb{N}$.</li></ol><br>Then $M_\\tau$ is integrable and $\\mathbb{E} \\left[ M_\\tau \\right] = \\mathbb{E} \\left[ M_0 \\right] $.</div><div></div><div></div><div>[Probability B1, Page 1]</div>"
  },
  {
    "front": "Sufficient condition for [$]\\mathbb{E} \\left[ \\tau \\right] &lt; \\infty[/$].",
    "back": "<div>Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space and $\\tau$ a $(\\mathcal{F}_n)$-stopping time. Then if there exist $K \\in \\mathbb{N}$ and $\\varepsilon&gt;0$ such that \\[\\mathbb{P} \\left(\\tau \\le n + K \\,\\middle\\vert\\, \\mathcal{F}_n\\right) \\ge \\varepsilon \\quad \\text{a.s.} \\] for all $n\\in \\mathbb{N}$, then $\\mathbb{E} \\left[ \\tau \\right] &lt; \\infty$.</div><div></div><div></div><div>[Probability B1, Page 2]</div>"
  },
  {
    "front": "Doob's maximal inequality.",
    "back": "<div>Let $(X_n)$ be a non-negative submartingale. Then for any $\\lambda&gt;0$ and $n\\in \\mathbb{N}$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\max_{k\\le n}X_k \\ge \\lambda \\right) \\le \\frac{\\mathbb{E} \\left[ X_n \\right] }{\\lambda}.\\] If $(M_n)$ is a martingale and $f\\colon \\mathbb{R}\\to \\mathbb{R}$ is non-negative, increasing and convex such that $\\mathbb{E} \\left[ f(|M_n|) \\right] &lt;\\infty$ for all $n\\in \\mathbb{N}$, then for all $\\lambda&gt;0$ and $n\\in \\mathbb{N}$, \\[\\mathbb{P}\\left( \\max_{k\\le n}|M_k| \\ge \\lambda \\right) \\le \\frac{\\mathbb{E} \\left[ f(|M_n|) \\right] }{f(\\lambda)}.\\]</div><div></div><div></div><div></div><div>[Probability B1, Page 2]</div>"
  },
  {
    "front": "Definition upcrossings and connection with convergence.",
    "back": "<div>Let $x = (x_n)\\in \\mathbb{R}^\\mathbb{N}$ and $a &lt; b$. Then define $(S_k)_{k\\ge 0}= (S_k(x,[a,b]))$ and $(T_k)_{k\\ge 0} = (T_k(x,[a,b]))$ recursively by $T_0 = 0$ and \\[S_k := \\inf \\left\\{ n\\ge T_{k-1}\\colon x_n \\le a \\right\\}, \\qquad T_k := \\inf \\left\\{ n\\ge S_k\\colon x_n \\ge b \\right\\} \\] for $k\\ge 1$. Then set <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; U_n(x,[a,b]) &amp;:= \\max\\left\\{ k\\ge 0\\colon T_k \\le n \\right\\}, \\\\&nbsp;&nbsp;&nbsp; U(x,[a,b]) &amp;:= \\sup\\left\\{ k\\ge 0\\colon T_k &lt; \\infty \\right\\} \\left(= \\sup_{n\\in\\mathbb{N}} U_n(x,[a,b]\\right),\\\\\\end{align*}[/$$]<br>the number of upcrossings of $x$ up to time $n$ and in total respectively.<br><br><b>Connection with convergence:</b> $x$ converges if and only if $U(x,[a,b]) &lt; \\infty$ for all $a,b\\in \\mathbb{Q}, \\, a &lt; b$.</div><div></div><div></div><div></div><div>[Probability B1, Page 2]</div>"
  },
  {
    "front": "Doob's upcrossing lemma for discrete supermartingales.",
    "back": "<div>Let $(X_n)$ be a supermartingale and $a &lt; b$. Then for every $n\\in \\mathbb{N}$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ U_n\\left( X, [a,b] \\right)&nbsp; \\right] \\le \\frac{\\mathbb{E} \\left[ \\left( X_n - a \\right) ^- \\right] }{b-a}.\\]</div><div></div><div></div><div></div><div>[Probability B1, Page 2]</div>"
  },
  {
    "front": "Forward convergence for [$]L^1[/$]-bounded discrete *martingales.",
    "back": "<div>Let $(X_n)$ be an $L^1$-bounded sub-/supermartingale. Then there exists an integrable $X_\\infty$ such that \\[X_n \\stackrel{  }{\\longrightarrow} X_\\infty \\quad \\text{a.s.} \\] This, in particular, applies to supermartingales bounded from below and to submartingales bounded from above.</div><div></div><div></div><div></div><div>[Probability B1, Page 2]</div>"
  },
  {
    "front": "Pythagoras for square-integrable martingales and characterisation of [$]L^2[/$]-boundedness.",
    "back": "<div>Let $(M_n)$ be a square-integrable martingale. Then for $n,m\\in \\mathbb{N}, \\, m &gt; n,$ <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ M_n^2 \\right] = \\mathbb{E} \\left[ M_0^2 \\right] + \\sum_{k=1}^n \\mathbb{E} \\left[ (M_k - M_{k-1})^2 \\right] ,\\\\&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ (M_m - M_n)^2 \\right] = \\sum_{k=n+1}^m \\mathbb{E} \\left[ (M_k-M_{k-1})^2 \\right] .\\end{align*}[/$$]</div><div>Consequently, a martingale $(M_n)$ is bounded in $L^2$ if and only if \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ M_0^2 \\right] &lt;\\infty \\quad \\text{and} \\quad \\sum_{k=1}^\\infty \\mathbb{E} \\left[ (M_k - M_{k-1})^2 \\right] &lt;\\infty.\\]</div><div></div><div></div><div></div><div>[Probability B1, Page 2/3]</div>"
  },
  {
    "front": "Forward convergence for [$]L^2[/$]-bounded discrete martingales.",
    "back": "<div>Let $(M_n)$ be an $L^2$-bounded martingale. Then there exists a random variable $M_\\infty\\in L^2$ such that \\[M_n \\stackrel{ \\text{a.s.} }{\\longrightarrow}&nbsp; M_\\infty \\quad \\text{and} \\quad M_n \\stackrel{ L^2 }{\\longrightarrow}&nbsp; M_\\infty.\\]</div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Definition and tail characterisation of uniform integrability.",
    "back": "<div>A collection $\\mathcal{C}$ of real random variables is called <i>uniformly integrable</i> if \\[&nbsp;&nbsp;&nbsp; \\sup_{X\\in \\mathcal{C}} \\mathbb{E} \\left[ |X| \\boldsymbol{1}\\left\\{|X| \\ge K\\right\\} \\right] \\stackrel{  }{\\longrightarrow} 0, \\quad K\\to \\infty.\\] This is the case if and only if \\[\\sup_{X\\in \\mathcal{C}}\\mathbb{E} \\left[ \\left( |X| - K \\right) ^+ \\right] \\stackrel{  }{\\longrightarrow} 0, \\quad K\\to \\infty.\\]</div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Connection between boundedness in [$]L^p[/$] and uniform integrability.",
    "back": "<div>Let $\\mathcal{C}$ be a collection of random variables and $p &gt; 1$. Then \\[\\mathcal{C} \\text{ is bounded in $L^p$} \\implies \\mathcal{C} \\text{ is uniformly integrable} \\implies \\mathcal{C} \\text{ is bounded in $L^1$} \\implies \\mathcal{C} \\text{ is tight.} \\]</div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Characterisation of uniform integrability (involving [$]\\mathbb{E}\\left[ |X| \\boldsymbol{1}_A\\right][/$]).",
    "back": "<div>Let $\\mathcal{C}$ be a collection of random variables. Then $\\mathcal{C}$ is uniformly integrable if and only if<br><ol>  <li>$\\mathcal{C}$ is tight,&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_{X\\in \\mathcal{C}} \\mathbb{E} \\left[ |X| \\boldsymbol{1}_{A} \\right] \\stackrel{  }{\\longrightarrow} 0$ if $\\mathbb{P}(A) \\to 0$.</li></ol></div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Vitali's Convergence Theorem.",
    "back": "<div>Let $X_n \\mathrel{\\stackrel{\\mathbb{P}}{\\longrightarrow}} X$ and $X_n$ integrable for all $n\\in \\mathbb{N}$. Then the following are equivalent.<br><ol>  <li>$(X_n)$ is uniformly integrable,&nbsp;&nbsp;&nbsp;</li>  <li>$X_n \\stackrel{ L^1 }{\\longrightarrow} X$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ |X_n| \\right] \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ |X| \\right] &lt;\\infty$.</li></ol><br>In particular, for any sequence $(X_n)$ we have $X_n \\stackrel{ L^1 }{\\longrightarrow} X$ if and only if $X_n \\mathrel{\\stackrel{\\mathbb{P}}{\\longrightarrow}} X$ and $(X_n)$ is uniformly integrable.</div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Families of the form [$]\\left( \\mathbb{E} \\left[ X\\,\\middle\\vert\\, \\mathcal{F}_\\alpha\\right]\\right)_\\alpha[/$] are uniformly integrable.",
    "back": "<div>Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and $(\\mathcal{F}_\\alpha)_{\\alpha\\in I}$ a family of sub-$\\sigma$-algebras of $\\mathcal{F}$. Then if $X$ is integrable, the family $(X_\\alpha)$ defined by \\[X_\\alpha := \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}_\\alpha\\right], \\quad \\alpha \\in I\\\\\\] is uniformly integrable.</div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Discrete Martingale convergence theorem.",
    "back": "<div>Let $(M_n)$ be an $(\\mathcal{F}_n)$-martingale. The following are equivalent.<br><ol>  <li>$(M_n)$ is uniformly integrable,&nbsp;&nbsp;&nbsp;</li>  <li>There exists an integrable $M_\\infty$ such that $M_n \\stackrel{ \\text{a.s.} }{\\longrightarrow} M_\\infty$ and $M_n \\stackrel{ L^1 }{\\longrightarrow} M_\\infty$.&nbsp;&nbsp;&nbsp;</li>  <li>There exists an integrable $M_\\infty$ such that $M_n = \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_n\\right]$ a.s. for all $n\\in \\mathbb{N}$.</li></ol></div><div></div><div></div><div></div><div>[Probability B1, Page 3]</div>"
  },
  {
    "front": "Discrete Optional Stopping for uniformly integrable martingales.",
    "back": "<div>Let $(M_n)$ be a uniformly integrable martingale and $\\tau$ a stopping time ($\\mathbb{P}(\\tau = \\infty) &gt; 0$ is allowed!). Then $M_\\tau$ is integrable and $\\mathbb{E} \\left[ M_\\tau \\right] = \\mathbb{E} \\left[ M_0 \\right] $.</div><div></div><div></div><div></div><div>[Probability B1, Page 4]</div>"
  },
  {
    "front": "Variant of Tschebyscheff's inequality.",
    "back": "<div>Let $X$ be a real random variable and $t &gt; 0$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( X - \\mathbb{E} \\left[ X \\right] \\ge t \\right) \\le \\frac{\\mathbb{V}(X)}{t^2 + \\mathbb{V}(X)}.\\] This improves the Tschebyscheff if $\\mathbb{V}(X) &lt; t^2$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 1]</div>"
  },
  {
    "front": "Hoeffding's lemma and theorem (Concentration for [$]a_i \\le X_i \\le b_i[/$]).",
    "back": "<div><b>Hoeffding's lemma:</b> Let $a \\le X \\le b$ a.s. and $\\mathbb{E} \\left[ X \\right] = 0$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\mathrm{e}^{sX} \\right] \\le \\exp \\left(\\frac{s^2 (b-a)^2}{8} \\right) .\\] <br><br><b>Hoeffding's theorem:</b> Let $X_1,\\ldots ,X_n$ be independent with $a_i \\le X_i \\le b_i$ a.s. and $S_n = \\sum_{i=1}^n X_i$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( S_n - \\mathbb{E} \\left[ S_n \\right] \\ge t(\\le -t) \\right) \\le \\exp\\left( - \\frac{2t^2}{\\sum_i (b_i-a_i)^2} \\right)\\] for all $t &gt; 0$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 1]</div>"
  },
  {
    "front": "Concentration of Binomial distribution (Okamoto and Chernoff)",
    "back": "<div>Let $S_n = \\sum_{i=1}^n X_i$, where $X_i \\sim \\text{Bin}(1,p_i)$ are independent and set $p := \\frac{1}{n}\\sum_{i=1}^n p_i$. Then, for any $\\varepsilon &gt; 0$,<br><ol>  <li>If $p \\ge \\frac{1}{2}$, then $\\mathbb{P}\\left( S_n - np \\ge \\varepsilon n \\right) \\le \\exp\\left( -\\frac{n\\varepsilon^2}{2p(1-p)} \\right) $.&nbsp;&nbsp;&nbsp;</li>  <li>[(ii)] If $p\\le \\frac{1}{2}$, then $\\mathbb{P}\\left( S_n - np \\le -\\varepsilon n \\right) \\le \\exp \\left( - \\frac{n\\varepsilon^2}{2p(1-p)} \\right) $.&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}\\left( |S_n-np| \\ge \\varepsilon n \\right) \\le 2\\exp \\left( -2n\\varepsilon^2 \\right) $.</li></ol></div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 1]</div>"
  },
  {
    "front": "Bernstein's inequality (Concentration for [$]|X_i - \\mathbb{E} X_i| \\le c[/$]).",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be independent and $\\left| X_i - \\mathbb{E} \\left[ X_i \\right]&nbsp; \\right| \\le c$ a.s., set $S_n = \\sum_{i=1}^n X_i$ and $\\sigma^2 := \\frac{1}{n}\\sum_{i=1}^n \\mathbb{V}(X_i)$. Then, for $t &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( S_n - \\mathbb{E} \\left[ S_n \\right] \\ge t (\\le -t) \\right) \\le \\exp \\left( - \\frac{t^2}{2\\sigma^2n + \\frac{2ct}{3}} \\right) .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 1]</div>"
  },
  {
    "front": "Concentration for [$]0 \\le X_i \\le 1[/$] (Corollary to Bernstein).",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be independent and $0\\le X_i \\le 1$ a.s., set $S_n = \\sum_{i=1}^n X_i$. Then if $t &gt; 0$ and $\\varepsilon &gt; 0$,<br><ol>  <li>$\\mathbb{P}\\left( S_n - \\mathbb{E} \\left[ S_n \\right] \\ge t \\right) \\le \\exp\\left( - \\frac{t^2}{2 \\mathbb{E} \\left[ S_n \\right] + \\frac{2t}{3}} \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}\\left( S_n - \\mathbb{E} \\left[ S_n \\right] \\ge \\varepsilon \\mathbb{E} \\left[ S_n \\right]&nbsp; \\right) \\le \\exp \\left( - \\frac{\\varepsilon^2 \\mathbb{E} \\left[ S_n \\right] }{2 + \\frac{2\\varepsilon}{3}} \\right) $.</li></ol></div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 1]</div>"
  },
  {
    "front": "Definition backwards martingale.",
    "back": "<div>Given $\\sigma$-algebras $(\\mathcal{F}_{-n})_{n\\in \\mathbb{N}_0}$ with $\\mathcal{F}_{-n -1}\\subset \\mathcal{F}_{-n}$ for all $n\\in \\mathbb{N}_0$, an adapted sequence $(M_{-n})_{n\\in \\mathbb{N}_0}$ of integrable real random variables is called a <i>backwards martingale</i> if \\[\\mathbb{E} \\left[M_{-n+1} \\,\\middle\\vert\\,\\mathcal{F}_{-n} \\right] = M_{-n} \\quad \\text{a.s.}\\] for all $n\\in \\mathbb{N}_0$. In that case, since $M_{-n} = \\mathbb{E} \\left[M_0 \\,\\middle\\vert\\, \\mathcal{F}_{-n}\\right]$ for all $n\\in \\mathbb{N}_0$, $(M_{-n})$ is automatically uniformly integrable.</div><div></div><div></div><div></div><div>[Probability B1, Page 4]</div>"
  },
  {
    "front": "Backward martingale convergence.",
    "back": "<div>Let $(M_{-n})$ be a backwards martingale. Then there exists an integrable random variable $M_{-\\infty}$ such that \\[M_{-n} \\stackrel{ \\text{a.s.} }{\\longrightarrow}&nbsp; M_{-\\infty} \\quad \\text{and} \\quad M_{-n} \\stackrel{ L^1 }{\\longrightarrow} M_{-\\infty}.\\] Furthermore, if $\\mathcal{F}_{-\\infty} := \\bigcap_{n=1}^\\infty \\mathcal{F}_{-n}$, \\[M_{-\\infty} = \\mathbb{E} \\left[M_{-n} \\,\\middle\\vert\\, \\mathcal{F}_{-\\infty}\\right]\\] for all $n\\in \\mathbb{N}$.</div><div></div><div></div><div></div><div>[Probability B1, Page 4]</div>"
  },
  {
    "front": "Proof of Kolmogorov's strong law using backwards martingales.",
    "back": "<div>Let $(X_n)$ be integrable i.i.d. real random variables with $\\mu := \\mathbb{E} \\left[ X_1 \\right] $. Then \\[\\frac{1}{n} \\sum_{k=1}^n X_k \\stackrel{  }{\\longrightarrow} \\mu \\quad \\text{a.s. and in $L^1$}. \\]</div><div></div><div></div><div></div><div>[Probability B1, Page 4]</div>"
  },
  {
    "front": "Quadratic variation of Brownian motion.",
    "back": "<div>Let $B$ be a standard real Brownian motion. Then&nbsp; \\[&nbsp;&nbsp;&nbsp; QV_t^\\pi(B) \\stackrel{ L^2 }{\\longrightarrow} t\\\\\\] if $\\delta(\\pi)\\to 0$, so in particular $\\left&lt;B \\right&gt; _t = t$ for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 1]</div>"
  },
  {
    "front": "Brownian paths are almost surely of infinite variation on every interval.",
    "back": "[Probability B8.2, Page 1]"
  },
  {
    "front": "Brownian paths are almost surely nowhere Hoelder continuous of order [$]\\gamma &gt; 1/2[/$].",
    "back": "[Probability B8.2, Page 1]"
  },
  {
    "front": "Blumenthal's 0-1-law.",
    "back": "<div>Let $B$ be a standard real Brownian motion. Let $\\mathcal{F}_t := \\sigma\\left(\\mathcal{F}_t^B \\cup \\mathcal{N}(\\mathbb{P})\\right)$ (augmented natural filtration) and $\\mathcal{F}_{t+} := \\bigcap_{s&gt;t}\\mathcal{F}_s$. Then $\\mathcal{F}_{0+}$ is trivial, that is, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(A) \\in \\left\\{ 0,1 \\right\\}, \\quad A\\in \\mathcal{F}_{0+}.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 1]</div>"
  },
  {
    "front": "Brownian motion is almoset surely surjective.",
    "back": "[Probability B8.2, Page 1]"
  },
  {
    "front": "Behaviour of Brownian motion around the origin (Corollary to Blumenthal).",
    "back": "<div>Let $B$ be a standard real Brownian motion and $\\varepsilon &gt; 0$. Then with probability one, \\[\\sup_{s&lt; \\varepsilon}B_s &gt; 0 \\quad \\text{and} \\quad \\inf_{s &lt; \\varepsilon} B_s &lt; 0.\\] In particular, $\\inf \\left\\{ t&gt; 0\\colon B_t = 0 \\right\\} = 0$ a.s.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 1]</div>"
  },
  {
    "front": "The usual conditions.",
    "back": "<div>A filtration $(\\mathcal{F}_t)$ on $(\\Omega, \\mathcal{A}, \\mathbb{P})$ satisfies the <i>usual conditions</i> if it is right-continuous and $\\mathcal{N}(\\mathbb{P})\\subset \\mathcal{F}_0$. If $(\\mathcal{F}_t)$ is a filtrations, then \\[&nbsp;&nbsp;&nbsp; \\widetilde{\\mathcal{F}}_t := \\sigma \\left( \\mathcal{F}_{t+} \\cup \\mathcal{N}(\\mathbb{P}) \\right) \\] satisfies the usual conditions.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 2]</div>"
  },
  {
    "front": "Definition progressively measurable and connection with measurable and adapted.",
    "back": "<div>A stochastic process $(X_t)$ on $(\\Omega, \\mathcal{F}, \\mathbb{P})$ is <i>progressively measurable (with respect to $(\\mathcal{F}_t))$</i> if for all $t\\ge 0$, \\[&nbsp;&nbsp;&nbsp; [0,t] \\times \\Omega \\to \\mathbb{R}; \\, (s, \\omega) \\mapsto X_s(\\omega)\\] is $\\mathcal{B}([0,t]) \\otimes \\mathcal{F}_t - \\mathcal{B}(\\mathbb{R})$ measurable. In that case, $X$ is adapted and measurable.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 2]</div>"
  },
  {
    "front": "Continuity criterion for progressive measurability.",
    "back": "An adapted process with (left-)right-continuous paths is progressively measurable.<div></div><div></div><div>[Probability B8.2, Page 2]</div>"
  },
  {
    "front": "First hitting times and two examples.",
    "back": "<div>Let $X$ be an adapted stochastic process on $(\\Omega, \\mathcal{F}, \\mathbb{P})$ with respect to a filtration $(\\mathcal{F})_t$ and $\\Gamma \\in \\mathcal{B}(\\mathbb{R})$. Then \\[H_\\Gamma := \\inf\\left\\{ t\\ge 0\\colon X_t \\in \\Gamma \\right\\} \\colon \\Omega \\to [0,\\infty]\\] is called a <i>first hitting time</i>.<br><ol>  <li>If $X$ has right-continuous paths and $\\Gamma$ is open, then $H_\\Gamma$ is a stopping time with respect to $(\\mathcal{F}_{t+})$.&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ has continuous paths and $\\Gamma$ is closed, then $H_\\Gamma$ is a stopping time with respect to $(\\mathcal{F}_t)$.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 2]</div>"
  },
  {
    "front": "Definition and connection between [$]\\mathcal{F}_\\tau[/$] and [$]\\mathcal{F}_{\\tau+}[/$] for a stopping time [$]\\tau[/$].",
    "back": "<div>Let $\\tau$ be a stopping time with respect to $(\\mathcal{F}_t)$. Then define the $\\sigma$-algebras<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathcal{F}_\\tau &amp;:= \\left\\{ A\\in \\mathcal{F}\\colon \\forall t\\ge 0\\colon A \\cap \\left\\{ \\tau \\le t \\right\\} \\in \\mathcal{F}_t \\right\\} ,\\\\&nbsp;&nbsp;&nbsp; \\mathcal{F}_{\\tau+} &amp;:= \\left\\{ A\\in \\mathcal{F}\\colon \\forall t\\ge 0\\colon A \\cap \\left\\{ \\tau &lt; t \\right\\} \\in \\mathcal{F}_t \\right\\} \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left\\{ A\\in \\mathcal{F}\\colon \\forall t\\ge 0 \\colon A\\cap \\left\\{ \\tau \\le t \\right\\} \\in \\mathcal{F}_{t+} \\right\\} .\\end{align*}[/$$]<br>Then $\\tau$ is $\\mathcal{F}_\\tau$-measurable, $\\mathcal{F}_{\\tau}\\subset \\mathcal{F}_{\\tau+}$ and if $(\\mathcal{F}_t)$ is right-continuous, then $\\mathcal{F}_\\tau = \\mathcal{F}_{\\tau+}$. If $\\tau \\equiv t\\ge 0$, then $\\mathcal{F}_{\\tau(+)}= \\mathcal{F}_{t(+)}$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 2]</div>"
  },
  {
    "front": "Let $\\tau,\\rho$ be stopping times.<br><ol>  <li>$\\tau\\wedge\\rho$, $\\tau\\vee\\rho$ and $\\tau + \\rho$ are stopping times,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left\\{ \\tau \\le \\rho \\right\\} \\in \\mathcal{F}_{\\tau \\wedge \\rho}$,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\tau \\le \\rho$, then $\\mathcal{F}_\\tau \\subset \\mathcal{F}_\\rho$.</li></ol>",
    "back": "[Probability B8.2, Page 2]"
  },
  {
    "front": "Stopped (pr.) measurable process is (pr.) measurable.",
    "back": "<div>Let $\\tau$ be a stopping time and $X$ (progressively) measurable. Then $(X_{t\\wedge \\tau})_{t\\ge 0}$ is (progressively) measurable.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 3]</div>"
  },
  {
    "front": "Sufficient condition for first hitting time [$]H_\\Gamma[/$] to be a stopping time for all [$]\\Gamma \\in \\mathcal{B}(\\mathbb{R})[/$].",
    "back": "[$]X[/$] progressively measurable with respect to a filtration that satisfies the usual conditions.<div></div><div></div><div>[Probability B8.2, Page 3]</div>"
  },
  {
    "front": "Strong markov property of Brownian motion.",
    "back": "<div>Let $B$ be a standard Brownian motion on $(\\Omega,\\mathcal{F}, (\\mathcal{F}_t), \\mathbb{P})$ and $\\tau$ a stopping time. Then conditional on $\\tau &lt; \\infty$, the process \\[&nbsp;&nbsp;&nbsp; B^{(\\tau)}_t := B_{\\tau + t}- B_\\tau, \\quad t\\ge 0\\\\\\] defines a standard Brownian motion independent of $\\mathcal{F}_\\tau$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 3]</div>"
  },
  {
    "front": "Reflection principle for Brownian motion.",
    "back": "<div>Let $B$ be a standard Brownian motion and set $S_t :=&nbsp; \\sup_{s\\le t}B_s$. Then for $t\\ge 0$ and $0 \\le b \\le a$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(S_t \\ge a, B_t \\le b) = \\mathbb{P}(B_t \\ge 2a - b).\\] In particular, $S_t$ and $|B_t|$ have the same distribution.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 3]</div>"
  },
  {
    "front": "Occupation of bins: Upper bound.",
    "back": "<div>Let $(X_1,\\ldots ,X_n) \\sim \\text{Mult} \\left( n; \\frac{1}{n},\\ldots ,\\frac{1}{n} \\right) $ and set $Z_n := \\max_{1\\le k\\le n} X_k$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( Z_n \\le 2e \\frac{\\log n}{\\log \\log n} \\right) \\ge 1 - \\frac{1}{e} \\left( \\frac{1}{n} \\right) ^{e-1}\\] for all $n\\ge 16$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 2]</div>"
  },
  {
    "front": "Poisson approximation.",
    "back": "<div>Let $(X_1,\\ldots ,X_n) \\sim \\text{Mult}\\left( m; p_1,\\ldots ,p_n \\right) $ and $Y_i \\sim \\text{Po}(mp_i)$ independent. Then the following hold.<br><ol>  <li>$\\mathbb{P}\\left( (Y_1,\\ldots ,Y_n) \\in \\cdot \\mid \\sum_{i=1}^n Y_i = m \\right) = \\mathbb{P}\\left( (X_1,\\ldots ,X_n) \\in \\cdot&nbsp; \\right) $.&nbsp;&nbsp;&nbsp;</li>  <li>If $f\\colon (\\mathbb{N}_0)^n \\to [0,\\infty)$, then&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ f(X_1,\\ldots ,X_n) \\right] \\le \\mathrm{e} \\sqrt{m} \\mathbb{E} \\left[ f(Y_1,\\ldots ,Y_n) \\right] .&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] In particular $\\mathbb{P}\\left( (X_1,\\ldots ,X_n)\\in \\cdot&nbsp; \\right) \\le \\mathrm{e} \\sqrt{m} \\mathbb{P}\\left( (Y_1,\\ldots ,Y_n) \\in \\cdot&nbsp; \\right) $.</li></ol></div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 2]</div>"
  },
  {
    "front": "Occupation of bins: Lower bound.",
    "back": "<div>Let $(X_1,\\ldots ,X_n) \\sim \\text{Mult} \\left( n; \\frac{1}{n},\\ldots ,\\frac{1}{n} \\right) $ and set $Z_n := \\max_{1\\le k\\le n} X_k$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( Z_n \\ge \\frac{\\log n}{\\log \\log n} \\right) \\ge 1 - \\frac{1}{n^2}\\] for almost all $n\\in \\mathbb{N}$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 2]</div>"
  },
  {
    "front": "Efron-Stein inequality, I",
    "back": "<div>Let $X_i\\colon \\Omega\\to \\Gamma_i$ be independent random variables and $f\\colon \\times_{i=1}^n \\Gamma_i \\to \\mathbb{R}$ measurable such that $Y:= f(X_1,\\ldots ,X_n)$ is square-integrable. Set $\\mathcal{C}_i' := \\sigma \\left( X_j\\colon j \\neq i \\right) $. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(Y) \\le \\sum_{i=1}^n \\mathbb{E} \\left[ \\left( Y - \\mathbb{E} \\left[Y \\,\\middle\\vert\\, \\mathcal{C}_i'\\right] \\right) ^2 \\right] .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 2]</div>"
  },
  {
    "front": "Efron-Stein inequality, II.",
    "back": "<div>Let $X_i\\colon \\Omega\\to \\Gamma_i$ be independent random variables and $f\\colon \\times_{i=1}^n \\Gamma_i \\to \\mathbb{R}$ measurable such that $Y:= f(X_1,\\ldots ,X_n)$ is square-integrable. Let $(X_1',\\ldots ,X_n')$ be an independent copy and set $Y_i' := f(X_1,\\ldots ,X_i',\\ldots ,X_n)$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(Y) \\le \\frac{1}{2}\\sum_{i=1}^n \\mathbb{E} \\left[ (Y - Y_i')^2 \\right] .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 3]</div>"
  },
  {
    "front": "<div>Doobs maximal inequality for discrete supermartingales.</div>",
    "back": "<div>Let $(X_n)$ be a supermartingale. Then for $n\\in \\mathbb{N}$ and $\\lambda &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\max_{k\\le n}|X_k| \\ge \\lambda \\right) \\le \\frac{1}{\\lambda} \\left( \\mathbb{E} \\left[ X_0 \\right] + 2 \\mathbb{E} \\left[ |X_n| \\right]&nbsp; \\right) .\\]</div><div></div><div></div><div></div><div>[Probability B8.1, Page 4]</div>"
  },
  {
    "front": "Doob's [$]L^p[/$]-inequality.",
    "back": "<div>Let $(X_n)$ be a martingale or a non-negative submartingale. Then for $n\\in \\mathbb{N}$ and $p &gt; 1$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\max_{k\\le n}|X_k|^p \\right] \\le \\left( \\frac{p}{p-1} \\right) ^p \\mathbb{E} \\left[ \\left| X_n \\right| ^p \\right] .\\]</div><div></div><div></div><div></div><div>[Probability B8.1, Page 4]</div>"
  },
  {
    "front": "Martingales obtainable from an adapted process with independent increments. Specifically in the case of Brownian motion.",
    "back": "<div>Let $(Z_t)$ be a process adapted to $(\\mathcal{F}_t)$ such that $Z_t - Z_s$ is independent of $\\mathcal{F}_s$ for $0\\le s &lt; t$. Then <br><ol>  <li>$\\widetilde{Z}_t := Z_t - \\mathbb{E} \\left[ Z_t \\right] $ defines a martingale,&nbsp;&nbsp;&nbsp;</li>  <li>If $Z$ is square-integrable, then $\\widetilde{Z}_t^2 - \\mathbb{E} \\left[ \\widetilde{Z}_t^2 \\right] $ defines a martingale,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\vartheta \\in \\mathbb{R}$ such that $\\mathbb{E} \\left[ \\mathrm{e}^{\\vartheta Z_t} \\right] &lt;\\infty$ for all $t\\ge 0$, then $\\frac{\\mathrm{e}^{\\vartheta Z_t}}{\\mathbb{E} \\left[ \\mathrm{e}^{\\vartheta Z_t} \\right] }$ defines a martingale.</li></ol><br><br>If $(B_t)$ is a Brownian motion, then $B_t$, $B_t^2 - t$ and $\\mathrm{e}^{\\vartheta B_t - \\vartheta^2 t / 2}, \\, \\vartheta \\in \\mathbb{R},$ define martingales.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 3]</div>"
  },
  {
    "front": "Doob's (sub-)martingale inequalities in continuous time (for non-continuous processes)",
    "back": "<div>Let $(X_t)$ be a martingale or a non-negative submartingale. Then for $T\\ge 0$ and $\\lambda &gt; 0$,<br><ol>  <li>$\\displaystyle\\forall p\\ge 1\\colon&nbsp; \\mathbb{P}\\left( \\sup_{t\\in [0,T]\\cap \\mathbb{Q}} |X_t| \\ge \\lambda \\right) \\le \\frac{1}{\\lambda^p} \\mathbb{E} \\left[ \\left| X_T \\right| ^p \\right] $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle \\forall p &gt; 1\\colon&nbsp; \\mathbb{E} \\left[ \\sup_{t\\in [0,T]\\cap \\mathbb{Q}} |X_t|^p \\right] \\le \\left( \\frac{p}{p-1} \\right) ^p \\mathbb{E} \\left[ \\left| X_T \\right| ^p \\right] $.</li></ol><br>In particular, $\\sup_{t\\in [0,T]\\cap \\mathbb{Q}}|X_t| &lt; \\infty$ a.s. for all $T\\ge 0$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 4]</div>"
  },
  {
    "front": "Doob's maximal inequality for supermartingales in continuous time (as a consequence of the discrete version).",
    "back": "<div>Let $(X_t)$ be a supermartingale. Then for $T\\ge 0$ and $\\lambda &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{t\\in [0,T]\\cap \\mathbb{Q}}|X_t| \\ge \\lambda \\right) \\le \\frac{1}{\\lambda} \\left( \\mathbb{E} \\left[ X_0 \\right] + 2\\mathbb{E} \\left[ |X_T| \\right]&nbsp; \\right) .\\] In particular, $\\sup_{t\\in [0,T]\\cap \\mathbb{Q}}|X_t| &lt; \\infty$ a.s. for all $T\\ge 0$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 4]</div>"
  },
  {
    "front": "Bound on [$]\\mathbb{P}\\left( \\sup_{s\\le t}B_s \\ge \\lambda t \\right)[/$], respectively [$]\\mathbb{P}\\left( \\left| \\mathcal{N}(0,1) \\right| \\ge x \\right)[/$].",
    "back": "<div>If $t\\ge 0$ and $\\lambda &gt; 0$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{s\\le t}B_s \\ge \\lambda t \\right) \\le \\mathrm{e}^{-\\lambda^2 t / 2}.\\] Equivalently, for any $x\\in \\mathbb{R}$, \\[\\mathbb{P}\\left( \\left| \\mathcal{N}(0,1) \\right| \\ge x \\right) \\le \\mathrm{e}^{-\\frac{x^2}{2}}.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 4]</div>"
  },
  {
    "front": "Gaussian Poincaré inequality",
    "back": "<div>Let $X_1,\\ldots ,X_n \\sim \\mathcal{N}(0,1)$ and $f\\colon \\mathbb{R}^n\\to \\mathbb{R}$ continuously differentiable such that $\\mathbb{E} \\left[ f(X)^2 \\right] &lt;\\infty$, where $X := (X_1,\\ldots ,X_n)$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(f(X))\\le \\mathbb{E} \\left[ \\left\\|\\nabla f(X)\\right\\|^2 \\right] = \\sum_{j=1}^n \\mathbb{E} \\left[ \\partial_j f(X)^2 \\right] .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 3]</div>"
  },
  {
    "front": "Variance bound for [$]f(X_1,\\ldots,X_n)[/$] if [$]f[/$] has the BDP property.",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be independent and $Y=f(X_1,\\ldots ,X_n)$ square-integrable with $f$ having the BDP for constants $c_1,\\ldots ,c_n \\ge 0$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(Y) \\le \\frac{1}{2} \\sum_{k=1}^n c_k^2.\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 3/4]</div>"
  },
  {
    "front": "Variance bound for [$]g_n(X_1,\\ldots,X_n)[/$] if [$]g_n[/$] is a configuration function.<div>(Two examples)</div>",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be $S$-valued and independent and $g_n\\colon S^n \\to \\mathbb{N}_0$ a configuration function, $Y:= g_n(X_1,\\ldots ,X_n)$ square-integrable. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(Y) \\le \\mathbb{E} \\left[ Y \\right] .\\] <br><br><b>Examples:</b> <br><ol>  <li>&nbsp; $g_n(x_1,\\ldots ,x_n) = \\left| \\left\\{ x_i\\colon 1\\le i\\le n \\right\\}&nbsp; \\right| $. If $S = \\mathbb{N}$ and $X_1,\\ldots ,X_n$ independent, then $\\mathbb{E} \\left[ Y_n \\right] /n \\longrightarrow 0$ and thus $\\mathbb{V}(Y) = o(n)$.&nbsp;&nbsp;&nbsp;</li>  <li>$S=\\mathbb{R}$ and $\\text{inc} (x_1,\\ldots ,x_n)$ longest increasing subsequence. Then if $X_1,\\ldots ,X_n$ independent and absolutely continuous, we have (literature) $\\mathbb{E} \\left[ \\text{inc} (X_1,\\ldots ,X_n \\right] \\sim 2\\sqrt{n} $ and thus $\\mathbb{V}(\\text{inc} (X_1,\\ldots ,X_n)) = O(\\sqrt{n} )$.</li></ol></div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 4]</div>"
  },
  {
    "front": "McDiarmid's inequality (concentration for [$]f(X_1,\\ldots,X_n)[/$] if [$]f[/$] has BDP).",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be indepentent and $Y=f(X_1,\\ldots ,X_n)$ integrable, where $f$ has the BDP with constants $c_1,\\ldots ,c_n \\ge 0$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| Y - \\mathbb{E} \\left[ Y \\right]&nbsp; \\right| \\ge t \\right) \\le 2 \\exp \\left( -\\frac{2t^2}{\\sum_{i=1}^n c_i^2} \\right) .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 4; Proof of simple version of Page 5]</div>"
  },
  {
    "front": "Application of McDiarmid to random graphs.",
    "back": "<div>For $n\\in \\mathbb{N}$ let $G[n] := \\left\\{ G = (V,E) \\text{graph} \\colon V = \\left\\{ 1,\\ldots ,n \\right\\}&nbsp; \\right\\} $ and for $p \\in [0,1]$ let $G_{n,p}\\colon \\Omega\\to G[n]$ denote the Erdos-Renyi random graph.<br><br>\\noindent<b>Lemma.</b> Let $\\binom{V}{2} = \\sum_{j=1}^m B_j$ and $f\\colon G[n]\\to \\mathbb{R}$ such that $\\left| f(G) - f(G') \\right| \\le 1$ whenever $E(G) \\Delta E(G') \\subset B_k$ for some $k\\in \\left\\{ 1,\\ldots ,m \\right\\} $. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left(\\left| f(G_{n,p}) - \\mathbb{E} \\left[ f \\left( G_{n,p} \\right)&nbsp; \\right]&nbsp; \\right| \\ge t\\right) \\le 2\\exp \\left( -\\frac{2t^2}{m} \\right) .\\]<br><br>\\noindent <b>Corollary.</b> Let $f\\colon G[n]\\to \\mathbb{R}$ such that $\\left| f(G) - f(G') \\right| \\le 1$ whenever $G$ and $G'$ differ only by edges incident to a common vertex. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| f(G_{n,p})-\\mathbb{E} \\left[ f(G_{n,p}) \\right]&nbsp; \\right| \\ge t \\right) \\le 2\\exp\\left( -\\frac{2t^2}{n-1} \\right) .\\] <br><br>\\noindent <b>Example.</b> $f = \\chi\\colon G[n] \\to \\mathbb{N}$ chromatic number.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 4]</div>"
  },
  {
    "front": "Backwards (super-)submartingale convergence theorem.",
    "back": "<div>Let $(X_{-n})$ be a (super-)submartingale for which one of the two equivalent conditions hold.<br><ol>  <li>&nbsp;$(X_{-n})$ is $L^1$-bounded.&nbsp;&nbsp;&nbsp;</li>  <li>$\\lim\\limits_{n\\to \\infty} \\mathbb{E} \\left[ X_{-n} \\right] $ exists.</li></ol><br>Then $(X_{-n})$ is uniformly bounded and there exists $X_{-\\infty}\\in L^1$ which is $\\mathcal{F}_{-\\infty} := \\bigcap_{n=1}^\\infty \\mathcal{F}_{-n}$-measurable and such that \\[&nbsp;&nbsp;&nbsp; X_{-n} \\stackrel{  }{\\longrightarrow} X_{-\\infty} \\text{ almost surely and in $L^1$.}\\] Furthermore, \\[\\mathbb{E} \\left[X_{-n} \\,\\middle\\vert\\, \\mathcal{F}_{-\\infty}\\right] \\downarrow (\\uparrow) \\, X_{-\\infty} \\text{ almost surely and in $L^1$.}\\]</div><div></div><div></div><div></div><div>[Probability B8.1, Page 4/5]</div>"
  },
  {
    "front": "Existence of [$]\\lim\\limits_{t\\to \\infty} X_t[/$] for *martingales.",
    "back": "<div>Let $(X_t)$ be a (right-/left-)continuous *martingale such that $\\sup\\limits_{t\\ge 0}\\mathbb{E} \\left[ X_t^- \\right] &lt;\\infty$. Then $\\lim\\limits_{t\\to \\infty}X_t$ exists almost surely.</div><div></div><div></div><div></div><div>[Probability Theory B8.2, Page 4]</div>"
  },
  {
    "front": "Existence of rational limits for *martingales.",
    "back": "<div>Let $(X_t)$ be a *martingale. Then for $\\mathbb{P}$-almost all $\\omega\\in \\Omega$, \\[&nbsp;&nbsp;&nbsp; \\forall t &gt; 0\\colon \\, \\lim_{\\substack{r\\uparrow t \\\\ r\\in \\mathbb{Q}}} X_r(\\omega) \\text{ and } \\lim_{\\substack{r\\downarrow t\\\\r\\in \\mathbb{Q}}}X_r(\\omega) \\text{ exist and are finite} .\\]</div><div></div><div></div><div></div><div>[Probability Theory B8.2, Page 4]</div>"
  },
  {
    "front": "Regularisation theorem for *martingales (Existence of càdlàg modification of a *martingale).",
    "back": "<div>If $(X_t)$ is a *martingale with respect to a filtration $(\\mathcal{F}_t)$ that satisfies the usual conditions such that $t\\mapsto \\mathbb{E} \\left[ X_t \\right] $ is right-continuous, then $X$ admits a modification $\\widetilde{X}$ which is also a $(\\mathcal{F}_t)$-*martingale and has càdlàg paths.</div><div></div><div></div><div></div><div>[Probability Theory B8.2, Page 4]</div>"
  },
  {
    "front": "Conditional Hoeffding inequality.",
    "back": "<div>Let $Y$ be a random variable on $(\\Omega, \\mathcal{A}, \\mathbb{P})$, $\\mathcal{F}\\subset \\mathcal{A}$ and $X$ a random variable such that<br><ol>  <li>$X \\le Y \\le X + c$ a.s. for some $c \\ge 0$ and $\\mathbb{E} \\left[Y \\,\\middle\\vert\\, \\mathcal{F}\\right] = 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$X$ is $\\mathcal{F}$-measurable and $\\mathbb{E} \\left[ \\mathrm{e}^{sX} \\right] &lt; \\infty$ for all $s &gt; 0$.</li></ol><br>Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[\\mathrm{e}^{sY} \\,\\middle\\vert\\, \\mathcal{F}\\right]\\le \\exp \\left( \\frac{s^2 c^2}{8} \\right) \\] almost surely for all $s &gt; 0$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 4]</div>"
  },
  {
    "front": "Azuma-Hoeffding inequality (and its generalisation).",
    "back": "<div>Let $\\left\\{ \\varnothing, \\Omega \\right\\} = \\mathcal{F}_0 \\subset \\ldots \\subset \\mathcal{F}_n$ be a filtration, $(Z_i)$ be a predictable sequence and $c_i \\ge 0$ such that either hold.<br><ol>  <li>$X$ is an $\\mathcal{F}_n$-measurable random variable such that $Z_i \\le \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}_i\\right] \\le Z_i + c_i$ almost surely,&nbsp;&nbsp;&nbsp;</li>  <li>$(D_i)$ is an $(\\mathcal{F}_i)$-MDS with $Z_i \\le D_i \\le Z_i + c_i$ almost surely.</li></ol><br>Then for all $t &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| X - \\mathbb{E} \\left[ X \\right]&nbsp; \\right| \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\sum_{i=1}^n D_i \\right| \\end{cases} \\ge t \\right) \\le 2 \\exp \\left( -\\frac{2t^2}{\\sum_{i=1}^n c_i^2} \\right) .\\] <br><br><b>Azuma-Hoeffding inequality:</b> Let $(D_i)$ be a $(\\mathcal{F}_i)$-MDS with $\\mathcal{F}_0 = \\left\\{ \\varnothing, \\Omega \\right\\} $ and $D_i \\in [a_i,b_i]$ for all $i$ for some $a_i,b_i$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| \\sum_{i=1}^n D_i \\right| \\ge t \\right) \\le 2\\exp \\left( - \\frac{2t^2}{\\sum_{i=1}^n (b_i-a_i)^2} \\right) .\\] One-sided versions hold in all cases.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Definition (weighted) Hamming distance.",
    "back": "<div>Let $\\Gamma = \\times_{i=1}^n \\Gamma_i$ and $\\alpha \\in [0,\\infty)^n$. Then for $x,y\\in \\Gamma$ and $A \\subset \\Gamma$ measurable set \\[&nbsp;&nbsp;&nbsp; d_\\alpha(x,y) := \\sum_{i=1}^n \\alpha_i \\boldsymbol{1}\\left\\{x_i \\neq y_i\\right\\}, \\qquad d_\\alpha(x,A) := \\inf_{y\\in A} d_\\alpha(x, y).\\] This is a pseudo-metric and a metric if $\\alpha_i &gt; 0$ for all $i$.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Definition Talagrand-distance [$]d_T(x,A)[/$] and connection with [$]d_H(x,A)[/$].",
    "back": "<div>Let $\\Gamma = \\times_{i=1}^n \\Gamma_i$. Then for $x\\in \\Gamma$ and $A \\subset \\Gamma$ measurable set \\[&nbsp;&nbsp;&nbsp; d_T(x,A) := \\sup_{\\left\\|\\alpha\\right\\|=1} d_\\alpha(x,A).\\] Then \\[\\frac{1}{\\sqrt{n} } d_H(x,A) \\le d_T(x,A) \\le \\sqrt{d_H(x,A)} \\le d_H(x,A).\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Talagrand's theorem.",
    "back": "<div>Let $\\Gamma = \\times_{i=1}^n \\Gamma_i$, $X_1,\\ldots ,X_n$ independent, and $A\\subset \\Gamma$ measurable. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(X\\in A) \\mathbb{P}^\\star(d_T(X,A) \\ge t) \\le \\exp\\left( -\\frac{t^2}{4} \\right) .\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Definition [$]c[/$]-configuration function and connection with configuration function of a hereditary property.",
    "back": "<div>Let $\\Gamma = \\times_{i=1}^n \\Gamma_i$ and $c &gt; 0$. Then $f\\colon \\Gamma \\to [0,\\infty)$ is called a <i>$c$-configuration function</i> if for each $x\\in \\Gamma$ there exists an $\\alpha\\in [0,1]^n$, $\\left\\|\\alpha\\right\\| =1$ such that \\[&nbsp;&nbsp;&nbsp; f(y) \\ge f(x) - \\sqrt{cf(x)} d_\\alpha(x,y)\\] for all $y \\in \\Gamma$.<br><br>Then the configuration function $g_n\\colon S^n \\to \\left\\{ 0,\\ldots ,n \\right\\} $ of some hereditary property is a $1$-configuration function.</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Baby-Talagrand.",
    "back": "<div>Let $X_1,\\ldots ,X_n$ be independent with values in $\\Gamma_1,\\ldots ,\\Gamma_n$ and $A\\subset \\Gamma$ measurable. Then if $\\alpha \\in [0,1]^n$ with $\\left\\|\\alpha\\right\\|_2 = 1$ and $d_\\alpha(\\cdot ,A)$ is integrable (in particular measurable), then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(X \\in A) \\mathbb{P}(d_\\alpha(X,A) \\ge t) \\le \\exp\\left( -\\frac{t^2}{2} \\right) .\\] for all $t &gt; 0$. Equivalently, \\[\\mathbb{P}(d_\\alpha(X,A) &lt; t) \\ge 1 - \\frac{1}{\\mathbb{P}(X\\in A)} \\exp\\left( -\\frac{t^2}{2} \\right).\\]</div><div></div><div></div><div></div><div>[Probabilistic Combinatorics, Page 5]</div>"
  },
  {
    "front": "Discrete optional sampling for bounded stopping times.",
    "back": "<div>Let $(M_n)$ be a *martingale and $\\tau \\le \\rho$ bounded stopping times. Then (if $M_n \\stackrel{  }{\\longrightarrow} M_\\infty$ in $L^1$ and a.s.) \\[&nbsp;&nbsp;&nbsp; M_\\tau \\phantom{i}^\\star\\!\\!= \\mathbb{E} \\left[M_\\rho \\,\\middle\\vert\\, \\mathcal{F}_\\tau\\right] \\left( \\phantom{i}^\\star\\!\\!= \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_\\tau\\right] \\right) .\\]</div><div></div><div></div><div></div><div>[Probability B8.1, Page 5]</div>"
  },
  {
    "front": "Continuous martingale convergence theorem. What if [$]M[/$] is bounded in [$]L^p[/$]?",
    "back": "<div>Let $(X_t)$ be a rightcontinuous martingale. Then TFAE.<br><ol>  <li>$(X_t)$ is uniformly integrable,&nbsp;&nbsp;&nbsp;</li>  <li>$\\exists X_\\infty\\in L^1$ such that $X_t \\stackrel{  }{\\longrightarrow} X_\\infty$ a.s. and in $L^1$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\exists X_\\infty\\in L^1$ such that $X_t = \\mathbb{E} \\left[X_\\infty \\,\\middle\\vert\\, \\mathcal{F}_t\\right]$ for all $t\\ge 0$.</li></ol> If, additionally, $X$ is $L^p$-bounded, then $X_t \\stackrel{ L^p }{\\longrightarrow}X_\\infty$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 4]</div>"
  },
  {
    "front": "Characterisation of *martingales using [$]X_\\tau[/$] for bounded stopping times [$]\\tau[/$].",
    "back": "<div>An adapted right-continuous process $(X_t)$ is a martingale if and only if $X_\\tau \\in L^1$ and $\\mathbb{E} \\left[ X_\\tau \\right] = \\mathbb{E} \\left[ X_0 \\right] $ for all bounded stopping times $\\tau$.<br><br>An adapted right-continuous process $(X_t)$ is a *martingale if and only if $X_\\tau \\in L^1$ and $\\mathbb{E} \\left[ X_\\tau \\right] \\phantom{i}^\\star\\!\\!= \\mathbb{E} \\left[ X_T \\right] $ for all bounded stopping times $\\tau\\le T$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 5]</div>"
  },
  {
    "front": "Definition (total) variation [$]V_t(a)[/$], bounded and finite variation. Rightcontinuity of [$]V_\\cdot(a)[/$].",
    "back": "<div>Let $a\\colon (0,\\infty)\\to \\mathbb{R}$ be a right-continuous function. Then<br><ol>  <li>The <i>(total) variation</i> of $a$ over $(0,t]$ is \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V_t(a) := \\sup_{\\pi} \\sum_{k=1}^{n(\\pi)} \\left| a(t_k) - a(t_{k-1}) \\right|.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>$a$ is said to be of <i>finite variation</i> if $V_t(a) &lt;&nbsp; \\infty$ for all $t&gt; 0$.&nbsp;&nbsp;&nbsp;</li>  <li>$a$ is said to be of <i>bounded variation</i> if $\\sup_{t &gt; 0}V_t(a) &lt; \\infty$.</li></ol><br>The map $t \\mapsto&nbsp; V_t(a)$ is non-negative, increasing and right-continuous.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "Characterisation of functions with finite variation.",
    "back": "<div>Let $a\\colon [0,\\infty)\\to \\mathbb{R}$ (be right-continuous). Then $a$ is of finite variation if and only if $a$ is the difference of two (right-continuous) increasing functions.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "Definition signed measure and Jordan-decomposition.",
    "back": "<div>A <i>signed measure</i> on a measurable space $(E, \\mathcal{E})$ is a difference of two $\\sigma$-finite measures, $\\mu = \\mu_1 - \\mu_2$. Then $\\mu$ has a unique Jordan-decomposition \\[\\mu = \\mu_+ - \\mu_-\\] with $\\mu_+$ and $\\mu_-$ $\\sigma$-finite measures with disjoint supports.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "One-to-one correspondence between functions and signed measures.<div></div><div>Connection with Jordan-decomposition.</div>",
    "back": "<div>There is a one-to-one correspondence between right-continuous functions of finite variation $a$ with $a(0) = 0$ and signed measures $\\mu$ via $a(t) = \\mu((0,t])$.<br><br>If $\\mu = \\mu_+ - \\mu_-$ is the Jordan-decomposition and $\\left| \\mu \\right| := \\mu_+ + \\mu_-$, then \\[&nbsp;&nbsp;&nbsp; V_t(a) = \\left| \\mu\\right|((0,t])&nbsp; , \\quad \\mu_{\\pm}((0,t]) = \\frac{1}{2}\\left( V_t(a) \\pm a(t) \\right) .\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "Definition and properties of [$] (f\\cdot a)(t) = \\int_0^t f(t) \\mathop{}\\!\\mathrm{d} a(s)[/$].",
    "back": "<div>Let $a$ be right-continuous and of finite variation with associated signed measure (asm) $\\mu = \\mu_+ - \\mu_-$ and $f\\colon (0,\\infty) \\to \\mathbb{R}$ measurable. Then we say that $f$ is $a$<i>-integrable</i> if $f$ is $|\\mu|$-integrable. In that case, define \\[&nbsp;&nbsp;&nbsp; \\left( f\\cdot a \\right) (t) := \\int_0^t f(s) \\mathop{}\\!\\mathrm{d} a(s) := \\int_{(0,t]} f \\mathop{}\\!\\mathrm{d} \\mu := \\int_{(0,t]} f \\mathop{}\\!\\mathrm{d} \\mu_+ - \\int_{(0,t]} f \\mathop{}\\!\\mathrm{d} \\mu_-.\\] Then $f\\cdot a$ is right-continuous and of finite variation.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "Associativity of integration against signed measures ([$]g\\cdot(f\\cdot a) = gf \\cdot a[/$]).",
    "back": "<div>Let $a$ be right-continuous and of finite variation and $f,g \\colon (0,\\infty)\\to \\mathbb{R}$ measurable such that $f$ is $a$-integrable and $g$ is $(f\\cdot a)$-integrable. Then \\[&nbsp;&nbsp;&nbsp; \\int_0^t g(s) \\mathop{}\\!\\mathrm{d} (f\\cdot a)(s) = \\int_0^t g(s)f(s) \\mathop{}\\!\\mathrm{d} a(s),\\\\\\] that is, $g\\cdot (f\\cdot a) = gf \\cdot a$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 6]</div>"
  },
  {
    "front": "Integration by parts for Stieltjes integral.",
    "back": "<div>Let $a,b$ be continuous functions of finite variation. Then \\[&nbsp;&nbsp;&nbsp; a(t)b(t) - a(0)b(0) = \\int_0^t a(s) \\mathop{}\\!\\mathrm{d} b(s) + \\int_0^t b(s) \\mathop{}\\!\\mathrm{d} a(s).\\]</div><div></div><div></div><div>[Probability B8.2, Page 7]</div>"
  },
  {
    "front": "[$]int f(s) \\mathop{}\\!\\mathrm{d} a(s)[/$] for [$] a\\in C^2[/$] and Chain-rule ([$]F(a(t)) - F(a(0)) = ?[/$])",
    "back": "<div>If $a\\in C^2$, then [$]a[/$] is of finite variation and $\\int f(s) \\mathop{}\\!\\mathrm{d} a(s) = \\int f(s) a'(s) \\mathop{}\\!\\mathrm{d} s$.\\\\</div><div><br><div>If $F\\in C^1$ and $a$ is continuous of finite variation then $F \\circ a$ is continuous of finite variation and \\[&nbsp;&nbsp;&nbsp; F(a(t)) - F(a(0)) = \\int_0^t F'(a(s)) \\mathop{}\\!\\mathrm{d} a(s).\\]</div><div></div><div></div><div>[Probability B8.2, Page 7]</div></div>"
  },
  {
    "front": "Definition finite variation process [$]A[/$] and [$](K\\cdot A)_t[/$].",
    "back": "<div>An adapted process $A$ is called a <i>finite variation process</i> if $A_0(\\omega) = 0$ and $t\\mapsto A_t(\\omega)$ is right-continuous and of finite variation for all $\\omega\\in \\Omega$.<br><br>In that case, let $K$ be progressively measurable and <i>$A$-integrable</i>, that is, \\[&nbsp;&nbsp;&nbsp; \\int_0^t \\left| K_s(\\omega) \\right| \\left|\\mathop{}\\!\\mathrm{d} A_s(\\omega) \\right| &lt; \\infty,\\quad t &gt; 0, \\omega\\in \\Omega.\\] Then the process $K\\cdot A$ defined by \\[&nbsp;&nbsp;&nbsp; (K\\cdot A)_t(\\omega) := \\int_0^t K_s(\\omega) \\mathop{}\\!\\mathrm{d} A_s(\\omega), \\quad t &gt; 0, \\omega\\in \\Omega\\\\\\] is a finite variation process.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 7]</div>"
  },
  {
    "front": "Integration against stopped finite variation process [$](K\\cdot A^\\tau)_t[/$].",
    "back": "<div>Let $A$ be a finite variation process and $K$ progressively measurable and $A$-integrable. Then if $\\tau$ is a stopping time, \\[&nbsp;&nbsp;&nbsp; \\left( K\\cdot A^\\tau \\right) _t = \\left( K\\cdot A \\right) _{t\\wedge \\tau} = \\left( K \\boldsymbol{1}_{[0,\\tau]}\\cdot A \\right) _t,\\quad t\\ge 0.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 7]</div>"
  },
  {
    "front": "Definition continuous local martingale. Sums of and stopped clm's are clm's.",
    "back": "<div>A <i>continuous local martingale</i> is an adapted process $(M_t)$ with continuous paths such that there exists a sequence $\\tau_n \\uparrow \\infty$ of stopping times such that $M^{\\tau_n}$ is a martingale for each $n\\in \\mathbb{N}$.<br><br>If $N$ is another continuous local martingale and $\\rho$ a stopping time, then $M+N$ and $M^{\\rho}$ are also continuous local martingales.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 8]</div>"
  },
  {
    "front": "When is a clm a (super-)martingale?",
    "back": "<div>Let $M$ be a continuous local martingale.<br><ol>  <li>If $M_t \\ge 0$ for all $t\\ge 0$, then $M$ is a supermartingale,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\left| M_t \\right| \\le \\xi$ for all $t\\ge 0$ for some $\\xi \\in L^1$, then $M$ is a martingale&nbsp;&nbsp;&nbsp;</li>  <li>$M$ is a martingale if and only if $\\left\\{ M_\\tau\\colon \\tau \\le N \\text{ stopping time}&nbsp; \\right\\} $ is uniformly integrable for all $N &gt; 0$.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 8]</div>"
  },
  {
    "front": "Characterisation of a clm [$]M[/$] with [$]M_0 = 0[/$] a.s. being indistinguishable from zero.",
    "back": "<div>Let $M$ be a continuous local martingale with $M_0 = 0$ a.s. Then TFAE.<br><ol>  <li>$M$ is indistinguishable from zero,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left&lt;M \\right&gt; _t = 0$ for all $t \\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$M$ is a finite variation process.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 9]</div>"
  },
  {
    "front": "Quadratic variation of a continuous local martingale.<div></div><div>What is the quadratic variation of [$]M^\\tau[/$]? Connection with stochastic integral?</div>",
    "back": "<div>Let $M$ be a continuous local martingale. Then there exists a unique (up to indistinguishability) increasing adapted process $\\left&lt;M,M \\right&gt; _t$ with $\\left&lt;M,M\\right&gt;_0 = 0$ such that (i) holds. In that case, (ii) holds as well.<br><ol>  <li>$\\left( M_t^2 - \\left&lt;M,M \\right&gt; _t \\right) _{t\\ge 0}$ is a continuous local martingale,&nbsp;&nbsp;&nbsp;</li>  <li>$\\forall T &gt; 0\\colon \\text{QV} _T^{\\pi} (M) \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\left&lt;M,M \\right&gt; _t$ as $\\delta(\\pi) \\stackrel{  }{\\longrightarrow} 0$.</li></ol><br><br><b>Remark:</b> <ol>  <li>If $\\tau$ is a stopping time, then $\\left&lt;M^\\tau,M^\\tau \\right&gt;_t = \\left&lt;M,M \\right&gt; _{t\\wedge \\tau}$,&nbsp;&nbsp;&nbsp;</li>  <li>$M_t^2 - \\left&lt;M,M \\right&gt; _t = 2 \\int_0^t M_s \\mathop{}\\!\\mathrm{d} M_s$.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 8]</div>"
  },
  {
    "front": "[$]\\mathbb{H}^2[/$] and [$]H^2[/$] (Definition, identification with [$]L^2[/$], [$]H^2[/$] closed subset)",
    "back": "<div>\\[&nbsp;&nbsp;&nbsp; \\mathbb{H}^2:= {\\raisebox{.2em}{$\\left\\{ M\\colon M \\text{ is an $L^2$-bounded cadlag martingale}&nbsp; \\right\\}$}\\left/\\raisebox{-.2em}{$\\sim$}\\right.} \\cong L^2(\\Omega,\\mathcal{F}_\\infty,\\mathbb{P})\\] where $\\sim$ indicates indistinguishability and the identification goes via $M \\mapsto M_\\infty$. Hence $\\mathbb{H}^2$ becomes a Hilbert space with <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left&lt;M,N \\right&gt; _{\\mathbb{H}^2}&amp;:= \\left&lt;M_\\infty,N_\\infty \\right&gt; _{L^2} = \\sqrt{\\mathbb{E} \\left[ M_\\infty N_\\infty \\right] } \\\\&nbsp;&nbsp;&nbsp; \\left\\|M\\right\\|_{\\mathbb{H}^2} &amp;= \\left\\|M_\\infty\\right\\|_{L^2}.\\end{align*}[/$$]<br>Then \\[H^2 := \\left\\{ M\\in \\mathbb{H}^2\\colon M \\text{ is continuous}&nbsp; \\right\\} \\] is a closed subset of $\\mathbb{H}^2$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 8]</div>"
  },
  {
    "front": "Characterisation of a clm [$]M[/$] with [$]M_0 = 0[/$] a.s. being an [$]L^2[/$]-bounded/square-integrable martingale.<div><div></div><div>If [$]M\\in H^2_0[/$], what's the relationship between [$]\\left\\|M\\right\\|_{\\mathbb{H}^2}[/$] and [$] \\left&lt;M\\right&gt;_\\infty[/$]?</div></div>",
    "back": "<div>Let $M$ be a continuous local martingale with $M_0 = 0$ a.s. Then <br><ol>  <li>$M$ is an $L^2$-bounded martingale ($M\\in H^2$) iff $\\mathbb{E} \\left[ \\left&lt;M \\right&gt; _\\infty \\right] &lt;\\infty $. In that case, $M^2 - \\left&lt;M \\right&gt; $ is a uniformly integrable martingale,&nbsp;&nbsp;&nbsp;</li>  <li>$M$ is a square-integrable martingale iff $\\mathbb{E} \\left[ \\left&lt;M \\right&gt; _t \\right] &lt; \\infty$ for all $t\\ge 0$. In that case, $M^2- \\left&lt;M \\right&gt; $ is a martingale.</li></ol><br>If $M\\in H^2$ with $M_0=0$ a.s., then $\\mathbb{E} \\left[ M_\\infty^2 \\right] = \\mathbb{E} \\left[ \\left&lt;M \\right&gt; _\\infty \\right] $.</div><div></div><div><div></div></div><div></div><div>[Probability B8.2, Page 8/9]</div>"
  },
  {
    "front": "Definition and characterisation of quadratic co-variation.<div></div><div>Properties of the map [$](M,N) \\mapsto \\left&lt;M,N\\right&gt;[/$].</div><div>What is [$]\\left&lt;M^\\tau,N^\\tau\\right&gt;[/$]?</div>",
    "back": "<div>Im $M$ and $N$ are continuous local martingales, then the <b>quadratic co-variation</b> or <b>cross-variation</b> between them is \\[&nbsp;&nbsp;&nbsp; \\left&lt;M,N \\right&gt; := \\frac{1}{2} \\left( \\left&lt;M+N \\right&gt; - \\left&lt;M \\right&gt; -\\left&lt;N \\right&gt;&nbsp; \\right) .\\] It is the unique (up to indist.) finite variation process for which $(MN - \\left&lt;M,N \\right&gt; )$ is a continuous local martingale. For every $T &gt; 0$, \\[\\sum_{j=1}^{n(\\pi)} \\left( M_{t_j}- M_{t_{j-1}} \\right) \\left( N_{t_j} - N_{t_{j-1}} \\right) \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\left&lt;M,N \\right&gt; _t\\\\\\] as $\\delta(\\pi) \\to 0$. The following hold.<br><ol>  <li>The map $(M,N) \\mapsto \\left&lt;M,N \\right&gt; $ is bilinear and symmetric,</li>  <li>If $\\tau$ is a stopping time, then \\[\\left&lt;M^\\tau,N^\\tau \\right&gt; _t = \\left&lt;M^\\tau,N \\right&gt;_t = \\left&lt;M,N^\\tau \\right&gt; _t = \\left&lt;M,N \\right&gt; _{t\\wedge\\tau}, \\quad t\\ge 0.\\]</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 9]</div>"
  },
  {
    "front": "Kunita-Watanabe inequality",
    "back": "<div><b>Kunita-Watanabe inequality:</b> Let $M,N$ be clm's and $K,H$ progressively measurable. Then for all $t\\in [0,\\infty]$,\\[&nbsp;&nbsp;&nbsp; \\int_0^t \\left| K_s \\right| \\left|H_s \\right| \\left|\\mathop{}\\!\\mathrm{d} \\left&lt;M,N \\right&gt; _s \\right| \\le \\left( \\int_0^tK_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;M \\right&gt; _s \\right) ^{1 / 2} \\left( \\int_0^t H_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;N \\right&gt; _s \\right) ^{1 / 2}.\\] In particular, if $p\\ge 1$ and $\\frac{1}{p} + \\frac{1}{q} = 1$, then for all $t\\in [0,\\infty]$,<br>\\begin{equation*}<br>\\mathbb{E} \\left[ \\int_0^t \\left| K_s \\right| \\left| H_s \\right| \\left| \\mathop{}\\!\\mathrm{d} \\left&lt;M,N \\right&gt; _s \\right|&nbsp; \\right] \\le \\left( \\mathbb{E} \\left[ \\left( \\int_0^t K_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;M \\right&gt; _s \\right) ^{p / 2} \\right]&nbsp; \\right) ^{1 / p} \\left( \\mathbb{E} \\left[ \\left( \\int_0^t H_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;N \\right&gt; _s \\right) ^{q /2} \\right]&nbsp; \\right) ^{1 / q}.<br>\\end{equation*}</div><div></div><div></div><div></div><div>[Probability B8.2, Page 9]</div>"
  },
  {
    "front": "Continuous semimartingales (definition, quadratic (co-)variation).",
    "back": "<div>A stochastic process $X$ is called <b>continuous semimartingale</b> (w.r.t. $(\\mathcal{F}_t)$ and $\\mathbb{P}$) if it is of the form \\[X_t = X_0 + M_t + A_t, \\quad t\\ge 0\\\\\\] for a continuous local martingale $M$ and a continuous finite variation process $A$ with $M_0 = A_0 = 0$ a.s. The decomposition is unique.<br><br><ol>  <li>A continuous semimartingale is of finite quadratic variation and $\\left&lt;X \\right&gt; = \\left&lt;M \\right&gt; $,&nbsp;&nbsp;&nbsp;</li>  <li>If $Y = Y_0 + N + A'$ is a continuous semimartingale, then $\\left&lt;X,Y \\right&gt; = \\left&lt;M,N \\right&gt; $.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 9]</div>"
  },
  {
    "front": "Stochastic integration w.r.t. [$]L^2[/$]-bounded martingales: Ansatz with limiting procedure<div></div><div>1. Definition for simple functions</div><div>2. [$]L^2(M)[/$]</div><div>3. Extension to linear isometry [$]L^2(M) \\to H_0^2;\\, \\varphi \\mapsto \\varphi \\bullet M[/$]</div>",
    "back": "<div>Let $M\\in H^2$.<br><br><b>Simple functions:</b> Define the set of simple functions \\[&nbsp;&nbsp;&nbsp; \\mathcal{E}:= \\left\\{ \\sum_{j=1}^m \\varphi ^{(i)} \\boldsymbol{1}_{[t_{j-1},t_j)}\\colon&nbsp; 0\\le t_0 &lt; \\ldots &lt;t_m, \\varphi ^{(i)}\\, \\mathcal{F}_{t_i}\\text{-mb and bounded} \\right\\}.\\] For $\\varphi \\in \\mathcal{E}$, define $\\left( \\varphi \\bullet M \\right) _t := \\sum_{i=1}^m \\varphi ^{(i)} \\left( M_{t\\wedge t_{j}} - M_{t\\wedge t_{j-1}} \\right) $. Then $\\varphi \\mapsto \\varphi \\bullet M \\in H^2_0$ is linear and $\\left&lt;\\varphi \\bullet M,N \\right&gt; = \\varphi \\cdot \\left&lt;M,N \\right&gt; $ for $M,N\\in H^2$.<br><br>Set<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; L^2(M) &nbsp;&nbsp;&nbsp; &amp;:= \\left\\{ K \\text{ pr. mb. with } \\left\\|K\\right\\|_{L^2(M)} := \\mathbb{E} \\left[ \\left( K^2 \\cdot \\left&lt;M \\right&gt;&nbsp; \\right) _{\\infty} \\right] &lt;\\infty&nbsp; \\right\\}\\\\&nbsp;&nbsp;&nbsp; &amp;= L^2\\left( [0,\\infty) \\otimes \\Omega, \\mathcal{F}_\\text{pr} , \\mu_M \\right) ,\\\\\\end{align*}[/$$]<br>where $\\mathcal{F}_\\text{pr} := \\sigma\\left( K\\colon K \\text{ pr mb}&nbsp; \\right) $ and $\\mu_M(\\cdot ) := \\int_\\Omega \\int_0^\\infty \\boldsymbol{1}\\left\\{(s,\\omega)\\in \\cdot \\right\\} \\mathop{}\\!\\mathrm{d} \\left&lt;M \\right&gt; _s(\\omega) \\mathbb{P}(\\mathop{}\\!\\mathrm{d} \\omega)$. Then $\\mathcal{E}\\subset L^2(M)$ is dense.<br><br><b>Theorem:</b> The mapping $\\varphi \\mapsto \\varphi \\bullet M$ can be extended to a linear isometry $L^2(M) \\to H_0^2$, denote $\\left( K\\bullet M \\right) _t =: \\int_0^t K_s \\mathop{}\\!\\mathrm{d} M_s$.<br><ol>  <li>Isometry property: $\\mathbb{E} \\left[ \\int_0^\\infty K_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;M \\right&gt; _s \\right] = \\mathbb{E} \\left[ \\left( \\int_0^t K_s \\mathop{}\\!\\mathrm{d} M_s \\right) ^2 \\right] $.&nbsp;&nbsp;&nbsp;</li>  <li>If $\\varphi_n \\stackrel{ L^2(M) }{\\longrightarrow} K$, then $\\varphi_n \\bullet M \\stackrel{ H_0^2 }{\\longrightarrow} K \\bullet M$.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 10]</div>"
  },
  {
    "front": "Stochastic integration w.r.t. [$]L^2[/$]-bounded martingales: Intrinsic characterisation using co-variation.",
    "back": "<div>Let $M\\in H^2$. Then for any $K\\in L^2(M)$ there exists a unique $K\\bullet M\\in H_0^2$ such that \\[\\left&lt;K\\bullet M,N \\right&gt; = K\\cdot \\left&lt;M,N \\right&gt; ,\\quad N\\in H^2.\\] Furthermore, the map $L^2(M) \\ni K \\mapsto K\\bullet M \\in H_0^2$ is a linear isometry.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 10]</div>"
  },
  {
    "front": "Associativity of stochastic integration against [$]M\\in H^2[/$] and stopped stochastic integral.",
    "back": "<div>Let $M\\in H^2$ and $K\\in L^2(M)$.<br><ol>  <li>If $F\\in L^2(K\\bullet M)$, then $FK\\in L^2(M)$ and&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; F\\bullet \\left( K\\bullet M \\right) = FK \\bullet M.&nbsp;&nbsp;&nbsp; \\]</li>  <li>If $\\tau$ is a stoppting time, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left( K\\bullet M \\right) ^\\tau = K\\bullet M^\\tau = K \\boldsymbol{1}_{[0,\\tau]} \\bullet M.\\]</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 10/11]</div>"
  },
  {
    "front": "Extension of [$]K\\bullet M[/$] to continuous local martingales [$]M[/$] (and [$]K \\in L^2_\\text{loc}[/$])",
    "back": "<div>Let $M$ be a continuous local martingale. Then set \\[&nbsp;&nbsp;&nbsp; L^2_{\\text{loc}}(M) := \\left\\{ K \\text{ pr. mb.} \\colon \\forall t\\ge 0\\colon \\int_0^t K_s^2 \\mathop{}\\!\\mathrm{d} \\left&lt;M \\right&gt; _s &lt; \\infty \\text{ a.s.}&nbsp; \\right\\} .\\]<br><br><b>Theorem.</b> Let $M$ be a continuous local martingale. Then for any $K\\in L^2_\\text{loc} (M)$ there exists a unique continuous local martingale $K\\bullet M$ such that \\[\\left&lt;K\\bullet M, N \\right&gt; = K\\cdot \\left&lt;M,N \\right&gt;\\] for all continuous local martingales $N$. If $M\\in H^2$ and $K\\in L^2(M)$, this definition coincides with the previous one.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 11]</div>"
  },
  {
    "front": "Definition local boundedness of a process and connection to continuity and integrability w.r.t. finite variation processes and continuous local martingales.",
    "back": "<div>A process $K$ is called <b>locally bounded</b> if it is progressively measurable and, almost-surely, \\[\\forall t\\ge 0\\colon \\sup_{s\\le t} \\left| K_s \\right| &lt; \\infty.\\] <br><ol>  <li>If $K$ is locally bounded and $A$ is a finite variation process, then a.s., \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^t \\left| K_s \\right| \\left| \\mathop{}\\!\\mathrm{d} A_s \\right| &lt; \\infty\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>If $K$ is locally bounded and $M$ a continuous local martingale, then $K\\in L^2_\\text{loc} (M)$.&nbsp;&nbsp;&nbsp;</li>  <li>If $K$ is continuous, it is locally bounded.</li></ol></div><div></div><div></div><div></div><div>[Probability B8.2, Page 11]</div>"
  },
  {
    "front": "Definition Itô-integral.<div></div><div>Properties:</div><div>-integral for simple integrands</div><div>-associativity</div><div>-stopped integral</div><div>-sensitivity to change of measure</div>",
    "back": "<div>Let $X = X_0 + M + A$ be a continuous semimartingale and $K$ locally bounded. Then the <b>It\\^{o}-integral</b> of $K$ with respect to $X$ is defined by \\[K \\bullet X := K\\bullet M + K\\cdot A.\\] The following properties hold.<br><ol>  <li>The map $(K,X)\\mapsto K\\bullet X$ is bilinear,&nbsp;&nbsp;&nbsp;</li>  <li>For $\\varphi \\in \\mathcal{E}$, $\\left( \\varphi \\bullet X \\right) _t = \\sum_{j=1}^m \\varphi^{(j)} \\left( X_{t_j\\wedge t}- X_{t_{j-1}\\wedge t} \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>If $F$ is locally bounded, then $F\\bullet\\left( K\\bullet X \\right) = FK \\bullet X$,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\tau$ is a stopping time, then $K\\bullet X^\\tau = \\left( K\\bullet X \\right) ^\\tau = K \\boldsymbol{1}_{[0,\\tau]}\\bullet X$.</li></ol><br><b>Remark:</b> The It\\^{o}-integral is invariant under equivalent change of measure.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 11]</div>"
  },
  {
    "front": "Stochastic dominated convergence (DOM for Itô-integral)",
    "back": "<div>Let $X$ be a continuous semimartingale, $K^{(n)}$ a sequence of locally bounded processes with $K^{(n)}_t \\to 0$ for all $t\\ge 0$ a.s. and $\\left| K^{(n)} \\right| \\le K$ a.s. for a locally bounded process $K$. Then \\[&nbsp;&nbsp;&nbsp; \\forall t\\ge 0\\colon \\sup_{s\\le t} \\left| \\int_0^s K^{(n)}_u \\mathop{}\\!\\mathrm{d} X_u \\right| \\stackrel{ \\mathbb{P} }{\\longrightarrow} 0.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 11]</div>"
  },
  {
    "front": "Itô-integral as limit of simple integrals.",
    "back": "<div>Let $X$ be a continuous semimartingale and $K$ a locally bounded, <i>left-continuous</i> process. Then \\[&nbsp;&nbsp;&nbsp; \\sum_{t_j\\in \\pi_n}K_{t_j}\\left( X_{t_{j+1}}-X_{t_j} \\right) \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\int_0^t K_s \\mathop{}\\!\\mathrm{d} X_s\\\\\\] locally uniformly if $\\delta(\\pi_n)\\stackrel{ n\\to \\infty }{\\longrightarrow} 0$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 11]</div>"
  },
  {
    "front": "Integration by parts for Itô-integral.",
    "back": "<div>If $X$ and $Y$ are continuous semimartingales then a.s., \\[X_t Y_t = X_0Y_0 + \\int_0^t X_s \\mathop{}\\!\\mathrm{d} Y_s + \\int_0^t Y_s \\mathop{}\\!\\mathrm{d} X_s + \\left&lt;X,Y \\right&gt; _t, \\quad t\\ge 0.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 12]</div>"
  },
  {
    "front": "Itô&nbsp;formula",
    "back": "<div>Let $d\\in \\mathbb{N}$, $X^1, \\ldots ,X^d$ continuous semimartingales and $F\\in C^2(\\mathbb{R}^d,\\mathbb{R})$. Then $\\left( F\\left( X^1_t,\\ldots ,X^d_t \\right)&nbsp; \\right) _{t\\ge 0}$ is a continuous semimartingale and a.s. for all $t\\ge 0$,<br>\\begin{multline*}<br>&nbsp;&nbsp;&nbsp; F\\left( X^1_t,\\ldots ,X^d_t \\right) = F\\left( X^1_0,\\ldots ,X_0^d \\right) + \\sum_{i=1}^d \\int_0^t \\frac{\\partial F}{\\partial x_i} \\left( X^1_s,\\ldots ,X^d_s \\right) \\mathop{}\\!\\mathrm{d} X^i_s \\\\+ \\frac{1}{2} \\sum_{i,j=1}^d \\int_0^t \\frac{\\partial ^2 F}{\\partial x_i \\partial x_j} \\left( X^1_s,\\ldots ,X^d_s \\right) \\mathop{}\\!\\mathrm{d} \\left&lt;X^i,X^j \\right&gt; _s.<br>\\end{multline*}<br>For $d=1$, this becomes&nbsp; \\[&nbsp;&nbsp;&nbsp; F(X_t) = F(X_0) + \\int_0^t F'(X_s) \\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2} \\int_0^t F''(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s.\\]</div><div></div><div></div><div></div><div>[Probability B8.2, Page 12]</div>"
  },
  {
    "front": "Solution to [$] \\mathop{}\\!\\mathrm{d} Y_t = \\lambda Y_t \\mathop{}\\!\\mathrm{d} M_t[/$] for [$]\\lambda \\in \\mathbb{C}[/$] and [$]M[/$] a clm.",
    "back": "<div>Let $M$ be a clm (or $\\in H^2$) and $\\lambda \\in \\mathbb{C}$. Then \\[&nbsp;&nbsp;&nbsp; Y_t := \\exp \\left( \\lambda M_t - \\frac{\\lambda^2}{2} \\left&lt;M \\right&gt; _t \\right) , \\quad t\\ge 0\\\\&nbsp;&nbsp;&nbsp; \\] defines a clm (or $\\in H^2_0$) (real- and imaginary part-wise). It is the solution to the stochastic differential equation \\[\\mathop{}\\!\\mathrm{d} Y_t = \\lambda Y_t \\mathop{}\\!\\mathrm{d} M_t,\\\\\\] which is shorthand for $Y_t = Y_0 + \\lambda \\int_0^t Y_s \\mathop{}\\!\\mathrm{d} M_s$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 12]</div>"
  },
  {
    "front": "Levy's characterisation of Brownian motion.",
    "back": "<div>Let $M$ be a continuous local martingale with $M_0 = 0$ a.s. Then $M$ is a standard Brownian motion if and only if $\\left&lt;M \\right&gt; _t = t$ a.s. for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[Probability B8.2, Page 12]</div>"
  },
  {
    "front": "Theorem of Dambis, Dubins and Schwarz.",
    "back": "<div>Let $M$ be a continuous $\\left( \\left( \\mathcal{F}_t \\right) ,\\mathbb{P} \\right) $-local martingale with $M_0 = 0$ and $\\left&lt;M \\right&gt; _\\infty = \\infty$ a.s. Set $$\\sigma_t := \\inf\\left\\{ s\\ge 0\\colon \\left&lt;M \\right&gt; _s &gt; t \\right\\} = \\max\\left\\{s\\ge 0 \\colon \\left&lt;M\\right&gt; = t\\right\\} .$$ for $t\\ge 0$. Then the process \\(B := \\left( M_{\\sigma_t} \\right) _{t\\ge 0}\\) is an $(( \\mathcal{F}_{\\sigma_t} ) ,\\mathbb{P} )$ - standard Brownian motion and \\[M_t = B_{\\left&lt;M \\right&gt; _t} \\text{ a.s. } \\forall t\\ge 0.\\]</div><div></div><div></div><div></div><div>[SDEs Page 5/6]</div>"
  },
  {
    "front": "Definition [$]X \\sim \\text{Markov}(\\lambda, P)[/$] and [$]p_{ij}^{(n)}[/$] and distribution of [$]X_n[/$].",
    "back": "<div>Let $I\\neq \\varnothing$ be countable and $\\lambda = (\\lambda_i)_{i\\in I}$ a probability measure on $I$ and $P = (p_{ij})_{i,j\\in I} \\in [0,1]^{I\\times I}$ a matrix in which each row sums to one. Then a <i>(homogenous) Markov chain</i> $X\\sim \\text{Markov} (\\lambda,P)$ is a sequence of random variables $X = (X_n)_{n\\in \\mathbb{N}_0}$ such that $X_0\\sim \\lambda$ and \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(X_n = i_{n+1} \\,\\middle\\vert\\, X_1 = i_1,\\ldots ,X_n = i_n\\right) = \\mathbb{P} \\left(X_{n+1} = i_{n+1} \\,\\middle\\vert\\,X_n = i_n \\right) = p_{i_n,i_{n+1}}\\] for all $n\\in \\mathbb{N},\\, i_1,\\ldots ,i_{n+1} \\in I$. Then<br><ol>  <li>$p_{ij}^{(n)} := \\mathbb{P} \\left(X_n = j \\,\\middle\\vert\\, X_0 = i\\right) = (P^n)_{ij}$,&nbsp;&nbsp;&nbsp;</li>  <li>$X_n \\sim \\lambda P^n$ for $n\\in \\mathbb{N}$.</li></ol></div><div></div><div></div><div></div><div>[Markov Chains, Page 1]</div>"
  },
  {
    "front": "Definition of [$]i\\to j[/$], (open/closed) classes and irreducibility.",
    "back": "<div>Let $i,j\\in I$. We say that <i>$i$ leads to $j$</i>, written $i\\to j$, if $p_{ij}^{(n)} &gt; 0$ for some $n\\in \\mathbb{N}_0$. Then \\[i \\leftrightarrow j \\mathrel{:\\Longleftrightarrow} i\\to j \\text{ and } j \\to i\\\\\\] defines an equivalence relation on $I$, the equivalence classes are called <i>(communicating) classes</i> of the Markov chain.<br><ol>  <li>A class $C\\subset I$ is called <i>closed</i> if $i\\not\\to j$ whenever $i\\in C,\\, j\\not\\in C$, and <i>open</i> otherwise.&nbsp;&nbsp;&nbsp;</li>  <li>A Markov chain/transition matrix is called <i>irreducible</i> if there is only one class, that is, if $i\\to j$ for all $i,j\\in I$.</li></ol></div><div></div><div></div><div></div><div>[Markov Chains, Page 1]</div>"
  },
  {
    "front": "Definition of period and (a-)periodicity. The period is a class property.",
    "back": "<div>The <i>period</i> $p$ of a state $i\\in I$ is the gcd of the set $\\{ n\\in \\mathbb{N}\\colon p_{ii}^{(n)} &gt; 0 \\} $, or undefined if it is empty. A state is called <i>aperiodic</i> if $p=1$ and <i>periodic</i> otherwise.<br><br>The period is a <i>class property</i>, that is, all states in a class have the same period.</div><div></div><div></div><div></div><div>[Markov Chains, Page 1]</div>"
  },
  {
    "front": "Characterisation of aperiodicity of a state.",
    "back": "<div>A state $i\\in I$ is aperiodic if and only if $p_{ii}^{(n)} &gt; 0$ for all but finitely many $n\\in \\mathbb{N}$.</div><div></div><div></div><div></div><div>[Markov Chains, Page 1]</div>"
  },
  {
    "front": "Markov Chains: hitting probabilities and characterising equations.",
    "back": "<div>Let $i\\in I$ and $A\\subset I$. The <i>hitting probability of $A$ starting from $i$</i> is \\[&nbsp;&nbsp;&nbsp; h_i^A := \\mathbb{P} \\left(X_n \\in A \\text{ for some $n\\in \\mathbb{N}_0$}&nbsp; \\,\\middle\\vert\\, X_0 = i\\right).\\] The vector $(h_i^A)_{i\\in I}$ is the unique minimal non-negative solution to the set of equations \\[h_i^A = \\begin{cases}&nbsp;&nbsp;&nbsp; 1 &amp;,i\\in A,\\\\&nbsp;&nbsp;&nbsp; \\sum_{j\\in I} p_{ij}h_j^A&amp;,i\\not\\in A.\\end{cases}\\]</div><div></div><div></div><div></div><div>[Markov Chains, Page 1]</div>"
  },
  {
    "front": "Definition of recurrence and transience, characterisation of recurrence.<div>Distribution of numbers of (re-)visits to a state [$]i[/$] if [$]X_0 = i[/$]</div><div>Recurrence is a class property.</div>",
    "back": "<div>Set $p_i := \\mathbb{P} \\left(X_n = i \\text{ for some $i\\in \\mathbb{N}$}&nbsp; \\,\\middle\\vert\\, X_0 = i\\right)$ for $i\\in I$. Put $R_i := \\left| \\left\\{ n\\in \\mathbb{N}\\colon X_n = i \\right\\}&nbsp; \\right| $. Then $i$ is called<br><ol>  <li><i>transient</i> if $p_i &lt; 1$. In that case, $R_i \\sim \\text{G} (1-p)$ under the probability measure $\\mathbb{P} \\left(\\,\\cdot \\, \\,\\middle\\vert\\, X_0 = i\\right)$,&nbsp;&nbsp;&nbsp;</li>  <li><i>recurrent</i> if $p_i = 1$. In that case, $\\mathbb{P} \\left(R_i = \\infty \\,\\middle\\vert\\, X_0 = i\\right)= 1$.</li></ol><br>Recurrence/transience is a class property and $i\\in I$ is recurrent if and only if \\[&nbsp;&nbsp;&nbsp; \\sum_{n=1}^\\infty p_{ii}^{(n)} = \\infty.\\]</div><div></div><div></div><div></div><div>[Markov Chains, Page 2]</div>"
  },
  {
    "front": "Connection between recurrent and closed classes.<div></div><div>Characterisation of recurrence in finite chains?</div>",
    "back": "Every recurrent class is closed and every finite closed class is recurrent.<div></div><div>Hence a state in a finite chain is recurrent if and only if its class is closed.</div>"
  },
  {
    "front": "Random walk in one and two dimensions (including the case [$]p\\neq q[/$] in 1d).",
    "back": "<div>$\\underline{d = 1}:$ With step probabilities $p\\in [0,1]$ and $q = 1-p$,\\[&nbsp;&nbsp;&nbsp; p_{00}^{2m+1} = 0,\\quad p_{00}^{(2m)} = \\binom{2m}{m} (pq)^m \\sim \\frac{1}{\\sqrt{\\pi m} } (4pq)^m\\\\\\] for $m\\in \\mathbb{N}_0$. Hence<br><ol>  <li>If $p = q = 1 / 2$, the chain is recurrent,&nbsp;&nbsp;&nbsp;</li>  <li>If $p \\neq q$, the chain is transcient.</li></ol><br><br>$\\underline{d=2}:$ Considering the projections of $X_n\\in \\mathbb{Z}^2$ on the diagonals, one obtains $p_{00}^{(2m)} \\sim \\frac{1}{\\pi m}$, hence the chain is recurrent.</div><div></div><div></div><div></div><div>[Markov Chains, Page 2]</div>"
  },
  {
    "front": "Random walk in [$]d\\ge 3[/$] dimensions.",
    "back": "<div>If $d=3$, one obtains $p_{00}^{(2m)}\\sim m^{-3 / 2}$, so the chain is transcient. This implies (by projection) transience in $d\\ge 4$ dimensions.</div><div></div><div></div><div></div><div>[Markov Chains, Page 2]</div>"
  },
  {
    "front": "Mean hitting time [$]k_i^A[/$] definition and characterisation. What if [$]h_i^A &lt; 1[/$]?",
    "back": "<div>Let $A\\subset I$ and $i\\in I$. Then put $H^A := \\inf \\left\\{ n\\ge 0\\colon X_n \\in A \\right\\} $ and call $k_i^A := \\mathbb{E} \\left[H^A \\,\\middle\\vert\\, X_0 = i\\right]$ the <i>mean hitting time of $A$ (starting at $i$)</i>. The vector $(k_i^A)_{i\\in I}$ is the uniqe minimal solution to the set of equations \\[k_i^A = \\begin{cases}&nbsp;&nbsp;&nbsp; 0 &amp;, i\\in A,\\\\&nbsp;&nbsp;&nbsp; 1 + \\sum_{j\\in I} p_{ij} k_j^A &amp;, i\\not\\in A.\\end{cases}\\]&nbsp; If $h_i^A &lt; 1$, then $k_i^A = \\infty$.</div><div></div><div></div><div></div><div>[Markov Chains, Page 2]</div>"
  },
  {
    "front": "Definition mean return time, null-recurrence and positive recurrence (class property?)",
    "back": "<div>The <i>mean return time</i> of a state $i\\in I$ is \\[&nbsp;&nbsp;&nbsp; m_i := \\mathbb{E} \\left[\\inf \\left\\{ n\\ge 1\\colon X_n = i \\right\\}&nbsp; \\,\\middle\\vert\\, X_0 = i\\right] = 1 + \\sum_{j\\in I} p_{ij} k_j^{\\left\\{ i \\right\\} }.\\] <br><ol>  <li>If $i$ is transient, then $m_i = \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li>If $i$ is recurrent and $m_i = \\infty$, $i$ is called <i>null-recurrent</i>, otherwise <i>positive recurrent</i>,</li></ol><br>Null-/positive recurrence is a class property.</div><div></div><div></div><div></div><div>[Markov Chains, Page 2]</div>"
  },
  {
    "front": "Ergodic theorem for Markov chains.",
    "back": "<div>Let $P$ be irreducible and $X\\sim \\text{Markov} (\\lambda,P)$. Set $N_i(n) :=&nbsp; \\sum_{k=0}^{n-1} \\boldsymbol{1}\\left\\{X_k = i\\right\\}$ for $i\\in I$ and $n\\in \\mathbb{N}$. Then \\[&nbsp;&nbsp;&nbsp; \\frac{N_i(n)}{n}\\stackrel{  }{\\longrightarrow} \\frac{1}{m_i} \\, \\text{a.s. and in $L^1$} , \\quad i\\in I.\\] This also holds if $m_i = \\infty$.</div><div></div><div></div><div></div><div>[Markov Chains, Page 3]</div>"
  },
  {
    "front": "Existence of stationary distribution of Markov chain.",
    "back": "<div>A distribution $\\pi = \\left( \\pi_i \\right) _{i\\in I}$ is called a <i>stationary, invariant or equilibirum distribution</i> with respect to a transition matrix $P$ if $\\pi P = \\pi$.<br><br>Let $P$ be an irreducible transition matrix. Then $P$ has a stationary distribution if and only if $P$ is positive recurrent. In that case, the stationary distribution is unique and given by \\[\\pi_i = \\frac{1}{m_i},\\quad i\\in I.\\]</div><div></div><div></div><div></div><div>[Markov Chains, Page 3]</div>"
  },
  {
    "front": "Convergence to equilibrium for Markov Chains.",
    "back": "<div>Let $P$ be irreducible, aperiodic, and with a stationary distribution $\\pi$. Then if $X\\sim \\text{Markov} (\\lambda,P)$ with arbitrary $\\lambda$, \\[X \\stackrel{ d }{\\longrightarrow} \\pi,\\\\\\] i.e. $\\mathbb{P}\\left( X_n = j \\right) \\to \\pi_j$. In particular, $p_{ij}^{(n)}\\to \\pi_j$.</div><div></div><div></div><div></div><div>[Markov Chains, Page 3]</div>"
  },
  {
    "front": "Partitioning of a class [$]C[/$] of a Markov chain with period [$]d[/$] into [$]d[/$] subclasses.",
    "back": "<div><b>Definition:</b> If $A,B\\subset \\mathbb{Z}$, say that <i>$A$ has tail $B$</i> if $A\\subset B$ and $B\\setminus A$ is finite.<br><br>If $C$ is a class with period $d\\in \\mathbb{N}$, then $C$ can be partitioned into $d$ subclasses $C_1,\\ldots ,C_{d}$ such that if $i\\in C_k$ and $j\\in C_l$, then $\\left\\{ n\\in \\mathbb{N}\\colon p_{ij}^{(n)}&gt;0 \\right\\} $ has tail $\\left| k-l \\right|&nbsp; + d\\mathbb{Z}$.</div><div></div><div></div><div></div><div>[Markov Chains, Page 4]</div>"
  },
  {
    "front": "Definition of Wright-Fisher model and [$]p_{ij},\\, \\tau_j,\\, T_j[/$].",
    "back": "<div>Constant population size $M\\in \\mathbb{N}$, each haplotype chooses its parent uniformly at random from the previous generation, independently and independent in subsequent generations. Given a sample of size $n\\le N$ at some point in time, let $\\xi(k)$ denote the number of remaining lineages after $k$ steps backwards.<br><ol>  <li>$p_{ij}:= \\mathbb{P} \\left(\\xi(k+1) = j \\,\\middle\\vert\\, \\xi(k) = i\\right)$ for $0\\le j\\le i$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\tau_n := \\text{number of backwards time steps with no coalescence event} $.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "[$]p_{ij}[/$] in Wright-Fisher model and approximation for large [$]M[/$].<br>",
    "back": "<div><ol>  <li>$p_{ij} = \\frac{1}{M^i} \\binom{M}{j} S(i,j)$,&nbsp;&nbsp;&nbsp;</li>  <li>$p_{ii} = 1 - \\frac{1}{M} \\binom{i}{2} + O(M^{-2})$,&nbsp;&nbsp;&nbsp;</li>  <li>$p_{ii-1} = \\frac{1}{M} \\binom{i}{2} + O(M^{-2})$.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "Wright-Fisher model: distributional limit of [$]T_j[/$] for [$]M\\to \\infty[/$].",
    "back": "[$$]T_j \\stackrel{ d }{\\longrightarrow} \\text{Exp} \\left( \\binom{j}{2} \\right) .[/$$]<br>"
  },
  {
    "front": "Definition coalescent.<div></div><div>Coalescent as limiting distribution? [$]M[/$] for humans, and year equivalent? What are the abstract individuals in the real world?</div>",
    "back": "<div>The coalescent is a distribution over binary trees with a continuous vertical time scale. Starting from $n\\in \\mathbb{N}$ lineages, pairs of lineages coalesce uniformly and independently at random after independent times $T_n,\\ldots ,T_2$ with $T_j \\sim \\text{Exp} \\left( \\binom{j}{2} \\right) $.<br><br><ol>  <li>The coalescent is the limiting distribution of many models (including the WF model)&nbsp;&nbsp;&nbsp;</li>  <li>For humans, $M\\sim 20.000 - 50.000$, which amounts to about $1$ million years. so low bc fewer humans in the past, and most parents have either no or at least two children, so coalescence more likely.</div><div>&nbsp; &nbsp;</li>  <li>Individuals correspond to genome sequences or chromosome halfs. could, f.ex. ask for the MRCA of the two halfs of my X chromosome.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "Coalescence: Time [$]W_n[/$] to most recent common ancestor (TMRCA), expectation and variance.",
    "back": "<div>Set $W_n := T_2+\\ldots +T_n$ for $n\\in \\mathbb{N}$. Then<br>\\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ W_n \\right] = 2 \\left( 1 - \\frac 1n \\right),\\quad \\mathbb{V}(W_n) \\le 4\\\\\\]</div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "Distribution of number of descendants of [$]k[/$] lineages in a coalescent of size [$]n[/$].",
    "back": "<div>Uniform over all possible combinations. That is, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left( Z_1,\\ldots ,Z_k \\right) = \\left( z_1,\\ldots ,z_k \\right)&nbsp; \\right) = \\binom{n-1}{k-1}^{-1}\\] for all $z_i \\ge 1$ with $\\sum_{i=1}^k z_i = n$.</div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "Genetics, Mutations: Types of mutations and mutations in the coalescent model.",
    "back": "<div><ol>  <li>Point mutations ($\\sim 2 \\cdot 10^{-8}$), deletions ($&lt;10^{-8}$), duplications ($&lt;10^{-8}$),&nbsp;&nbsp;&nbsp;</li>  <li>Tandem repeat variation: If $s$ is a sequence repeated $n\\in \\mathbb{N}$ times, new DNA has $m\\approx n$ repitions ($\\le 5\\&nbsp;&nbsp;&nbsp;</li>  <li>Insertion of transposons (selfish genetic elements).</li></ol>\\textbf{Coalescent model:} Assume a fixed mutation probability $\\mu$ per generation. In coalescent times, this means $\\mu M$ mutations per unit time. Now put $\\theta := 2 M \\mu$ and assume that $\\theta$ is fixed (as $M\\to \\infty$). Now mutations happen along edges of the coalescent according to a poisson process with rate $\\theta / 2$ (for a given tree).</div><div></div><div></div><div></div><div>[Genetics, Page 1]</div>"
  },
  {
    "front": "Genetics, Mutation: Definition and distribution of [$]M_j[/$] and [$]S[/$], and interpretation. Expectation and variance of [$]M_j[/$]",
    "back": "<div>$M_j$ is the number of mutations while there are $j$ lineages, $S:= M_2+\\ldots +M_n$ is the total number of mutations. Then $S$ has probability generating function \\[&nbsp;&nbsp;&nbsp; f_{S_n}(z) = \\prod_{j=2}^n f_{M_j}(z) = \\prod_{j=2}^n \\left( 1 - \\frac{(z-1)\\theta}{j-1} \\right) ^{-1}.\\] In particular, $M_j \\sim \\text{G} \\left( p_j \\right) $ with $p_j = \\frac{j-1}{\\theta + j-1}$ are independent, so \\[\\mathbb{E}\\left[ M_j\\right] = \\frac{\\theta}{j-1},\\quad \\mathbb{V} \\left(M_j\\right) = \\left(\\frac{\\theta}{j-1}\\right)^2 + \\frac{\\theta}{j-1}.\\]<br><br><b>Interpretation:</b> The process that records mutation and coalescence events is a poisson process with rate $\\frac{j(j-1)}{2} + \\theta j / 2$ (while there are $j$ lineages).</div><div></div><div></div><div></div><div>[Genetics, Page 1/2]</div>"
  },
  {
    "front": "Genetics, Mutations: Definition and consistency of Watterson-estimator for [$]\\theta[/$]<div></div><div>How to estimate [$]\\mu[/$] and [$]M[/$]?</div>",
    "back": "<div>The <i>Watterson estimator</i> (1975) \\[\\widehat{\\theta}:= \\frac{S}{\\sum_{j=1}^{n-1} 1 / j}\\] is unbiased and $L^2$-consistent. However $\\mathbb{V}(\\widehat{\\theta}) \\sim \\theta / \\log n$ goes to zero only very slowly with sample size.<br><br><ol>  <li>Estimation of $\\mu$: Compare DNA of chimpanzees and humans (MRCA $16$ million years ago), or directly compare DNA of parents and children in many families,&nbsp;&nbsp;&nbsp;</li>  <li>Estimation of $M$: Sample $n$ human chromosomes (across the world, same type of chromosome each), and find $S$ (number of variant sites). Given the mutation probability, calculate $\\widehat{\\theta}$, which then gives $\\widehat{M} = \\widehat{\\theta} / (2\\mu)$.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 2]</div>"
  },
  {
    "front": "Definition uniform spanning tree of a finite graph, RW in a (locally-finite) graph, hitting time [$]\\tau_v[/$], cover time [$]t_\\text{cov}[/$].<div></div><div>When is [$]\\tau_v &lt; \\infty[/$] a.s. for all [$]v\\in V(G)[/$]?</div>",
    "back": "<div><b>Definition:</b> Let $\\mathcal{T}$ be the set of spanning trees of a finite, connected graph $G$. Then a random variable $T\\sim \\text{Unif} (\\mathcal{T})$ is called a <i>uniform spanning tree</i>.<br><br><b>Definition:</b> A <i>random walk (RW)</i> on a locally finite graph $G$ is a Markov chain $(X_n)_{n\\in \\mathbb{N}_0}$ on $V(G)$ with transition probabilities $p_{vw} = \\,\\boldsymbol{1}\\!\\left\\{\\left\\{ v,w \\right\\} \\in E(G)\\right\\} d(v)^{-1}$.<br><ol>  <li>The <i>hitting time</i> $\\tau_v$ of $v\\in V(G)$ is $\\tau_v := \\inf \\left\\{ n\\ge 0 \\colon&nbsp; X_n = v \\right\\} $,&nbsp;&nbsp;&nbsp;</li>  <li>The <i>cover time</i> is $t_\\text{cov} = \\max_{v} \\tau_v = \\inf \\left\\{ n\\ge 0\\colon \\left\\{ X_0,\\ldots ,X_n \\right\\} = V(G) \\right\\} $.</li></ol><br>If $G$ is connected and recurrent, then $\\tau_v &lt; \\infty$ a.s. for all $v\\in V(G)$. If $G$ is finite and connected, then $t_\\text{cov} &lt; \\infty$ a.s., otherwise $t_\\text{cov}= \\infty$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Aldous-Browler algorithm.",
    "back": "<div>Let $G$ be connected and recurrent, and $(X_n)$ a RW on $G$. Then the subgraph containing edges \\[&nbsp;&nbsp;&nbsp; \\left\\{ X_{\\tau_v - 1}, X_{\\tau_v} \\right\\} = \\left\\{ X_{\\tau_v - 1},v \\right\\} ,\\quad v\\in V(G),\\\\\\] gives a spanning tree of $G$. This is, in fact, a uniform spanning tree.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Definition loop-erased walk, loop-erased random walk (LERW).",
    "back": "<div>Let $\\gamma = (x_0,\\ldots ,x_n)$ be a walk in $G$. For $v\\in V(\\gamma)$, put $\\ell_v := \\max \\left\\{ m\\colon x_m = v \\right\\} $, the <i>last</i> visit to $v$. Define<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; v_0 &amp;:= x_0,\\\\&nbsp;&nbsp;&nbsp; v_{i+1} &amp;:= x_{\\ell_{v_i}+1},\\\\\\end{align*}[/$$]<br>until $v_{n'}= x_n$ (equivalently, $\\ell_{v_{n'}} = n$). This can also be applied to infinite walks, if they contain every vertex at most finitely often.<br><br>The loop erasure of a random walk is called a <i>loop-erased random walk (LERW)</i>.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Wilson's algorithm for uniform spanning trees.",
    "back": "<div>Let $G$ be finite and connected, and $V(G) = \\left\\{ v_0,\\ldots ,v_{n-1} \\right\\} $. Define a sequence of subtrees $T_0\\subset T_1\\subset \\ldots $ of $G$ by <br><ol>  <li>$T_0 := \\left\\{ v_0 \\right\\} $,&nbsp;&nbsp;&nbsp;</li>  <li>If $V(T_i) = V(G)$, stop. Otherwise, choose minimally labeled $v\\not\\in V(T_i)$ and add a LERW from $v$ to $T_i$ to form $T_{i+1}$.</li></ol><br><br><b>Theorem (Wilson, 1996).</b> Wilson's algorithm gives a uniform spanning tree.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Cayley's formula (spanning trees)",
    "back": "<div>For $n\\in \\mathbb{N}$, $\\left|\\mathcal{T}(K_n)\\right| = n^{n-2}$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Uniform spanning trees: negative association",
    "back": "<div>Let $G$ be finite and connected, and $T$ a UST of $G$.<br><ol>  <li>For all disjoint and acyclic $A,B\\subset E(G)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(B\\subset E(T) \\,\\middle\\vert\\, A\\subset E(T)\\right) \\le \\mathbb{P}\\left( B\\subset E(T) \\right) .&nbsp;&nbsp;&nbsp; \\]</li>  <li>Let $H\\le G$ be connected and $T_H$ a UST of $H$. Then for all $A\\subset E(H)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( A\\subset E(T_H) \\right) \\ge \\mathbb{P}\\left( A\\subset E(T) \\right) .\\]</li></ol></div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 1]</div>"
  },
  {
    "front": "Infinite graphs: [$]\\Omega_G[/$], (increasing) cylinders, and characterisation of weak convergence on&nbsp;[$]\\mathcal{B}_G[/$].",
    "back": "<div>Let $G$ be an infinite, connected graph. Then $\\Omega_G:= \\left\\{ 0,1 \\right\\} ^{E(G)}$ is a compact topological space. <i>Cylinders</i> are sets of the form \\[\\left\\{ \\omega\\in \\Omega_G\\colon \\omega\\!\\!\\restriction_{A} \\equiv 1, \\omega\\!\\!\\restriction_{B} \\equiv 0&nbsp; \\right\\} ,\\\\\\] where $A,B\\subset E(G)$ are finite and disjoint. If $B = \\varnothing$, it is called an <i>increasing cylinder</i> and denoted by $\\mathcal{C}_A$. All cylinders are clopen, and they form a countable base of the topology. In particular, $\\mathcal{B}_G$ is generated by cylinders.<br><br><b>Proposition.</b> Let $\\mu_n,\\,n\\in \\mathbb{N},$ and $\\mu$ be probability measures on $\\mathcal{B}_G$. Then the following are equivalent.<br><ol>  <li>$\\mu_n \\implies \\mu$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\forall \\text{ cylinders } \\mathcal{C}\\colon \\mu_n(\\mathcal{C}) \\stackrel{  }{\\longrightarrow} \\mu(\\mathcal{C})$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\forall \\text{ increasing cylinders } \\mathcal{C}_A\\colon \\mu_n(\\mathcal{C}_A) \\stackrel{  }{\\longrightarrow} \\mu(\\mathcal{C}_A)$.</li></ol></div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 2]</div>"
  },
  {
    "front": "Infinite graphs: Existence of weak limit of a sequence [$](\\mu_n)[/$] on [$]\\Omega_G[/$].",
    "back": "<div>If $(\\mu_n)$ is a sequence of probability measures on $\\mathcal{B}_G$ such that for all (increasing) cylinders $\\mathcal{C}$, the limit $\\lim_{n\\to \\infty}\\mu_n(\\mathcal{C})$ exists, then there exists a unique probability measure $\\mu$ on $\\mathcal{B}_G$ with \\[&nbsp;&nbsp;&nbsp; \\mu(\\mathcal{C}) = \\lim_{n\\to \\infty} \\mu_n(\\mathcal{C})\\] for all cylinders $\\mathcal{C}$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 2]</div>"
  },
  {
    "front": "Definition exhaustion of a graph and free uniform spanning forest.",
    "back": "<div>An <i>exhaustion</i> of an infinite, connected graph $G$ is a sequence of connected subgraphs $(G_n)$ induced by a sequence $V_n \\uparrow V(G)$.<br><br>If $(G_n)$ is an exhaustion of $G$ and $T_n$ is a UST of $G_n$ for $n\\in \\mathbb{N}$ (as random element in $\\Omega_G$), then the weak limit of $\\mu_{T_n}$ exists, and is called the <i>free unifrom spanning forest (FUSF)</i> of $G$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 2]</div>"
  },
  {
    "front": "The FUSF and WUSF almost surely contain no cycles and almost surely have no finite component.",
    "back": "[Graphs and Lattices, Page 2]"
  },
  {
    "front": "FUSF: Definition graph automorphism, invariance under a g.a., FUSF is invariant under every g.a.",
    "back": "[Graphs and Lattices, Page 2]"
  },
  {
    "front": "Sperner's lemma and proof using Hall's theorem.",
    "back": "<div><b>Sperner's lemma.</b> An antichain in $\\mathcal{P}(n)$ has at most $\\binom{n}{\\left\\lfloor n / 2 \\right\\rfloor }$ elements.</div><div></div><div></div><div></div><div>[Combinatorics, Page 1]</div>"
  },
  {
    "front": "Local LYM inequality.",
    "back": "<div>The <i>(lower) shadow</i> of $\\mathcal{F}\\subset X^{(k)}$ is the set $\\partial \\mathcal{F} := \\left\\{ B\\in X^{(k-1)}\\colon B\\subset A \\text{ for some } A\\in \\mathcal{F} \\right\\} $.<br><br><b>Lemma (Local LYM inequality).</b> Let $n\\in \\mathbb{N}$, $r\\in \\left[ n \\right] $, $\\mathcal{A}\\subset \\left[ n \\right] ^{(r)}$. Then \\[&nbsp;&nbsp;&nbsp; \\frac{\\left| \\partial \\mathcal{A} \\right| }{\\binom{n}{r-1}} \\ge \\frac{\\left| \\mathcal{A} \\right| }{\\binom{n}{r}}.\\] Equality holds if and only if $\\mathcal{A}=\\varnothing$ or $\\mathcal{A}=\\left[ n \\right] ^{(r)}$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 1]</div>"
  },
  {
    "front": "LYM inequality. (Two proofs).",
    "back": "<div>Let $n\\in \\mathbb{N}$ and $\\mathcal{F}\\subset \\mathcal{P}(n)$. Then \\[&nbsp;&nbsp;&nbsp; \\sum_{F\\in \\mathcal{F}} \\binom{n}{|F|}^{-1} = \\sum_{i=0}^n \\frac{\\left| \\mathcal{F}\\cap \\left[ n \\right] ^{(i)} \\right| }{\\binom{n}{i}} \\le 1.\\] Equality holds if and only iff $\\mathcal{F} = \\left[ n \\right] ^{(i)}$ for some $i\\in \\left\\{ 0,\\ldots ,n \\right\\} $.</div><div></div><div></div><div></div><div>[Combinatorics, Page 1]</div>"
  },
  {
    "front": "Proof of Sperner's lemma with LYM inequality. What are the maximal antichains?",
    "back": "<div>If $\\mathcal{F}\\subset \\mathcal{P}(n)$ is a maximal antichain, then $\\mathcal{F} = \\left[ n \\right] ^{(i)}$ with $i = \\left\\lfloor n / 2 \\right\\rfloor $ or $i = \\left\\lceil n / 2 \\right\\rceil $.</div><div></div><div></div><div></div><div>[Combinatorics, Page 1]</div>"
  },
  {
    "front": "Definition of the wired UST [$]T_n^W[/$], and the WUSF on a connected, locally finite graph. General connection between [$]\\mu^F(\\mathcal{C}_A)[/$] and [$]\\mu^W(\\mathcal{C}_A)[/$].",
    "back": "<div>Let $G$ be connected and locally finite, and $(G_n)$ an exhaustion. The <i>boundary</i> $\\partial G_n$ of $G_n$ is the set of vertices in $V_n$ that are adjacent to a vertex in $V(G) \\setminus V_n$. The&nbsp; <i>wired version of $G_n$</i>, denoted $G_n^W$, is obtained from $G_n$ by adding a new vertex with edges to the boundary $\\partial G_n$. $T_n^W$ denotes a UST on $G_n^W$.<br><br>Let $\\mu_{T_n}^W$ be the law of $T_n\\cap E(G_n)$. Then for any finite $A\\subset E(G)$, \\[&nbsp;&nbsp;&nbsp; \\mu_{T_{n+1}}^W(\\mathcal{C}_A) \\ge \\mu_{T_n}^W (\\mathcal{C}_A).\\] Hence, there exists a distributional limit $\\mu_{T_n}^W \\implies \\mu^W,$&nbsp; which does not depend on the exhaustion, and is called the <i>wired uniform spanning forest (WUSF)</i> of $G$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 3]</div>"
  },
  {
    "front": "FUSF and WUSF on recurrent graphs. Uniform spanning tree of a recurrent graph.",
    "back": "<div>If $G$ is recurrent, connected, and locally finite, then FUSF $\\stackrel{d}{=} $ WUSF, both are connected almost surely, and generated by both the Aldous-Browler and Wilson's algorithm. In this case, one refers to the <i>uniform spanning tree</i> of $G$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 3]</div>"
  },
  {
    "front": "Wilson's algorithm on transient graphs. When is the WUSF on a transient graph almost surely connected?",
    "back": "<div>Given an enumeration $(v_i)_{i\\in \\mathbb{N}}$, let $T_0$ be an LERW started at $v_0$ (a RW almost surely visits every vertex only finitely often). Then for $i\\in \\mathbb{N}$, stop if $V(T_i) = V(G)$. Otherwise, choose minimally labelled $v\\in V(G)\\setminus V(T_j)$, and add a LERW started at $v$, absorbed at $T_j$ (which might not happen), and set $T = \\bigcup_{i=1} ^\\infty T_i$. This is also called <i>Wilson's algorithm rooted at infinity</i>.<br><br><b>Proposition.</b> If $G$ is transient, connected, and locally finite, Wilson's algorithm rooted at inifity generates the WUSF on $G$. It is connected almost surely iff two independent RW's on $G$ intersect almost surely.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 3]</div>"
  },
  {
    "front": "<div>Let $(X_t)$ be adapted and $\\tau$ a stopping time.<br><ol>  <li>When is $X_\\tau \\,\\boldsymbol{1}\\!\\left\\{\\tau &lt; \\infty\\right\\}$ $\\mathcal{F}_\\tau$-measurable?&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is a *martingale, when is $X^\\tau$, too?</li></ol></div><div></div>",
    "back": "<div>Let $(X_t)$ be adapted and $\\tau$ a stopping time.<br><ol>  <li>If $X$ is progressively measurable, then $X_\\tau \\,\\boldsymbol{1}\\!\\left\\{\\tau &lt; \\infty\\right\\}$ is $\\mathcal{F}_\\tau$-measurable,&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is a rightcontinuous *martingale, then so is $X^\\tau$.</li></ol></div><div></div><div></div><div></div><div>[C8.1, Page 2]</div>"
  },
  {
    "front": "If [$]B[/$] is a Brownian motion, and [$]a &lt; 0 &lt; b[/$], what is [$]\\mathbb{P} \\left( \\tau_a &lt; \\tau_b\\right)[/$]?",
    "back": "[$$] \\mathbb{P}\\left(\\tau_a&lt;\\tau_b\\right) = \\frac{b}{|a| + b}.[/$$]<div></div><div></div><div>[C8.1, Page 2]</div>"
  },
  {
    "front": "Continuous optional sampling (bounded stopping times or uniformly integrable *martingale).",
    "back": "<div>Let $(M_t)$ be a rightcontinuous *martingale and $\\sigma \\le \\tau$ stopping times. If $\\sigma$ and $\\tau$ are bounded or $M$ is uniformly integrable, then \\[&nbsp;&nbsp;&nbsp; M_\\sigma *= \\mathbb{E} \\left[M_\\tau \\,\\middle\\vert\\, \\mathcal{F}_\\sigma\\right].\\] In particular, if $(M_t)$ is a martingale, $\\left\\{ M_\\tau\\colon \\tau \\text{ stopping time}&nbsp; \\right\\} $ is uniformly integrable.</div><div></div><div></div><div></div><div>[C8.1, Page 1]</div>"
  },
  {
    "front": "Continuous optional sampling for finite stopping times.",
    "back": "<div>Let $(M_t)$ be a rightcontinuous *martingale and $\\sigma \\le \\tau$ almost surely finite stopping times such that $M_\\tau$ and $M_\\sigma$ are integrable and \\[\\lim_{n\\to \\infty}\\mathbb{E} \\left[\\left| M_n \\right| \\colon \\tau &gt; n \\,\\middle\\vert\\, \\right] = 0.\\] Then, $M_\\sigma *= \\mathbb{E} \\left[M_\\tau \\,\\middle\\vert\\, \\mathcal{F}_\\sigma\\right]$.</div><div></div><div></div><div></div><div>[C8.1 Page 1]</div>"
  },
  {
    "front": "Doob's inequalities for (right-)continuous submartingales.",
    "back": "<div>Let $(M_t)$ be a rightcontinuous non-negative submartingale&nbsp; (or martingale), $\\lambda &gt; 0$, and $f\\colon \\mathbb{R}\\to [0,\\infty)$ increasing and convex with $\\mathbb{E} \\left[ f(\\left| M_t \\right| ) \\right] &lt;\\infty$ for all $t\\ge 0$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{t\\le T}\\left| M_t \\right| \\ge \\lambda \\right) \\le \\frac{\\mathbb{E} \\left[ f(\\left| M_t \\right| ) \\colon M_T^\\star \\ge \\lambda\\right] }{f(\\lambda)}\\] for all $T &gt; 0$. If $p &gt; 1$, then \\[\\mathbb{E} \\left[ \\sup_{t\\le T} \\left| M_t \\right| ^p \\right] \\le \\left( \\frac{p}{p-1} \\right) ^p \\mathbb{E} \\left[ \\left| M_T \\right| ^p \\right] .\\]</div><div></div><div></div><div></div><div>[C8.1 Page 1]</div>"
  },
  {
    "front": "Doob's (down-)upcrossing lemma for rightcontinuous (sub-)supermartingales.",
    "back": "<div>Let $(X_t)$ be a rightcontinuous supermartingale. Then for any $a &lt; b$ and $T &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ U(a,b;X,T) \\right] \\le \\frac{E\\left[ \\left( X_T - a)^- \\right)&nbsp; \\right] }{b - a}.\\] If $X$ is a submartingale, then \\[\\mathbb{E} \\left[ D(b,a;X,T) \\right] \\le \\frac{E\\left[ \\left( X_T - b \\right) ^+ \\right] }{b - a}.\\]</div><div></div><div></div><div></div><div>[C8.1 Page 1]</div>"
  },
  {
    "front": "Dilworth's theorem on finite posets.",
    "back": "<div>Let $(P,\\le)$ be a finite poset. The minimum number of chains needed to cover $P$ is equal to the size of the largest antichain.</div><div></div><div></div><div></div><div>[Combinatorics, Page 2]</div>"
  },
  {
    "front": "Real version of Littlewood-Offord (by Erdos).",
    "back": "<div>Suppose that $x_1,\\ldots ,x_n \\in \\mathbb{R}$ with $\\left| x_i \\right| \\ge 1$. Then for any $\\alpha \\in \\mathbb{R}$, there are at most $\\binom{n}{\\left\\lfloor n / 2 \\right\\rfloor }$ subsets $I\\subset \\left[ n \\right] $ such that \\[&nbsp;&nbsp;&nbsp; \\sum_{i\\in I}x_i \\in [\\alpha,\\alpha+1).\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 2]</div>"
  },
  {
    "front": "Partition of [$]\\mathcal{P}(n)[/$] into symmetric chains. What is the distribution of sizes of such a symmetric chain decomposition?",
    "back": "<div>Let $n\\in \\mathbb{N}$. A chain $A_1\\subset \\ldots \\subset A_m$ in $\\mathcal{P}(n)$ is called <i>symmetric</i> if $\\left| A_{i+1} \\right| = \\left| A_i \\right| +1$ for all $i\\le m-1$ and $\\left| A_1 \\right| + \\left| A_m \\right| = n$.<br><br><b>Proposition.</b> There exists a decomposition of $\\mathcal{P}(n)$ into $\\binom{n}{\\left\\lfloor n / 2 \\right\\rfloor }$ symmetric chains. There are \\[&nbsp;&nbsp;&nbsp; \\binom{n}{i} - \\binom{n}{i-1}\\] chains of size $n - 2i + 1$ for every $0\\le i \\le n / 2$. These run from the $i$'th to the $n-i$'th layer.</div><div></div><div></div><div></div><div>[Combinatorics, Page 2]</div>"
  },
  {
    "front": "Littlewood-Offord theorem.",
    "back": "<div>Let $n,d\\in \\mathbb{N}$ and $x_1,\\ldots ,x_n \\in \\mathbb{R}^d$ with $\\left\\|x_i\\right\\|_2 \\ge 1$. Then for any $K\\subset \\mathbb{R}^d$ with $\\text{diam} (K) &lt; 1$, there are at most $\\binom{n}{\\left\\lfloor n / 2 \\right\\rfloor }$ subsets $I\\subset [n]$ such that \\[\\sum_{i\\in I}x_i \\in K.\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 2]</div>"
  },
  {
    "front": "<div>In the context of <b>percolation</b>, definitions of<br><ol>  <li>a <i>vertex-transitive</i> graph,&nbsp;&nbsp;&nbsp;</li>  <li>The probability measure $\\mathbb{P}_p$ on $\\Omega_G$ for $p\\in [0,1]$,&nbsp;&nbsp;&nbsp;</li>  <li>The random variable $\\mathcal{C}(x) \\subset V(G)$ for $x\\in V(G)$ and the notation $x\\leftrightarrow \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li>The percolation probability $\\theta(p)$ for $G = \\mathbb{Z}^d$.</li></ol></div><div></div>",
    "back": "[Graphs and Lattices, Page 4]"
  },
  {
    "front": "Percolation: Proof that [$]\\theta(\\cdot)[/$] is increasing and that there are [$]p,p'\\in (0,1)[/$] such that [$]\\theta(p) = 0[/$] and [$] \\theta(p') &gt; 0[/$].",
    "back": "[Graphs and Lattices, Page 4]"
  },
  {
    "front": "Percolation: Definition of critical probability [$]p_c(d)[/$] and (un-)known facts regarding continuity.",
    "back": "<div>If $d\\ge 2$, then \\[&nbsp;&nbsp;&nbsp; p_c := p_c(d) := \\sup\\left\\{ t\\in (0,1)\\colon \\theta(p) = 0 \\right\\} \\in (0,1).\\] It is known that $\\theta$ is continuous on $(p_c,1)$ and rightcontinuous at $p_c$, and it is conjectured that it is also leftcontinuous, that is, $\\theta(p_c) = 0$. This has been proven for $d = 2$ and $d\\ge 11$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 4]</div>"
  },
  {
    "front": "Percolation: Probability that an infinite cluster exists somewhere.",
    "back": "<div>Let $d\\ge 2$ and $p\\in [0,1]$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}_p \\left( \\exists x\\in V(G)\\colon x \\leftrightarrow \\infty \\right)&nbsp; = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 &amp;, \\theta(p) &gt; 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, \\theta(p) = 0.&nbsp;&nbsp;&nbsp; \\end{cases}\\]</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 4]</div>"
  },
  {
    "front": "Percolation: Harris/FKG inequality.",
    "back": "<div>Let $f,g\\colon \\Omega_G \\to \\mathbb{R}$ be increasing functions. Then \\[ \\mathbb{E}_p\\left[ fg \\right] \\ge \\mathbb{E}_p\\left[ f \\right] \\mathbb{E}_p \\left[ g \\right] .\\] In particular, if $A,B\\subset \\Omega_G$ are increasing and measurable, then $\\mathbb{P}\\left( A\\cap B \\right) \\ge \\mathbb{P}(A)\\mathbb{P}(B)$ and \\[\\mathbb{E}_p \\left[f \\,\\middle\\vert\\, A\\right]\\ge \\mathbb{E}_p\\left[ f \\right] .\\]</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 4/5]</div>"
  },
  {
    "front": "Conditional distribution of [$]T_j[/$] given [$]M_j = 0[/$] (for constant population size). Hence, what is [$]\\mathbb{E}\\left[ W_n \\,\\middle\\vert\\, S = 0\\right][/$].",
    "back": "<div>Conditional on $M_j = 0$, $T_j$ has distribution $\\text{Exp} \\left( \\frac{j(j-1+\\Theta)}{2} \\right) $. Consequently, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[W_n \\,\\middle\\vert\\, S = 0\\right] = \\sum_{j=2}^n \\mathbb{E} \\left[T_j \\,\\middle\\vert\\, M_j = 0\\right] = \\sum_{j=2}^n \\frac{2}{j(j-1+\\Theta)}\\]</div><div></div><div></div><div></div><div>[Mathematical Genetics, Page 2]</div>"
  },
  {
    "front": "Definition of coalescence tree with variable population size.",
    "back": "<div>Assume population size evolves (backwards) according to $N(t) = \\nu(t) N(0)$. Then the instantaneous rate (from WF model) for a coalescence is $\\binom{j}{2} \\frac{1}{N(t)}$.<br><br><b>Definition:</b> The <i>coalescence with variable population size</i> $N(t) = N(0) \\nu(t)$ is a distribution on binary trees with vertical length scale. Starting at $n = N(0)$ offsprings, uniformly and independently chosen pairs coalesce at instantaneous rate $\\frac{j(j-1)}{2 \\nu(t)}$ (while $j$ lineages remain). Equivalently, coalescence occurs at times $T_n,\\ldots ,T_2$, where \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(T_j &gt; t \\,\\middle\\vert\\, T_n + \\ldots + T_{j+1} = s\\right) = \\exp \\left( - \\binom{j}{2} \\int_s^{s+t} \\frac{1}{\\nu(u)}\\mathop{}\\!\\mathrm{d} u \\right) .\\] Mutations occur along edges with rate $\\Theta / 2$ as before.</div><div></div><div></div><div></div><div>[Genetics, Page 2]</div>"
  },
  {
    "front": "Time rescaling that transforms [$]\\nu(t)[/$]-coalescent into constant population size coalescent. Hence, how to simulate the former? Explicit transformation for [$]\\nu(t) = \\mathrm{e}^{-\\beta t}[/$]?",
    "back": "<div>If the time is rescaled via \\[&nbsp;&nbsp;&nbsp; t' = \\int_0^t \\frac{1}{\\nu(s)}\\mathop{}\\!\\mathrm{d} s,\\\\\\] the resulting coalescent is one with constant population size. Hence, to simulate a $\\nu$-coalescent, first simulate an ordinary one, then transform $S_j' \\to S_j$, and put mutations on the resulting edges.<br><br>If $\\nu(t) = \\mathrm{e}^{-\\beta t}$, then \\[&nbsp;&nbsp;&nbsp; t' = \\frac{\\mathrm{e}^{\\beta t}-1}{\\beta},\\qquad t = \\frac{\\log\\left( 1 + \\beta t' \\right) }{\\beta}.\\]</div><div></div><div></div><div></div><div>[Mathematical Genetics, Page 2/3]</div>"
  },
  {
    "front": "Mutation frequency spectrum: Definition and result for [$]q_{nb}[/$] for general coalescent under the infinite-sites model.",
    "back": "<div>Given a coalescence tree with random times $T_n,\\ldots ,T_2$, what is the probability $q_{nb}$ ($1\\le b \\le n$) of a randomly chosen mutation to appear in $b$ offsprings? (Assuming the <i>infinite-sites model</i>, that is, that every mutation occurs at a unique site).<br><br>If $p_k$ denotes the probability of the mutation having occured while $k$ ancestors were present, then \\[&nbsp;&nbsp;&nbsp; q_{nb} = \\sum_{k=2}^n \\frac{\\binom{n-b-1}{k-2}}{\\binom{n-1}{k-1}} p_k, \\quad 1 \\le b \\le n,\\\\\\] where \\[p_k = \\frac{k \\mathbb{E} \\left[ T_k \\right] }{\\sum_{j=2}^n j \\mathbb{E} \\left[ T_j \\right] }, \\quad 1 \\le k \\le n.\\]</div><div></div><div></div><div></div><div>[Genetics, Page 3]</div>"
  },
  {
    "front": "Mutation frequency spectrum: explicit formula for [$]q_{nb}[/$] in the case of constant population size.",
    "back": "[$$] q_{nb} = \\frac 1b \\left( \\sum_{k=1}^{n-1} \\frac 1k\\right)^{-1}.[/$$]<div></div><div></div><div>[Mathematical Genetics, Page 3]</div>"
  },
  {
    "front": "Number of alleles/haplotypes: Definition haplotype, allele, infinite-alleles model (and relationship to inifinite-sites model).<div></div><div>What is the haplotype of an offspring determined by (in the infinite-alleles model)?</div>",
    "back": "<div>A <i>haplotype</i> is a specific DNA sequence. Given the sequence of the MRCA of a sample, it is characterised by the mutations that affected it. <i>Alleles</i> are distinct types.<br><br><i>I</i>n the <i>infinitely-many-alleles model</i>, every mutation gives rise to a new, unique haplotype. This is strictly weaker than the inifinite-sites model. For example, it allows for multiple mutations at the same base, as long as they have different outcomes.<br><br>In the inifinite-alleles model, the haplotype of an offspring is thus determined by the most recent mutation that affected it.</div><div></div><div></div><div></div><div>[Genetics, Page 3]</div>"
  },
  {
    "front": "Number of haplotypes:<div>&nbsp;- Rates in the coalescent</div><div>&nbsp;- Representation of the number [$]K[/$] of alleles in the infinite-alleles model, pgf, and distribution</div>",
    "back": "<div><div>Under the infinite-alleles model for the standard coalescent of size $n$ with rate $\\theta$, we have \\[K = 1 + \\sum_{j=2}^n I_j,\\\\\\] where $I_j \\sim \\text{Bin} \\left(\\frac{\\theta}{\\theta + j -1}\\right)$ are independent.\\\\</div><div></div><div>Reasoning: Consider the death process where we kill each lineage when a mutation occurs. Then the number of lineages reduces by one at each coalescence and mutation event, and precisely the latter contribute to $K$.\\\\</div><div></div><div>We thus have \\[f_K(z) = \\frac{(\\theta z)^{(n)}}{\\theta ^{(n)}}, \\quad z\\in [0,1],\\\\\\] from which we can conclude \\[\\mathbb{P}\\left( K = k \\right) = \\frac{S(n,k) \\theta ^k}{\\theta ^{(n)}}, \\quad 1 \\le k \\le n.\\]</div><div></div><div></div><div></div><div>[Genetics, Page 4]</div></div>"
  },
  {
    "front": "Number of haplotypes: Expectation and variance of [$]K[/$] and asymptotic behaviour, and comparison to [$]\\mathbb{E}[S][/$].<div></div><div>What does this tell us about the number of mutations that define their own haplotype?</div>",
    "back": "<div>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ K \\right] &amp;= 1 + \\sum_{j=2}^n \\frac{\\theta}{\\theta + j-1} = 1 + \\mathbb{E} \\left[ S \\right] - \\sum_{j=1}^{n-1} \\frac{\\theta^2}{j(j+\\theta)} \\sim \\theta \\log n,\\\\&nbsp;&nbsp;&nbsp; \\mathbb{V}(K) &amp;= \\sum_{j=0}^{n-1} \\frac{\\theta j}{(\\theta+j)^2}\\sim \\theta \\log n.\\end{align*}[/$$]<br>The former means that asymptotically (and in expectation), the number of mutations that do <i>not</i><br>define their own haplotype remains finite.</div><div></div><div></div><div></div><div>[Genetics, Page 4]</div>"
  },
  {
    "front": "Number of haplotypes: Ewen's sampling formula.",
    "back": "\"<div>Set $\\alpha(j):= \\# \\text{haplotypes that have frequency $j$ in the sample} $ for $j = 1,\\ldots ,n$. Then $\\sum_{j=1}^n \\alpha(j) = K$ and $\\sum_{j=1}^n j\\alpha(j) = n$. Now if $\\alpha_1,\\ldots ,\\alpha_n \\in \\mathbb{N}$ such that $\\sum_{j=1}^n j\\alpha_j = n$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\forall j\\in [n]\\colon&nbsp; \\alpha(j) = \\alpha_j \\right) = \\frac{\\theta^k}{\\theta ^{(n)}} \\frac{n!}{\\prod_{j=1}^n j^{\\alpha_j} \\alpha_j!}.\\]</div><div></div><div></div><div></div><div>[Genetics, Page 4]</div>\""
  },
  {
    "front": "Definition gene tree and how to obtain it from coalescent tree.<div></div><div>Representation of variation data (with known ancestral type) as incidence matrix.</div>",
    "back": "<div>Gene tree has the ancestral type es root, the $n$ offsprings as leaves, and mutations as the remaining vertices. It is obtained from the coalescent by drawing an edge from every vertex to the most recent previous mutation (or to the root if there is none).<br><br>Given $n$ sequences and $s$ mutant sites, define $S\\in \\left\\{ 0,1 \\right\\} ^{n\\times s}$ by \\[&nbsp;&nbsp;&nbsp; s_{ij}:= \\,\\boldsymbol{1}\\!\\left\\{\\text{sequence $i$ exhibits the mutant type at site $j$} \\right\\}.\\]</div><div></div><div></div><div></div><div>[Genetics, Page 4]</div>"
  },
  {
    "front": "Gene trees: Bijection between incidence matrices with certain properties and (rooted) gene trees (Gusfield's alogrithm)<div><div></div><div>In particular, what is a sufficient and necessary condition for a data set (with known ancestral type) to be compatible with the infinite-sites model?</div></div>",
    "back": "<div>An incidence matrix corresponds to a gene tree if and only if it has the property that pairs of mutation carrier sets \\[&nbsp;&nbsp;&nbsp; O_j := \\left\\{ i\\in [n]\\colon s_{ij} = 1 \\right\\} , \\quad j = 1,\\ldots ,s,\\\\\\] are pairwise disjoint or comparable.<br><br>One direction is clear, the other direction is Gusfield's algorithm.<br><br>A data set (with known ancestral type) is compatible with the infinite-sites model if and only if its incidence matrix has the inclusion property.</div><div></div><div></div><div></div><div>[Genetics, Page 4/5]</div>"
  },
  {
    "front": "Unrooted gene tree [$]\\leftrightarrow[/$] rooted gene tree.<div></div><div>In particular, how many possible ancestral types exist for a given set of variational data (under the infinite-sites model)?</div>",
    "back": "<div>\\textbf{rooted $\\rightarrow$ unrooted:}<br><ol>  <li>slide mutations up&nbsp;&nbsp;&nbsp;</li>  <li>collapse edges without mutations&nbsp;&nbsp;&nbsp;</li>  <li>Give vertices that are not a former leaf a new label (those are sequences that must have existed but are not present in the final sample.</li></ol><br><br>\\textbf{unrooted $\\rightarrow$ rooted:} Choose any point on any edge as root (of which there are $s+1$ by induction)<br><ol>  <li>extend edges for vertices that are haplotypes in the final sample&nbsp;&nbsp;&nbsp;</li>  <li>slide mutations downwards (with respect to the new root)</li></ol><br><br><b>Corollary.</b> Of the $2^s$ possible choices of the ancestral type, only $s+1$ are valid (under the infinite-sites model).</div><div></div><div></div><div></div><div>[Genetics, Page 5]</div>"
  },
  {
    "front": "Unrooted gene trees:<div>&nbsp;(i) Characterisation of a data set with known ancestral type to have the inclusion property/have a gene tree/be compatible with infinite-sites</div><div>&nbsp;(ii) Characterisation of a data set with unknown ancestral type to have an unrooted gene tree/be compatible with infinite-sites</div>",
    "back": "<div><ol>  <li>if and only if the incidence matrix does not contain the pattern $\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} $.&nbsp;&nbsp;&nbsp;</li>  <li>if and only if the incidence matrix does not conatin the pattern $\\begin{pmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} $ for any $\\Leftrightarrow$ all togglings.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 5]</div>"
  },
  {
    "front": "Probability of datasets: definition of histories and how to obtain the probability of a given history.",
    "back": "<div>A timescale-free representation of a coalescent tree is the history $H = (H_1,\\ldots ,H_{n-1+s})$ of its coalescence/mutation events (in the order they happened, starting at the bottom of the tree). Coalescence of lineages $m$ and $n$, while $k$ lineages are left is denoted $C_k(m,n)$, mutation at lineage $m$ by $M_k(m)$.<br><br><b>Proposition.</b> Let $k_j := k_j(H_1,\\ldots ,H_{j-1})$ be the number of lineages left after the first $j-1$ events of a history. Then<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(H_j = C_k(m,n) \\,\\middle\\vert\\, H_1,\\ldots ,H_{j-1}\\right) &amp;= \\mathbb{P} \\left(H_j = C_k(m,n) \\,\\middle\\vert\\, k_j\\right) = \\frac{2}{k_j(k_j - 1 + \\theta)},\\\\&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(H_j = M_k(m) \\,\\middle\\vert\\, H_1,\\ldots ,H_{j-1}\\right) &amp;= \\mathbb{P} \\left(H_j = M_k(m) \\,\\middle\\vert\\, k_j\\right) = \\frac{\\theta}{k_j(k_j - 1 + \\theta)}.\\end{align*}[/$$]<br>Then $\\mathbb{P}(H= (H_1,\\ldots ,H_f)) = \\prod_{j=1}^f \\mathbb{P} \\left(H_j \\,\\middle\\vert\\, k_j\\right)$.</div><div></div><div></div><div></div><div>[Genetics, Page 5]</div>"
  },
  {
    "front": "Probability of datasets: Times [$]E_j[/$] between events and their (conditional) distribution.",
    "back": "<div>Let $E_j$ be the time between the $j-1$'th and $j$'th event. Then the $E_j$ are conditionally independent given the history, and their conditional distribution given $k_j$ is \\[&nbsp;&nbsp;&nbsp; E_j \\sim \\text{Exp} \\left( \\frac{k_j(k_j - 1 + \\theta)}{2} \\right) .\\]</div><div></div><div></div><div></div><div>[Genetics, Page 5]</div>"
  },
  {
    "front": "Probability of datasets: joint conditional density of [$]T_2,\\ldots,T_n[/$] given a history.",
    "back": "<div>By Bayes' formula, $f(t_2,\\ldots ,t_n \\,|\\, H) \\propto \\mathbb{P} \\left(H \\,\\middle\\vert\\, T_j = t_j\\right) f(t_2,\\ldots ,t_n)$. Then<br><ol>  <li>integrating gives $\\mathbb{P}(H)$,&nbsp;&nbsp;&nbsp;</li>  <li>normalisation gives the conditional, joined distribution of $T_2,\\ldots ,T_n$ given the history $H$.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 6]</div>"
  },
  {
    "front": "Percolation: Witness sets, disjoint occurrence, and BK inequality.",
    "back": "<div>Let $A,B\\subset \\Omega_G$.<br><ol>  <li>If $\\omega\\in A$, then $I = I(\\omega)\\subset E(G)$ is a <i>witness set of $A$ for $\\omega$</i> if $\\omega'\\!\\!\\restriction_{I} =\\omega\\!\\!\\restriction_{I} \\implies \\omega'\\in A$ for all $\\omega'\\in \\Omega_G$.&nbsp;&nbsp;&nbsp;</li>  <li>$A\\,\\circ\\, B := \\left\\{ \\omega\\in A\\cap B\\colon \\exists \\text{ disjoint witness sets $I_A(\\omega), I_B(\\omega)$ of $A$ and $B$ for $\\omega$}&nbsp; \\right\\} $.</li></ol><br><br><b>Proposition (BK inequality).</b> Let $A,B\\subset \\Omega_G$ be increasing events that depend on finitely many edges. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}_p\\left( A\\circ B \\right) \\le \\mathbb{P}_P(A) \\mathbb{P}_p(B).\\]</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 5]</div>"
  },
  {
    "front": "Percolation: Exponential bound on [$]\\mathbb{P}(0\\leftrightarrow \\partial \\Lambda(n))[/$] in the subcritical regime. Corollary on [$]\\mathbb{E}\\left[|\\mathcal{C}|\\right][/$].",
    "back": "<div>For all $p\\in [0,p_c)$ exists $C_p &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}_p\\left( 0\\leftrightarrow \\partial \\Lambda(n) \\right) \\le \\mathrm{e}^{-C_pn}\\] for all $n\\in \\mathbb{N}$. In particular, $\\mathbb{E} \\left[ \\left| \\mathcal{C} \\right|&nbsp; \\right] &lt;\\infty$.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 5]</div>"
  },
  {
    "front": "Percolation:<div>&nbsp;- Approximation of events by events that depend on finitely many edges,</div><div>&nbsp;- 0-1-law for invariant events on [$]\\mathbb{Z}^d[/$].</div>",
    "back": "<div><b>Lemma.</b> Let $A\\in \\mathcal{B}_G$. Then for all $\\varepsilon&gt;0$ there exists $B\\in \\mathcal{B}_G$ that depends only on finitely many edges such that $\\mathbb{P}_p(A\\Delta B) &lt; \\varepsilon$.<br><br>\\textbf{Proposition ($0$-$1$-law).} Let $A\\in \\mathcal{B}_G$ be invariant under graph automorphisms on $\\mathbb{Z}^d$. Then $\\mathbb{P}_p(A) \\in \\left\\{ 0,1 \\right\\} $.</div><div></div><div></div><div></div><div>[Graphs and Lattices, Page 5]</div>"
  },
  {
    "front": "Percolation: Uniqueness of infinite cluster (in the supercritical regime).",
    "back": "If [$]\\theta(p) &gt; 0[/$], then there is a unique infinite cluster with probability one.<div></div><div></div><div>[Graphs and Lattices, Page 5]</div>"
  },
  {
    "front": "Definition (co-)lexicographical order on [$][n]^{(r)}[/$].",
    "back": "<div>Let $n\\in \\mathbb{N}$ and $r\\in [n]$. Let $A,B\\in [n]^{(r)}$, $A\\neq B$.<br><ol>  <li>$A &lt;_{\\text{lex}}B \\mathrel{:\\Longleftrightarrow} \\min \\left( A\\Delta B \\right) \\in A$. (small $1$'s make small),&nbsp;&nbsp;&nbsp;</li>  <li>$A&lt;_\\text{colex} B\\mathrel{:\\Longleftrightarrow} \\max(A\\Delta B) \\in B$ (large $1$'s make large).</li></ol></div><div></div><div></div><div></div><div>[Combinatorics, Page 2]</div>"
  },
  {
    "front": "[$]UV[/$]-compression operator (defined on sets and set systems over [$]\\mathcal{P}(n)[/$] and lemma in preparation for Kruskal-Katona.",
    "back": "<div><b>Definition.</b> Let $U,V\\subset [n]$ with $|U| = |V|$ and $U\\cap V = \\varnothing$. Then <i>$UV$-compression operator</i> is given by \\[&nbsp;&nbsp;&nbsp; C_{UV}\\colon \\mathcal{P}(n) \\to \\mathcal{P}(n);\\, A\\mapsto \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (A\\setminus V) \\cup U &amp;, \\text{ if } U\\cap A = \\varnothing \\text{ and } V \\subset A,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A &amp;, \\text{ otherwise.} &nbsp;&nbsp;&nbsp; \\end{cases}\\] If $\\mathcal{F}\\subset \\mathcal{P}(n)$, then \\[C_{UV}(\\mathcal{F}) := \\left\\{ C_{UV}(A)\\colon A\\in \\mathcal{F} \\right\\} \\cup \\left\\{ A\\in \\mathcal{F}\\colon C_{UV}(A) \\in \\mathcal{F} \\right\\} .\\] Then we call $\\mathcal{F}$ <i>$(U,V)$-compressed</i> if $C_{UV}(\\mathcal{F}) = \\mathcal{F}$.<br><br><b>Lemma.</b> Let $U,V\\subset [n]$ disjoint and of same size. Then if $\\mathcal{F}\\subset [n]^{(r)}$ satisfies \\[&nbsp;&nbsp;&nbsp; \\forall u\\in U \\exists v\\in V\\colon \\mathcal{F}\\text{ is $(U\\setminus u,V\\setminus v)$-compressed.} \\] Then $\\left| \\partial C_{UV}(\\mathcal{F}) \\right| \\le \\left| \\partial \\mathcal{F} \\right| $.</div><div></div><div></div><div></div><div>[Combinatorics, Page 3]</div>"
  },
  {
    "front": "Kruskal-Katona theorem.",
    "back": "<div>Let $n\\in \\mathbb{N}$, $r\\le n$, and $\\mathcal{F}\\subset [n]^{(r)}$. Let $\\mathcal{A}\\subset [n]^{(r)}$ contain the first $\\left| \\mathcal{F} \\right| $ sets with respect to $&lt;_\\text{colex} $. Then \\[\\left| \\partial \\mathcal{F} \\right| \\le \\left| \\partial \\mathcal{A} \\right| .\\] In words, initial segments with respect to colex have the smallest shadows.</div><div></div><div></div><div></div><div>[Combinatorics, Page 3]</div>"
  },
  {
    "front": "Percolation: Kersten's theorem, [$]p_c \\le \\frac 12[/$].",
    "back": "[Graphs and Lattices, Page 6]"
  },
  {
    "front": "Percolation: Kersten's theorem, [$]p_c \\ge \\frac 12[/$].",
    "back": "[Graphs and Lattices, Page 6]"
  },
  {
    "front": "The stochastic exponential (of a zero in zero csm) and its SDE characterisation.",
    "back": "<div>If $X$ is a continuous semimartingale (clm) with $X_0 = 0$, then \\[&nbsp;&nbsp;&nbsp; \\mathcal{E}(X)_t := \\exp\\left( X_t - \\frac{\\left&lt;X \\right&gt; _t}{2} \\right) ,\\quad t\\ge 0,\\\\\\] is a continuous semimartingale (clm), the <i>stochastic exponential</i> of $X$.<br><br><b>Proposition.</b> $\\mathcal{E}(X)$ is the unique (continuous semimartingale) solution to the stochastic differential equation \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} Z_t = Z_t \\mathop{}\\!\\mathrm{d} X_t\\\\&nbsp;&nbsp;&nbsp; Z_0 = 1\\\\\\end{cases}\\] Furthermore, if $Z$ is a clm with $\\mathop{}\\!\\mathrm{d} Z_t = Z_t \\mathop{}\\!\\mathrm{d} X_t$, then $Z_t = Z_0\\, \\mathcal{E}(X)_t$ for $t\\ge 0$.</div><div></div><div></div><div></div><div>[SDEs, Page 4/5]</div>"
  },
  {
    "front": "Geometric Brownian motion.",
    "back": "<div>Let $B$ be a standard BM and $\\mu,\\sigma\\in \\mathbb{R}$. Then the stochastic differential equation \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathrm{d} S_t = \\mu S_t \\mathrm{d} t + \\sigma S_t \\mathrm{d} B_t\\\\&nbsp;&nbsp;&nbsp; S_0 = 1\\\\\\end{cases}\\] has a unique (continuous semimartingale) solution given by \\[S_t = \\exp\\left[ \\sigma B_t + \\left( \\mu - \\frac{\\sigma^2}{2} \\right) t \\right] ,\\quad t\\ge 0.\\]</div><div></div><div></div><div></div><div>[SDEs, Page 5]</div>"
  },
  {
    "front": "Lévy's characterisation of Brownian motion.",
    "back": "<div>Let $d\\in \\mathbb{N}$ and $(M^1,\\ldots ,M^d)$ be a $d$-dimensional continuous local martingale that vanishes at zero. Then $M$ is a $d$-dimensional standard Brownian motion if and only if \\[\\left&lt;M^i,M^j \\right&gt; _t = \\delta_{ij}t,\\quad t\\ge 0,\\\\\\] for all $i,j\\in \\left\\{ 1,\\ldots ,d \\right\\} $.</div><div></div><div></div><div></div><div>[SDEs, Page 5]</div>"
  },
  {
    "front": "Maximal size of an intersecting family [$]\\mathcal{A} \\subset \\mathcal{P}(n)[/$].",
    "back": "[$$] \\left|\\mathcal{A}\\right| \\le 2^{n-1}[/$$]<div></div><div></div><div>[Combinatorics, Page 3]</div>"
  },
  {
    "front": "Erdos-Ko-Rado theorem on intersecting families (two proofs).<div></div><div>When does equality hold?</div>",
    "back": "<div>Let $r\\le \\frac{n}{2}$ and $\\mathcal{A} \\subset \\left[ n \\right] ^{(r)}$ be an intersecting family. Then $\\left| \\mathcal{A} \\right| \\le \\binom{n-1}{r-1}$. Equality holds if and only if $\\mathcal{A} = \\left\\{ A\\subset [n]^{(r)}\\colon i\\in A \\right\\} $ for some $i\\in [n]$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 3]</div>"
  },
  {
    "front": "Liggett's theorem on convex combinations of Bernoulli random variables.",
    "back": "<div>Suppose that $n\\in \\mathbb{N}$, $X_1,\\ldots ,X_n\\sim \\text{Bin} (1,p)$ are independent with $p\\ge \\frac{1}{2}$ and $\\alpha_1,\\ldots ,\\alpha_n\\ge 0$ with $\\sum_{i=1}^n \\alpha_i = 1$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sum_{i=1}^n \\alpha_i X_i \\ge \\frac{1}{2} \\right) \\ge p.\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "Two Families Theorem. Is the bound sharp?",
    "back": "<div><b>Two Families Theorem.</b> Let $k\\in \\mathbb{N}$, and $A_1,\\ldots ,A_k$ and $B_1,\\ldots ,B_k$ be finite sets such that $A_i \\cap B_i = \\varnothing$ and $A_i \\cap B_j \\neq \\varnothing$ for all $i,j\\in [k]$, $i\\neq j$. Then \\[&nbsp;&nbsp;&nbsp; \\sum_{i=1}^k \\binom{|A_i| + |B_i|}{|A_i|}^{-1} \\le 1.\\] In particular, if $|A_i| = a$ and $|B_i| = b$ for all $i\\in [k]$, then $k\\le \\binom{a+b}{a}$.<br><br><b>Remark.</b> The bound is sharp: $\\left\\{ A_i \\right\\} := \\left[ a+b \\right] ^{(a)}$, $B_i := [a+b]\\setminus A_i$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "Bollobás' theorem on [$]r[/$]-uniform hypergraphs. Is the bound sharp? What does it say for [$]r=2,s=1[/$]?",
    "back": "<div><b>Theorem (Bollobás).</b> Let $G$ be an $r$-uniform hypergraph on $[n]$ and $s\\in \\mathbb{N}$ such that adding any edge to $G$ creates a copy of $K^{r}_{r+s}$. Then \\[&nbsp;&nbsp;&nbsp; \\left| E(G) \\right| \\ge \\binom{n}{r} - \\binom{n-s}{r}.\\] <br><br><b>Remark.</b> This bound is sharp: $E(G) := \\left\\{ A\\in [n]^{(r)}\\colon A\\cap \\left[ s \\right] \\neq \\varnothing \\right\\} $. For $r = 2,\\,s=1$, the bound is $n-1$ and the attaining graphs are stars.</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "Definition <i>[$]r[/$]-uniform hypergraph</i> and <i>complete [$]r[/$]-uniform hypergraph on [$]k[/$] vertices</i>.",
    "back": "<div>Let $r\\in \\mathbb{N}$. Then an <i>$r$-uniform hypergraph</i> is a pair $G = (V,E)$ with $V\\neq \\varnothing$ finite and $E\\subset V^{(r)}$. For $k\\ge r$, the <i>complete $r$-uniform hypergraph on $k$ vertices</i> is \\[&nbsp;&nbsp;&nbsp; K^{(r)}_k := \\left( [k], [k]^{(r)} \\right) .\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "<b>Ising model:</b> <br><ol>  <li>$\\Omega$, $\\Omega_G$, and $\\Omega_G^\\eta$ for $G\\subset \\mathbb{Z}^d$ finite and $\\eta \\in \\Omega$.&nbsp;&nbsp;&nbsp;</li>  <li>Ising Hamiltonian (free and with boundary conditions) $\\mathcal{H}_{G;\\beta,h}^{(\\eta)}$, finite-volume Gibbs measure $\\mu_{G;\\beta,h}^{(\\eta)} $ and partition function $\\mathcal{Z}_{G;\\beta,h}^{(\\eta)} $.&nbsp;&nbsp;&nbsp;</li>  <li>(Increasing) cylinders, weak convergence on $(\\Omega,\\mathcal{B}(\\Omega))$, and $\\mathcal{G}(\\beta,h)$.</li></ol>",
    "back": "<div>$\\Omega := \\left\\{ \\pm 1 \\right\\} ^{\\mathbb{Z}^d}$ is a compact topological space, a countable base of which is given by cylinders. Increasing cylinders are $\\cap$-stable generators of $\\mathcal{B}(\\Omega)$. Weak convergence is equivalent to pointwise convergence on increasing cylinders. For $G\\subset \\mathbb{Z}^d$, put $\\Omega_G:= \\left\\{ \\pm 1 \\right\\} ^{V(G)}$. If $\\eta \\in \\Omega$, put \\[\\Omega_G^\\eta := \\left\\{ \\omega\\in \\Omega\\colon \\omega = \\eta \\text{ on } \\mathbb{Z}^d\\setminus V(G) \\right\\}&nbsp; \\subset \\Omega,\\\\\\] which is finite. If $\\beta\\ge 0, h \\in \\mathbb{R}$, then \\[\\mathcal{H}_{G;b,h}^{(\\eta)} := -\\beta \\sum_{(i,j)\\in E(G) (\\cup\\, \\partial G)}\\sigma_i \\sigma_j - h \\sum_{i\\in V(G)}\\sigma_i \\colon \\Omega_G^{(\\eta)} \\to \\mathbb{R},\\\\\\] where $\\sigma_i\\colon \\Omega \\to \\left\\{ \\pm 1 \\right\\}; \\, (\\omega_j)_{j} \\mapsto \\omega_i$. Then $\\mu_{G;\\beta,h}^{(\\eta)} $ is a probability measure on $(\\Omega,\\mathcal{B}(\\Omega))$, resp. $(\\Omega_G, \\mathcal{P}(\\Omega_G))$, defined by \\[\\mu_{G;\\beta,h}^{(\\eta)} (\\left\\{ \\omega \\right\\} ) = \\frac{\\mathrm{e}^{-\\mathcal{H}_{G;\\beta,h}^{(\\eta)} (\\omega)}}{\\mathcal{Z}_{G;\\beta,h}^{(\\eta)} },\\quad \\omega\\in \\Omega_G^{(\\eta)}.\\] where $\\mathcal{Z}_{G;\\beta,h}^{(\\eta)} = \\sum_{\\omega\\in \\Omega_G^{(\\eta)}}\\mathrm{e}^{-\\mathcal{H}_{G;\\beta,h}^{(\\eta)} (\\omega)}$ is the <i>partition function</i>.<br><br>We put $\\mathcal{G}(\\beta,h)$ to be the set of probability measures on $(\\Omega,\\mathcal{B})$ that are the weak limit of a sequence $(\\mu_{G_n;\\beta,h}^{(\\eta_n)})_{n\\in \\mathbb{N}}$ for an exhaustion $(G_n)$ and a sequence $(\\eta_n)\\in \\Omega^{\\mathbb{N}}$.</div><div></div><div></div><div></div><div>[PGL Page 6/7]</div>"
  },
  {
    "front": "<b>Ising model:</b> Total and average magnetisation $M$ and $m$, pressure $\\psi_G(\\beta,h)$ and relation to $\\left&lt;m \\right&gt; $. Definitions of thermodynamic limit pressure and first-order phase transition.",
    "back": "<div>Total magnetisation is $M := \\sum_{i\\in V(G)}\\sigma_i \\colon \\Omega_G \\to \\mathbb{R}$, average magnetisation is $m = M / \\left| V(G) \\right| $. Define the <i>pressure</i> of the model by \\[&nbsp;&nbsp;&nbsp; \\psi_G(\\beta,h) := \\frac{\\log \\mathcal{Z}_{G;\\beta,h} }{\\left| V(G) \\right| },\\quad \\beta \\ge 0, h\\in \\mathbb{R}.\\] Then $\\left&lt;m \\right&gt; _{G;\\beta,h} = \\frac{\\partial}{\\partial h}\\psi_G(\\beta,h)$.<br><br><b>Definition.</b> The <i>thermodynamic limit pressure</i> of the model is given by the pointwise limit \\[&nbsp;&nbsp;&nbsp; \\psi(\\beta,h) = \\lim_{n\\to \\infty} \\psi_{\\Lambda(n)}(\\beta,h).\\] Then $\\psi$ is a real-valued, convex function with left- and right derivatives. We say that the model has a <i>first-order phase transition</i> at $(\\beta,h)\\in \\mathbb{R}_{\\ge 0}\\times \\mathbb{R}$ if $\\psi(\\beta,\\cdot )$ is not differentiable at $h$.</div><div></div><div></div><div></div><div>[PGL Page 6]</div>"
  },
  {
    "front": "Ising model: FKG inequality and Spatial Markov property (and proof of the latter).",
    "back": "<div><b>FKG Inequality.</b> For $\\beta\\ge 0, h\\in \\mathbb{R}$, and $\\eta\\in \\Omega$, and increasing functions $f,g\\colon \\Omega\\to \\mathbb{R}$, and every finite $G\\subset \\mathbb{Z}^d$, we have \\[\\left&lt;fg \\right&gt; _{G;\\beta,h}^{\\eta} \\ge \\left&lt;f \\right&gt; _{G;\\beta,h}^\\eta \\left&lt;g \\right&gt; _{G;\\beta,h}^\\eta.\\] In particular, $\\mu_G(\\mathcal{C}_{A\\cup B}) \\ge \\mu_G(\\mathcal{C}_A) \\mu_G(\\mathcal{C}_B)$ for any $A,B\\subset \\mathbb{Z}^d$. Also holds w.r.t. Gibbs measures $\\mu\\in \\mathcal{G}(\\beta,h)$ and continuous/local $f,g$.<br><br><b>Spatial Markov Porperty.</b> For $\\beta \\ge 0, h\\in \\mathbb{R}, \\eta \\in \\Omega$, and $G'\\subset G\\subset \\mathbb{Z}^d$ finite, \\[&nbsp;&nbsp;&nbsp; \\mu_{G';\\beta,h}^\\eta (\\,\\cdot\\,) = \\mu_{G;\\beta,h}^\\eta \\left(\\,\\cdot\\,\\middle\\vert\\, \\Omega_{G'}^\\eta\\right) = \\frac{\\mu_{G;\\beta,h}^\\eta(\\,\\cdot\\,\\cap \\Omega_{G'}^\\eta)}{\\mu_{G;\\beta,h}^\\eta(\\Omega_{G'}^\\eta)}.\\]</div><div></div><div></div><div></div><div>[PGL Page 7]</div>"
  },
  {
    "front": "Ising model: Measures [$]\\mu_{\\beta,h}^{\\pm} \\in \\mathcal{G}(\\beta,h)[/$].",
    "back": "<div>Given an exhaustion $(G_n)$, we have $\\mu_{G_n;\\beta,h}^+ \\implies \\mu_{\\beta,h}^+$ for a translation invariant probability measure $\\mu_{\\beta,h}^+$ on $(\\Omega,\\mathcal{B})$, which does not depend on $(G_n)$. Analogously for $\\mu_{\\beta,h}^-$.</div><div></div><div></div><div></div><div>[PGL Page 7]</div>"
  },
  {
    "front": "Recombination:<div>&nbsp;- Motivation (recombination in biology)</div><div>&nbsp;- Model</div><div>&nbsp;- ARG (marginal trees, embedded subgraphs)</div><div></div><div>Does an ARG contain the same information as the collection of marginal trees?</div>",
    "back": "<div><b>Motiviation.</b> In reality, parents don't pass on one of their chromosome-halves, but a combination of the two, knit together at certain points. This doesn't change anything when considering single sites, but now different sites within a gene may have different histories, with correlation between spatially close sites.<br><br><b>Model.</b> Assume that we look at a sequence of $s$ consecutive sites in the genome, small enough so that we can assume that at most one recombination occurs within this sequence, which we model by $[0,1]$. Then in case of recombination at a <i>breakpoint</i> $B\\in [0,1]$, the left- and right-hand sites of $B$ choose a distinct parent each.<br><br><b>Ancestral recombination graph (ARG).</b> The ARG is a unique representation of the ancestral history of a sample. On top of coalescent events and mutations, it has recombination events, and the latter two have labels in $[0,1]$.<br><ol>  <li>The <i>marginal trees</i> $T(x)$ for $x\\in [0,1]$ can be obtained from the ARG by taking appropriate branches.&nbsp;&nbsp;&nbsp;</li>  <li>The ARG of the sites $[a,b]\\subset [0,1]$ can be obtained as <i>embedded subgraphs</i> by deleting the appropriate branches in recombination events that occured in $[0,a)$ and $(b,1]$.</li></ol><br>The ARG has more information then the collection of marginal trees. For example, pairs of recombination and coalescence are invisible in all marginals.</div><div></div><div></div><div></div><div>[Genetics, Page 6]</div>"
  },
  {
    "front": "4-gamete-test for recombinations. (and an example where this effect can be seen)",
    "back": "<div>Under the infinite-sites model, given $n\\times s$ variational data with unknown ancestral type, if the pattern $\\begin{pmatrix} 0&amp;0\\\\0&amp;1\\\\1&amp;0\\\\1&amp;1 \\end{pmatrix} $ is present in any pair of sites $i,j\\in [s]$, then the history must have a recombination event that happened between sites $i$ and $j$.</div><div></div><div></div><div></div><div>[Genetics, Page 6]</div>"
  },
  {
    "front": "Hudson's [$]R_M[/$].",
    "back": "<div>Let variational data be given for $n$ sequences and $s$ ordered sites (corresponding to positions $0&lt;x_1&lt;\\ldots &lt;x_s&lt;1$), and define \\[&nbsp;&nbsp;&nbsp; R_{ij}:= \\,\\boldsymbol{1}\\!\\left\\{\\text{columns $i$ and $j$ show all $4$ gametes} \\right\\}\\] for $i,j\\in [s]$.<br><ol>  <li>$l\\leftarrow 1$, $r\\leftarrow 2$, $R_M \\leftarrow 0$.&nbsp;&nbsp;&nbsp;</li>  <li>If&nbsp; $\\displaystyle \\max_{l\\le k\\le r-1} R_{kr} = 1$, put $l\\leftarrow r$ and $R_M \\leftarrow R_M + 1$.&nbsp;&nbsp;&nbsp;</li>  <li>If $r =s$, terminate. Otherwise, put $r\\leftarrow r+1$ and go to ($2$).</li></ol><br>Then $R_M$ is the minimal number of recombination events required to produce the matrix $R$.</div><div></div><div></div><div></div><div>[Genetics, Page 6]</div>"
  },
  {
    "front": "\"Haplotype bound [$]H_M[/$]<div>&nbsp;- Motivating example (where [$]R_M = 1[/$] but at least 2 recombination events)</div><div>&nbsp;- \"\"[$]H-s-1[/$]\"\"-bound</div><div>&nbsp;- Haplotype bound [$]H_M[/$]</div>\"",
    "back": "<div><b>Proposition.</b> Suppose $n\\times s$-variational data is given, with $H$ haplotypes (unique rows). Then, under the infinite-sites model, at least $H-s-1$ recombinations must have occurred.<br><br>\\textbf{Haplotype bound $H_M$.} Let $n\\times s$-variational data be given, and let $R_{ij}$ for $i&lt;j$ be the best $(H-s-1)$-bound obtained from subsets of $[i,j]$. Then put $H_M^{(1)} := 0$ and \\[&nbsp;&nbsp;&nbsp; H_M^{(j)} := \\max_{1\\le k &lt; j} \\left( H_M^{(k)}+R_{kj} \\right) ,\\quad 2\\le j \\le n.\\] Then, $H_M:= H_M^{(s)}$is the minimum number of recombination events needed to satisfy all $(H-s-1)$-bounds of subsets of $[s]$.</div><div></div><div></div><div></div><div>[Genetics, Page 6/7]</div>"
  },
  {
    "front": "Ising model: monotonicity of [$]\\left&lt; f \\right&gt;_{G;\\beta,h}^\\eta[/$] as function of [$]\\eta[/$] for increasing [$]f[/$]. Corollary on sandwiching [$]\\left&lt; f \\right&gt; _\\mu[/$] for [$]\\mu\\in \\mathcal{G}(\\beta,h)[/$].",
    "back": "<div>Let $\\beta \\ge 0, h \\in \\mathbb{R}$, $G\\subset \\mathbb{Z}^d$ finite, and $f\\colon \\Omega\\to \\mathbb{R}$ increasing. Then if $\\eta,\\eta'\\in \\Omega$ with $\\eta \\le \\eta'$, then \\[\\left&lt;f \\right&gt; _{G;\\beta,h}^{\\eta}\\le \\left&lt;f \\right&gt; _{G;\\beta,h}^{\\eta'}.\\] In particular, if $f$ is local (or, more generally, continuous), and $\\mu\\in \\mathcal{G}(\\beta,h)$, then \\[\\left&lt;f \\right&gt; _{\\beta,h}^- \\le \\left&lt;f \\right&gt; _\\mu \\le \\left&lt;f \\right&gt; _{\\beta,h}^+.\\]</div><div></div><div></div><div></div><div>[PGL, Page 7]</div>"
  },
  {
    "front": "Ising model: Characterisation of [$]\\left| \\mathcal{G}(\\beta,h)\\right| = 1[/$].<div></div><div>Connection between [$]\\Psi(\\beta,h)[/$] and [$]\\left&lt; \\sigma_0\\right&gt;^{\\pm}[/$], thus characterisation of existence of 1st order phase transition at [$](\\beta,h)[/$] in terms of [$]\\mathcal{G}(\\beta,h)[/$].</div>",
    "back": "<div><b>Proposition.</b> Let $\\beta \\ge 0, h\\in \\mathbb{R}$. Then the following are equivalent.<br><ol>  <li>$\\left| \\mathcal{G}(\\beta,h) \\right| =1$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mu_{\\beta,h}^+ = \\mu_{\\beta,h}^-$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left&lt;\\sigma_0 \\right&gt; _{\\beta,h}^+ = \\left&lt;\\sigma_0 \\right&gt; _{\\beta,h}^-$.</li></ol></div><div></div><div><b>Theorem.</b> $\\left&lt; \\sigma_0\\right&gt;^\\pm_{\\beta;0} = \\frac{\\partial}{\\partial h^\\pm} \\Psi(\\beta,h)$. In particular, there is a 1st order phase transition at $(\\beta,h)$ iff $\\left| \\mathcal{G}(\\beta,h)\\right| &gt; 1$.</div><div></div><div></div><div></div><div>[PGL, Page 8]</div>"
  },
  {
    "front": "Wright-Fisher model with recombination:<div>&nbsp;- Definition</div><div>&nbsp;- Probabilities of number and kind of events per generation for large [$]N[/$]</div><div>&nbsp;- Asymptotic (rescaled) times [$]T_j[/$] and distributional limit for large [$]N[/$]</div>",
    "back": "<div>Consider WF model with population size $2N$, where every descendant recombines with probability $r\\in [0,1]$ (in which case he chooses two parents independently), and mutates with probability $\\mu\\in [0,1]$. Put $\\rho := 4Nr$ and $\\theta := 4N\\mu$, and consider a sub-sample of size $j&gt;1$.<br><ol>  <li>$\\mathbb{P}\\left( \\text{only $1$ mutation}&nbsp; \\right) = \\frac{1}{2N} \\frac{\\theta j}{2} + O(N^{-2})$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}\\left( \\text{only $1$ recombination}&nbsp; \\right) = \\frac{1}{2N} \\frac{\\rho j}{2} + O(N^{-2})$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}\\left( \\text{only $1$ coalescence}&nbsp; \\right) = \\frac{1}{2N} \\binom{j}{2} + O(N^{-2})$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}\\left( \\text{at least $2$ events}&nbsp; \\right) = O(N^{-2})$.</li></ol><br>Now let $t_j$ be the number of consecutive generations (at $j$ lineages) without any events and put $T_j := \\frac{t_j}{2N}$. Then \\[&nbsp;&nbsp;&nbsp; T_j \\stackrel{ d }{\\longrightarrow} \\text{Exp} \\left( \\binom{j}{2} + \\frac{\\theta j}{2} + \\frac{\\rho j}{2} \\right) .\\] Furthermore, if $E$ denotes the next event (there is a unique one with asymptotic probability $1$), then \\[\\mathbb{P}\\left( \\text{$E$ is a mutation at a specific lineage}&nbsp; \\right) \\stackrel{  }{\\longrightarrow}&nbsp; \\frac{\\theta / 2}{\\binom{j}{2} + \\frac{\\theta j}{2} + \\frac{\\rho j}{2}} = \\frac{\\theta}{j(j-1 + \\theta + \\rho)}.\\] For recombination and coalescence, replace numerator with $\\rho/2$ and $1$, respectively.</div><div></div><div></div><div></div><div>[Genetics, Page 7]</div>"
  },
  {
    "front": "Coalescent with recombination (two equivalent definitions). What is the last remaining lineage called?",
    "back": "<div>The <i>coalescent with recombination</i> describes the history of a sample of size $n$ as a Markov process. While $j&gt;1$ lineages remain, wait for time $T_j \\sim \\text{Exp} \\left( \\binom{j}{2} + \\frac{\\theta j}{2}+ \\frac{\\rho j}{2} \\right) $ until next event, which is a coalescence / recombination / mutation of a uniformly chosen pair of lineages / lineage / lineage with probabilities $(j-1)$ / $\\rho$ / $\\theta$ $\\cdot \\left( j-1+\\theta + \\rho \\right) ^{-1}$.<br><br><i>Equivalently</i>, each pair of lineages coalesces independently with rates $1$, and each lineage mutates / recombines independently with rates $\\theta / 2$ / $\\rho / 2$.<br><br><b>Remark.</b> <br><ol>  <li>Distribution of brakepoints in recombination events is completely general,&nbsp;&nbsp;&nbsp;</li>  <li>Allows for general mutation model, so an ancestral type is chosen, and then at every mutation, a new mutant type according to some model (usually a function of the old type),&nbsp;&nbsp;&nbsp;</li>  <li>Last remaining lineage is called <i>grand most recent common ancestor</i>.</li></ol></div><div></div><div></div><div></div><div>[Genetics, Page 7]</div>"
  },
  {
    "front": "Ising model: What is known about first-order phase transitions for general [$]d[/$]? Curie temperature?",
    "back": "<div>For $d = 1$, there is no phase transition for arbitrary $\\beta \\ge 0$, $h\\in \\mathbb{R}$. For $d\\ge 2$, there exists $\\beta_c \\ge 0$ such that there is a phase transition (i.e. $\\mu^+ \\neq \\mu^-$) at $(\\beta,0)$ for all $\\beta &gt; \\beta_c$, but no phase transitions for $h\\neq 0$. This means that spontaneous magnetisation occurs for vanishing external field and temperature $T &lt; T_C$, where $T_C \\sim 1 / \\beta_c$ is the Curie temperature.</div><div></div><div></div><div></div><div>[PGL Page 8]</div>"
  },
  {
    "front": "Ising model: proof of phase transition in [$]d=2[/$]",
    "back": "<div>If $d=2$, then $\\left&lt;\\sigma_0 \\right&gt; ^{\\pm}_{\\beta,0}\\stackrel{ \\beta \\to \\infty }{\\longrightarrow} \\pm 1$. In particular, there exists $\\beta_c (\\le 2)$ such that there is a phase transition at $(\\beta,0)$ for all $\\beta &gt; \\beta_c$.</div><div></div><div></div><div></div><div>[PGL Page 8]</div>"
  },
  {
    "front": "Definition of a time change [$](\\sigma_t)[/$] and [$](\\sigma_t)[/$]-continuity of a continuous martingale [$]M[/$].",
    "back": "<div>A <i>time change</i> is a family $(\\sigma_t)_{t\\ge 0}$ of $\\mathcal{F}_t$-stopping times such that<br><ol>  <li>$\\sigma_s \\le \\sigma_t$ almost surely for all $0\\le s\\le t$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\lim_{s\\downarrow t}\\sigma_s = \\sigma_t$ almost surely for all $t\\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\sigma_t \\stackrel{  }{\\longrightarrow} \\infty$ almost surely as $t\\to \\infty$.</li></ol><br>Note that if $(M_t)$ is a uniformly integrable $\\mathcal{F}_t$-martingale, then $(M_{\\sigma_t})_{t\\ge 0}$ is an $\\mathcal{F}_{\\sigma_t}$-martingale. For $t\\ge 0$ set&nbsp; \\[\\sigma_{t-}:= \\lim_{s\\uparrow t}\\sigma_s \\le \\sigma_t.\\] Then a martingale $M$ is called <i>$(\\sigma_t)$-continuous</i> if $M_{\\sigma_{t-}}= M_{\\sigma_t}$ almost surely for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[SDEs, Page 5]</div>"
  },
  {
    "front": "The intervals of constancy of a clm [$]M[/$] with [$]M_0 = 0[/$] and [$]\\left&lt; M\\right&gt;[/$] coincide almost surely.",
    "back": "<div>Let $(M_t)$ be a continuous local martingale, $M_0 = 0$. Then for all $\\omega\\in \\Omega_0$ for a one-set&nbsp; $\\Omega_0$, we have that for all $0\\le a \\le b&lt;\\infty$, \\[&nbsp;&nbsp;&nbsp; \\forall t\\in [a,b]\\colon M_t(\\omega) = M_a(\\omega) \\iff \\forall t\\in [a,b]\\colon \\left&lt;M \\right&gt; _t(\\omega) = \\left&lt;M \\right&gt; _a(\\omega).\\]</div><div></div><div></div><div></div><div>[SDEs, Page 5]</div>"
  },
  {
    "front": "Burkholder-Davis-Gundy inequality.",
    "back": "<div>For every $p\\in (0,\\infty)$ there exist constants $c_p,C_p &gt; 0$ such that for every continuous local martingale $M$ with $M_0 = 0$ and every finite stopping time $T$, \\[&nbsp;&nbsp;&nbsp; c_p \\mathbb{E} \\left[ \\left( M_T^\\star \\right) ^{2p} \\right] \\le \\mathbb{E} \\left[ \\left&lt;M \\right&gt; _T^p \\right] \\le C_p \\mathbb{E} \\left[ \\left( M_T^\\star \\right) ^{2p} \\right] .\\]</div><div></div><div></div><div></div><div>[SDEs, Page 6]</div>"
  },
  {
    "front": "Martingale representation theorem.",
    "back": "<div>Let $B$ be a standard Brownian motion with natural filtration $(\\mathcal{F}_t^B)_{t\\ge 0}$. Then for any $M\\in L^2(\\Omega,\\mathcal{F}_\\infty^B,\\mathbb{P})$, there exists a unique $H\\in L^2(B)$ such that \\[M = \\mathbb{E} \\left[ M \\right] + \\int_0^\\infty H_s \\mathop{}\\!\\mathrm{d} B_s.\\]</div><div></div><div></div><div></div><div>[SDEs, Page 6]</div>"
  },
  {
    "front": "Definition of VC dimension. What is VC dimension of [$][n]^{(\\le d)}[/$] and of the set of half-planes in [$]\\mathbb{R}^2[/$]?",
    "back": "<div><b>Definition.</b> Let $X\\neq \\varnothing$ and $\\mathcal{F}\\subset \\mathcal{P}(X)$.<br><ol>  <li>For $S\\subset X$, the <i>trace of $\\mathcal{F}$ on $S$</i> is $\\mathcal{F}\\,\\vert\\, S := \\mathcal{F}\\cap S := \\left\\{ F\\cap S\\colon F\\in \\mathcal{F} \\right\\} $, and $\\text{tr} _{\\mathcal{F}}(S) := \\left| \\mathcal{F}\\cap S \\right| $. We say that $S$ is <i>shattered</i> by $\\mathcal{F}$ if $\\mathcal{F}\\cap S = \\mathcal{P}(S)$,&nbsp;&nbsp;&nbsp;</li>  <li>The <i>VC-dimension</i> (Vapnik-Chervonenkis) of $\\mathcal{F}$ is $\\max \\left\\{ \\left| S \\right| \\colon \\text{$S\\subset X$ is shattered by $\\mathcal{F}$}&nbsp; \\right\\} $.</li></ol><br><br>$\\left[ n \\right]^{(\\le d)}$ has VC dimension $d$, half planes in $\\mathbb{R}^2$ have VC dimension $3$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "Theorem (Sauer-Selah) on VC dimensions. Two proofs.",
    "back": "<div>If $n\\in \\mathbb{N}$ and $\\mathcal{F}\\subset \\mathcal{P}(n)$ has VC-dimension at most $d\\le n$, then \\[&nbsp;&nbsp;&nbsp; \\left| \\mathcal{F} \\right| \\le \\left| \\left[ n \\right] ^{(\\le d)} \\right| = \\sum_{i=0}^d \\binom{n}{i}.\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 4]</div>"
  },
  {
    "front": "Kleitman's theorem on downsets.",
    "back": "<div>Let $\\mathcal{A}, \\mathcal{B}\\subset \\mathcal{P}(n)$ be downsets. Then \\[\\left| \\mathcal{A}\\cap \\mathcal{B} \\right| \\ge 2^{-n} \\left| \\mathcal{A} \\right| \\left| \\mathcal{B} \\right| .\\]&nbsp;</div><div></div><div></div><div></div><div>[Combinatorics, Page 5]</div>"
  },
  {
    "front": "Grönwall's inequality",
    "back": "<div>Let $\\phi\\colon [0,\\infty) \\to [0,\\infty)$ be locally bounded, and $a,b\\ge 0$ such that \\[&nbsp;&nbsp;&nbsp; \\phi(t) \\le a + b\\int_0^t \\phi(s) \\mathop{}\\!\\mathrm{d} s\\\\\\] for all $t\\ge 0$. Then $\\phi(t) \\le a\\mathrm{e}^{bt}$ for all $t\\ge 0.$</div><div></div><div></div><div></div><div>[SDEs, Page 7]</div>"
  },
  {
    "front": "Existence and uniqueness for SDEs with Lipschitz coefficients.",
    "back": "<div>Let $m,d\\in \\mathbb{N}$, $(Z_t)$ an $m$-dimensional continuous $\\mathcal{F}_t$-semimartingale, and $f\\colon [0,\\infty) \\times C\\left( [0,\\infty),\\mathbb{R}^d \\right) \\to \\mathbb{R} ^{d\\times m}$ measurable such that<br><ol>  <li>$f(\\cdot ,0)$ is locally bounded (and thus $f(\\cdot ,x)$ for all $x\\in C$),&nbsp;&nbsp;&nbsp;</li>  <li>There exists $K&gt;0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| f(t,x) - f(t,y) \\right| \\le K \\sup_{0\\le s \\le t} \\left| x(s) - y(s) \\right| &nbsp;&nbsp;&nbsp; \\] for all $x,y\\in C$ and $t\\ge 0$.</li></ol><br>Then for every $x\\in \\mathbb{R}^d$, there exists an $\\mathcal{F}^Z_t$-adapted continuous semimartingale $X$ with \\[&nbsp;&nbsp;&nbsp; X_t = x + \\int_{0}^t f(s,X) \\mathop{}\\!\\mathrm{d} Z_s,\\quad t\\ge 0.\\] Furthermore, the solution of this SDE is unique up to indistinguishability.</div><div></div><div></div><div></div><div>[SDEs, Page 7/8]</div>"
  },
  {
    "front": "Definition Pott's model and connection with Ising model.",
    "back": "<div>Let $G$ be a finite, connected graph, $\\beta \\ge 0$, and $q\\in \\mathbb{N}$. Then for $\\omega\\in \\left\\{ 1,\\ldots ,q \\right\\} ^{V(G)}$, \\[&nbsp;&nbsp;&nbsp; \\pi_{G;q,\\beta}(\\left\\{ \\omega \\right\\} ) := \\frac{1}{\\mathcal{Z}_{G;\\beta,h}^\\text{Potts} } \\exp \\left( \\beta \\sum_{(i,j)\\in E(G)} \\,\\boldsymbol{1}\\!\\left\\{\\omega_i = \\omega_j\\right\\} \\right) .\\] With $G \\subset \\mathbb{Z}^d$ and $q=2$, one obtains the Ising model with $h = 0$ and $\\beta' = \\beta / 2$.</div><div></div><div></div><div></div><div>[PGL, Page 8]</div>"
  },
  {
    "front": "Definition RCM (Random Cluster Model). Connection with bond percolation?",
    "back": "\"<div>Let $G$ be a finite, connected graph, $q&gt;0$, and $p\\in (0,1)$. Then for $\\omega\\in \\left\\{ 0,1 \\right\\} ^{E(G)}$, \\[&nbsp;&nbsp;&nbsp; \\phi_{p,q}(\\left\\{ \\omega \\right\\} ) := \\frac{1}{\\mathcal{Z}_{p,q}^{\\text{RCM}}} \\left( \\prod_{e\\in E(G)} p^{\\omega(e)} (1-p)^{1-\\omega(e)} \\right)&nbsp; q^{K(\\omega)},\\\\\\] where $K(\\omega) := \\# \\text{connected components of $\\omega$} $. Then $\\phi_{p,1}$ is bond-percolation.</div><div></div><div></div><div></div><div>[PGL, Page 8]</div>\""
  },
  {
    "front": "Coupling of RCM and Potts. What does this imply for obtaining a sample of one via a sample of the other?",
    "back": "<div>Let $G$ be finite, connected, $p\\in (0,1)$, and $q\\in \\mathbb{N}$. Set \\[&nbsp;&nbsp;&nbsp; F:= \\left\\{ (\\sigma,\\omega) \\colon \\forall e = (i,j)\\in E(G)\\colon \\omega(e) = 1 \\implies \\sigma_i = \\sigma_j \\right\\} \\subset [q]^{V(G)}\\times \\left\\{ 0,1 \\right\\} ^{E(G)},\\\\\\] the set of <i>compatible pairs</i>. Then the measure defined by \\[\\mu\\left( \\left\\{ (\\sigma,\\omega) \\right\\}&nbsp; \\right) \\propto \\boldsymbol{1}_{F}(\\sigma,\\omega) \\phi_{p,1}(\\omega)\\] has marginals $\\pi_{q,\\beta(q)}$ and $\\phi_{p,q}$, where $\\beta(q) = - \\log(1-p)$. Furthermore,<br><ol>  <li>$\\mu (\\sigma \\,|\\, \\omega) = \\boldsymbol{1}_{F}(\\sigma,\\omega) \\left( \\frac{1}{q} \\right) ^{K(\\omega)}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mu(\\omega\\,|\\, \\sigma) = \\boldsymbol{1}_{F}(\\sigma,\\omega) \\frac{\\phi_{p,1}(\\omega)}{(1-p)^{\\left| E^\\sigma_{\\neq} \\right| }}$.</li></ol> This means that sampling and RCM and then colouring the connected components uniformly gives a Potts sample; Sampling Potts and then performing ordinary bond percolation on allowed edges gives an RCM sample.</div><div></div><div></div><div></div><div>[PGL, Page 8/9]</div>"
  },
  {
    "front": "Expectation and variance of number of segregating sites [$]S_n[/$] in coalescent with recombination.",
    "back": "<div>Assume that mutations and recombinations are placed uniformly at random in $[0,1]$ (in particular, infinite-sites). Then let $S_n$ be the number of segregating sites in a coalescent of size $n$ (with recombination). Then<br><ol>  <li>$\\mathbb{E} \\left[ S_n \\right] = \\theta \\sum_{j=1}^{n-1}\\frac{1}{j}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{V}(S_n) = \\theta \\sum_{j=1}^{n-1}\\frac{1}{j} + \\frac{\\theta^2}{2}\\int_0^1 (1-z) f_n(\\rho z) \\mathop{}\\!\\mathrm{d} z$,</li></ol> where $f_n(\\rho z)$ is the covariance between total marginal tree lengths of sites at distance $z$. We have \\[\\frac{\\theta^2}{2} \\int_0^1 (1-z) f_n(\\rho z) \\mathop{}\\!\\mathrm{d} z \\to \\begin{cases}&nbsp;&nbsp;&nbsp; 0 &amp;, \\rho \\to \\infty,\\\\&nbsp;&nbsp;&nbsp; \\theta^2 \\sum_{j=1}^n 1 / j^2 &amp;, \\rho \\to 0.\\end{cases}\\] In particular, expectation is the same and variance lower than in the no recombination case (but asymptotically the same).</div><div></div><div></div><div></div><div>[Genetics, Page 7/8]</div>"
  },
  {
    "front": "(Upper bound for) expectation of number of distinct times to MRCA of marginals in a coalescent with recombination. Is it a lot?",
    "back": "\"<div>Assume that recombinations happen uniformly at random in $[0,1]$. Then the number $C_n$ of changes of the TMRCA when moving through marginal trees from $0$ to $1$ in a coalescent with recombination is \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ C_n \\right] = \\rho \\left( 1 - \\frac{2}{n(n+1)} \\right) .\\] In particular, $\\mathbb{E} \\left[ \\# \\text{distinct TMRCAs}&nbsp; \\right] \\le 1+\\mathbb{E} \\left[ C_n \\right] \\le 1+\\rho$ for all $n\\in \\mathbb{N}$. That's surprisingly few!</div><div></div><div></div><div></div><div>[Genetics, Page 8]</div>\""
  },
  {
    "front": "\"Distribution of subgraphs of ARG and \"\"small ARG\"\"\"",
    "back": "<div><b>Subgraphs:</b> The ARG obtained by deleting events that happened outside $[a,b]\\subset [0,1]$ has same distribution as an ARG with mutation / recombination rates $\\theta / 2 (b-a)$ / $\\rho / 2 (b-a)$. In particular, subgraph $T([x,x+\\delta x])$ for $\\delta \\to 0$ has no recombinations, so $T(x)$ is standard coalescent.<br><br><b>small ARG:</b> The subgraph obtained by deleting edges that contribute no genetic material to the final sample, and edges that carry only material for sites that have already reached their MRCA. Can be simulated by dynamically reducing recombination and mutation rates accordingly.</div><div></div><div></div><div></div><div>[Genetics, Page 7]</div>"
  },
  {
    "front": "Mixing times: Definitions of [$]d(t),\\, \\rho(t), \\, t_\\text{mix}(\\varepsilon)[/$], asymptotic behaviour of the former and relation between the former two.",
    "back": "<div>Let $P$ be a Markov transition kernel over a finite state space $S$ that converges to a stationary distribution $\\pi$. For $t\\ge 0$ and $\\varepsilon &gt; 0$, define<br><ol>  <li>$d(t) := \\max_{x\\in S}\\left| P_t^x - \\pi \\right| _\\text{TV} $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\rho(t) := \\max_{x,y\\in S} \\left| P_t^x - P_t^y \\right| _\\text{TV} $,&nbsp;&nbsp;&nbsp;</li>  <li>$t_\\text{mix} (\\varepsilon) := \\inf \\left\\{ t\\ge 0\\colon d(t) \\le \\varepsilon \\right\\} $.</li></ol><br>Then we have $d(t) \\stackrel{  }{\\longrightarrow} 0$ as $t\\to \\infty$ and $d(t) \\le \\rho(t) \\le 2d(t)$ for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[PGL, Page 9]</div>"
  },
  {
    "front": "Mixing times: Definition coupling and coupling lemma.",
    "back": "<div>Let $\\mu$ and $\\nu$ be probability measures on $S$. Then a <i>coupling</i> of $\\mu$ and $\\nu$ is a random variable $(X,Y)\\colon (\\Omega,\\mathcal{A},\\mathbb{P})\\to S$ such that $X\\sim \\mu$ and $Y\\sim \\nu$.<br><br><b>Coupling lemma.</b> It holds that \\[&nbsp;&nbsp;&nbsp; \\left| \\mu-\\nu \\right| _\\text{TV} = \\min \\left\\{ \\mathbb{P}\\left( X\\neq Y \\right) \\colon (X,Y)\\colon (\\Omega,\\mathcal{A},\\mathbb{P})\\to S \\text{ is a coupling of $\\mu$ and $\\nu$}&nbsp; \\right\\} .\\]</div><div></div><div></div><div></div><div>[PGL, Page 9]</div>"
  },
  {
    "front": "Mixing times: submultiplicativity of [$]\\rho[/$]",
    "back": "<div>If $t,s\\ge 0$, then $\\rho(t+s) \\le \\rho(t) \\rho(s)$.</div><div></div><div></div><div></div><div>[PGL, Page 9]</div>"
  },
  {
    "front": "Mixing times: Behaviour of [$]t_\\text{mix}[/$] as a function of [$]\\varepsilon[/$].<br>",
    "back": "<div>Let $0&lt;\\varepsilon&lt;\\delta &lt; \\frac{1}{2}$. Then \\(<br>&nbsp;&nbsp;&nbsp; t_\\text{mix} (\\delta) \\le t_\\text{mix} (\\varepsilon) \\le \\left\\lceil \\frac{\\log \\varepsilon}{\\log (2\\delta)} \\right\\rceil t_\\text{mix} (\\delta).<br>\\) Putting $t_\\text{mix} := t_\\text{mix}(1/4) $, this means that for $0&lt;\\varepsilon&lt;1 / 4$, \\[t_\\text{mix}&nbsp; \\le t_\\text{mix} (\\varepsilon) \\le \\left\\lceil \\frac{\\log(\\varepsilon^{-1})}{\\log 2} \\right\\rceil t_\\text{mix}.\\]</div><div></div><div></div><div></div><div>[PGL, Page 9]</div>"
  },
  {
    "front": "Lazy random walk on [$]\\{0,1\\}^n[/$] and mixing time.",
    "back": "<div>$S=\\left\\{ 0,1 \\right\\} ^n$. In the $t$'th step, flip a uniformly chosen coordinate with probability $\\frac{1}{2}$, otherwise do nothing. That is, \\[&nbsp;&nbsp;&nbsp; P(x,x) = \\frac{1}{2},\\quad P(x,x^i) = \\frac{1}{2n},\\qquad x\\in S, i\\in [n],\\\\\\] where $x^i$ is $x$ with flipped $i$'th coordinate. This Markov chain is irreducible, aperiodic, and has unique stationary distribution $\\pi = \\text{Unif} (S)$. Then \\[t_\\text{mix} (\\varepsilon) \\le \\left\\lceil n\\log n + n \\log (1 / \\varepsilon) \\right\\rceil ,\\quad \\varepsilon &gt; 0,\\\\\\] so $t_\\text{mix} = O(n \\log n)$. In fact, $t_\\text{mix} \\sim \\frac{1}{2}n \\log n$.</div><div></div><div></div><div></div><div>[PGL, Page 10]</div>"
  },
  {
    "front": "[$]q[/$]-colouring Markov chain on finite graph [$]G[/$]. When is it irreducible/aperiodic, what is stationary distribution, what is mixing time?",
    "back": "<div>Let $G$ be a finite graph, $n = \\left| V(G) \\right| $, $q\\in \\mathbb{N}$, $S\\subset [q]^{V(G)}$ the set of proper $q$-colourings. If $S \\neq \\varnothing$, define MC $(\\eta_t)$ on $S$ by choosing, for $t\\in \\mathbb{N}$, $v_t\\in V(G)$ and $c_t \\in [q]$ uniformly at random, and attempting to recolour $v_t$ with colour $c_t$ (if that would yield invalid colouring, do nothing). This MC is aperiodic, irreducible if $q\\ge \\Delta +2$, and has unique stationary distribution $\\text{Unif} (S)$.<br><br><b>Theorem.</b> If $q\\ge 4\\Delta + 1$, then \\[&nbsp;&nbsp;&nbsp; t_\\text{mix} (\\varepsilon) \\le&nbsp; \\left\\lceil\\frac{q}{q-4\\Delta} (n\\log n + n \\log( 1 / \\varepsilon))&nbsp; \\right\\rceil ,\\quad \\varepsilon &gt; 0,\\\\\\] so $t_\\text{mix} = O(n\\log n)$.</div><div></div><div></div><div></div><div>[PGL, Page 10]</div>"
  },
  {
    "front": "Theorem (Bubley, Dyer) on mixing times.",
    "back": "<div>Suppose $S$ is the finite state space for a MC (which converges to a unique stationary distribution), and suppose that $E_S \\subset S^{(2)}$ implies a connected graph structure on $S$. If there exists $\\alpha &gt; 0$ such that for any $(x,y)\\in E_S$ there exists a coupling $(X,Y)$ of $P_x^1$, $P_y^1$ with $\\mathbb{E} \\left[ d_S(X,Y) \\right] \\le 1 - \\alpha$, then \\[&nbsp;&nbsp;&nbsp; t_\\text{mix} (\\varepsilon) \\le \\frac{1}{\\alpha}\\left( \\log \\text{diam} (S) + \\log (1 / \\varepsilon) \\right) ,\\quad \\varepsilon &gt; 0.\\]</div><div></div><div></div><div></div><div>[PGL, Page 10]</div>"
  },
  {
    "front": "Carathéodory (Definition of semiring and ring).",
    "back": "<div><b>Definition.</b> Let $\\Omega\\neq \\varnothing$.<br><ol>  <li>A <i>semiring</i> $\\mathcal{H}\\subset \\mathcal{P}(\\Omega)$ contains $\\varnothing$, is $\\cap$-stable, and for $A,B\\in \\mathcal{H}$ there exists $k\\in \\mathbb{N}$ and $C_1,\\ldots ,C_k\\in \\mathcal{H}$ with $A\\setminus B = \\sum_{i=1}^k C_i$,&nbsp;&nbsp;&nbsp;</li>  <li>A <i>ring</i> $\\mathcal{R}\\subset&nbsp; \\mathcal{P}(\\Omega)$ contains $\\varnothing$, is $\\cup$-stable, and closed under relative complement. In particular, it is $\\cap$-stable,&nbsp;&nbsp;&nbsp;</li>  <li>A <i>measure</i> on $\\mathcal{H}$ is a $\\sigma$-additive map $\\mu\\colon \\mathcal{H}\\to [0,\\infty]$.</li></ol><br><b>Carathéodory.</b> If $\\mathcal{H}\\subset \\mathcal{P}(\\Omega)$ is a semiring and $\\mu\\colon \\mathcal{H}\\to [0,\\infty]$ is a measure, then there exists an extension of $\\mu$ to a measure on $\\sigma(\\mathcal{H})$. It is unique if there exists $(A_n)\\in \\mathcal{H}^\\mathbb{N}$ with $A_n\\uparrow \\Omega$ and $\\mu(A_n)&lt;\\infty$ for all $n\\in \\mathbb{N}$.</div><div></div><div></div><div></div><div>[WT Page 1]</div>"
  },
  {
    "front": "Radon-Nikodym.",
    "back": "<div>If $\\mu$ and $\\nu$ are measures on $(\\Omega,\\mathcal{A})$, $\\mu$ is $\\sigma$-finite, and $\\nu \\ll \\mu$, then $\\nu$ has a $\\mu$-density.</div><div></div><div></div><div></div><div>[WTheorie Page 1]</div>"
  },
  {
    "front": "Lebesgue-decomposition.",
    "back": "<div>If $\\mu$ and $\\nu$ are measures on $(\\Omega,\\mathcal{A})$ and $\\nu$ is $\\sigma$-finite, then there is a unique decomposition $\\nu = \\nu_a + \\nu_s$ such that \\[\\nu_a \\ll \\mu,\\qquad \\nu_s \\perp \\mu.\\]</div><div></div><div></div><div></div><div>[WTheorie, Page 1]</div>"
  },
  {
    "front": "If $f\\colon (\\Omega,\\mathcal{A})\\to (\\Sigma,\\mathcal{F})$ and $\\mathcal{M}\\subset \\mathcal{F}$, then $\\sigma(f^{-1}(\\mathcal{M})) = f^{-1}(\\sigma(\\mathcal{M}))$.",
    "back": "[WTheorie, Page 1]"
  },
  {
    "front": "If $\\mathcal{F}\\subset \\mathcal{P}(\\Omega)$ is a $\\cap$-stable, then $\\delta(\\mathcal{F}) = \\sigma(\\mathcal{F})$.",
    "back": "[General Stochastics Page 1]"
  },
  {
    "front": "If $(\\mathcal{F}_j)_{j\\in J} \\in \\mathcal{P}(\\Omega)^J$ is an independent family, then so is $(\\delta(\\mathcal{F}_j))$.",
    "back": "[General Stochastics, Page 1]"
  },
  {
    "front": "How to prove that $\\mathcal{F}\\perp \\sigma(X_i\\colon i\\in I)$ for $\\mathcal{F}\\subset \\mathcal{A}$ and random variables $X_i\\colon (\\Omega,\\mathcal{A},\\mathbb{P}) \\to (\\Sigma,\\mathcal{E}),\\, i\\in I$.",
    "back": "<div></div><div>Show that $\\mathcal{F}\\perp (X_j\\colon j\\in J)$ for all $J\\subset I$ finite, that is, that for all $F\\in \\mathcal{F}$ and $A_j$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(F\\cap\\{&nbsp; \\forall j\\in J\\colon X_j\\in A_j\\}) = \\mathbb{P}(F) \\mathbb{P}( \\forall j\\in J\\colon X_j\\in A_j).\\]</div><div></div><div></div>[General Stochastics Page 1]"
  },
  {
    "front": "Kolmogorov's zero-one law.",
    "back": "<div>Let $(X_j\\colon \\Omega\\to \\Omega_j)_{j\\in \\mathbb{N}}$ be independent random variables. Then \\[&nbsp;&nbsp;&nbsp; \\mathcal{A}_\\infty := \\bigcap_{k=1}^\\infty \\sigma\\left( X_j\\colon j\\ge k \\right) \\] is $\\mathbb{P}$-trivial.</div><div></div><div></div><div></div><div>[General Stochastics, Page 1]</div>"
  },
  {
    "front": "Merging independent [$]\\sigma[/$]-algebras in blocks results in independent [$]\\sigma[/$]-algebras.",
    "back": "<div>If $(\\mathcal{F}_j)_{j\\in J}$ is an independent family of $\\sigma$-algebras, $J = \\sum_{s\\in S}J_s$, and $\\mathcal{A}_s := \\sigma \\left( \\bigcup_{j\\in J_s} \\mathcal{F}_j \\right) $, then $(\\mathcal{A})_{s\\in S}$ is independent.</div><div></div><div></div><div></div><div>[General Stochastics, Page 1]</div>"
  },
  {
    "front": "Convex functions: Continuity and differentiability and [$]f_{\\pm}'[/$].",
    "back": "<div>Let $f\\colon \\mathbb{R}\\to \\mathbb{R}$ be convex. Then for all $x\\in \\mathbb{R}$ the limits \\[&nbsp;&nbsp;&nbsp; f_-'(x) := \\lim_{h\\downarrow 0} \\frac{f(x) - f(x-h)}{h},\\qquad f_+'(x) := \\lim_{h\\downarrow 0}\\frac{f(x+h) - f(x)}{h}\\] exist. $f_-'$ and $f_+'$ are left- and right-continuous, respectively, and coincide on a co-countable set. In particular, $f$ is continuous and continuously differentiable almost-everywhere.</div><div></div><div></div><div></div><div>[C8.1 Page 8]</div>"
  },
  {
    "front": "Integration by parts formula for convex functions.",
    "back": "<div>If $f\\colon \\mathbb{R}\\to \\mathbb{R}$ is convex and $\\phi \\in C_c^\\infty(\\mathbb{R})$, then \\[&nbsp;&nbsp;&nbsp; \\int_\\mathbb{R} f'(x) \\phi(x) \\mathop{}\\!\\mathrm{d} x = - \\int_{\\mathbb{R}}f(x) \\phi'(x) \\mathop{}\\!\\mathrm{d} x.\\]</div><div></div><div></div><div></div><div>[C8.1 Page 8]</div>"
  },
  {
    "front": "Convex functions: measure [$]\\mu_f[/$] and [$]\\int \\phi \\mathop{}\\!\\mathrm{d} \\mu_f[/$] for functions [$]\\phi\\in C_c^\\infty(\\mathbb{R})[/$].<div></div><div>In what sense is this measure the second derivative of [$]f[/$]?</div>",
    "back": "<div>$f_+'$ is right-continuous, so defines a measure on $[0,\\infty)$ via $\\mu_f((a,b]) := f_+'(b) - f_+'(a)$ for $0\\le a \\le b$. Then if $\\phi \\in C_c^\\infty(\\mathbb{R})$, \\[&nbsp;&nbsp;&nbsp; \\int_\\mathbb{R} \\phi(x) \\mu_f(\\mathop{}\\!\\mathrm{d} x) = - \\int_\\mathbb{R} \\phi'(x) f'(x) \\mathop{}\\!\\mathrm{d} x \\left( = \\int_\\mathbb{R} \\phi''(x) f(x) \\mathop{}\\!\\mathrm{d} x \\right) .\\] If $f\\in C^2(\\mathbb{R})$, then $\\mathop{}\\!\\mathrm{d} \\mu_f = f'' \\mathop{}\\!\\mathrm{d} x$.</div><div></div><div></div><div></div><div>[C8.1 Page 8]</div>"
  },
  {
    "front": "Version of Ito for convex [$]f\\colon \\mathbb{R}\\to \\mathbb{R}[/$].",
    "back": "<div>Let $(X_t)$ be a continuous semimartingale, and $f\\colon \\mathbb{R}\\to \\mathbb{R}$ convex. Then there exists an increasing, continuous process $(A^f_t)_{t\\ge 0}$ such that \\[&nbsp;&nbsp;&nbsp; f(X_t) = f(X_0) + \\int_0^t f_-'(X_s)\\mathop{}\\!\\mathrm{d} X_s + A^f_t\\\\\\] almost-surely for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[SDEs Page 9]</div>"
  },
  {
    "front": "Definition and existence of local times. What is [$] L_t^a(X)[/$] informally (by Ito)?",
    "back": "<div>Let $(X_t)$ be a continuous semimartingale. Then for every $a\\in \\mathbb{R}$ there exists a unique continuous increasing process $(L^a_t)_{t\\ge 0}$ such that any of the following hold almost surely for all $t\\ge 0$. Then all of them hold.<br><ol>  <li>$(X_t-a)^+ = (X_0-a)^+ + \\int_0^t \\,\\boldsymbol{1}\\!\\left\\{X_s &gt; a\\right\\}\\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2}L_t^a$,&nbsp;&nbsp;&nbsp;</li>  <li>$(X_t - a)^- = (X_0-a)^- - \\int_0^t \\,\\boldsymbol{1}\\!\\left\\{X_s \\le a\\right\\}\\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2}L_t^a$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| X_t- a \\right| = \\left| X_0-a \\right|&nbsp; + \\int_0^t \\text{sgn} (X_s - a) \\mathop{}\\!\\mathrm{d} X_s + L_t^a$,</li></ol><br>where $\\text{sgn} (x) := \\begin{cases}&nbsp;&nbsp;&nbsp; 1 &amp;, x &gt; 0\\\\&nbsp;&nbsp;&nbsp; -1 &amp;, x \\le 0\\\\\\end{cases}.$ These processes are called <i>local times</i> of $X$. Informally, we have (by Ito) \\[L_t^a(X) = \\int_0^t \\delta_a(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt; X \\right&gt;_s.\\]</div><div></div><div></div><div></div><div>[SDEs Page 9]</div>"
  },
  {
    "front": "Support of the (random) measure [$]\\mathop{}\\!\\mathrm{d} L_t^a(X)[/$] for a continuous semimartingale [$]X[/$] and [$]a\\in \\mathbb{R}[/$].",
    "back": "<div>Let $(X_t)$ be a continuous semimartingale, with local times $(L_t^a)_{t\\ge 0, a\\in \\mathbb{R}}$. Then for $a\\in \\mathbb{R}$, the (random) measure $\\mathop{}\\!\\mathrm{d} L_t^a$ on $[0,\\infty)$ is supported on $\\left\\{ t\\ge 0\\colon X_t = a \\right\\} $. That is, almost surely, \\[&nbsp;&nbsp;&nbsp; \\forall B\\in \\mathcal{B}(\\mathbb{R})\\colon&nbsp; \\int_B \\mathop{}\\!\\mathrm{d} L_s^a = \\int_B \\,\\boldsymbol{1}\\!\\left\\{X_s = a\\right\\}\\mathop{}\\!\\mathrm{d} L_s^a.\\] In particular, we almost surely have for all $0\\le s \\le t$, \\[\\forall r\\in [s,t]\\colon X_r\\neq a \\implies L_t^a = L_s^a.\\]</div><div></div><div></div><div></div><div>[SDEs Page 9]</div>"
  },
  {
    "front": "For $(X_n), X$ real random variables, we have $X_n \\stackrel{ d }{\\longrightarrow} X$ iff $F_n(x) \\stackrel{  }{\\longrightarrow} F(x)$ for all $x \\in \\mathcal{C}(F)$.",
    "back": "[General Stochstics, Page 2]"
  },
  {
    "front": "Borel-Cantelli lemma.",
    "back": "<div>Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space and $(A_n) \\in \\mathcal{A}^\\mathbb{N}$. Then<br><ol>  <li>$\\displaystyle \\sum_{n=1}^\\infty\\mathbb{P}(A_n) &lt; \\infty \\implies \\mathbb{P}\\left( \\limsup_{n\\to \\infty}A_n \\right) = 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle \\text{If $(A_n)$ is independent}\\colon&nbsp; \\sum_{n=1}^\\infty \\mathbb{P}(A_n) = \\infty \\implies \\mathbb{P}\\left( \\limsup_{n\\to \\infty}A_n \\right) = 1$.</li></ol></div><div></div><div></div><div></div><div>[General Stochastics, Page 1]</div>"
  },
  {
    "front": "Existence and uniqueness of conditional expectation.",
    "back": "<div>Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space and $X\\colon \\Omega\\to \\mathbb{R}$ integrable, and $\\mathcal{F}\\subset \\mathcal{A}$ a $\\sigma$-algebra. Then $\\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]$ exists and is $\\mathbb{P}$-a.s. unique.</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "[$]L^2[/$]-projection property of conditional expectation.",
    "back": "<div>If $X\\in L^2(\\Omega,\\mathcal{A},\\mathbb{P})$ and $\\mathcal{F}\\subset \\mathcal{A}$ is a $\\sigma$-algebra, then $\\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]\\in L^2$ and $\\mathbb{E} \\left[ \\left( X - \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right] \\right) W \\right] = 0$ for all $W\\in L^2$. In particular, \\[\\left\\|X - \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]\\right\\| = \\min \\left\\{ \\left\\|X - W\\right\\|\\colon W\\in L^2 \\right\\} .\\]</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "&nbsp;Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space, $X,Y\\colon \\Omega\\to \\mathbb{R}$ integrable, and $\\mathcal{F}\\subset \\mathcal{A}$ a $\\sigma$-algebra. Then, up to integrability:<br><ol>  <li>If $Y$ is $\\mathcal{F}$-measurable, then $\\mathbb{E} \\left[XY \\,\\middle\\vert\\,&nbsp; \\mathcal{F}\\right]= Y \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right] \\right| \\le \\mathbb{E} \\left[\\left| X \\right|&nbsp; \\,\\middle\\vert\\, \\mathcal{F}\\right]$,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\mathcal{H}\\perp \\!\\!\\! \\perp \\sigma\\left( \\mathcal{F}\\cup \\sigma(X) \\right) $, then $\\mathbb{E} \\left[X \\,\\middle\\vert\\, \\sigma\\left( \\mathcal{F} \\cup \\mathcal{H} \\right) \\right] = \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]$.</li></ol>",
    "back": "[General Stochastics, Page 2]"
  },
  {
    "front": "Jensen's inequality for (conditional) expectation",
    "back": "<div>Let $X\\in L^1(\\Omega,\\mathcal{A},\\mathbb{P})$, $\\mathcal{F}\\subset \\mathcal{A}$ a $\\sigma$-algebra, and $g\\colon \\mathbb{R}\\to \\mathbb{R}$ convex. Then \\[&nbsp;&nbsp;&nbsp; g\\left( \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right] \\right) \\le \\mathbb{E} \\left[g(X) \\,\\middle\\vert\\, \\mathcal{F}\\right].\\]</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "MCT and DCT for conditional expectation.",
    "back": "<div>Suppose that $X_n, X \\in L^1(\\Omega,\\mathcal{A},\\mathbb{P}), \\, n\\in \\mathbb{N}$, and that $\\mathcal{F}\\subset \\mathcal{A}$ is a $\\sigma$-algebra.<br><ol>  <li>If $0\\le X_n \\uparrow X$ a.s., then $\\mathbb{E} \\left[X_n \\,\\middle\\vert\\, \\mathcal{F}\\right]\\uparrow \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]$ a.s.&nbsp;&nbsp;&nbsp;</li>  <li>If $X_n \\stackrel{  }{\\longrightarrow} X$ a.s. and $\\left| X_n \\right| \\le Y \\in L^1(\\Omega,\\mathcal{A},\\mathbb{P})$, then $\\mathbb{E} \\left[X_n \\,\\middle\\vert\\, \\mathcal{F}\\right]\\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[X \\,\\middle\\vert\\, \\mathcal{F}\\right]$ a.s.</li></ol></div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "Factorisation lemma for measurable maps.",
    "back": "<div>If $Z\\colon (\\Omega,\\mathcal{A})\\to (\\Sigma,\\mathcal{E})$ and $Y\\colon (\\Omega,\\mathcal{A}) \\to (\\overline{\\mathbb{R}},\\overline{\\mathcal{B}})$ are such that $\\sigma(Y) \\subset \\sigma(Z)$, then there exists a map $h\\colon (\\Sigma,\\mathcal{E}) \\to (\\overline{R},\\overline{\\mathcal{B}})$ such that $Y = h \\circ Z$. If $\\mathbb{P}$ is a probability measure on $(\\Omega,\\mathcal{A})$, then $h$ is $\\mathbb{P}^Z$-a.s. unique.</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "Meyer-Tanaka formula.",
    "back": "<div>If $(X_t)$ is a continuous semimartingale with local times $(L_t^a)_{t\\ge 0}, \\, a\\in \\mathbb{R},$ and $f\\colon \\mathbb{R}\\to \\mathbb{R}$ is convex, then \\[&nbsp;&nbsp;&nbsp; f(X_t) = f(X_0) + \\int_0^t f_-'(X_s) \\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2} \\int_\\mathbb{R} L_t^a(X) \\mu_f(\\mathop{}\\!\\mathrm{d} a),\\\\\\] almost surely for $t\\ge 0$. In particular, $(f(X_t))_{t\\ge 0}$ is a continuous semimartingale.</div><div></div><div></div><div></div><div>[SDEs Page 9/10]</div>"
  },
  {
    "front": "Occupation formula for local times.",
    "back": "<div>If $(X_t)$ is a continuous semimartingale, and $\\Phi\\colon \\mathbb{R}\\to [0,\\infty)$ is measurable, then almost surely for $t \\ge 0$, \\[&nbsp;&nbsp;&nbsp; \\int_0^t \\Phi(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt;_s = \\int_\\mathbb{R} \\Phi(a) L_t^a(X) \\mathop{}\\!\\mathrm{d} a.\\]</div><div></div><div></div><div></div><div>[SDEs Page 10]</div>"
  },
  {
    "front": "Bound on size of [$]t[/$]-intersecting systems.",
    "back": "<div>Let $k,t\\in \\mathbb{N}$, $1 &lt; t \\le k$. Then for all $n\\in \\mathbb{N}$ that are sufficiently large, we have that any $t$-intersecting system $\\mathcal{A} \\subset&nbsp; [n]^{(k)}$ satisfies \\[&nbsp;&nbsp;&nbsp; \\left| \\mathcal{A} \\right| \\le \\binom{n-t}{k-t}.\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 5]</div>"
  },
  {
    "front": "Fisher's inequality on systems with [$]\\left|A\\cap B\\right| = k \\forall A\\neq B[/$] and example for sharpness.",
    "back": "<div>Suppose that $n,k\\in \\mathbb{N}$, $0\\le k\\le n$, and $\\mathcal{A} \\subset \\mathcal{P}(n)$ satisifes $\\left| A\\cap B \\right| = k$ for all $A,B\\in \\mathcal{A},\\, A\\neq B$. Then $\\left| \\mathcal{A} \\right| \\le n$.<br><br><b>Example:</b> $k=1$ and $\\mathcal{A} = \\left\\{ \\{1\\},\\{1,2\\},\\ldots ,\\{1,n\\} \\right\\} $.</div><div></div><div></div><div></div><div>[Combinatorics, Page 5]</div>"
  },
  {
    "front": "Oddtown theorem on intersecting systems (two very short proofs).",
    "back": "<div>Suppose that $\\mathcal{A}\\subset \\mathcal{P}(n)$ is such that<br><ol>  <li>$\\left| A \\right|$ is odd for all $A\\in \\mathcal{A}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| A\\cap B \\right|$ is even for all $A,B\\in \\mathcal{A},\\, A \\neq B$.</li></ol><br>Then $\\left| \\mathcal{A} \\right| \\le n$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 5]</div>"
  },
  {
    "front": "Modular Frankl-Wilson theorem on intersecting systems.",
    "back": "<div>Let $p\\in \\mathbb{N}$ be prime and $S\\subset \\left\\{ 0,\\ldots ,p-1 \\right\\} $. Suppose that $\\mathcal{A} \\subset \\mathcal{P}(n)$ satsifies<br><ol>  <li>$\\left| A \\right| \\not\\in S \\text{ (mod $p$)} $ for all $A\\in \\mathcal{A}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| A\\cap B \\right| \\in S \\text{ (mod $p$)} $ for all distinct $A,B\\in \\mathcal{A}$.</li></ol><br>Then $\\left| \\mathcal{A} \\right| \\le \\sum_{i=0}^{|S|}\\binom{n}{i}$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 5]</div>"
  },
  {
    "front": "Disintegration theorem.",
    "back": "<div>Let $\\mu$ be a measure on $X_1\\times X_2$ where $(X_1,\\mathcal{B}(X_1))$ is a polish space and $(X_2,\\mathcal{A}_2)$ is a measurable space, such that $\\pi_2 \\mu := \\mu(X_1 \\times \\cdot )$ is $\\sigma$-finite. Then there exists a stochastic kernel $K\\colon X_2\\times \\mathcal{B}(X_1)\\to [0,1]$ such that $\\mu = (\\pi_2 \\mu) \\otimes K$, that is, \\[&nbsp;&nbsp;&nbsp; \\mu(\\cdot ) = \\int_{X_2}\\int_{X_1} \\,\\boldsymbol{1}\\!\\left\\{(x_1,x_2) \\in \\cdot \\right\\} K(x_2,\\mathop{}\\!\\mathrm{d} x_1) (\\pi_2 \\mu) (\\mathop{}\\!\\mathrm{d} x_2).\\] If $\\widetilde{K}$ is another such kernel, then there exists a $\\pi_2 \\mu$-nullset $N\\in \\mathcal{A}_2$ such that $K(x,\\cdot ) = \\widetilde{K}(x.\\cdot )$ for all $x\\in X_2\\setminus N$.</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "Total variation distance of probability measures.",
    "back": "<div>If $\\mu$ and $\\nu$ are probability measures on $(\\Omega,\\mathcal{A})$, then $\\left| \\mu-\\nu \\right| _\\text{TV} := \\sup_{A\\in \\mathcal{A}} \\left| \\mu(A) - \\nu(A) \\right| $. If $\\mu$ and $\\nu$ have densities $f$ and $g$ with respect to some measure $\\lambda$ on $\\mathcal{A}$, then \\[\\left| \\mu - \\nu \\right| _{\\text{TV} } = \\frac{1}{2}\\int_\\Omega \\left| f-g \\right| \\mathop{}\\!\\mathrm{d} \\lambda.\\]</div><div></div><div></div><div></div><div>[General Stochastics, Page 2]</div>"
  },
  {
    "front": "[$]\\varepsilon[/$]-[$]\\delta[/$]-criterion for absolute continuity.",
    "back": "<div>If $\\mu$ and $\\nu$ are measures on a measurable space $(\\Omega,\\mathcal{A})$ and $\\nu$ is finite, then \\[&nbsp;&nbsp;&nbsp; \\nu \\ll \\mu \\iff \\forall \\varepsilon&gt;0 \\exists \\delta &gt; 0 \\forall A\\in \\mathcal{A} \\colon \\mu(A) &lt; \\delta \\implies \\nu(A) &lt; \\varepsilon.\\]</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Young's and Hölder's inequality.",
    "back": "<div><b>Young's inequality.</b> If $a,b\\ge 0$ and $p,q\\in (1,\\infty)$ with $\\frac{1}{p} + \\frac{1}{q} = 1$, then \\[ab \\le \\frac{a^p}{p} + \\frac{b^q}{q}.\\] <br><br><b>Hölder's inequality.</b>&nbsp; If $f \\in L^p(\\Omega,\\mathcal{A},\\mu)$ and $g\\in L^q(\\Omega,\\mathcal{A},\\mu)$, where $p,q\\in [1,\\infty]$ with $\\frac{1}{p} + \\frac{1}{q} = 1$, then \\[\\left\\|fg\\right\\|_1 \\le \\left\\|f\\right\\|_p \\left\\|g\\right\\|_q.\\]</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Fatou's lemma.",
    "back": "<div>If $f_n \\ge 0$ are measurable on $(\\Omega,\\mathcal{A},\\mu)$, then $\\displaystyle\\int_\\Omega \\liminf_{n\\to \\infty}f_n \\mathop{}\\!\\mathrm{d} \\mu \\le \\liminf_{n\\to \\infty} \\int_\\Omega f_n \\mathop{}\\!\\mathrm{d} \\mu$.</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Monotone convergence theorem.",
    "back": "<div>If $0\\le f_n \\uparrow f$ on $(\\Omega,\\mathcal{A},\\mu)$, then $\\displaystyle \\int_\\Omega f_n \\mathop{}\\!\\mathrm{d} \\mu \\uparrow \\int_\\Omega f \\mathop{}\\!\\mathrm{d} \\mu$.</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Dominated convergence theorem.",
    "back": "<div>If $f_n \\to f$ $\\mu$-a.e. on $(\\Omega,\\mathcal{A},\\mu)$ and $\\left| f_n \\right| \\le g \\in L^1(\\Omega,\\mathcal{A},\\mu)$, then $f\\in L^1$ and $\\displaystyle \\int_\\Omega f_n \\mathop{}\\!\\mathrm{d} \\mu \\stackrel{  }{\\longrightarrow} \\int_\\Omega f\\mathop{}\\!\\mathrm{d} \\mu$.</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Uniqueness of measure.",
    "back": "<div>If $\\mu$ and $\\nu$ are measures on $(\\Omega,\\mathcal{A})$, which coincide on a $\\cap$-stable generator $\\mathcal{M}$ of $\\mathcal{A}$, such that $\\mathcal{M}$ contains an exhausting sequence (or countable cover) $(A_n)\\in \\mathcal{M}^\\mathbb{N}$ with $\\mu(A_n) &lt; \\infty$ for all $n\\in \\mathbb{N}$, then $\\mu = \\nu$.</div><div></div><div></div><div></div><div>[General Stochastics Page 3]</div>"
  },
  {
    "front": "Skorokhod's theorem ([$]X_n \\stackrel{ d }{\\longrightarrow} X \\implies Y_n \\stackrel{ \\text{a.s.} }{\\longrightarrow} Y[/$]).<div></div><div>(Proof in the real-valued case)</div>",
    "back": "<div>If $E$ is a separable metric space and $X_n,X,\\,n\\in \\mathbb{N}$ are $E$-valued random variables with $X_n \\stackrel{ d }{\\longrightarrow} X$, then there exist real random variables $Y_n,Y,\\,n\\in \\mathbb{N}$ on a common probability space with $Y_n \\stackrel{d}{=} X_n$ for all $n\\in \\mathbb{N}$ and $Y\\stackrel{d}{=} X$ such that $Y_n \\stackrel{ \\text{a.s.} }{\\longrightarrow} Y$.</div><div></div><div></div><div></div><div>[Asymptotic Stochastics (Interview Summary) Page 1]</div>"
  },
  {
    "front": "If $P_n \\stackrel{ d }{\\longrightarrow} P$ on $(S,\\rho)$ and $Q_n \\stackrel{ d }{\\longrightarrow} Q$ on $(S',\\rho')$, and $S$ and $S'$ are separable, then $P_n \\otimes Q_n \\stackrel{ d }{\\longrightarrow} P \\otimes Q$.",
    "back": "[General Stochastics, Page 3]"
  },
  {
    "front": "<div>If $f_n,f,g,\\,n\\in \\mathbb{N}$ are $(\\Omega,\\mathcal{A})$-$(\\overline{\\mathbb{R}},\\overline{\\mathcal{B}})$-measurable, then so are $\\sup_{n\\in \\mathbb{N}}f_n$, $\\inf_{n\\in \\mathbb{N}}f_n$, $\\limsup_{n\\to \\infty}f_n$, $\\liminf_{n\\to \\infty}f_n$, $fg$, $\\alpha f + \\beta g$ for $\\alpha,\\beta \\in \\mathbb{R}$, and, if $f \\neq 0$, $1 / f$.</div><div></div>",
    "back": "[General Stochastics, Page 3]"
  },
  {
    "front": "Criterion for $L_t^0(X) = 0$ for a continuous semimartingale $X$ involving increasing $\\rho\\colon [0,\\infty)\\to [0,\\infty)$ with $\\int_0^\\varepsilon \\frac{1}{\\rho(s)}\\mathop{}\\!\\mathrm{d} s = \\infty$.",
    "back": "[SDEs, Page 10]"
  },
  {
    "front": "Girsanov: If $\\mathbb{Q}\\ll \\mathbb{P}$ with continuous $D$, then<br><ol>  <li>$D$ is $\\mathbb{Q}$-almost surely strictly positive,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\tau$ is a stopping time, $\\mathop{}\\!\\mathrm{d} \\mathbb{Q}\\!\\!\\restriction_{\\mathcal{F}_\\tau} = D_\\tau \\mathop{}\\!\\mathrm{d} \\mathbb{P}\\!\\!\\restriction_{\\mathcal{F}_\\tau} $.</li></ol>",
    "back": "[SDEs Page 11]"
  },
  {
    "front": "Definition of Girsanov pair [$](\\mathbb{P},\\mathbb{Q})[/$], [$]D[/$] and [$]L[/$], expression of [$]\\widetilde{M}[/$] in terms of [$]L[/$], and what is [$]\\mathcal{E}(L)^{-1}[/$]?",
    "back": "<div>On a filtered probability space $(\\Omega,\\mathcal{F},\\left( \\mathcal{F}_t \\right) _{t\\ge 0}, \\mathbb{P})$ (with usual conditions), a <i>Girsanov pair</i> $(\\mathbb{P},\\mathbb{Q})$ is a tuple of equivalent probability measures for which the $\\mathbb{P}$-martingale $(D_t)$ is continuous. Then $D$ is uniformly integrable and there exists a unique continuous $\\mathbb{P}$-martingale $L$ with $D = \\mathcal{E}(L)$. Then for a $\\mathbb{P}$-clm $M$ we have \\[\\widetilde{M} = M - D^{-1}\\left&lt;M,D \\right&gt; = M - \\left&lt;M,L \\right&gt; ,\\\\\\] and $\\mathcal{E}(L)^{-1} = \\mathcal{E}(-\\widetilde{L})$.</div><div></div><div></div><div></div><div>[SDEs Page 11]</div>"
  },
  {
    "front": "Girsanov theorem (general and in the case where [$]D = \\mathcal{E}(L)[/$]).",
    "back": "<div>If $\\mathbb{Q}\\triangleleft\\mathbb{P}$ with continuous $D$, and $M$ is a continuous $\\mathbb{P}$-local martingale, then \\[\\widetilde{M} := M - D^{-1} \\cdot \\left&lt;M,D \\right&gt; \\] is a continuous $\\mathbb{Q}$-local martingale. In particular, every continuous $\\mathbb{P}$-semimartingale is a continuous $\\mathbb{Q}$-semimartingale.</div><div></div><div>If $D = \\mathcal{E}(L)$ for a continuous $\\mathbb{P}$-local martingale $L$, then $\\widetilde{M} = M - \\left&lt; M,L\\right&gt;$.</div><div></div><div></div><div></div><div>[SDEs, Page 11]</div>"
  },
  {
    "front": "If $D$ is an almost surely everywhere strictly positive continuous local martingale, then there exists a unique continuous local martingale $L$ such that $D = \\mathcal{E}(L)$. What does $L$ look like?",
    "back": "[$$]L_t = \\log D_0 + \\int_0^t D_s^{-1} \\mathop{}\\!\\mathrm{d} D_s[/$$]<div></div><div></div><div>[SDEs Page 11]</div>"
  },
  {
    "front": "Girsanov transform and behaviour under composition.",
    "back": "<div>For a Girsanov pair $(P,Q)$, the map $M \\mapsto \\widetilde{M}$ is denoted by $G_P^Q$ and called the <i>Girsanov transform</i> from $P$ to $Q$. If $(P,Q)$ and $(Q,R)$ are Girsanov pairs, then so is $(P,R)$, and \\[G_P^R = G_Q^R \\circ Q_P^Q.\\] In particular, the Girsanov transfrom is a bijection.</div><div></div><div></div><div></div><div>[SDEs Page 11]</div>"
  },
  {
    "front": "Kazamaki and Novikov criterion.",
    "back": "<div>Let $L$ be a continuous local martingale with $L_0 = 0$.<br><br><b>Kazamaki.</b> If $\\left( \\exp\\left( \\frac{1}{2}L_t \\right)&nbsp; \\right) _{t\\ge 0}$ is a uniformly integrable submartingale, then $\\mathcal{E}(L)$ is a uniformly integrable martingale.<br><br><b>Novikov.</b> If $\\mathbb{E} \\left[ \\exp\\left( \\frac{1}{2}\\left&lt;L \\right&gt; _\\infty \\right)&nbsp; \\right] &lt;\\infty$, then $\\mathcal{E}(L)$ is a uniformly integrable martingale. If $\\mathbb{E} \\left[ \\exp \\left( \\frac{1}{2}\\left&lt;L \\right&gt; _t \\right)&nbsp; \\right] &lt; \\infty$ for all $t\\ge 0$, then $\\mathcal{E}(L)$ is a martingale.</div><div></div><div></div><div></div><div>[SDEs Page 11]</div>"
  },
  {
    "front": "The space [$]C([0,\\infty),\\mathbb{R}^d)[/$] (metric, Borel sigma algebra, filtration), and measure extension theorem.",
    "back": "<div>$C := C([0,\\infty),\\mathbb{R}^d)$ is separable and complete via $\\rho(x,y) = \\sum_{k=1}^\\infty 2^{-k} \\left( 1\\wedge \\left\\|r_k(x) - r_k(y)\\right\\|_\\infty \\right) $. Then $\\mathcal{B}(C) = \\sigma\\left( \\pi_t\\colon t\\ge 0 \\right) $, and a filtration is defined by<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathcal{F}_t &nbsp;&nbsp;&nbsp; &amp;:= \\sigma(r_t) = r_t ^{-1} (\\mathcal{B}(C[0,T])) = \\sigma \\left( \\pi_s\\colon s\\le t \\right) \\\\&nbsp;&nbsp;&nbsp; &amp;= \\left\\{ A\\in \\mathcal{B}(C) \\colon \\forall x\\in C\\colon x\\in A \\iff x^t \\in A \\right\\} ,\\\\\\end{align*}[/$$] for $t\\ge 0$, where $r_t\\colon C \\to C[0,t]; \\, x \\mapsto x \\!\\!\\restriction_{[0,t]} $. Then $\\mathcal{F}_\\infty = \\mathcal{B}(C)$.<br><br><b>Theorem.</b> If $Q_t,\\, t\\ge 0,$ are probability measures on $\\mathcal{F}_t,\\,t\\ge 0,$ such that $Q_t \\!\\!\\restriction_{\\mathcal{F}_s} = Q_s$ for $0\\le s \\le t,$ then there exists a unique probability measure $Q$ on $\\mathcal{B}(C)$ such that $Q_t = Q\\!\\!\\restriction_{\\mathcal{F}_t} $ for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[SDEs Page 12]</div>"
  },
  {
    "front": "Weak solution for [$]\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t[/$] by time change.",
    "back": "<div>Suppose that $(B_t)$ is a Brownian motion, and $\\sigma\\colon \\mathbb{R}\\to \\mathbb{R}$ is bounded and measurable such that $\\sigma \\ge \\varepsilon$ for some $\\varepsilon &gt; 0$. Then for any probability measure $\\mu$ on $\\mathbb{R}$, there exists a weak solution $(X,\\widetilde{B})$ to the SDE \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t,\\\\&nbsp;&nbsp;&nbsp; X_0 \\sim \\mu,\\\\\\end{cases}\\] and we have uniqueness in law.</div><div></div><div></div><div></div><div>[SDEs Page 12]</div>"
  },
  {
    "front": "Transformation of weak solution to [$]\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t[/$] to weak solution of [$]\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t[/$] by change of measure<div>(and thus existence and uniqueness in law of solutions to the latter).</div>",
    "back": "<div>Suppose that $\\sigma,b \\colon \\mathbb{R}\\to \\mathbb{R}$ are measurable and bounded such that there exists $\\varepsilon &gt; 0$ with $\\sigma \\ge \\varepsilon$. Then for any probability measure $\\mu$ on $\\mathbb{R}$, weak solutions of the SDEs \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t,\\\\&nbsp;&nbsp;&nbsp; X_0 \\sim \\mu\\\\\\end{cases}\\] and \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t,\\\\&nbsp;&nbsp;&nbsp; X_0 \\sim \\mu\\\\\\end{cases}\\] can be transformed into one another by a change of measure. In particular, weak solutions to the latter exist and are unique in law.</div><div></div><div></div><div></div><div>[SDEs Page 12]</div>"
  },
  {
    "front": "Transformation of weak solutions to [$]\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t[/$] to weak solutions that live on [$]C([0,\\infty))[/$].",
    "back": "<div>If $\\sigma,b\\colon \\mathbb{R}\\to \\mathbb{R}$ are bounded and measurable such that $\\sigma \\ge \\varepsilon$ for some $\\varepsilon &gt; 0$. Then any weak solution $(X,B)$ to the SDE \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t,\\\\&nbsp;&nbsp;&nbsp; X_0 \\sim \\mu\\\\\\end{cases}\\] gives rise to a weak solution $(\\widetilde{X},\\widetilde{B})$ to the same SDE that lives on $(C[0,\\infty),(\\mathcal{F}_t))$.</div><div></div><div></div><div></div><div>[SDEs Page 12]</div>"
  },
  {
    "front": "(Non-modular) Frankl-Wilson Theorem. Is the bound sharp?",
    "back": "<div>Let $n\\in \\mathbb{N}$, $S\\subset \\mathbb{N}_0$, and $\\mathcal{F}\\subset \\mathcal{P}(n)$ be $S$-intersecting. Then $\\left| \\mathcal{F} \\right|\\le \\sum_{i=0}^{|S|}\\binom{n}{i} $. The bound is achieved by $\\mathcal{F} = [n]^{(\\le d)}$ and $S = \\{0,\\ldots,d-1\\}$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 6]</div>"
  },
  {
    "front": "Ray-Chaudhuri-Wilson Theorem (Frankl-Wilson for layers).",
    "back": "<div>If $\\mathcal{F}\\subset \\left[ n \\right] ^{(k)}$ is $S$-intersecting with $S\\subset \\left\\{ 0,\\ldots ,k-1 \\right\\} $, then $\\left| \\mathcal{F} \\right| \\le \\binom{n}{|S|}$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 6]</div>"
  },
  {
    "front": "Combinatorial Nullstellensatz.",
    "back": "<div>Let $\\mathbb{F}$ be a field, $f\\in \\mathbb{F}[X_1,\\ldots ,X_n]$ with degree $t\\in \\mathbb{N}_0$, and suppose that the coefficient in front of $\\prod_{i=1}^n X_i^{t_i}$ with $\\sum t_i = t$ is non-zero. Then if $S_1,\\ldots ,S_n \\subset \\mathbb{F}$ with $\\left| S_i \\right| \\ge t_i + 1$ for all $i\\in [n]$, then there exists $(s_1,\\ldots ,s_n) \\in \\times_{i=1}^n S_i$ with \\[&nbsp;&nbsp;&nbsp; f(s_1,\\ldots ,s_n) \\neq 0.\\]</div><div></div><div></div><div></div><div>[Combinatorics, Page 6]</div>"
  },
  {
    "front": "Cauchy-Davenport Theorem on [$]A+B[/$] if [$]A,B\\subset \\mathbb{Z}_p[/$].",
    "back": "<div>If $p\\in \\mathbb{P}$ and $A,B\\subset \\mathbb{Z}_p$, then $\\left| A+B \\right| \\ge \\left( |A| + |B| - 1 \\right) \\wedge p$, and $\\left| A \\hat{+} B \\right| \\ge \\left( |A| + |B| - 3\\right) \\wedge p$.</div><div></div><div></div><div></div><div>[Combinatorics, Page 6]</div>"
  },
  {
    "front": "Selection in the WF-model. [$]s,\\gamma,X_k,Z_k[/$], and moments of [$]X_{k+1} - x[/$] given [$] X_k = x[/$].<br><br>Wright-Fisher diffusion",
    "back": "<div>Assume a population of $2N$ alleles of types $A$ and $a$. Suppose that $s &gt; 0$ and every offspring chooses each individual of type $A$, resp. $a$, with probability proportional to $1+s$, resp. $1$. Let $\\gamma := 2N s$, $Z_k$ the number of type $A$ alleles in generation $k$, and $X_k :=&nbsp; Z_k / 2N$. Then, given $X_k = x\\in [0,1]$, \\[&nbsp;&nbsp;&nbsp; p_k := \\mathbb{P}\\left( \\text{offspring chooses parent of type $A$}&nbsp; \\right) = \\frac{(1+s)x}{1+sx} = x + s x(1-x) + o(s),\\\\\\] and $Z_{k+1}\\sim \\text{Bin} (2N,p_k)$. Furthermore, again given $X_k = x$,<br><ol>  <li>$\\displaystyle\\mathbb{E} \\left[ X_{k+1} - x \\right] = \\frac{\\gamma}{2N}x(1-x) + o\\left( \\frac{1}{N} \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle\\mathbb{E} \\left[ \\left( X_{k+1}- x \\right) ^2 \\right] = \\frac{1}{2N} x(1-x) + o\\left( \\frac{1}{N} \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle\\mathbb{E} \\left[ \\left( X_{k+1}- x \\right) ^{3} \\right] = o\\left( \\frac{1}{N} \\right) $.</li></ol></div><div>This means that $(Y_t := X_{\\left\\lfloor 2N t \\right\\rfloor })_{t\\ge 0}$ converges to a diffusion process with $a(x) = x(1-x)$ and $b(x) = \\gamma x(1-x)$, called the <i>Wright-Fisher diffusion with selection</i>.</div><div></div><div></div><div></div><div>[Genetics, Page 8]</div>"
  },
  {
    "front": "Genetics: Definition of diffusion process.",
    "back": "<div>Let $a &gt; 0$ and $b$ be smooth functions. Then a continuous Markov process $(X_t)_{t\\ge 0}$ is called a <i>diffusion process</i> with <i>infinitesimal mean / drift parameter $b(x)$</i> and <i>infinitesimal variance / diffusion paramter $a(x)$</i>, if for all $t\\ge 0$, given $X_t = x$,<br><ol>  <li>$\\displaystyle \\mathbb{E} \\left[ X_{t+\\delta t}- x \\right] = b(x) \\delta t + o(\\delta t)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle \\mathbb{E} \\left[ \\left( X_{t+\\delta t} - x \\right) ^2 \\right] = a(x) \\delta t + o(\\delta t)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle \\mathbb{E} \\left[ \\left( X_{t+\\delta t}- x \\right)^3 \\right] = o(\\delta t)$.</li></ol><br>This is the unique solution to the SDE $\\mathop{}\\!\\mathrm{d} X_t = b(X_t) \\mathop{}\\!\\mathrm{d} t + \\sqrt{a(X_t)} \\mathop{}\\!\\mathrm{d} B_t$.</div><div></div><div></div><div></div><div>[Genetics Page 8]</div>"
  },
  {
    "front": "Genetics: Definition of the generator of a Markov process and characterisation of diffusion by generators.",
    "back": "<div>Let $(X_t)$ be a Markov process. Then its <i>generator</i> is defined by \\[&nbsp;&nbsp;&nbsp; L\\colon \\mathbb{R} ^{\\mathbb{R}} \\supset D(L) \\to \\mathbb{R} ^{\\mathbb{R}}; \\, f \\mapsto \\left[ x \\mapsto \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{E} \\left[f(X_t) \\,\\middle\\vert\\, X_0 = x\\right] \\!\\!\\restriction_{t = 0} \\right],\\\\\\] where $D(L)$ is the space of functions for which this is well-defined. A Markov process is uniquely determined by its generator.<br><br>Let $a &gt; 0$ and $b$ be smooth. Then a continuous Markov process $(X_t)_{t\\ge 0}$ is a diffusion with parameters $a$ and $b$ if and only if its generator satisfies \\[&nbsp;&nbsp;&nbsp; L(f) = \\frac{a}{2} \\frac{\\mathop{}\\!\\mathrm{d} ^2 f}{\\mathop{}\\!\\mathrm{d} x^2} + b \\frac{\\mathop{}\\!\\mathrm{d} f}{\\mathop{}\\!\\mathrm{d} x}, \\quad f\\in D(L) \\supset C^2(\\mathbb{R}).\\]</div><div></div><div></div><div></div><div>[Genetics, Page 8/9]</div>"
  },
  {
    "front": "Fixation probability for a diffusion process. General solution and solution for a Wright-Fisher diffusion.",
    "back": "<div>Suppose a diffusion process $(X_t)$ has absorbing states $l &lt; r$ and that $\\tau := \\tau_l \\wedge \\tau_r &lt; \\infty$ almost surely. Then the function $h\\colon [l,r] \\to [0,1]; \\, x \\mapsto \\mathbb{P} \\left(X_\\tau = r \\,\\middle\\vert\\, X_0 = x\\right)$ satisfies $L(h) = 0$. It is thus given by \\[&nbsp;&nbsp;&nbsp; h(x) = \\frac{\\int_l^x \\exp \\left( -\\int_0^y \\frac{2b(z)}{a(z)}\\mathop{}\\!\\mathrm{d} z \\right) \\mathop{}\\!\\mathrm{d} y}{\\int_l^r \\exp \\left( -\\int_0^y \\frac{2b(z)}{a(z)}\\mathop{}\\!\\mathrm{d} z \\right) \\mathop{}\\!\\mathrm{d} y}, \\quad x \\in [l,r].\\] In the case of a Wright-Fisher diffusion, the solution is \\[h(x) = \\frac{1 - \\mathrm{e}^{-2\\gamma x}}{1 - \\mathrm{e}^{-2\\gamma}}\\quad (\\gamma \\neq 0),\\qquad h(x) = x \\quad (\\gamma = 0).\\]</div><div></div><div></div><div></div><div>[Genetics, Page 9]</div>"
  },
  {
    "front": "Substitution probability and substitution rate for Wright-Fisher diffusion.<div></div><div>What is the impact of a large population size on the effect of selection and evolution?</div>",
    "back": "<div>The <i>substitution probability</i> is the probability with which a new mutation fixes, so $p := h(1 / 2N)$. For Wright-Fisher diffusion, assuming $\\left| s \\right| \\ll 1$, we have<br><ol>  <li>$s &gt; 0, \\gamma \\gg 1\\colon p \\approx 2s$, (<b>beneficial</b>)&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| \\gamma \\right| \\ll 1\\colon p \\approx \\frac{1}{2N} + s$, (<b>nearly neutral</b>)&nbsp;&nbsp;&nbsp;</li>  <li>$s &lt; 0, \\left| \\gamma \\right| \\ll 1\\colon&nbsp; p \\approx 2\\left| s \\right| \\mathrm{e}^{-4 N \\left| s \\right| }$. (<b>deleterious)</b>.</li></ol><br><br>The <i>substitution rate</i> is the number of new mutations that will fixate per generation, so $r = 2N\\mu \\cdot p$. So<br><ol>  <li>beneficial: $r = 2\\mu \\gamma$,&nbsp;&nbsp;&nbsp;</li>  <li>nearly neutral: $r = \\mu$,&nbsp;&nbsp;&nbsp;</li>  <li>deleterious: $r = 2\\mu|\\gamma| \\mathrm{e}^{-2|\\gamma|}$.</li></ol><br>Hence, large populations are very good at suppressing bad mutations, and a bit better at spreading advantageous ones, thus better at adapting in general.</div><div></div><div></div><div></div><div>[Genetics, Page 9]</div>"
  },
  {
    "front": "Synonymous and non-synonymous mutations.",
    "back": "synonymous mutations are mutations that occur in non-coding DNA or have no effect on the protein they affect. These have [$]s = 0[/$]. Non-synonymous mutations are mutations that occur in coding DNA and do have an effect on the corresponding protein. Most of these have [$]s &lt; 0[/$], rarely [$]s &gt; 0[/$]."
  },
  {
    "front": "Criterion for $L^0_t(X^1 - X^2) = 0$ for all $t\\ge 0$ if $X^1$ and $X^2$ are solutions to diffusion equations $\\mathop{}\\!\\mathrm{d} X_t^i = b_i(X_t^i,t) \\mathop{}\\!\\mathrm{d} t + \\sigma(X_t^i,t) \\mathop{}\\!\\mathrm{d} B_t$.",
    "back": "<div>Suppose that $\\sigma,b_1,b_2\\colon \\mathbb{R} \\times [0,\\infty) \\to \\mathbb{R}$ are locally bounded and measurable such that $\\left| \\sigma(x,t) - \\sigma(y,t) \\right| ^2 \\le \\rho(\\left| x-y \\right| )$ for all $x,y\\in \\mathbb{R}$ and $t\\ge 0$ for some increasing $\\rho\\colon [0,\\infty) \\to [0,\\infty)$ with $\\int_0^\\varepsilon \\rho(s)^{-1}\\mathop{}\\!\\mathrm{d} s = \\infty$ for all $\\varepsilon &gt; 0$. Then if $B$ is a Brownian motion and $X^1$ and $X^2$ are solutions to \\[&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t^i = b_i(X_t^i,t) \\mathop{}\\!\\mathrm{d} t + \\sigma(X_t^i,t) \\mathop{}\\!\\mathrm{d} B_t\\\\\\] on the same probability space, then $L_t^0(X^1 - X^2) = 0$ almost surely for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[SDEs Page 10]</div>"
  },
  {
    "front": "Pathwise uniqueness for the SDE [$]\\mathop{}\\!\\mathrm{d} X_t = b(X_t,t) \\mathop{}\\!\\mathrm{d} t + \\sigma(X_t,t) \\mathop{}\\!\\mathrm{d} B_t[/$] with [$]\\rho[/$]-continuous [$]\\sigma[/$] and locally Lipschitz [$]b[/$].",
    "back": "<div>Suppose that $\\sigma,b\\colon \\mathbb{R} \\times [0,\\infty)\\to \\mathbb{R}$ are locally bounded and measurable such that<br><ol>  <li>$b$ is locally Lipschitz, i.e. $\\forall R &gt; 0 \\exists K &gt; 0\\colon \\left| b(x,t) - b(y,t) \\right| \\le K \\left| x - y \\right| $ for all $x,y,\\in [-R,R],t\\in [0,R]$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left| \\sigma(x,t) - \\sigma(y,t) \\right|^2 \\le \\rho(\\left| x-y \\right| )$ for all $x,y\\in \\mathbb{R}$ and $t\\ge 0$.</li></ol><br>Then the SDE $\\mathop{}\\!\\mathrm{d} X_t = b(X_t,t) \\mathop{}\\!\\mathrm{d} t + \\sigma(X_t,t) \\mathop{}\\!\\mathrm{d} B_t$ satisfies pathwise uniqueness.</div><div></div><div></div><div></div><div>[SDEs Page 10]</div>"
  },
  {
    "front": "Existence, uniqueness in law, and pathwise uniqueness for SDEs of the form [$]\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t[/$].",
    "back": "<div>Let $\\sigma,b\\colon \\mathbb{R}\\to \\mathbb{R}$ be bounded and measurable such that $\\sigma \\ge \\varepsilon$ for some $\\varepsilon &gt; 0$. Then there exists a (weak) solution to the SDE \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_0\\sim \\mu,\\\\&nbsp;&nbsp;&nbsp; \\end{cases}\\] which is unique in law. If, furthermore, there exists an increasing $\\rho\\colon [0,\\infty)\\to [0,\\infty)$ with $\\int_0^\\varepsilon \\rho(s)^{-1}\\mathop{}\\!\\mathrm{d} s = \\infty$ for all $\\varepsilon &gt; 0$ and $\\left| \\sigma(x) - \\sigma(y) \\right| ^2 \\le \\rho(\\left| x-y \\right| )$ for all $x,y\\in \\mathbb{R}$, then we also have pathwise uniqueness, and every weak solution is strong.</div><div></div><div></div><div></div><div>[SDEs Page 12/13]</div>"
  },
  {
    "front": "Definition of $\\mathcal{H}^s_\\delta$ and $\\mathcal{H}^s$. Connection with Lebesgue measure.",
    "back": "<div>Let $s\\ge 0$ and $\\delta &gt; 0$. Then for $n\\in \\mathbb{N}$ and $F\\subset \\mathbb{R}^n$,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathcal{H}^s(F) &amp;:= \\lim_{\\delta \\downarrow 0}\\mathcal{H}^s_\\delta(F),\\\\&nbsp;&nbsp;&nbsp; \\mathcal{H}^s_\\delta(F) &amp;:= \\inf \\left\\{ \\sum_{i}\\left| U_i \\right| ^s\\colon (U_i) \\text{ is a $\\delta$-cover of $F$}\\right\\} \\uparrow \\mathcal{H}^s(F),\\\\\\end{align*}[/$$]<br>where $\\left| \\cdot&nbsp; \\right| := \\text{diam}(\\cdot) $. Then $\\mathcal{H}^s$ is an outer measure on $\\mathbb{R}^n$ and a Borel measure (so increasing and subadditive on $\\mathcal{P}(\\mathbb{R}^n)$). For $n\\in \\mathbb{N}$, we have $\\mathcal{H}^n = c_n^{-1} \\lambda^n$ on Borel sets, where $c_n = \\lambda^n(B(0,1 / 2))$.</div><div></div>"
  },
  {
    "front": "Hausdorff dimension.",
    "back": "<div><b>Lemma.</b> The map $s\\mapsto \\mathcal{H}^s(\\cdot )$ is increasing and if $0\\le s &lt; t$, then $\\mathcal{H}^t_\\delta(\\cdot ) \\le \\delta ^{t-s} \\mathcal{H}^s_\\delta(\\cdot )$.<br><br><b>Definition.</b> Putting $\\dim_H F := \\sup \\left\\{ s\\ge 0\\colon \\mathcal{H}^s(F) = \\infty \\right\\} $, we have \\[&nbsp;&nbsp;&nbsp; \\mathcal{H}^s(F) = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &amp;, 0 \\le s &lt; \\dim_H F,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ? &amp;, s = \\dim_H F,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, s &gt; \\dim_H F.&nbsp;&nbsp;&nbsp; \\end{cases}\\] If $0 &lt; ? &lt; \\infty$, then $F$ is called an <i>$s$-set</i>.</div><div></div><div></div><div></div><div>[Fractals Page 1]</div>"
  },
  {
    "front": "Behaviour of [$]\\mathcal{H}^s[/$] and [$]\\dim_H[/$] under similarity transforms, Hölder-continuous, and bi-Lipschitz functions.",
    "back": "<div>Let $S,f,g\\colon \\mathbb{R}^n\\to \\mathbb{R}^m$ be such that $S$ is a similarity transform with scaling factor $\\lambda &gt; 0$, $f$ is Hölder-continuous with exponent $\\alpha &gt; 0$ and constant $c &gt; 0$, and $f$ is bi-Lipschitz.<br><ol>  <li>$\\mathcal{H}^s(S(\\cdot )) = \\lambda^s \\mathcal{H}^s(\\cdot )$ and $\\dim_H (S(\\cdot )) = \\dim_H(\\cdot )$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathcal{H}^{\\frac{s}{\\alpha}}(f(\\cdot )) \\le c^{\\frac{s}{\\alpha}} \\mathcal{H}^s(\\cdot )$ and $\\dim_H(f(\\cdot )) \\le \\frac{1}{\\alpha}\\dim_H(\\cdot )$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\dim_H(g(\\cdot )) = \\dim_H(\\cdot )$.</li></ol></div><div></div><div></div><div></div><div>[Fractals Page 1]</div>"
  },
  {
    "front": "Properties of [$]\\dim_H[/$]: Monotonicity, stability, countable sets, open sets, smooth manifolds.",
    "back": "<div><ol>  <li>If $E\\subset F\\subset \\mathbb{R}^n$, then $\\dim_H E \\le \\dim_H F$,&nbsp;&nbsp;&nbsp;</li>  <li>If $(F_i) \\in \\mathbb{R} ^{\\mathbb{N}}$, then $\\dim_H \\left( \\bigcup_{i=1} ^\\infty \\right) = \\sup_{i\\in \\mathbb{N}}\\dim_H(F_i)$,&nbsp;&nbsp;&nbsp;</li>  <li>If $F$ is countable, then $\\dim_H F = 0$,&nbsp;&nbsp;&nbsp;</li>  <li>If $F$ is open, then $\\dim_H F = n$,&nbsp;&nbsp;&nbsp;</li>  <li>If $F$ is a smooth $m$-dimensional manifold, then $\\dim_H F = m$.</li></ol></div><div></div><div></div><div></div><div>[Fractals, Page 1]</div>"
  },
  {
    "front": "What do you know about [$]F\\subset \\mathbb{R}^n[/$] if [$]\\dim_H F &lt; 1[/$]?",
    "back": "[$]F[/$] is totally disconnected.<div></div><div></div><div>[Fractals Page 1]</div>"
  },
  {
    "front": "Equivalent definitions of Hausdorff dimension.",
    "back": "<div>If we only use covers by open/closed sets/balls in the definition of $\\mathcal{H}^s$, we get the same definition of dimension. If $F$ is compact, we may consider only finite covers.</div><div></div><div></div><div></div><div>[Fractals Page 2]</div>"
  },
  {
    "front": "Definition (upper and lower) box-dimensions.",
    "back": "<div>If $F\\subset \\mathbb{R}^n$ is non-empty and bounded, then the <i>lower</i> and <i>upper box-dimensions</i> of $F$ are \\[&nbsp;&nbsp;&nbsp; \\underline{\\dim}_B F := \\liminf_{\\delta \\to 0} \\frac{\\log N_\\delta(F)}{-\\log \\delta},\\qquad \\overline{\\dim}_B F := \\limsup_{\\delta \\to 0} \\frac{\\log N_\\delta(F)}{-\\log \\delta}.\\] In case of equality, the common value $\\dim_B F$ is the <i>box counting dimension of $F$</i>. Here $N_\\delta(F)$ for $\\delta &gt; 0$ is one of the following.<br><ol>  <li>smallest size of a $\\delta$-cover of $F$,&nbsp;&nbsp;&nbsp;</li>  <li>smallest number of closed radius $\\delta$-balls that cover $F$,&nbsp;&nbsp;&nbsp;</li>  <li>smallest number of side $\\delta$-cubes that cover $F$,&nbsp;&nbsp;&nbsp;</li>  <li>largest number of disjoint open radius $\\delta$-balls with centres in $F$,&nbsp;&nbsp;&nbsp;</li>  <li>number of $\\delta$-mesh cubes that intersect $F$.</li></ol></div><div></div><div></div><div></div><div>[Fractals Page 2]</div>"
  },
  {
    "front": "Connection between [$]\\dim_H[/$] and [$]\\dim_B[/$] and limiting behaviour of [$]N_\\delta(F) \\delta^s[/$].<div></div><div>Corollary: Upper bound for [$]\\dim_H F[/$] in terms of specific [$]\\delta_k[/$]-coverings.</div>",
    "back": "<div>If $F\\subset \\mathbb{R}^n$ is non-empty and bounded, then $\\dim_H F \\le \\underline{\\dim}_B F \\le \\overline{\\dim}_B F$. More precisely, \\[&nbsp;&nbsp;&nbsp; \\mathcal{H}_\\delta^s(F) \\le \\inf \\left\\{ \\sum_i \\delta^s \\colon (U_i) \\text{ is a finite $\\delta$-cover of $F$}&nbsp; \\right\\} = N_\\delta(F) \\delta^s \\stackrel{  }{\\longrightarrow} \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &amp;, s &lt; \\underline{\\dim}_B F,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, s &gt; \\overline{\\dim}_B F.&nbsp;&nbsp;&nbsp; \\end{cases}\\] Hence if $F$ can be covered by $n_k$ sets of diameter at most $\\delta_k \\downarrow 0$, then </div>\\[\\dim_H F \\le \\underline{\\dim}_B F \\le \\liminf_{k\\to \\infty} \\frac{\\log n_k}{-\\log \\delta_k}.\\]<div></div><div></div><div></div><div>[Fractals Page 2]</div>"
  },
  {
    "front": "Mass distribution principle (for lower bounds on [$]\\dim_H[/$]).",
    "back": "<div>If $\\mu$ is a finite outer measure on $\\mathbb{R}^n$ with $\\mu(F) &gt; 0$ such that there exist $\\varepsilon,c &gt; 0$ such that all sets $U$ with $\\left| U \\right| \\le \\varepsilon$ have $\\mu(U) \\le c \\left| U \\right| ^{s}$, then $\\mathcal{H}^s(F) \\ge \\mu(F) /c$. In particular, \\[\\dim_H F \\ge s.\\]</div><div></div><div></div><div></div><div>[Fractals Page 3]</div>"
  },
  {
    "front": "Connection between Hausdorff-measure and behaviour of [$]\\frac{\\mu(B(x,r))}{r^s}[/$] for a finite (outer) measure [$]\\mu[/$] on [$]\\mathbb{R}^n[/$].",
    "back": "<div>Let $\\mu$ be a finite (outer) measure on $\\mathbb{R}^n$, $c &gt; 0$ and $F\\in \\mathcal{B}(\\mathbb{R}^n)$.<br><ol>  <li>If $\\limsup_{r\\to 0}\\frac{\\mu(B(x,r))}{r^s} &lt; c$ for all $x\\in F$, then $\\mathcal{H}^s(F) \\ge \\mu(F) / c$, so $\\dim_H F \\ge s$ (if $\\mu(F) &gt; 0$).&nbsp;&nbsp;&nbsp;</li>  <li>If $\\limsup{r\\to 0} \\frac{\\mu(B(x,r))}{r^s}&gt; c$ for all $x\\in F$, then $\\mathcal{H}^s(F) \\le 2^s \\mu(\\mathbb{R}^n) / c$, so $\\dim_H F \\le s$.</li></ol><br>In particular, if $\\lim_{r\\to 0}\\frac{\\log \\mu(B(x,r))}{\\log r} = s$ for all $x\\in F$ and $\\mu(F) &gt; 0$, then $\\dim_H F = s$.</div><div></div><div></div><div></div><div>[Fractals Page 3]</div>"
  },
  {
    "front": "Subsets of finite Hausdorff measure.",
    "back": "<div>If $F\\in \\mathcal{B}(\\mathbb{R}^n)$ and $0 &lt; \\mathcal{H}^s(F) \\le \\infty$, then there exist a compact set $E\\subset F$ with $0 &lt; \\mathcal{H}^s(E) &lt; \\infty$ and $b &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp; \\mathcal{H}^s(E \\cap B(x,r)) \\le b r^s,\\quad x\\in \\mathbb{R}^n, r &gt; 0.\\]</div><div></div><div></div><div></div><div>[Fractals Page 3]</div>"
  },
  {
    "front": "[$]s[/$]-potential and [$]s[/$]-energy and relation to Hausdorff measure (lower bound on [$]\\dim_H F[/$]).",
    "back": "<div><b>Definition.</b> Let $\\mu$ be a finite (outer) measure on $\\mathbb{R}^n$ and $s\\ge 0$. Then \\[&nbsp;&nbsp;&nbsp; \\varphi_s \\colon \\mathbb{R}^n \\to \\mathbb{R}; \\, x \\mapsto \\int_{\\mathbb{R}^n} \\frac{1}{\\left| x-y \\right|^s }\\mu(\\mathop{}\\!\\mathrm{d} y), \\qquad I_s(\\mu) := \\int \\varphi_s(x) \\mu(\\mathop{}\\!\\mathrm{d} x),\\\\\\] are called the <i>$s$-potential</i> and <i>$s$-energy</i> of $\\mu$, respectively.<br><br><b>Theorem.</b> If $F\\in \\mathcal{B}(\\mathbb{R}^n)$ and $\\mu$ is a mass distribution on $F$ (finite positive outer measure with $\\operatorname{supp} \\mu \\subset F$) and $I_s(\\mu) &lt; \\infty$, then $\\mathcal{H}^s(F) = \\infty$, in particular $\\dim_H F \\ge s$.</div><div></div><div></div><div></div><div>[Fractals Page 4]</div>"
  },
  {
    "front": "[$]s[/$]-potential and [$]s[/$]-energy and relation to Hausdorff measure (existence of [$]\\mu[/$]).",
    "back": "<div><b>Definition.</b> Let $\\mu$ be a finite (outer) measure on $\\mathbb{R}^n$ and $s\\ge 0$. Then \\[&nbsp;&nbsp;&nbsp; \\varphi_s \\colon \\mathbb{R}^n \\to \\mathbb{R}; \\, x \\mapsto \\int_{\\mathbb{R}^n} \\frac{1}{\\left| x-y \\right| }\\mu(\\mathop{}\\!\\mathrm{d} y), \\qquad I_s(\\mu) := \\int \\varphi_s(x) \\mu(\\mathop{}\\!\\mathrm{d} x),\\\\\\] are called the <i>$s$-potential</i> and <i>$s$-energy</i> of $\\mu$, respectively.<br><br><b>Theorem.</b> If $F\\in \\mathcal{B}(\\mathbb{R}^n)$ and $\\mathcal{H}^s(F) &gt; 0$, then there exists a mass distribution (finite positive outer measure with $\\operatorname{supp} \\mu \\subset F$) with $I_t(\\mu) &lt; \\infty$ for all $0 &lt; t &lt; s$.</div><div></div><div></div><div></div><div>[Fractals Page 4]</div>"
  },
  {
    "front": "Upper bound for dimension of graph and range of locally Hölder continuous function.",
    "back": "<div>Let $f\\colon [0,\\infty)\\to \\mathbb{R}^d$ be locally $\\alpha$-Hölder continuous for some $\\alpha \\in (0,1)$. Then<br><ol>  <li>$\\dim_H \\text{Graph} _f([0,\\infty)) \\le \\frac{1}{\\alpha}\\wedge (1 + d(1-\\alpha))$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\dim_H f(A) \\le \\frac{\\dim_H A}{\\alpha}$ for all $A\\subset [0,1]$.</li></ol></div><div></div><div></div><div></div><div>[Fractals (BM) Page 1]</div>"
  },
  {
    "front": "Upper bound on Hausdorff dimension of [$]F \\subset \\mathbb{R}^d[/$] using a single sequence of coverings.",
    "back": "<div>Let $F\\subset \\mathbb{R}^d$ and $(U_i^{(n)})$ a sequence of coverings with $\\delta_n := \\max_i \\left| U_i^{(n)} \\right| \\to 0$. Then, for all $s\\ge 0$, \\[&nbsp;&nbsp;&nbsp; \\mathcal{H}^s(F) \\le \\liminf_{n\\to \\infty} \\sum_i \\left| U_i^{(n)} \\right| ^s.\\]</div><div></div><div></div><div></div><div>[Fractals BM Page 1]</div>"
  },
  {
    "front": "[$]\\dim_H \\texttt{Rec} \\stackrel{d}{=} \\dim_H \\texttt{Zeros} \\ge \\frac 12[/$] almost surely.",
    "back": "[Fractals BM Page 1]"
  },
  {
    "front": "[$](\\dim_H \\texttt{Rec} \\stackrel{d}{=}) \\dim_H \\texttt{Zeros} \\le \\frac 12[/$] almost surely.",
    "back": "[Fractals BM Page 1]"
  },
  {
    "front": "Upper bound on [$]\\mathbb{P} \\left( 0\\in B([t,t+\\varepsilon])\\right)[/$] for [$]t,\\varepsilon &gt; 0[/$].",
    "back": "<br>Let $B$ be a standard Brownian motion&nbsp;and $t,\\varepsilon &gt; 0$. Then \\[&nbsp;\\mathbb{P} \\left( 0\\in B([t,t+\\varepsilon])\\right)&nbsp;&nbsp;\\le \\sqrt{\\frac{\\varepsilon}{t + \\varepsilon}} .\\]<div></div><div></div><div></div><div>[Fractals BM Page 1]&nbsp;</div>"
  },
  {
    "front": "If $d\\ge 2$, then $\\dim_H \\texttt{Range} (B) = \\dim_H \\texttt{Graph} (B) = 2$ almost surely.",
    "back": "[Fractals BM Page 1]"
  },
  {
    "front": "If $d=1$, then $\\dim_H \\texttt{Graph} (B) = 3 / 2$ almost surely.",
    "back": "[Fractals BM Page 2]"
  },
  {
    "front": "Monotone class theorem (set version).",
    "back": "<div>Let $\\Omega\\neq \\varnothing$, $\\mathcal{M}\\subset \\mathcal{P}(\\Omega)$ a $\\pi$-system, and $\\mathcal{H}\\subset \\mathbb{R} ^{\\Omega}$ a vector space such that<br><ol>  <li>$1\\in \\mathcal{H}$ and $ \\boldsymbol{1}_{M}\\in \\mathcal{H}$ for all $M\\in \\mathcal{M}$,&nbsp;&nbsp;&nbsp;</li>  <li>If $0\\le f_n \\uparrow f$ bounded with $f_n \\in \\mathcal{H}$, then $f\\in \\mathcal{H}$.</li></ol><br>Then $\\mathcal{H}$ contains all bounded $\\sigma(\\mathcal{M})$-measurable functions.</div><div></div><div></div><div></div><div>[General Stochastics, Page 3]</div>"
  },
  {
    "front": "Monotone class theorem (functional version).",
    "back": "<div>Let $\\Omega\\neq \\varnothing$ and $\\mathcal{H} \\subset \\mathbb{R} ^{\\Omega}$ be a vector space with <br><ol>  <li>$1\\in \\mathcal{H}$,&nbsp;&nbsp;&nbsp;</li>  <li>If $0\\le f_n \\uparrow f$ bounded with $f_n\\in \\mathcal{H}$, then $f\\in \\mathcal{H}$.</li></ol><br>If $\\mathcal{C}\\subset \\mathcal{H}$ is closed under pointwise multiplication and all $f\\in \\mathcal{C}$ are bounded, then $\\sigma(\\mathcal{C})_b\\subset \\mathcal{H}$ (i.e. all $\\sigma(\\mathcal{C})$-measurable bounded functions are in $\\mathcal{H}$).</div><div></div><div></div><div></div><div>[General Stochastics, Page 3/4]</div>"
  },
  {
    "front": "Progressive sets, the $\\sigma$-algebra $\\text{Prog} $, and connection with progressively measurable processes.",
    "back": "<div>A set $\\Gamma \\in \\mathcal{F}_\\infty \\otimes \\mathcal{B}([0,\\infty))$ is called <i>progressive</i> if $ \\boldsymbol{1}_{\\Gamma}$ is progressively measurable. Then \\[&nbsp;&nbsp;&nbsp; \\text{Prog} := \\left\\{ \\Gamma\\in \\mathcal{F}_\\infty\\otimes \\mathcal{B}([0,\\infty))\\colon \\Gamma&nbsp; \\text{ is progressive}&nbsp; \\right\\} = \\sigma \\left( X\\colon&nbsp; \\text{$X$ is progressively measurable}&nbsp; \\right) \\] is a $\\sigma$-algebra, and a stochastic process $X$ is progressively measurable if and only if it is $\\text{Prog}$-$\\mathcal{B}(\\mathbb{R})$-measurable.</div><div></div><div></div><div></div><div>[General Stochastic Analysis, Page 1]</div>"
  },
  {
    "front": "The predictable [$]\\sigma[/$]-algebra and predictable processes.",
    "back": "<div>The $\\sigma $-algebras on $\\Omega\\times [0,\\infty)$ generated by<br><ol>  <li>elementary adapted processes (linear span of $K \\boldsymbol{1}_{(s,t]}$ with $K$ $\\mathcal{F}_s$-measurable),&nbsp;&nbsp;&nbsp;</li>  <li>adapted left-continuous processes,&nbsp;&nbsp;&nbsp;</li>  <li>adapted continuous processes,&nbsp;&nbsp;&nbsp;</li>  <li>adapted bounded continuous processes,</li></ol><br>coincide. This $\\sigma$-algebra is denoted by $\\mathcal{P}$ and called the <i>predictable</i> $\\sigma$-algebra. A process $X\\colon \\Omega\\times [0,\\infty) \\to \\mathbb{R}$ is called <i>predictable</i> if it is $\\mathcal{P}$-measurable.</div><div></div><div></div><div></div><div>[General Stochastic Analysis&nbsp;Page 1]</div>"
  },
  {
    "front": "Predictable processes are [$](\\mathcal{F}_{t-})[/$]-adapted.",
    "back": "[General Stochastic Analysis, Page 1]"
  },
  {
    "front": "Stochastic Fubini Theorem.",
    "back": "<div>Let $(A,\\mathcal{A})$ be measurable and $H\\colon A\\times [0,\\infty)\\times \\Omega\\to \\mathbb{R}$ be bounded and $\\mathcal{A} \\otimes \\mathcal{P}$-measurable. Then if $X$ is a continuous semimartingale, there exists a $\\mathcal{A} \\otimes \\mathcal{P}$-measurable $K$ such that for all $a\\in A$, \\[&nbsp;&nbsp;&nbsp; (K(a,t ,\\cdot ))_{t\\ge 0} \\text{ and } \\left(\\int_0^t H(a,s,\\cdot ) \\mathop{}\\!\\mathrm{d} X_s\\right)_{t\\ge 0} \\text{ are indistinguishable,} \\] and such that if $\\nu$ is a finite measure on $(A,\\mathcal{A})$, then \\[\\int_A \\left( \\int_0^t H(a,s,\\cdot ) \\mathop{}\\!\\mathrm{d} X_s \\right) \\nu(\\mathop{}\\!\\mathrm{d} a) := \\int_A K(a,t,\\cdot ) \\nu(\\mathop{}\\!\\mathrm{d} a) = \\int_0^t \\left( \\int_A H(a,s,\\cdot ) \\nu(\\mathop{}\\!\\mathrm{d} a) \\right) \\mathop{}\\!\\mathrm{d} X_s\\\\\\] almost surely for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[General Stochastic Analysis Page 1]</div>"
  },
  {
    "front": "If $X_n(a,\\cdot ) \\stackrel{ \\mathbb{P} }{\\longrightarrow} X^a$ for $\\mathcal{A} \\otimes \\mathcal{F}$-measurable $X_n$, we can choose an $\\mathcal{A}\\otimes \\mathcal{F}$-measurable $X$ with $X_n(a,\\cdot ) \\stackrel{ \\mathbb{P} }{\\longrightarrow} X(a,\\cdot )$ for all $a\\in A$.",
    "back": "<div>Let $(A,\\mathcal{A})$ be measurable, $(\\Omega,\\mathcal{F},\\mathbb{P})$ a probability space and $X_n\\colon A\\times \\Omega\\to \\mathbb{R}$ for $n\\in \\mathbb{N}$ be $\\mathcal{A}\\otimes \\mathcal{F}$-measurable such that $X_n(a,\\cdot )\\stackrel{ \\mathbb{P} }{\\longrightarrow} X^a$ for every $a\\in A$. Then there exists an $\\mathcal{A}\\otimes \\mathcal{F}$-measurable $X$ such that \\[&nbsp;&nbsp;&nbsp; X_n(a,\\cdot ) \\stackrel{ \\mathbb{P} }{\\longrightarrow} X(a,\\cdot )\\] for all $a\\in A$.</div><div></div><div></div><div></div><div>[General Stochastics, Page 4]</div>"
  },
  {
    "front": "If $(a_{nm})_{n,m\\in \\mathbb{N}}$ is increasing in both arguments and $k(n),l(n) \\uparrow \\infty$, then&nbsp; \\[&nbsp;&nbsp;&nbsp; \\lim_{n\\to \\infty}\\lim_{m\\to \\infty}a_{nm} = \\lim_{m\\to \\infty}\\lim_{n\\to \\infty}a_{nm} = \\lim_{n\\to \\infty}a_{k(n), l(n)}.\\]",
    "back": "[General Stochastics Page 4]"
  },
  {
    "front": "Nice generator of [$]\\mathcal{E}\\otimes \\mathcal{F}[/$] given nice generators of [$]\\mathcal{E}[/$] and [$]\\mathcal{F}[/$].",
    "back": "<div>If $(E,\\mathcal{E})$ and $(\\Sigma,\\mathcal{F})$ are measurable and $\\mathcal{M}_1 \\subset \\mathcal{E}$ and $\\mathcal{M}_2 \\subset \\mathcal{F}$ are $\\pi$-systems and generators that contain exhausting sequences, then \\[&nbsp;&nbsp;&nbsp; \\sigma\\left( \\left\\{ A\\times B\\colon A\\in \\mathcal{M}_1, B\\in \\mathcal{M}_2 \\right\\}&nbsp; \\right) = \\mathcal{E}\\otimes \\mathcal{F},\\\\\\] and the generator is again a $\\pi$-system that contains an exhausting sequence.</div><div></div><div></div><div></div><div>[General Stochastics Page 4]</div>"
  },
  {
    "front": "If $\\mathcal{C}$ is a class of stochastic processes stable under stopping, then $(\\mathcal{C}_\\text{loc} )_\\text{loc} = \\mathcal{C}_\\text{loc} $.",
    "back": "<div><b>Lemma.</b> If $\\mathcal{C}$ is a class of stochastic processes stable under stopping, then $X\\in \\mathcal{C}_\\text{loc} $ iff there exists a sequence $T_n \\stackrel{ \\text{a.s.}  }{\\longrightarrow} \\infty$ of stopping times with $X^{T_n}\\in \\mathcal{C}$ for all $n\\in \\mathbb{N}$ (so $(T_n)$ must not be monotonic).<br><br><b>Proposition.</b> If $\\mathcal{C}$ is a class of stochastic processes stable under stopping, then $(\\mathcal{C}_\\text{loc} )_\\text{loc} = \\mathcal{C}_\\text{loc} $.</div><div></div><div></div><div></div><div>[General Stochastic Analysis, Page 2]</div>"
  },
  {
    "front": "Kolmogorov continuity criterion.",
    "back": "<div>Let $(X_t)_{t\\in \\mathbb{R}^d}$ be a stochastic process with values in a complete metric space $(S,\\rho)$. If there exist $a,b &gt; 0$ and $c,h &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\rho(X_s,X_t)^a \\right] \\le C \\left| t-s \\right| ^{d + b}\\] for all $s,t\\in \\mathbb{R}^d$ with $\\left| s-t \\right| &lt; h$, then $X$ has a continuous version that is almost surely locally Hölder-continuous with every exponent $c \\in (0,b / a)$.</div><div></div><div></div><div></div><div>[General Stochastic Analysis, Page 2]</div>"
  },
  {
    "front": "Definition of a Markov process.",
    "back": "<div>A <i>Markov process</i> is an adapted stochastic process on $(\\Omega,\\mathcal{F},\\left( \\mathcal{F}_t \\right) _{t\\ge 0})$ with values in a measurable space $(E,\\mathcal{E})$ together with a family of probability kernels $P_t\\colon E\\times \\mathcal{E}\\to [0,1], \\, t\\ge 0,$ satsifying \\[&nbsp;&nbsp;&nbsp; P_{t+s}(x,\\cdot ) = \\int_E P_s(x,\\cdot ) P_t(x,\\mathop{}\\!\\mathrm{d} y),\\quad s,t\\ge 0,\\\\\\] and probability measures $(\\mathbb{P}^{x})_{x\\in E}$ on $(\\Omega,\\mathcal{F})$ such that \\[\\mathbb{P}^x \\left( X_{t+s}\\in \\cdot \\,\\middle\\vert\\, \\mathcal{F}_s\\right) = P_t(X_s,\\cdot ) \\quad \\mathbb{P}^{x}\\text{-a.s.} \\] for all $x\\in E$ and $s,t\\ge 0$. Equivalently, $\\mathbb{E}^x \\left[f(X_{t+s)} \\,\\middle\\vert\\, \\mathcal{F}_s\\right] = \\int_E f(y) P_t(X_s,\\mathop{}\\!\\mathrm{d} y)$ for $f$ measurable and non-negative or bounded.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs Page 1]</div>"
  },
  {
    "front": "<div>Definition strong Markov property.</div>",
    "back": "<div>Let $X$ be a cadlag Markov process with the usual augmented natural filtration. Then if $\\tau$ is a stopping time, $X$ is <i>strong Markov at $\\tau$</i> if \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^x \\left(X_{t+\\tau}\\in \\cdot&nbsp; \\,\\middle\\vert\\, \\mathcal{F}_\\tau\\right) = P_t(X_\\tau, \\cdot ) \\quad \\mathbb{P}^x\\text{-a.s.} \\] for all $t\\ge 0$ and $x\\in E$. Equivalently, $\\mathbb{E} \\left[f(X_{t+\\tau}) \\,\\middle\\vert\\, \\mathcal{F}_\\tau\\right] = \\int_E f(y) P_t(X_\\tau,\\mathop{}\\!\\mathrm{d} y)$ a.s. for $f$ measurable and non-negative or bounded. $X$ is a <i>strong Markov process</i> if it is strong Markov at every finite stopping time.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 1]</div>"
  },
  {
    "front": "Positive contraction operator [$]T[/$] associated with a transition kernel [$]P[/$] on [$]E[/$] and transition semigroup of a family [$](P_t)_{t\\ge 0}[/$].",
    "back": "<div>For a transition kernel $P\\colon E\\times \\mathcal{E}\\to [0,1]$, the linear operator \\[&nbsp;&nbsp;&nbsp; T\\colon B(E) \\to B(E); (T f)(x) := \\int_E f(y) P(x,\\mathop{}\\!\\mathrm{d} y) = \\mathbb{E}^x \\left[ f(X) \\right]\\] is a positive contraction operator, that is, $0\\le f \\le 1$ implies $0\\le T f \\le 1$.<br><br>If $(P_t)_{t\\ge 0}$ is a family of transition kernels on $E$, they satisfy the Chapman-Kolmogorov equations if and only if $(T_t)_{t\\ge 0}$ is a semigroup, that is, $T_{t+s} = T_t T_s$ for $s,t\\ge 0$. In that case, $(T_t)$ is called the <i>transition semigroup</i>.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 1]</div>"
  },
  {
    "front": "Transition semigroup of Brownian motion in [$]\\mathbb{R}^d[/$].",
    "back": "<div>For Brownian motion in $\\mathbb{R}^d$, the transition semigroup is the <i>heat semigroup</i>, that is, the family of operators for which $T_t f(\\cdot ) = u(t,\\cdot )$, where $u$ is the solution of \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\Delta u,\\\\&nbsp;&nbsp;&nbsp; u(0,\\cdot ) = f(\\cdot ).\\end{cases}\\] Indeed, we have for $t\\ge 0$ and $x\\in \\mathbb{R}^d$, \\[(T_t f)(x) = \\mathbb{E}^{x} \\left[ f(B_t) \\right] = \\frac{1}{(2\\pi t)^{d / 2}} \\int_{\\mathbb{R}^d} f(y) \\exp \\left( -\\frac{\\left\\|x - y\\right\\|^2}{2t} \\right) \\mathop{}\\!\\mathrm{d} y.\\]</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 1]</div>"
  },
  {
    "front": "Markov processes with finite state space: Transition semigroup [$]T_t = P(t)[/$] and characterisation when it is strongly continuous.",
    "back": "<div>If $X$ is a Markov process with finite state space $E$, then the transition semigroup is a semigroup of matrices given by $P(t) := T_t = (p_{ij}(t))_{i,j\\in E},\\,t\\ge 0$, where $p_{ij}(t) := P_t(i,\\left\\{ j \\right\\} )$ for $i,j\\in E$ and $t\\ge 0$. Moreover, <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; (P(t))_{t\\ge 0} \\text{ is strongly continuous} &nbsp;&nbsp;&nbsp; &amp;\\iff p_{ij} \\text{ is continuous for all $i,j\\in E$} \\\\&nbsp;&nbsp;&nbsp; &amp;\\iff p_{ij} \\text{ is rightcontinuous at zero for all $i,j\\in E$}\\\\&nbsp;&nbsp;&nbsp; &amp;\\iff p_{ij}(t) \\stackrel{ t\\downarrow 0 }{\\longrightarrow} \\delta_{ij} \\text{ for all $i,j\\in E$}\\end{align*}[/$$]</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 1]</div>"
  },
  {
    "front": "Markov processes with finite state space:&nbsp;[$]Q[/$]-matrix of a Markov process and connection with [$]P(t)[/$].",
    "back": "<div>If $X$ is a Markov process with finite state space $E$ and $p_{ij}$ is continuous for $i,j\\in E$, then there exists $Q\\in \\mathbb{R} ^{E\\times E}$ with $q_{ij} = p_{ij}'(0)$ for $i,j\\in E$. In particular,<br><ol>  <li>$q_{ij}\\ge 0$ for all $i\\neq j$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\sum_{j\\in E}q_{ij} = 0$ for all $i\\in E$,</li></ol><br>and $P'(t) = Q P(t) = P(t) Q$, so $P(t) = \\exp(tQ)$ for $t\\ge 0$. $Q$ is called the <i>$Q$-matrix</i> or <i>infinitesimal generator</i> of $X$.<br><br><b>Remark.</b> If $Q \\in \\mathbb{R}^{E\\times E}$ satisfies (i) and (ii) above, then there exists a unique transition semigroup $(P(t))_{t\\ge 0}$ with $Q$-matrix $Q$, namely $P(t) = \\exp(tQ),\\, t\\ge 0$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 1]</div>"
  },
  {
    "front": "Definition strongly continuous contraction semigroup.",
    "back": "<div>A family of continuous linear operators $(T_t)_{t\\ge 0}$ on a Banach space $C$ is called a <i>strongly continuous contraction semigroup</i> if<br><ol>  <li>$T_0 = \\operatorname{id}$,&nbsp;&nbsp;&nbsp;</li>  <li>$T_{t+s} = T_t T_s$ for all $s,t\\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left\\|T_t\\right\\|\\le 1$ for all $t\\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$t\\mapsto T_t z$ is continuous for all $z\\in C$.</li></ol></div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 2]</div>"
  },
  {
    "front": "Definition Feller semigroup, Feller process, and connection.",
    "back": "<div><b>Definition.</b> Let $E$ be a complete, compact, and separable normed space. A strongly continuous contraction semigroup $(T_t)$ on $C(E)$ is called a <i>Feller semigroup</i> if, for any $t\\ge 0$,<br><ol>  <li>$T_t 1 = 1$,&nbsp;&nbsp;&nbsp;</li>  <li>If $C(E) \\ni f\\ge 0$, then $T_t f \\ge 0$.</li></ol><br><br><b>Definition.</b> A Markov process in $E$ is called a <i>Feller process</i> if $T_t(C(E)) \\subset C(E)$ holds for any $t\\ge 0$ for the transition semigroup of $X$, that is, if $T_t f = \\left[ x \\mapsto \\mathbb{E}^x \\left[ f(X_t) \\right]&nbsp; \\right] \\in C(E)$ for any $f\\in C(E)$. Then the transition group is a Feller process iff $(T_t)$ is strongly continuous.<br><br><b>Theorem.</b> Let $(T_t)$ be a Feller semigroup on $C(E)$. Then there exists a distributionally unique cadlag Markov process $X$ in $E$ with probability measures $(\\mathbb{P}^{x})_{x\\in E}$ with $\\mathbb{P}^x(X_0 = x) = 1$ and transition semigroup $(T_t)$. Note that this is automatically a Feller process.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 2]</div>"
  },
  {
    "front": "Infinitesimal generator of a strongly continuous contraction semigroup.",
    "back": "<div>Let $(T_t)_{t\\ge 0}$ be a strongly continuous contraction semigroup on a Banach space $C$ and put<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; A_t := \\frac{1}{t} (T_t - \\operatorname{id}),\\quad t&gt; 0,\\qquad A\\colon C \\supset D(A) \\to C; \\, z \\mapsto \\lim_{t\\downarrow 0} A_t z,\\\\\\end{align*}[/$$]<br>where $D(A) := \\left\\{ z\\in C\\colon \\lim_{t\\downarrow 0}A_t z \\text{ exists}&nbsp; \\right\\} $. Then $A$ is a densely defined, closed linear operator with $T_t(D(A)) \\subset D(A)$ and \\[&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} (T_t z) = A (T_t z) = T_t (Az)\\] for $z\\in D(A)$ and $t\\ge 0$. $A$ is called the <i>infinitesimal generator</i> of the semigroup.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 2/3]</div>"
  },
  {
    "front": "Definition resolvent of a strongly continuous contraction semigroup on a Banach space and connection with infinitesimal generator.",
    "back": "<div>Let $(T_t)_{t\\ge 0}$ be a strongly continuous contraction semigroup on a Banach space $C$. Then its <i>resolvent</i> is the family $(R_\\lambda)_{\\lambda&gt;0}$ of bounded linear operators defined by \\[&nbsp;&nbsp;&nbsp; R_\\lambda z := \\int_0^\\infty \\mathrm{e}^{-\\lambda t} (T_t z)\\mathop{}\\!\\mathrm{d} t,\\quad z\\in C,\\lambda &gt; 0.\\] Then, for $\\lambda &gt; 0$, the operator $\\lambda - A\\colon D(A) \\to C$ is invertible with inverse $R_\\lambda = (\\lambda - A)^{-1}$. In particular, $R_\\lambda(C) \\subset D(A)$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 3]</div>"
  },
  {
    "front": "Definition Markov pregenerator (and sufficient criterion for second condition), and why [$]f\\in D(A)[/$] is determined by [$](1-\\lambda A)f[/$] for any [$]\\lambda \\ge 0[/$].",
    "back": "<div>A densely defined (usually unbounded) linear operator $A\\colon C(E)\\supset D(A) \\to C(E)$ (where $E$ is a compact, complete, and separable normed space) is called a <i>Markov pregenerator</i> if<br><ol>  <li>$1\\in D(A)$ and $A 1 = 0$,&nbsp;&nbsp;&nbsp;</li>  <li>For $f\\in D(A)$ and $\\lambda \\ge 0$, we have $\\min f \\ge \\min (1-\\lambda A) f$.</li></ol><br>For (ii) it is sufficient to have $(Af)(x_0) \\ge 0$ for some $x_0\\in E$ with $f(x_0) = \\min f$.<br><br><b>Remark.</b> (ii) implies that $\\left\\|f\\right\\|\\le \\left\\|(1-\\lambda A)f\\right\\|$ for $\\lambda \\ge 0$ and $f\\in D(A)$, so the map $f\\mapsto (1-\\lambda A)f$ is injective.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 3]</div>"
  },
  {
    "front": "Closure of a linear operator and criterion for existence.",
    "back": "<div>Let $A$ be a linear operator on a Banach space $C$. Then, if it exists, the smallest closed extension of $A$ is called the <i>closure</i> of $A$ and denoted $\\overline{A}$. It exists if and only if $\\overline{\\text{Gr} (A)}$ is the graph of a linear operator, so if and only if $D(A)\\ni x_n \\to 0$ and $Ax_n \\to y$ implies $y = 0$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 3]</div>"
  },
  {
    "front": "Closure of a Markov pregenerator.",
    "back": "A Markov pregenerator has a closure, which is again a Markov pregenerator.<div></div><div></div><div>[Stochastic Analysis and PDEs, Page 3]</div>"
  },
  {
    "front": "Maximum principle for Feller semigroups (i.e. Markov generators), and connection between Feller processes and elliptic operators.",
    "back": "<div>If $A$ is the generator of a Feller semigroup on $E\\subset \\mathbb{R}^d$ (i.e. a Markov generator) and $f\\in C(E)$ attains a non-negative maximum in the interior of $E$, and $Af \\ge 0$, then $f$ is constant.<br><br>If $X$ is a continuous Feller process on $[0,T]$ with infinitesimal generator $A$ and $C_c^\\infty\\subset D(A)$, then $A$ is an elliptic operator.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 3]</div>"
  },
  {
    "front": "If $A$ is a closed Markov pregenerator, then $\\mathcal{R}(I - \\lambda A)\\subset C(E)$ is closed for all $\\lambda &gt; 0$.",
    "back": "[Stochastic Analysis and PDEs, Page 4]"
  },
  {
    "front": "Definition Markov generator.",
    "back": "<div>A <i>Markov generator</i> is a closed Markov pregenerator $A$ with $\\mathcal{R}(I-\\lambda A) = C(E)$ for all $\\lambda &gt; 0$. It suffices for this to hold for sufficiently small $\\lambda &gt; 0$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "A bounded Markov pregenerator is a Markov generator.",
    "back": "<div>If $A$ is a Markov pregenerator that is everywhere defined and bounded, then $A$ is a Markov generator.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "Hille-Yosida Theorem.",
    "back": "<div>There is a one-to-one correspondence between Markov generators $A$ and Feller semigroups $(T_t)_{t\\ge 0}$ on $C(E)$.<br><ol>  <li>Given $(T_t)$, $A$ is the infinitesimal generator,&nbsp;&nbsp;&nbsp;</li>  <li>Given $A$, define $T_t f := \\lim_{n\\to \\infty} \\left( I - \\frac{t}{n}A \\right) ^{-n} f$ for $f\\in C(E)$ and $t\\ge 0$.</li></ol><br>Note that for $g\\in C(E)$ and $\\lambda &gt; 0$, the solution to $f - \\lambda A f = g$ is given by $f = \\frac{1}{\\lambda} R_{1 / \\lambda}g = \\int_0^\\infty \\mathrm{e}^{-t} T_{\\lambda t}g \\mathop{}\\!\\mathrm{d} t$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "Definition core [$]D[/$] of a Markov generator. Is [$]D[/$] dense in [$]C(E)[/$]?",
    "back": "<div>If $A$ is a Markov generator, a <i>core</i> of $A$ is a linear subset $D\\subset D(A)$ such that $A$ is the closure of $A \\!\\!\\restriction_{D} $. Note that $\\overline{D}= C(E)$ is necessary, but not sufficient.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "How to obtain a Markov generator from a Markov pregenerator. Core of the resulting Markov generator?",
    "back": "<div>If $A$ is a Markov pregenerator such that $\\mathcal{R}(1-\\lambda A)$ is dense in $C(E)$ for sufficiently small $\\lambda &gt; 0$, then the closure of $A$ is a Markov generator, of which $D(A)$ is a core.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "(Markov) generator of Wright-Fisher diffusion.",
    "back": "<div>Define $A$ on the set of polynomials by \\[&nbsp;&nbsp;&nbsp; (Af)(x) := \\frac{1}{2} x(1-x) f''(x), \\quad x\\in[0,1]\\] Then $A$ is a Markov pregenerator and it can be checked that $\\mathcal{R}(1-\\lambda A)$ is dense for all $\\lambda &gt; 0$. Thus the closure of $A$ is a Markov generator with domain \\[\\left\\{ f\\in C[0,1]\\colon \\exists \\text{ polynomials } p_n \\stackrel{ \\left\\|\\cdot \\right\\| }{\\longrightarrow} f \\text{ such that } x(1-x)p_n'' \\text{ has a uniform limit}&nbsp; \\right\\} .\\] The corresponding Feller process is the Wright-Fisher diffusion.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "Theorem (Trotter-Kurtz).",
    "back": "<div>Suppose that $A_n,A,\\, n\\in \\mathbb{N}$ are generators of Feller semigroups $(T_n(t)), (T_t),\\, n\\in \\mathbb{N}$. Then if $D$ is a core of $A$ with $D\\subset D(A_n)$ for all $n\\in \\mathbb{N}$ and $A_n f \\stackrel{  }{\\longrightarrow} A f$ for all $f\\in D$, \\[&nbsp;&nbsp;&nbsp; T_n(t) f \\stackrel{  }{\\longrightarrow} T(t)f\\\\\\] locally uniformly in $t\\ge 0$ for all $f\\in C(E)$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "Solution to [$]F'(t) = AF(t) + G(t)[/$] for a Markov generator [$]A[/$].",
    "back": "<div>If $A$ is the generator of a Feller semigroup $(T_t)_{t\\ge 0}$, and $F,G\\colon [0,\\infty)\\to C(E)$ are such that<br><ol>  <li>$G$ is continuous,&nbsp;&nbsp;&nbsp;</li>  <li>$F(t)\\in D(A)$ for all $t\\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$F$ is differentiable and $F'(t) = AF(t) + G(t)$ for all $t\\ge 0$,</li></ol><br>then $F(t) = T(t) F(0) + \\int_0^t T(t-s) G(s) \\mathop{}\\!\\mathrm{d} s$ for $t\\ge 0$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 4]</div>"
  },
  {
    "front": "Definition martingale problem [$](A,\\mu)[/$] and well-posedness.",
    "back": "<div>Let $(E,\\mathcal{E})$ be a metric space, $A$ a Markov pregenerator, $\\mu$ a probability measure on $E$. Then a probability measure $\\mathbb{P}$ on $D([0,\\infty),E)$ is said to <i>solve the martingale problem $(A,\\mu)$</i> if, for all $f\\in D(A) \\subset C(E)$,<br><ol>  <li>$X_0 \\sim \\mu$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left( f(X_t) - f(X_0) - \\int_0^t Af(X_s) \\mathop{}\\!\\mathrm{d} s \\right)_{t\\ge 0} $ defines a $\\mathbb{P}$-(local) martingale w.r.t. $(\\mathcal{F}_t)$ for all $f\\in D(A)$,</li></ol><br>where $X = \\id_D$ and $\\mathcal{F}_t = \\sigma(\\pi_s\\colon 0\\le s \\le t)$. The problem is <i>well-posed</i> if there exists a unique solution $\\mathbb{P}^x$ to the martingale problem $(A,\\delta_x)$ for every $x\\in E$.<div><br><div></div></div></div><div></div><div>[Stochastic Analysis and PDEs, Page 5]</div>"
  },
  {
    "front": "Unique solution to martingale problem [$](A,x)[/$] if [$]E[/$] is separable and compact and [$]\\bar{A}[/$] is a Markov generator.<div></div><div>Existence.</div>",
    "back": "<div>If $E$ is separable and compact and $A$ is a Markov pregenerator on $C(E)$ such that $\\overline{A}$ is a Markov generator, then the unique solutions $(\\mathbb{P}^x)_{x\\in E}$ to the martingale problem $(A,x)$ for $x\\in E$ are given by the unique Feller process associated with $\\overline{A}$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 5]</div>"
  },
  {
    "front": "Notation [$]E(\\sigma,b)[/$] for SDEs and Yamada-Watanabe theorem.",
    "back": "<div><b>Notation.</b> Let $m,d\\ge 1$ and $\\sigma \\colon \\mathbb{R}^d \\to \\mathbb{R} ^{d\\times m}$ and $b\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ measurable and locally bounded. Then the SDE $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t$ is denoted by $E(\\sigma,b)$.<br><br><b>Theorem (Yamada-Watanabe).</b> Suppose solutions to $E(\\sigma,b)$ exist. Then pathwise uniqueness implies uniqueness in law and, in that case, every solution/the unique solution is strong. Conversely, if every solution is strong, then we have pathwise uniqueness.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 5]</div>"
  },
  {
    "front": "Definition of martingale problem [$]M(a,b)[/$].",
    "back": "<div>Let $d\\ge 1$ and $\\sigma_{ij},b_i\\colon \\mathbb{R}\\to \\mathbb{R}$ for $i,j\\in [d]$ be measurable and locally bounded, and $a:= \\sigma \\sigma^\\top$. Then we say that an $\\mathbb{R}^d$-valued process $(X_t)_{t\\ge 0}$ on a probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$ solves the <i>martingale problem $M(a,b)$</i> if, for all $i,j\\in [d]$,<br><ol>  <li>$\\left( M^i_t := X_t^i - \\int_0^t b_i(X_s) \\mathop{}\\!\\mathrm{d} s&nbsp;&nbsp; \\right)_{t\\ge 0} $ is a local martingale, and&nbsp;&nbsp;&nbsp;</li>  <li>$\\left( M_t^i M_t^j - \\int_0^t a_{ij}(X_s)\\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0}$ is a local martingale, that is, $\\left&lt;M^i,M^j \\right&gt; _t = \\int_0^t a_{ij}(X_s) \\mathop{}\\!\\mathrm{d} s$.</li></ol></div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 6]</div>"
  },
  {
    "front": "Connection between solutions to [$]M(a,b)[/$] and [$]E(\\sigma,b)[/$].",
    "back": "<div>Let $d\\ge 1$ and $\\sigma_{ij},b_i\\colon \\mathbb{R}\\to \\mathbb{R}$ be measurable and locally bounded for $i,j\\in [d]$, and put $a:= \\sigma \\sigma ^\\top$. Then if $(X,B)$ solves $E(\\sigma,b)$, $X$ solves $M(a,b)$. Conversely, if $X$ solves $M(a,b)$, there exists a Brownian motion on an extension of the probability space such that $(X,B)$ is a solution to $E(\\sigma,b)$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 6]</div>"
  },
  {
    "front": "If [$]f\\colon S\\to \\mathbb{R}[/$] is measurable, then [$]\\mathcal{C}(f)[/$] is a [$]G_\\delta[/$]-set.",
    "back": "[General Stochastics, Page 4]"
  },
  {
    "front": "Connection between solutions to [$]M(a,b)[/$]/[$]E(\\sigma,b)[/$] and [$](A,x)[/$] for suitably defined [$]A[/$].",
    "back": "<div>Let $d\\ge 1$, $\\sigma\\colon \\mathbb{R}^d \\to \\mathbb{R} ^{d\\times d }$ and $b\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ measurable and locally bounded, and $a:= \\sigma \\sigma ^\\top$. Then define $A\\colon C^2_0(\\mathbb{R})\\to C_0(\\mathbb{R})$ by \\[&nbsp;&nbsp;&nbsp; (Af)(x) := \\frac{1}{2}\\sum_{i,j=1}^d a_{ij}(x) \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x) + \\sum_{i=1}^d b_i(x) \\frac{\\partial f}{\\partial x_i}(x),\\quad x\\ge 0,\\\\\\] which is a Markov pregenerator. Then solutions to $E(\\sigma,b)$ starting at $x$ are solutions to $(A,x)$, and solutions to $(A,x)$ are solutions to $M(a,b)$ starting at $x$.</div><div></div><div></div><div></div><div>[Stochastic Analysis and PDEs, Page 6]</div>"
  },
  {
    "front": "Under what conditions can we conclude from $N_n \\stackrel{ d }{\\longrightarrow} N$ and $X_n \\stackrel{ d }{\\longrightarrow} X$ that \\[&nbsp;&nbsp;&nbsp; \\sum_{k=1}^{N_n} X_n^{(k)} \\stackrel{ d }{\\longrightarrow} \\sum_{k=1}^N X^{(k)}.\\]",
    "back": "<div>If $N_n\\stackrel{ d }{\\longrightarrow} N$ are $\\mathbb{N}_0$-valued with $\\mathbb{E} \\left[ N \\right] &lt;\\infty$, and $X_n \\stackrel{ d }{\\longrightarrow} X$ with $(X_n)$ uniformly integrable, then \\[&nbsp;&nbsp;&nbsp; \\sum_{k=1}^{N_n}X_n^{(k)} \\stackrel{ d }{\\longrightarrow} \\sum_{k=1}^N X^{(k)},\\\\\\] where $X_n^{(k)},\\,k\\in \\mathbb{N},$ are independent copies of $X_n$.</div><div></div><div></div><div></div><div>[General Stochastics, Page 5]</div>"
  },
  {
    "front": "Asymptotic form of [$]\\mathbb{P}(Z_n &gt; 0)[/$] and weak limit of [$]\\frac{Z_n}{n} \\,\\vert\\, Z_n &gt; 0[/$] for critical Galton-Watson branching process.",
    "back": "<div>If $(Z_n)$ is a critical Galton-Watson branching process with offspring variance $\\gamma &gt; 0$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( Z_n &gt; 0 \\right) \\sim \\frac{2}{n\\gamma}.\\] Furthermore, \\[\\mathbb{P}\\left(\\frac{Z_n}{n} \\in \\cdot\\right) \\stackrel{ d }{\\longrightarrow} \\text{Exp}\\left(\\frac{\\gamma}{2}\\right).\\]</div><div></div><div></div><div></div><div>[Superprocesses, Page 1]</div>"
  },
  {
    "front": "Asymptotic form of first two moments of [$]\\frac{Z_n}{n} \\,\\vert\\, Z_n &gt; 0[/$] for critical Galton-Watson process.",
    "back": "<div>Let $(Z_n)$ be a critical Galton-Watson branching process with offspring variance $\\gamma$. Then $\\mathbb{E} \\left[ Z_n \\right] =1$, $\\mathbb{E} \\left[ Z_n^2 \\right] = 1 + n\\gamma$, and, putting $Y_n \\sim\\frac{Z_n}{n} \\,\\vert\\, Z_n &gt; 0 $,<br><ol>  <li>$\\mathbb{E} \\left[ Y_n \\right] \\sim \\frac{\\gamma}{2}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ Y_n^2 \\right] \\sim \\frac{\\gamma^2}{2}$.</li></ol><br>In particular, $(Y_n)$ is uniformly integrable.</div><div></div><div></div><div></div><div>[Superprocesses, Page 1]</div>"
  },
  {
    "front": "Definition of [$](X_t^{(n)})_{t\\ge 0}[/$] as rescaled Galton-Watson process, and weak limit of [$]X_1^{(n)}[/$].",
    "back": "<div>Let $Z^{(n)},\\,n\\in \\mathbb{N},$ be Galton-Watson branching processes with starting conditions $Z_0^{(n)}\\sim nx$, and put $X_t ^{(n)} := \\frac{Z^{(n)}_{\\left\\lfloor nt \\right\\rfloor }}{n}$ for $t\\ge 0$ and $n\\in \\mathbb{N}$. Then \\[&nbsp;&nbsp;&nbsp; X^{(n)}_1 \\stackrel{ d }{\\longrightarrow} \\sum_{k=1}^N \\text{Exp} \\left( \\frac{\\gamma}{2} \\right) ,\\\\\\] with $N\\sim \\text{Poi} \\left( \\frac{2x}{\\gamma} \\right) $.</div><div></div><div></div><div></div><div>[Superprocesses, Page 1]</div>"
  },
  {
    "front": "Definition of the Borel strong Markov process [$]Y[/$] we will work with.",
    "back": "<div>Let $E$ be a Polish space and $(D([0,\\infty),E), \\mathcal{D}, (\\mathcal{D}_t),Y,(\\mathbb{P}^x))$ be a Borel strong Markov process with $P_t(C_b(E)) \\subset C_b(E)$, where $(\\mathcal{D}_t)$ is the canonical right-continuous filtration. Borel means that $x\\mapsto \\mathbb{P}^x(A)$ is measurable for every $A\\in \\mathcal{B}(E)$. In particular, $Y$ is a <i>Hunt process</i>, that is, \\[&nbsp;&nbsp;&nbsp; Y(T_n) \\stackrel{  }{\\longrightarrow} Y(T) \\, \\text{$\\mathbb{P}^x$-a.s. for all $x\\in E$} \\] whenever $T_n \\uparrow T &lt; \\infty$ $\\mathbb{P}^x$-a.s. are stopping times. In particular, $(P_t)$ is bp-continuous, that is, $P_{t_n} f \\stackrel{ bp }{\\longrightarrow} P_t f$ whenever $t_n \\stackrel{  }{\\longrightarrow} t$ and $f\\in C_b(E)$.</div><div></div><div></div><div></div><div>[Superprocesses Page 1]</div>"
  },
  {
    "front": "Generator of the BSMP [$]Y[/$].",
    "back": "<div>Define a linear operator $A\\colon C_b(E) \\supset D(A) \\to C_b(E)$ by \\[&nbsp;&nbsp;&nbsp; A \\phi := \\lim_{t\\to 0}^{bp} \\frac{P_t - 1}{t} f ,\\quad f\\in D(A) := \\left\\{ f\\in C_b(E)\\colon&nbsp; \\lim_{t\\to 0}^{bp} \\frac{P_t - 1}{t} f \\text{ exists and is in $C_b(E)$}&nbsp; \\right\\}.\\] Then $P_t(C_b(E)) \\subset C_b(E)$ for all $t\\ge 0$ and \\[\\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t}(P_t \\phi) = P_t A \\phi = A P_t \\phi,\\quad t\\ge 0,\\, \\phi\\in D(A),\\\\\\] with derivative in a pointwise sense. Furthermore, \\[D(A) = \\left\\{ \\phi\\in C_b(E)\\colon \\exists \\psi\\in C_b(E) \\text{ s.t. } \\left( \\phi(Y_t) - \\phi(Y_0) - \\int_0^t \\psi(Y_s) \\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0} \\text{ is a $\\mathbb{P}^x$-mg $\\forall x\\in E$}&nbsp; \\right\\},\\\\\\] and then $A\\phi = \\psi$.</div><div></div><div></div><div></div><div>[Superprocesses Page 1/2]</div>"
  },
  {
    "front": "Resolvent of the BSMP [$]Y[/$].",
    "back": "<div>For $\\lambda &gt; 0$, define $U_\\lambda \\colon C_b(E) \\to C_b(E)$ by \\[&nbsp;&nbsp;&nbsp; (U_\\lambda f)(x) := \\int_0^\\infty \\mathrm{e} ^{-\\lambda t} (P_t f) (x) \\mathop{}\\!\\mathrm{d} t = \\mathbb{E}^x \\left[ \\int_0^\\infty \\mathrm{e}^{-\\lambda t} f(Y_t) \\mathop{}\\!\\mathrm{d} t \\right] ,\\quad f\\in C_b(E).\\] Then $(\\lambda - A)^{-1} = U_\\lambda$.</div><div></div><div></div><div></div><div>[Superprocesses Page 2]</div>"
  },
  {
    "front": "Behaviour of $\\lambda U_\\lambda f$ as $\\lambda \\to \\infty$ for $f\\in C_b(E)$.",
    "back": "<div>If $f\\in C_b(E)$, then $\\lambda U_\\lambda f \\stackrel{ bp }{\\longrightarrow} f$ as $\\lambda \\to \\infty$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 2]</div>"
  },
  {
    "front": "Generator [$]A[/$] of [$]Y[/$] is bp-densely defined and bp-closed.",
    "back": "<div>We have $\\overline{D(A)}^{bp} = b\\mathcal{E}$ (bounded measurable functions), $\\overline{(D(A)_+)}^{bp} = b\\mathcal{E}_+$, and $A$ is bp-closed. That is, if $D(A) \\ni \\phi_n \\stackrel{ bp }{\\longrightarrow} \\phi \\in C_b(E)$ and $A\\phi_n \\stackrel{ bp }{\\longrightarrow} \\psi\\in C_b(E)$, then $\\phi\\in D(A)$ and $A\\phi = \\psi$.</div><div></div><div></div><div></div><div>[Superprocesses Page 2]</div>"
  },
  {
    "front": "How to show that a process $(D(\\mathbb{R}_+,E), \\mathcal{D}, (\\mathcal{D}_t), Y, (\\mathbb{P}^x)_{x\\in E})$ with $\\mathbb{P}^x(Y_0 = x) = 1$ is a Markov process, that it is Borel, and that it satisfies (CC) (that is, $P_t(C_b(E)) \\subset C_b(E)$ for $t\\ge 0$).",
    "back": "<div>Let $(D(\\mathbb{R}_+,E), \\mathcal{D}, (\\mathcal{D}_t), Y, (\\mathbb{P}^x)_{x\\in E})$ be with $\\mathbb{P}^x(Y_0 = x) = 1$. Then this process is a Markov process if<br><ol>  <li>$x\\mapsto \\mathbb{E}^x \\left[ f(X_t) \\right] $ is measurable for all $f\\in C_b / B_b / B_+$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E}^x \\left[ f(X_{t+r}) \\,\\middle\\vert\\, \\mathcal{D}_t \\right] = \\mathbb{E}^{X_t} \\left[ f(X_r) \\right] $ $\\mathbb{P}^x$-a.s. for all $x\\in E,t,r\\ge 0, f\\in C_b / B_b / B_+$.</li></ol><br>Furthermore, the latter two conditions are equivalent to<br><ol>  <li>$x\\mapsto \\mathbb{E}^x \\left[ f(X_{t_1},\\ldots ,X_{t_n}) \\right] $ is measurable for all $t_i\\ge 0, f\\in C_b/B_b /B_+$,&nbsp;&nbsp;&nbsp;</li>  <li>$x\\mapsto \\mathbb{E}^x \\left[ f(X_t) \\right] $ is continuous for all $f\\in C_b(E)$ (i.e., $x\\mapsto \\mathbb{P}^x \\circ X_t ^{-1}$ is continuous for all $t\\ge 0$).</li></ol><br><b>Note!</b> (CC) implies (Borel).</div><div></div><div></div><div></div><div>[General Stochastic Analysis, Page 2]</div>"
  },
  {
    "front": "A Markov process [$](Y,(\\mathbb{P}^x))[/$] in [$]D(E)[/$] with [$]P_t(C_b(E)) \\subset C_b(E)[/$] is a strong Markov process.",
    "back": "[General Stochastic Analysis, Page 2]"
  },
  {
    "front": "Let $(Y, (\\mathbb{P}^x)_{x\\in E})$ be a Markov process in $D(\\mathbb{R}_+,E)$. What is $\\mathbb{E}^x \\left[ f(X ^{t+s}) \\,\\middle \\vert\\, \\mathcal{D}_t \\right] $?",
    "back": "<div>Let $(D(\\mathbb{R}_+,E), \\mathcal{D}, (\\mathcal{D}_t), Y, (\\mathbb{P}^x)_{x\\in E})$ be a Markov process. Then for $A\\in \\mathcal{D}_t$, $t,s\\ge 0$, $x\\in E$, and $f\\in B_b / B_+$, \\[&nbsp;&nbsp;&nbsp; \\int_{D(E)} \\boldsymbol{1}_{A}(\\omega) f(\\omega ^{t+s}) \\mathbb{P}^x(\\mathop{}\\!\\mathrm{d} \\omega) = \\int_{D(E)} \\boldsymbol{1}_{A}(\\omega) \\int_{D(E)} f(\\omega^t/t / (\\widetilde{\\omega}^s)) \\mathbb{P} ^{\\omega(t)}(\\mathop{}\\!\\mathrm{d} \\widetilde{\\omega}) \\mathbb{P}^x(\\mathop{}\\!\\mathrm{d} \\omega).\\] Sloppily speaking, $\\mathbb{E}^x \\left[ f(X^{t+s}) \\,\\middle\\vert\\, \\mathcal{D}_t\\right] = \\mathbb{E} ^{X_t} \\left[ f(X_t / t / (X^s)) \\right]$, with only the $X^s$ being random in the second expression.</div><div></div><div></div><div></div><div>[General Stochastic Anlaysis, Page 3]</div>"
  },
  {
    "front": "A (CC) Markov process satisfies that whenever [$]x_n \\stackrel{  }{\\longrightarrow}x[/$], then [$$]\\mathbb{P}^{x_n} \\stackrel{ \\text{fidi} }{\\longrightarrow} \\mathbb{P}^x[/$$].",
    "back": "<div>If $(Y,(\\mathbb{P}^x))$ is a Markov process in $D(E)$ with (CC), then $x_n \\to x$ in $E$ implies that $\\mathbb{P} ^{x_n}\\stackrel{ \\text{fidi} }{\\longrightarrow} \\mathbb{P}^x$, that is, \\[&nbsp;&nbsp;&nbsp; \\Big[x\\mapsto \\mathbb{E}^x \\left[ f(X_{t_1},\\ldots ,X_{t_n}) \\right]\\Big] \\in C_b(E)\\] whenever $n\\in \\mathbb{N}$, $t_i\\ge 0$, and $f\\in C_b(E^n)$. In particular, $Y$ is a Borel process.</div><div></div><div></div><div></div><div>[General Stochastic Analysis, Page 3]</div>"
  },
  {
    "front": "Criterion for [$]P_n \\stackrel{ d }{\\longrightarrow} P[/$] in product [$]S\\times S'[/$] involving [$]\\mathbb{E}^{(n)}\\left[ f(X) g(X)\\right][/$].",
    "back": "<div>Let $S,S'$ be separable metric spaces, and $P_n, P,\\,n\\in \\mathbb{N},$ probability measures on $S\\times S'$. Then $P_n \\stackrel{ d }{\\longrightarrow} P$ if and only if for any $f\\in C_b(S)$, $g\\in C_b(S')$, \\[&nbsp;&nbsp;&nbsp; \\int_{S\\times S'} f(x) g(y) P_n (\\mathop{}\\!\\mathrm{d} (x,y)) \\stackrel{  }{\\longrightarrow} \\int_{S\\times S'} f(x) g(y) P (\\mathop{}\\!\\mathrm{d} (x,y)).\\]</div><div></div><div></div><div></div><div>[General Stochastics, Page 5]</div>"
  },
  {
    "front": "Branching particle systems:<br><ol>  <li>Drift $g\\in C_b(E)$, branching variance $\\gamma \\in C_b(E)$, and offspring distribution kernel $\\nu^N(x,\\cdot )$.&nbsp;&nbsp;&nbsp;</li>  <li>Index notation for offsprings.</li></ol>",
    "back": "[Superprocesses, Page 3]"
  },
  {
    "front": "Branching particle systems: Definition of $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P}^N)$, and intuitive description.",
    "back": "[Superprocesses, Page 3]"
  },
  {
    "front": "Branching particle systems: Termination time $\\zeta_\\alpha$ of a particle $\\alpha \\in I$ and notation $\\alpha \\stackrel{\\omega}{\\sim} t$.",
    "back": "<div>For $\\alpha \\in I$, the termination time of $\\alpha$ is \\[\\zeta_\\alpha := \\begin{cases}&nbsp;&nbsp;&nbsp; 0 &amp;, x_{\\alpha_0} = \\perp,\\\\&nbsp;&nbsp;&nbsp; \\min \\left\\{ \\frac{i+1}{N}\\colon i&lt;|\\alpha| , N ^{\\alpha | i} &lt; \\alpha_{i+1} \\right\\} &amp;, \\text{ set is non-empty} ,\\\\&nbsp;&nbsp;&nbsp; \\frac{|\\alpha| + 1}{N} &amp;, \\text{else} .\\end{cases}\\] Write $\\alpha \\stackrel{\\omega}{\\sim} t$ if $\\frac{|\\alpha|}{N}\\le t &lt; \\frac{|\\alpha| + 1}{N} = \\zeta_\\alpha$. Note that $\\alpha \\sim t \\iff \\alpha \\sim \\underline{t}:= \\frac{\\left\\lfloor Nt \\right\\rfloor }{N}$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 3]</div>"
  },
  {
    "front": "Branching particle systems: Limiting behaviour of $X^0_N := \\frac{1}{N} \\sum_{k=1}^{M_N} \\delta_{x_i} \\sim \\frac{1}{N} \\text{Poi} (NX^0)$.",
    "back": "<div>We have $X^0_N \\stackrel{ \\mathbb{P} }{\\longrightarrow} X^0$ and, for all bounded measurable $f\\colon E\\to \\mathbb{R}$, \\[\\int_E f \\mathop{}\\!\\mathrm{d} X^0_N \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\int_E f \\mathop{}\\!\\mathrm{d} X^0\\\\\\]</div><div></div><div></div><div></div><div>[Superprocesses, Page 3]</div>"
  },
  {
    "front": "Bounds on $\\mathbb{E} \\left[ \\int \\psi \\mathop{}\\!\\mathrm{d} H_t^N \\right] $ and $\\mathbb{E} \\left[ \\int\\phi \\mathop{}\\!\\mathrm{d} X^N_t \\right] $ for $\\psi\\colon D(E)\\to [0,\\infty)$ and $\\phi\\colon E\\to [0,\\infty)$ measurable.",
    "back": "<div><b>Notation.</b> Put $\\mathbb{P}^{\\mu}(\\cdot ) := \\int_E \\mathbb{P}^x(\\cdot ) \\mu(\\mathop{}\\!\\mathrm{d} x)$ for $\\mu\\in M_F(E)$, and $g_\\infty:= \\sup_{x,N}\\left| g_N(x) \\right| $.<br><br><b>Lemma.</b> If $\\psi\\colon D(E)\\to [0,\\infty)$ is measurable and $t\\ge 0$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{\\alpha \\sim t} \\psi(Y^\\alpha_{\\cdot \\wedge t}) \\right] = \\mathbb{E} \\left[ \\int \\psi \\mathop{}\\!\\mathrm{d} H_t^N \\right] \\le \\mathrm{e}^{g_{\\infty}\\underline{t}} \\mathbb{E}^{X_0} \\left[ \\psi(Y^t) \\right] = \\mathrm{e}^{g_\\infty \\underline{t}} \\int \\mathbb{E}^x \\left[ \\psi(Y^t) \\right] X_0(\\mathop{}\\!\\mathrm{d} x).\\] If $\\phi \\colon E \\to [0,\\infty)$ is measurable, then $\\mathbb{E} \\left[ \\int \\phi \\mathop{}\\!\\mathrm{d} X_t^N \\right] \\le \\mathrm{e}^{g_\\infty \\underline{t}} \\mathbb{E}^{X_0} \\left[ \\phi(Y_t) \\right]$. If $g_N \\equiv 0$, then equality holds.</div><div></div><div></div><div></div><div>[Superprocesses, Page 4]</div>"
  },
  {
    "front": "Definitions of $X^N_t$ and $H^N_t$, why both have cadlag paths, adaptedness of $X^N$, expression of $\\int f \\mathop{}\\!\\mathrm{d} X_t^N$ in terms of $H^N$.",
    "back": "<div>For $N\\in \\mathbb{N}$ and $t\\ge 0$, put $X_t^N := \\frac{1}{N} \\sum_{\\alpha \\sim t} \\delta_{Y^\\alpha_t}\\in M_F(E)$ and $H_t^N := \\frac{1}{N} \\sum_{\\alpha \\sim t} \\delta_{Y^\\alpha_{\\cdot \\wedge t}} \\in M_F(D(E))$ (the <i>historical process</i>). Then $X^N \\in D(M_F(E))$, $H^N \\in D(M_F(D(E)))$, and \\[&nbsp;&nbsp;&nbsp; \\int_E f \\mathop{}\\!\\mathrm{d} X_t^N = \\int_{D(E)} f(y_t) H_t^N(\\mathop{}\\!\\mathrm{d} y)\\] for $f\\in B_b(E)$. Furthermore, $\\left\\{ \\alpha \\sim t \\right\\} \\in \\mathcal{F}_t$, and $X^N_t$ is $\\mathcal{F}_t$-measurable for all $t\\ge 0$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 4]</div>"
  },
  {
    "front": "Upper bound on $\\mathbb{P}\\left( \\sup_{t\\le T} X_t^N(E) \\ge \\lambda \\right) $ for $\\lambda,T &gt; 0$.",
    "back": "<div>If $\\lambda,T &gt; 0$, and $N \\ge N_0(g_\\infty)$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{t\\le T}X_t^N(E) \\ge \\lambda \\right) \\le \\frac{\\mathrm{e} ^{3g_\\infty T}X_0(E))}{\\lambda}.\\]</div><div></div><div></div><div></div><div>[Superprocesses, Page 4]</div>"
  },
  {
    "front": "Definition of [$](\\text{LMP})_\\nu[/$] and Dawson-Watanabe superprocess.",
    "back": "<div>Let $\\nu \\in M_1(M_F(E))$ and $X$ a continuous $M_F(E)$-valued process on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$. Then $X$ satisfies $(\\text{LMP})_\\nu$ if<br><ol>  <li>$X_0 \\sim \\nu$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\forall \\phi\\in D(A)\\colon \\left( M_t(\\phi) := X_t(\\phi) - X_0(\\phi) - \\int_0^t X_s(A^g\\phi)\\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0}$ is an $(\\mathcal{F}_t)$-continous local martingale,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left&lt;M(\\phi) \\right&gt; _t = \\int_0^t X_s(\\gamma \\phi^2) \\mathop{}\\!\\mathrm{d} s$ for $t\\ge 0$ and $\\phi\\in D(A)$.</li></ol><br>We write $(\\text{MP})_\\nu$ if we want a true martingale and $(\\text{LMP})_{X_0} := (\\text{LMP})_{\\delta_{X_0}}$.<br><br><b>Definition.</b> We call a process $X$ on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$ satisfying $(\\text{LMP})_{\\nu}$ an <i>$(\\mathcal{F}_t)$-$(A,\\gamma,g)$-Dawson-Watanabe superprocess</i> with initial distribution $\\nu$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 4/5]</div>"
  },
  {
    "front": "Well-posedness of [$](\\text{LMP})_\\nu[/$] and the corresponding BSMP. Corollary on [$]X^N \\stackrel{ d }{\\longrightarrow} ?[/$].",
    "back": "<div><b>Well-posedness.</b> $(\\text{LMP})_\\nu$ is well-posed for any $\\nu\\in M_1(M_F(E))$, that is, a solution always exists and its law on $C([0,\\infty),M_F(E))$ is unique.<br><br><b>BSMP.</b> There exists a BSMP $(C([0,\\infty),M_F(E)), \\mathcal{F}_X, (\\mathcal{F}^X_t),X,(\\mathbb{P}_{X_0}))$ with $T_t(C_b(M_F(E))) \\subset C_b(M_F(E))$ such that if $(Z_t) $ satisfies $(\\text{LMP})_\\nu$ on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$, then \\[&nbsp;&nbsp;&nbsp; \\text{law} (Z) = \\mathbb{P} \\circ Z^{-1} = \\int \\mathbb{P}_{X_0}(\\cdot ) \\nu(\\mathop{}\\!\\mathrm{d} X_0),\\\\\\] and whenever $T$ is an a.s. finite stopping time, \\[\\mathbb{P} \\left(Z_{T+\\cdot }\\in A \\,\\middle\\vert\\, \\mathcal{F}_T\\right) = \\mathbb{P}_{Z_T}(A)\\] almost surely.<br><br><b>Corollary.</b> $X^N \\stackrel{ d }{\\longrightarrow} \\mathbb{P}_{X_0}$ on $D(E)$.</div><div></div><div></div><div></div><div>[Superprocesses Page 5]</div>"
  },
  {
    "front": "In the context of $X$ satisfying $(\\text{LMP})_\\nu$, definitions of $L^2_\\text{loc}$, $L^2$, and $M(\\psi)$ for simply functions $\\psi \\in S$, extension to linear map $M\\colon L^2_\\text{loc} \\to \\mathcal{M}_\\text{loc}$.<br><br>When is $M(\\psi)$ a true martingale and what is $\\left&lt;M(\\psi) \\right&gt; $?",
    "back": "<div>Suppose that $X$ satisfies $(\\text{LMP})_\\nu$ on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$, and let $\\mathcal{P}\\subset \\mathcal{B}([0,\\infty)) \\otimes \\mathcal{F}$ be the predictable $\\sigma$-algebra. Then define<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; L^2_\\text{loc} &amp;:= \\left\\{ \\psi&nbsp; \\text{ $\\mathcal{P}\\otimes \\mathcal{E}$-measurable with $\\int_0^t X_s(\\gamma \\psi_s ^2)\\mathop{}\\!\\mathrm{d} s &lt; \\infty$ for all $t\\ge 0$} \\right\\} ,\\\\&nbsp;&nbsp;&nbsp; L^2 &amp;:= \\left\\{ \\psi \\text{ $\\mathcal{P}\\otimes \\mathcal{E}$-measurable with $\\mathbb{E} \\left[ \\int_0^t X_s(\\gamma \\psi_s ^2)\\mathop{}\\!\\mathrm{d} s \\right] &lt; \\infty$ for all $t\\ge 0$} \\right\\} .\\end{align*}[/$$]<br>Simple functions are sums of functions of the form $\\psi(t,\\omega,x) = \\phi(x) \\psi(\\omega) \\boldsymbol{1}_{(r,s]}(t) = \\phi(x) \\widetilde{\\psi}(\\omega,t)$ with $\\phi \\in D(A)$ and $\\psi \\in b\\mathcal{F}_r$, in which case we define \\[&nbsp;&nbsp;&nbsp; M(\\psi)_t := \\psi \\left( M_{t\\wedge s}(\\phi) - M_{t\\wedge r}(\\phi) \\right) = \\int_0^t \\widetilde{\\psi}_s \\mathop{}\\!\\mathrm{d} M(\\phi)_s,\\quad t\\ge 0.\\] Then $M$ can be uniquely extended to a linear map $M\\colon L^2_\\text{loc} \\to \\mathcal{M}_\\text{loc}$ with \\[\\left&lt;M(\\psi) \\right&gt; _t = \\int_0^t X_s(\\gamma \\psi_s ^2) \\mathop{}\\!\\mathrm{d} s,\\quad t\\ge 0,\\, \\psi \\in L^2_\\text{loc}.\\] If $\\psi \\in L^2$, then $M(\\psi)$ is a square-integrable martingale.</div><div></div><div></div><div></div><div>[Superprocesses, Page 5]</div>"
  },
  {
    "front": "Upper bound on $\\mathbb{E} \\left[ X_t(E) \\right] $ for solution of $(\\text{LMP})_\\nu$. In particular, sufficient criterion for $\\psi \\in B(\\mathcal{P}\\otimes \\mathcal{E})$ to be in $L^2$.",
    "back": "<div>Suppose $X$ is a solution of $(\\text{LMP})_\\nu$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X_t(E) \\right] \\le \\mathrm{e}^{\\overline{g}t}\\mathbb{E} \\left[ X_0(E) \\right] ,\\quad t\\ge 0.\\] In particular, if $\\psi$ is $\\mathcal{P}\\otimes&nbsp; \\mathcal{E}$-measurable and bounded on $[0,T]\\times \\Omega\\times E$ for every $T&gt;0$, then $\\psi \\in L^2$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 5]</div>"
  },
  {
    "front": "Uniqueness of solutions to $(\\text{LMP})_\\nu$ is implied by existence of solutions $(H_1)$ and uniqueness of one-dimensional distributions $(H_2)$.",
    "back": "<div>Suppose that<br><ol>  <li>For each $X_0\\in M_F(E)$, a solution to $(\\text{LMP})_\\nu$ exists,&nbsp;&nbsp;&nbsp;</li>  <li>For every $t\\ge 0$ there exists a kernel $p_t\\colon M_F(E) \\times \\mathcal{M}_F \\to [0,1]$ such that whenever $\\nu\\in M_1(M_F(E))$ and $Z$ solves $(\\text{LMP})_\\nu$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( Z_t \\in A \\right) = \\int_{M_F(E)} p_t(X_0,A) \\nu(\\mathop{}\\!\\mathrm{d} X_0),\\quad A\\in \\mathcal{M}_F.&nbsp;&nbsp;&nbsp; \\]</li></ol><br>Then solutions to $(\\text{LMP})_\\nu$ are unique in law, and there exists a strong Markov process $(C(M_F(E)), \\mathcal{F}_X, (\\mathcal{F}^X_t), X, (\\mathbb{P}_{X_0})_{X_0})$ such that $Z\\sim (\\text{LMP})_\\nu$ implies \\[&nbsp;&nbsp;&nbsp; \\textrm{law}(Z) = \\int \\mathbb{P}_{X_0}(\\cdot ) \\nu(\\mathop{}\\!\\mathrm{d} X_0).\\]</div><div></div><div></div><div></div><div>[Superprocesses, Page 6]</div>"
  },
  {
    "front": "Definition of $D(\\vec{A})_T$ for $T &gt; 0$, and $\\vec{A}\\phi$ for $\\phi \\in D(\\vec{A})_T$. Formula for $X_t(\\phi_t)$ and extension of martingale problem of $Y$ to functions $\\phi\\in D(\\vec{A})_T$.",
    "back": "<div>Let $T &gt; 0$. Then $D(\\vec{A})_T$ comprises all functions $\\phi\\colon [0,T]\\times E\\to \\mathbb{R}$ with<br><ol>  <li>For every $x\\in E$, $\\phi(\\cdot ,x)$ is absolutely continuous and there exists a version of $\\dot{\\phi}(t,x) = \\frac{\\partial \\phi}{\\partial t}(t,x)$ which is jointly Borel-measurable, bounded, and continuous in $x$ for all $t\\in [0,T]$,&nbsp;&nbsp;&nbsp;</li>  <li>For every $t\\in [0,T]$, $\\phi(t,\\cdot )\\in D(A)$ and $A\\phi_t$ is bounded on $[0,T]\\times E$.</li></ol><br>For $\\phi \\in D(\\vec{A})_T$, put $\\vec{A}\\phi(t,x) := \\dot{\\phi}(t,x) + A\\phi_t(x)$. Then \\[&nbsp;&nbsp;&nbsp; X_t(\\phi_t) = X_0(\\phi_0) + M_t(\\phi) + \\int_0^t X_s(\\dot{\\phi}_s + A^g \\phi_s) \\mathop{}\\!\\mathrm{d} s\\\\\\] almost surely for all $t\\in[0,T]$. Furthermore, \\[\\left( N_t := \\phi(t,Y_t) - \\phi(0,Y_0) - \\int_0^t \\vec{A}\\phi(s,Y_s)\\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0}\\] is a bounded cadlag $\\mathcal{D}_t$-martingale under $\\mathbb{P}^x$ for all $x\\in E$, and jumps are a.s. contained in jumps of $Y$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 6]</div>"
  },
  {
    "front": "Superprocesses: definition of $P_t^g\\colon b\\mathcal{E}\\to b\\mathcal{E}$, (CC) property and connection with $D(A)$ and $A^g$.",
    "back": "<div>For $t\\ge 0$ and $\\phi\\in b\\mathcal{E}$, define $(P^g_t\\phi)(x) := \\mathbb{E}^x\\left[ \\phi(Y_t) \\exp \\left( \\int_0^tg(Y_s)\\mathop{}\\!\\mathrm{d} s \\right)&nbsp; \\right] $ for $x\\in E$. Then,<br><ol>  <li>$P^g_t(C_b(E)) \\subset C_b(E)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\phi\\in D(A) \\iff \\frac{1}{t} (P^g_t - 1)\\phi \\stackrel{ bp }{\\longrightarrow} \\psi\\in C_b(E)$. In that case, $\\psi = A^g\\psi$,&nbsp;&nbsp;&nbsp;</li>  <li>$P_t^g(D(A)) \\subset D(A)$ and if $\\phi\\in D(A)$, then $\\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} (P_t^g \\phi) = A^g P_t^g \\phi = P_t^g A^g \\phi$, with derivative in a $bp$-sense.</li></ol></div><div></div><div></div><div></div><div>[Superprocesses, Page 6/7]</div>"
  },
  {
    "front": "Green function representation of $X_t(\\phi)$ for $X$ a solution of $(\\text{LMP})_\\nu$ and $\\phi\\in b\\mathcal{E}$. Corollary on $\\mathbb{E} \\left[ X_t(\\phi) \\right] $ and $\\mathbb{E} \\left[ X_s(\\phi)X_t(\\psi) \\right] $ if $\\nu = \\delta_{X_0}$.",
    "back": "<div>Suppose $X$ is a solution to $(\\text{LMP})_\\nu$ and $\\phi\\in b\\mathcal{E}$. Then \\[&nbsp;&nbsp;&nbsp; X_t(\\phi) = X_0(P^g_t\\phi) + \\int_0^t \\int P^g_{t-s}\\phi(x) \\mathop{}\\!\\mathrm{d} M(s,x)\\] almost surely for all $t\\ge 0$. In particular, if $\\nu = \\delta_{X_0}$ for some $X_0\\in M_F(E)$, and $0\\le s\\le t$, $\\psi\\in b\\mathcal{E}$,<br><ol>  <li>$\\mathbb{E} \\left[ X_t(\\phi) \\right] = X_0(P^g_{t}\\phi)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ X_s(\\phi)X_t(\\psi) \\right] = X_0(P^g_s\\phi)X_0(P^g_t\\psi) + \\int_0^s X_0(P^g_r(\\gamma P^g_{s-r}\\phi P^g_{t-r}\\psi)) \\mathop{}\\!\\mathrm{d} r$.</li></ol></div><div></div><div></div><div></div><div>[Superprocesses, Page 7]</div>"
  },
  {
    "front": "If $Y$ is a second countable topological space and a topology on $X$ is generated by maps $f_i\\colon X\\to Y,\\, i\\in I$, when can we conclude that $X$ is also second countable and that $\\mathcal{B}(X) = \\sigma(f_i\\colon i\\in I)$?<br><br>What does this mean in the case that $X = M_F(E)$ for Polish $E$ with topology of weak convergence?",
    "back": "<div>If $Y$ is second countable and $f_i\\colon X\\to Y,\\,i\\in I$ are maps that generate a topology $\\mathcal{T}$ on $X$, such that there exists countable $I_0\\subset I$ with \\[&nbsp;&nbsp;&nbsp; \\forall i\\in I_0\\colon f_i(x_n) \\stackrel{  }{\\longrightarrow} f_i(x) \\implies \\forall i\\in I\\colon f_i(x_n) \\stackrel{  }{\\longrightarrow} f_i(x)\\] for any $(x_n)\\in X^\\mathbb{N}, \\, x\\in X$, then<br><ol>  <li>$X$ is second countable and $\\mathcal{T}= \\mathcal{T}_0$,&nbsp;&nbsp;&nbsp;</li>  <li>$x_n \\stackrel{  }{\\longrightarrow} x$ in $X$ iff $f_i(x_n) \\stackrel{  }{\\longrightarrow} f_i(x)$ for all $i\\in I_0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathcal{B}(X) = \\sigma(f_i\\colon i\\in I_0) = \\sigma(f_i\\colon i\\in I)$.</li></ol><br><br><b>Corollary.</b> If $E$ is Polish, then so is $M_F(E)$, and $\\mathcal{B}(M_F(E)) = \\mathcal{M}_F = \\sigma(\\pi_f \\colon f\\in C_b(E))$, where $\\pi_f\\colon M_F(E) \\to \\mathbb{R}; \\mu \\mapsto \\mu(f) = \\int f \\mathop{}\\!\\mathrm{d} \\mu$.<div></div><div></div><div></div><div>[General Stochastics, Page 5]</div></div>"
  },
  {
    "front": "Superprocesses: Definition of [$]e_\\phi[/$] for [$]\\phi\\in b\\mathcal{E}_+[/$] and Laplace functional of [$]W\\colon \\Omega \\to M_F(E)[/$]. Why does [$]L_W[/$] characterise the law of [$]W[/$]?",
    "back": "<div>For $\\phi\\in b\\mathcal{E}_+$, put $e_\\phi\\colon M_F(E) \\to \\mathbb{R}; \\mu \\mapsto \\mathrm{e} ^{-\\mu(\\phi)}$. Then if $W$ is a random variable in $M_F(E)$, define its <i>Laplace functional</i> by \\[&nbsp;&nbsp;&nbsp; L_W\\colon b\\mathcal{E}_+ \\to \\mathbb{R}; \\, \\phi \\mapsto \\mathbb{E} \\left[ e_\\phi(W) \\right] = \\mathbb{E} \\left[ \\mathrm{e}^{-W(\\phi)} \\right] .\\] Then if $L_W$ and $L_{W'}$ aggree on $D_0\\subset b\\mathcal{E}_+$ with $\\overline{D_0}^{bp} = b\\mathcal{E}_+$, we must have $W \\stackrel{d}{=} W'$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 7]</div>"
  },
  {
    "front": "Definition of $(\\text{SE})_{\\phi,f}$ and $(\\text{ME})_{\\phi,f}$, connection, and theorem on unique solution to the latter.",
    "back": "<div>Define $a(x,\\lambda) := \\lambda g(x) - \\gamma(x) \\lambda^2 / 2$ for $x\\in E,\\lambda \\in \\mathbb{R}$. Now for $\\phi\\in D(A)_+, \\, f\\in C_b(E)_+$, and $V\\in D(\\vec{A})_t$ for some $t&gt; 0$, write<br>\\begin{equation}\\tag*{$(\\text{SE})_{\\phi,f}$}<br>&nbsp;&nbsp;&nbsp; \\begin{cases}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\partial V_s}{\\partial s} = AV_s + a(\\cdot ,V_s) + f, \\quad s\\le t,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V_0 = \\phi.<br>&nbsp;&nbsp;&nbsp; \\end{cases}<br>\\end{equation}<br>We also define for $\\phi,f\\in b\\mathcal{E}_+$, and jointly measurable $V_\\cdot (\\cdot )$,<br>\\begin{equation}\\tag*{$(\\text{ME})_{\\phi,f}$}<br>&nbsp;&nbsp;&nbsp; V_t = P_t \\phi + \\int_0^t P_{t-s}(f + a(\\cdot ,V_s)) \\mathop{}\\!\\mathrm{d} s.<br>\\end{equation}<br>Then a solution $V\\in D(\\vec{A})_t$ of $(\\text{SE})_{\\phi,f}$ also solves $(\\text{ME})_{\\phi,f}$.<br><br><b>Theorem.</b> Let $\\phi,f\\in b\\mathcal{E}_+$. Then there exists a unique, jointly measurable, non-negative solution $V^f \\phi$ of $(\\text{ME})_{\\phi,f}$ such that $V^f\\phi$ is bounded on $[0,T]\\times E$ for all $T &gt; 0$. If $\\phi\\in D(A)_+$ and $f\\in C_b(E)_+$, then $V^f\\phi \\!\\!\\restriction_{[0,T]\\times E} \\in D(\\vec{A})_T$ solves $(\\text{SE})_{\\phi,f}$, and $\\dot{V}^f \\phi$ and $AV^f\\phi$ are continuous in $x$ and $t$.</div><div></div><div></div><div></div><div>[Superprocesses, Page 7]</div>"
  },
  {
    "front": "Laplace functional equation $(\\text{LE})_{\\phi,f}$ for solutions $X$ of $(\\text{LMP})_\\nu$.\\\\<div></div><div>Proof in the case where $\\phi\\in D(A)_+$ and $f\\in C_b(E)_+$, and the existence of a non-negative solution $V\\in D(\\vec{A})_t$ of $(\\text{SE})_{\\phi,f}$ is assumed.</div>",
    "back": "<div>If $\\phi,f\\in b\\mathcal{E}_+$ and $X$ solves $(\\text{LMP})_\\nu$, then<br>\\begin{equation}\\tag*{$(\\text{LE})_{\\phi,f}$ }<br>&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\exp\\left( -X_t(\\phi) - \\int_0^t X_s(f)\\mathop{}\\!\\mathrm{d} s \\right)&nbsp; \\right] = \\mathbb{E} \\left[ \\mathrm{e}^{-X_0(V_t^f \\phi)} \\right] ,\\quad t\\ge 0.<br>\\end{equation}</div><div></div><div></div><div></div><div>[Superprocesses, Page 7]</div>"
  },
  {
    "front": "Superprocesses: Verification of [$](H_2)[/$].",
    "back": "[Superprocesses, Page 8]"
  },
  {
    "front": "Characterisation of relative compactness and total boundedness of subsets of metric spaces involving sequences, and relationship with each other.",
    "back": "<div>Let $(E,\\rho)$ be a metric space.<br><ol>  <li>A subset $A\\subset E$ is totally bounded iff every sequence in $A$ has a Cauchy subsequence,&nbsp;&nbsp;&nbsp;</li>  <li>A subset $A\\subset E$ is relatively compact iff every sequence in $A$ has a convergent subsequence.</li></ol><br>In particular, every relatively compact subset is totally bounded, and the converse holds (for all subsets) iff $(E,\\rho)$ is complete.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>\\textbf{$\\implies$:} Let $(x_n)\\in A^\\mathbb{N}$. Cover $A$ with finitely many $1$-balls. Then there is $I_1\\subset \\mathbb{N}$ infinite such that all $x_i,i\\in I_1$ lie in one of the balls $B_1$. Proceed with $1 / 2$-balls and $(x_i)_{i\\in I_1}$, and thus get a sequence of balls $(B_n)_{n\\in \\mathbb{N}}$ and infinite sets $\\mathbb{N} \\supset I_1 \\supset I_2 \\supset \\ldots $ such that $i \\in I_n \\implies x_i \\in B_n$. Choose $(k(n))$ increasing with $k(n) \\in I_n$. Then $(x_{k(n)})$ is a Cauchy sequence.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\textbf{$\\impliedby$:} Suppose $A$ is not totally bounded, $\\varepsilon&gt;0$ accordingly. Then let $x_{1}\\in A$, $x_2\\in A \\setminus B(x_1,\\varepsilon)$, $x_3\\in A\\setminus (B(x_1,\\varepsilon) \\cup B(x_2,\\varepsilon))$, etc. Then $(x_n)$ does not have a Cauchy subsequence.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>\\textbf{$\\implies$:} Clear. \\textbf{$\\impliedby$:} Let $(\\widetilde{x}_n)\\in \\overline{A}^\\mathbb{N}$ and $(x_n)\\in A^\\mathbb{N}$ with $\\rho(x_n,\\widetilde{x}_n) &lt; 1 / n$. Then $(x_n)$ has a convergent subsequence $(x_{k(n)})$, so $\\widetilde{x}_{k(n)}\\stackrel{  }{\\longrightarrow} x\\in \\overline{A}$, so $\\overline{A}$ is compact.&nbsp;&nbsp;&nbsp;</li></ol><br><br>&nbsp;&nbsp;&nbsp; Now suppose every totally bounded subset is relatively compact and let $(x_n)\\in E^\\mathbb{N}$ be a Cauchy sequence. Then $A:= \\left\\{ x_n\\colon n\\in \\mathbb{N} \\right\\} $ is totally bounded by (i), so relatively compact, so $(x_n)$ converges.</i> </p> </div><div></div>"
  },
  {
    "front": "Version of Arzela-Ascoli for [$]C([0,\\infty),\\mathbb{R}^d)[/$].",
    "back": "<div>A subset $A\\subset C([0,\\infty),\\mathbb{R}^d)$ is relatively compact (w.r.t. topology of uniform convergence on compacts) if and only if<br><ol>  <li>$\\sup_{x\\in A}\\left| x(0) \\right| &lt; \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li>For all $T&gt;0$, $\\sup_{x\\in A} w_x(\\delta,T) \\stackrel{  }{\\longrightarrow} 0$ as $\\delta\\to 0$,</li></ol><br>where $w_x(\\delta,T) = \\sup_{\\substack{\\left| s - t \\right| &lt;\\delta\\\\ s,t\\le T}} \\left| x(t) - x(s) \\right| $.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Follows from Arzela-Ascoli and the sequence characterisation of relative compactness.</i> </p> </div><div></div>"
  },
  {
    "front": "Daniell-Stone theorem on positive linear functionals [$]I\\colon \\mathcal{H}\\subset \\mathbb{R}^\\Omega\\to \\mathbb{R}[/$].",
    "back": "<div>Let $\\Omega\\neq \\varnothing$ and $H\\subset \\mathbb{R}^\\Omega$ be a real vector lattice (vector space that is closed under $\\wedge$ and $\\vee$), and let $I\\colon H\\to \\mathbb{R}$ be a positive, linear functional such that whenever $H\\ni f_n\\downarrow 0$, then $I(f_n)\\downarrow 0$. Then there exists a measure $\\mu$ on $(\\Omega,\\mathcal{F}:= \\sigma(H))$ such that $H\\subset L^1(\\Omega,\\mathcal{F},\\mu)$ and $I(f) = \\int f \\mathop{}\\!\\mathrm{d} \\mu$ for all $f\\in H$.<br><br>If $\\exists (f_n)\\in H^\\mathbb{N}$ with $0\\le f_n\\uparrow 1$, then $\\mu$ is $\\sigma$-finite and unique. In particular, if $1\\in \\mathcal{H}$, then $\\mu$ is finite and unique.</div><div></div>"
  },
  {
    "front": "Definition upper- and lower semicontinuity of real-valued functions and Dini lemma.",
    "back": "<div><b>Definition.</b> Let $(E,\\rho)$ be a metric space and $f\\colon E\\to [-\\infty,\\infty)$. Then $f$ is called <i>upper semi-continuous</i> if $\\left\\{ f &lt; a \\right\\} $ is open for all $a\\in \\mathbb{R}$. A function $f\\colon E\\to (-\\infty,\\infty]$ is called <i>lower semi-continuous</i> if $(-f)$ is upper semi-continuous, so if $\\left\\{ f &gt; a \\right\\} $ is open for all $a\\in \\mathbb{R}$.<br><br><b>Dini lemma.</b> If $E$ is a compact metric space and $(f_n\\colon E\\to [0,\\infty))$ are upper semi-continuous with $f_n\\downarrow 0$, then $\\left\\|f_n\\right\\|_\\infty \\downarrow 0$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\varepsilon &gt; 0$. Then $E \\subset \\bigcup_{n=1}^\\infty \\left\\{ f_n &lt; \\varepsilon \\right\\} $, so by compactness of $E$ and because it's an increasing union, there is some $N\\in \\mathbb{N}$ such that $E = \\left\\{ f_N &lt; \\varepsilon \\right\\} $, that is, $f_n(x) \\le \\varepsilon$ for all $x\\in E$ and $n\\ge N$, that is, $\\left\\|f_n\\right\\|\\le \\varepsilon$ for all $n\\ge N$.</i> </p> </div><div></div>"
  },
  {
    "front": "Riesz representation theorem on positive linear functionals on [$]C(E,\\mathbb{R})[/$] (resp. [$]C_c(E,\\mathbb{R})[/$] if [$]E[/$] is only locally compact).",
    "back": "<div>Let $E$ be a compact metric space and $I\\colon C(E,\\mathbb{R})\\to \\mathbb{R}$ a positive linear functional. Then there is a unique finite measure $\\mu$ on $(E,\\mathcal{B}(E))$ such that $I(f)= \\mu(f)$ for all $f\\in C(E,\\mathbb{R})$.<br><br><b>Remark.</b> If $E$ is locally compact, and $I\\colon C_c(E,\\mathbb{R})\\to \\mathbb{R}$ is a positive linear functional, then there exists a unique $\\sigma$-finite and regular measure $\\mu$ on $(E,\\mathcal{B}(E))$ with $I(f) = \\mu(f)$ for all $f\\in C_c(E,\\mathbb{R})$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $H:= C(E,\\mathbb{R})$ is a real vector lattice with $1\\in H$ and $\\mathcal{B}(E) = \\sigma(H)$. Have to check that $I$ is a Daniell integration, so let $H\\ni f_n \\downarrow 0$. By Dini's lemma, $f_n \\to 0$ uniformly. Hence if $\\varepsilon &gt; 0$, then for all $n\\ge N$ we have $0\\le f_n \\le \\varepsilon$ and thus \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 \\le I(f_n) \\le \\varepsilon I(1),\\quad n\\ge N.&nbsp;&nbsp;&nbsp; \\] This and $I(f_{n+1}) \\le I(f_n)$ for $n\\in \\mathbb{N}$ imply $I(f_n) \\downarrow 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Tychonoff's embedding theorem for separable metric spaces $(E,\\rho)$. Corollary on existence of countable CDC's $A\\subset C_b(E)$.",
    "back": "<div><b>Theorem (Tychonoff).</b> Let $(E,\\rho)$ be a separable metric space. Then there exists a compact and separable metric space $(\\overline{E},\\rho')$ such that $E\\subset \\overline{E}$ is dense and $\\rho$ and $\\rho'$ are equivalent on $E$. Then \\[&nbsp;&nbsp;&nbsp; C(\\overline{E}) = C_b(\\overline{E}) = U_{\\rho'}(\\overline{E}) = U_{\\rho'}(E)\\] is separable and complete. In particular, there exists a countable convergence determining set $A \\subset C_b(E)$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; No proof of the embedding theorem, only that separability of $\\overline{E}$ follows from separability of $E$. Then $C(\\overline{E})$ is separable, and $U_{\\rho'}(\\overline{E}) \\subset U_{\\rho'}(E)$ by restriction of maps, and $U_{\\rho'}(E)\\subset C(\\overline{E})$ because any bounded, uniformly continuous function on $E$ can be uniquely extended to a continuous function on $\\overline{E}$.<br><br>&nbsp;&nbsp;&nbsp; Now if $P_n,P,\\,n\\in \\mathbb{N}$ are probability measures on $E$, then $P_n\\stackrel{ d }{\\longrightarrow} P$ iff $P_n(f) \\stackrel{  }{\\longrightarrow} P(f)$ for all $f\\in U_{\\rho'}(E)$ iff the same holds for a countable dense subset $A\\subset U_{\\rho'}(E) \\subset C_b(E)$ (because $|P_n(f) - P(f)| \\le 2 \\left\\| f - g\\right\\| + |P_n(g) - P(g)|$).</i> </p> </div><div></div>"
  },
  {
    "front": "$M_1(E)$ for separable $E$ is metrisable (Prohorov metric). When is it Polish?",
    "back": "<div>Let $E$ be a separable metric space, and let $A = \\left\\{ f_k\\colon k\\in \\mathbb{N} \\right\\} \\subset C_b(E)$ be a countable convergence determining class. Then define the <i>Prohorov metric</i> by \\[&nbsp;&nbsp;&nbsp; d(P,Q) := \\sum_{k=1}^\\infty \\frac{1}{2^k} \\left( \\left| P(f_k) - Q(f_k) \\right| \\wedge 1 \\right) ,\\quad P,Q\\in M_1(E).\\] Then $(M_1(E),d)$ is a separable metric space with $d(P_n,P)\\stackrel{  }{\\longrightarrow} 0$ iff $P_n \\stackrel{ d }{\\longrightarrow} P$. If $E$ is Polish, then so is $M_1(E)$ (with the above metric).</div><div></div>"
  },
  {
    "front": "Finite sets of probability measures over Polish spaces are tight. (Direct proof)",
    "back": "<div><b>Theorem.</b> Let $E$ be a Polish space and $\\mathcal{L}\\subset M_1(E)$ finite. Then $\\mathcal{L}$ is tight.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It suffices to show this in the case where $\\mathcal{L} = \\left\\{ P \\right\\} $. Let $\\left\\{ x_i\\colon i\\in \\mathbb{N} \\right\\} $ be a dense subset of $E$. Let $\\varepsilon &gt; 0$. For $n\\in \\mathbb{N}$, we have $E = \\bigcup_{i\\in \\mathbb{N}} \\overline{B}(x_i,1 / n)$, so for some $k(n)\\in \\mathbb{N}$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P\\left( \\bigcup_{i=1} ^{k(n)} \\overline{B}(x_i,1 / n) \\right) &gt; 1 - \\frac{\\varepsilon}{2^n}.&nbsp;&nbsp;&nbsp; \\] Put $K := \\bigcap_{n=1}^\\infty \\bigcup_{i=1} ^{k(n)} \\overline{B}(x_i, 1 / n)$. Then $K$ is closed, and totally bounded by construction, so compact ($E$ is complete). Furthermore, \\[&nbsp;&nbsp;&nbsp; P\\left( K^{c} \\right) \\le \\sum_{n=1}^\\infty \\frac{\\varepsilon}{2^n}= \\varepsilon,\\\\&nbsp;&nbsp;&nbsp; \\] which completes the proof.</i> </p> </div><div></div>"
  },
  {
    "front": "<b>Theorem.</b> If $E$ is a separable metric space and $\\mathcal{L}\\subset M_1(E)$ is tight, then $\\mathcal{L}$ is (sequentially) relatively compact.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Since the claimed statement is topological, we may choose $\\rho$ according to the Tychonoff embedding $E\\subset \\overline{E}$, so $\\overline{E}$ is separable and compact and the closure of $E$, and $\\mathcal{B}(E) = E\\cap \\mathcal{B}(\\overline{E})$. Then $C(\\overline{E}) = U_\\rho(E)$ is separable, choose a dense subset $\\left\\{ \\phi_k\\colon k\\in \\mathbb{N} \\right\\}$ with $\\phi_1 = 1$.<br><br>&nbsp;&nbsp;&nbsp; Let $(P_n)\\in \\mathcal{L}^{\\mathbb{N}}$, and define $Q_n(\\cdot ):= P_n(E\\cap \\cdot )$ for $n\\in \\mathbb{N}$. Then by a diagonal argument, we find a subsequence $(Q_{k(n)})$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Q_{k(n)}(\\phi_k) \\stackrel{ n\\to \\infty }{\\longrightarrow} f_k \\in \\mathbb{R},\\quad k\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp; \\] Say $k(n) = n$ for simplicity. Now $I(\\phi) := \\lim_{n\\to \\infty}Q_n(\\phi)$ for $\\phi$ in the linear span of the $\\phi_k$ defines a linear, bounded, positive operator, that extends uniquely to $I\\colon C(\\overline{E}) \\to \\mathbb{R}$. By Riesz there exists $Q\\in M_F(\\overline{E})$ with $I(\\cdot ) = Q(\\cdot )$. $1 = Q_n(\\phi_1) \\to I(\\phi_1) = Q(E)$, so $Q\\in M_1(\\overline{E})$ and $Q_n \\to Q$ weakly.<br><br>&nbsp;&nbsp;&nbsp; For $m\\in \\mathbb{N}$, we find $K_m\\subset E$ compact with $P_n(K_m) \\ge 1 - 1 / m$ for all $n\\in \\mathbb{N}$. Then $K_m$ is also compact, thus closed and measurable in $\\overline{E}$ (!), so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Q(K_m) \\ge \\limsup_{n\\to \\infty}Q_n(K_m) = \\limsup_{n\\to \\infty}P_n(K_m) \\ge 1 - \\frac{1}{m},\\quad m\\in \\mathbb{N},\\\\&nbsp;&nbsp;&nbsp; \\] so $Q(B) = 1$, where $B := \\bigcup_{m=1} ^\\infty K_m\\subset E$ is in $\\mathcal{B}(E)$ and $\\mathcal{B}(\\overline{E})$.<br><br>&nbsp;&nbsp;&nbsp; Now $P(\\cdot ) := Q(\\cdot \\cap B)$ is a probability measure on $E$ (check that measurability issues work here!), and if $F = \\overline{F}\\cap E$ is closed in $E$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(F) = Q(F \\cap B) = Q(\\overline{F}) \\ge \\limsup_{n\\to \\infty}Q_n(\\overline{F}) = \\limsup_{n\\to \\infty}P_n(F),\\\\&nbsp;&nbsp;&nbsp; \\] so $P_n\\to P$ weakly.</i> </p> </div><div></div>"
  },
  {
    "front": "<b>Theorem.</b> If $E$ is a Polish space and $\\mathcal{L}\\subset M_1(E)$ is (sequentially) relatively compact, then $\\mathcal{L}$ is tight.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp;&nbsp; Let $\\varepsilon&gt;0$, $\\left\\{ x_i\\colon i\\in \\mathbb{N} \\right\\} \\subset E$ dense and $B_i(\\delta) := B(x_i,\\delta)$. Then claim that for any $n\\in \\mathbb{N}$ there exists $k(n)\\in \\mathbb{N}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall P\\in \\mathcal{L}\\colon P\\left( \\bigcup_{i=1} ^{k(n)} B_i(1 / n) \\right) &gt; 1 - \\frac{\\varepsilon}{2^n}.&nbsp;&nbsp;&nbsp; \\] Indeed, suppose that for every $k\\in \\mathbb{N}$ there exists $P_k\\in \\mathcal{L}$ with $P_k(U_k) \\le 1 - \\varepsilon'$. Then by relative compactness, WLOG $P_k \\stackrel{  }{\\longrightarrow} P\\in M_1(E)$ weakly, so \\[&nbsp;&nbsp;&nbsp; P(U_k) \\le \\liminf_{l\\to \\infty} P_l(U_k) \\le \\liminf_{l\\to \\infty}P_l(U_l) \\le 1 - \\varepsilon',\\\\&nbsp;&nbsp;&nbsp; \\] for all $k\\in \\mathbb{N}$, but $\\bigcup_{k=1} ^\\infty U_k = E$, a contradiction.<br><br>&nbsp;&nbsp;&nbsp; Now $G:= \\bigcap_{n=1}^\\infty \\bigcup_{i=1} ^{k(n)}B_i(1&nbsp; / n)$ is totally bounded and thus relatively compact, and $P(G) \\ge 1 - \\varepsilon$ for all $P\\in \\mathcal{L}$.</i> </p> </div><div></div>"
  },
  {
    "front": "Condition on $n,k\\in \\mathbb{N}$ under which $R(k,k) &gt; n$ (with first moment method), and explicit lower bound on $R(k,k)$ as a corollary.",
    "back": "<div><b>Theorem.</b> If $n,k\\in \\mathbb{N}$ are such that $\\binom{n}{k} 2^{1 - \\binom{k}{2}} &lt; 1$, then $R(k,k) &gt; n$. In particular, $R(k,k) \\ge 2^{k / 2}$ for $k\\ge 3$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Colour each edge of $K_n$ red or blue independently with probability $1 / 2$. Then a fixed $K_k$ is monochromatic with probability $2 (1 / 2)^{\\binom{k}{2} }$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\exists \\text{a monochromatic $K_k$} \\right) \\le \\binom{n}{k} 2^{1 - \\binom{k}{2} } &lt; 1.&nbsp;&nbsp;&nbsp; \\] In particular, there exists a good colouring of $K_n$, so $R(k,k) &gt; n$.<br><br>&nbsp;&nbsp;&nbsp; For the corollary, choose $n = \\left\\lfloor 2^{k / 2} \\right\\rfloor $, in which chase \\[&nbsp;&nbsp;&nbsp; \\binom{n}{k} 2^{1 - \\binom{k}{2} } \\le \\frac{n^k}{k!} 2^{1 - \\binom{k}{2} } \\le \\frac{2^{1 + k / 2}}{k!}&lt;1\\\\&nbsp;&nbsp;&nbsp; \\] for all $k\\ge 3$.</i> </p> </div><div></div>"
  },
  {
    "front": "Proof that $R(k,k) &gt; n - \\binom{n}{k} 2^{1 - \\binom{k}{2} }$ for $n,k\\in \\mathbb{N}$ and asymptotic lower bound on $R(k,k)$ as corollary.",
    "back": "<div><b>Theorem.</b> Let $n,k\\in \\mathbb{N}$. Then $R(k,k) &gt; n - \\binom{n}{k} 2^{1 - \\binom{k}{2} }$. In particular, $R(k,k) \\ge (1-o(1)) \\mathrm{e}^{-1}k 2^{k / 2}$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Colour all edges blue and red with probability $1 / 2$, independently. Then if $X$ is the number of monochromatic $K_k$'s, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X \\right] = \\binom{n}{k} 2^{1 - \\binom{k}{2} } =: \\mu.\\] Thus $\\mathbb{P}(X \\le \\mu) &gt; 0$, so there exists a colouring with $m\\le \\mu$ defects. Delete one vertex from each of them to obtain a good colouring of $K_{n-m}$, so $R(k,k) &gt; n - m \\ge n - \\mu$.</div><div></div><div>For the explicit lower bound, take $n = \\left\\lfloor \\mathrm{e}^{-1} k 2^{k / 2}\\right\\rfloor$.</i> </p> </div><div></div>"
  },
  {
    "front": "Let $S\\subset \\mathbb{Z}\\setminus \\left\\{ 0 \\right\\} $. Then there exists a sum-free $A\\subset S$ with $\\left| A \\right| &gt;\\, ?$.",
    "back": "<div><b>Proposition.</b> Let $S\\subset \\mathbb{Z}\\setminus \\left\\{ 0 \\right\\} $. Then there exists a sum-free $A\\subset S$ with $\\left| A \\right| &gt; \\left| S \\right| / 3$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $S = \\left\\{ s_1,\\ldots ,s_n \\right\\} $. Take $p\\in \\mathbb{P}$ of the form $p = 3k + 2$ and $p &gt; 2 \\max_i \\left| s_i \\right| $. Then $I := \\left\\{ k+1,\\ldots ,2k+1 \\right\\} $ is sum-free modulo $p$ (!). Take $r \\sim \\text{Unif}(\\left\\{ 1,\\ldots ,p-1 \\right\\} )$ and put $A =&nbsp; \\left\\{ s\\in S\\colon rs \\in I \\right\\} $, so $A$ is always sum-free. Indeed, if $a+b=c$ in $A$, then $ra + rb = rc$ in $I$, a contradiction. Now \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left| A \\right|&nbsp; \\right] = \\sum_{i=1}^n \\mathbb{P}(rs_i \\in I) = n \\frac{|I|}{p-1} = n \\frac{k+1}{3k+1} &gt; \\frac{n}{3},\\\\&nbsp;&nbsp;&nbsp; \\] so $\\mathbb{P}\\left( \\left| A \\right| &gt; n / 3 \\right) &gt; 0$, so there exists $A\\subset S$ sum-free with $\\left| A \\right| &gt; n / 3$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition colouring of a hypergraph and trivial upper bound on $m(r)$.",
    "back": "<div><b>Definition.</b> A hypergraph $H$ is <i>$k$-colourable</i> if there exists a $k$-colouring such that no edge of $H$ is monochromatic. Then \\[&nbsp;&nbsp;&nbsp; m(r) := \\min \\left\\{ e(H)\\colon&nbsp; \\text{$H$ is an $r$-uniform hypergraph that is not $2$-colourable} \\right\\} .\\] Since the complete $r$-uniform hypergraph on $2r$ vertices is not $2$-colourable, $m(r) \\le \\binom{2r}{r} \\le 4^r$.</div><div></div>"
  },
  {
    "front": "$2$-colourings of hypergraphs: Lower bound on $m(r)$",
    "back": "<div><b>Proposition.</b> $m(r) \\ge 2^{r-1}$ for all $r\\ge 2$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $H$ be an $r$-uniform hypergraph with $e(H) &lt; 2^{r-1}$, and colour every vertex blue and red with probability $1 / 2$ each, independently. Then for $e\\in E(H)$, $\\mathbb{P}(\\text{$e$ is monochromatic}) = 2^{1-r}$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(\\exists \\text{ monochromatic edge}) \\le e(H) 2^{1-r}&lt; 1,\\\\&nbsp;&nbsp;&nbsp; \\] so there exists a good colouring of $H$.</i> </p> </div><div></div>"
  },
  {
    "front": "$2$-colourings of hypergraphs: Strong upper bound on $m(r)$ (for large $r$).",
    "back": "<div><b>Theorem.</b> $m(r) \\le 3r^2 2^r$ for large $r\\in \\mathbb{N}$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $|V| = n :=&nbsp; r^2$ and $H = (V,\\left\\{ e_1,\\ldots ,e_m \\right\\} )$, where $e_i \\sim \\text{Unif}(\\binom{V}{r} )$ i.i.d. Then $H$ is a random $r$-uniform hypergraph with at most $m$ edges. If $c$ is a fixed $2$-colouring of $V$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(\\text{$c$ is proper}) = \\prod_{i=1}^m \\mathbb{P}(\\text{$e_i$ is not $c$-monochromatic}) = (1 - \\mathbb{P}(\\text{$e_1$ is $c$-monochromatic}))^m,\\\\&nbsp;&nbsp;&nbsp; \\] and <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{P}(\\text{$e_1$ is $c$-monochromatic}) &nbsp;&nbsp;&nbsp; &amp;\\ge \\binom{\\left\\lfloor \\frac{n}{2} \\right\\rfloor }{r} / \\binom{n}{r} = \\frac{n / 2}{n} \\frac{n / 2 - 1}{n - 1} \\cdots \\frac{n / 2 - r + 1}{n - r + 1} \\\\&nbsp;&nbsp;&nbsp; &amp;\\ge \\left( \\frac{n / 2 - r}{n - r} \\right) ^r = 2^{-r} \\left( 1 - \\frac{r}{n-r} \\right) ^r\\\\&nbsp;&nbsp;&nbsp; &amp;= 2^{-r} \\left( 1 - \\frac{1}{r-1} \\right) ^r \\ge \\frac{1}{3 \\cdot 2^r} =: p_0\\\\\\end{align*}[/$$] for large $r$. Thus \\[\\mathbb{P}\\left( \\text{$H$ is $2$-colourable} \\right) \\le 2^n (1-p_0)^m \\le 2^{n} \\mathrm{e}^{-m p_0} = 2^{r^2} \\mathrm{e}^{-r^2}&lt; 1,\\\\&nbsp;&nbsp;&nbsp; \\] so there exists an $r$-uniform hypergraph with at most $m$ edges which is not $2$-colourable, so $m(r) \\le m$.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of tightness in $C([0,\\infty),\\mathbb{R}^d)$, special case of sequences.",
    "back": "<div><b>Theorem.</b> Let $\\mathcal{L}\\subset M_1(C(\\mathbb{R}^d))$. Then $\\mathcal{L}$ is tight if and only if<br><ol>  <li>$\\sup_{P\\in \\mathcal{L}} P\\left[ x\\in C\\colon \\left| x(0) \\right| \\ge L \\right] \\stackrel{  }{\\longrightarrow} 0 $ as $L \\to \\infty$ (that is, $(P^{\\pi_0})_{P\\in \\mathcal{L}}$ is tight),&nbsp;&nbsp;&nbsp;</li>  <li>For all $T &gt; 0$ and $\\lambda &gt; 0$, $\\sup_{P\\in \\mathcal{L}} \\mathbb{P}\\left[ x\\in C\\colon w_T(x,\\delta) \\ge \\lambda \\right] \\stackrel{  }{\\longrightarrow} 0$ as $\\delta \\to 0$.</li></ol><br>For a sequence $(X_n)$ of $C$-valued stochastic processes, this means that $(X_n(0))$ is tight and $\\limsup_{n\\to \\infty} \\mathbb{P}\\left( w_T(X_n,\\delta) \\ge \\lambda \\right) \\to 0$ as $\\delta \\to 0$ for all $T,\\lambda &gt; 0$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\textbf{\\enquote{$\\implies$}:} Let $\\varepsilon&gt;0$ and choose $K_\\varepsilon \\subset C$ compact with $P(K_\\varepsilon) \\ge 1 - \\varepsilon$ for all $P\\in \\mathcal{L}$. Then there exists $\\alpha &gt; 0$ such that $\\sup_{x\\in K_\\varepsilon} \\left| x(0) \\right| &lt; \\alpha$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P\\left[ x\\in C\\colon \\left| x(0) \\right| \\le L \\right] \\ge P\\left( K_\\varepsilon&nbsp; \\right) \\ge 1 - \\varepsilon\\\\&nbsp;&nbsp;&nbsp; \\] for all $L &gt; \\alpha$ and $P\\in \\mathcal{L}$. Now if $T,\\lambda &gt; 0$, there exists $\\delta_0&gt; 0$ such that $\\sup_{x\\in K_\\varepsilon} w_T(x,\\delta) &lt; \\lambda$ for all $0&lt;\\delta&lt;\\delta_0$, so that \\[&nbsp;&nbsp;&nbsp; P\\left[ x\\in C\\colon w_T(x,\\delta) &lt; \\lambda \\right] \\ge P(K_\\varepsilon) \\ge 1 - \\varepsilon\\\\&nbsp;&nbsp;&nbsp; \\] for all such $\\delta$ and $P\\in \\mathcal{L}$.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\\enquote{$\\impliedby$}:} Let $\\varepsilon &gt; 0$. Then choose $L &gt; 0$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(B) = P\\left[ x\\in C\\colon \\left| x(0) \\right| \\le L \\right] \\ge 1 - \\frac{\\varepsilon}{2},\\\\&nbsp;&nbsp;&nbsp; \\] and, for $n\\in \\mathbb{N}$, $\\delta_n &gt; 0$ with \\[&nbsp;&nbsp;&nbsp; P(B_n) = P\\left[ x\\in C\\colon&nbsp; w_n(x,\\delta) \\le \\frac{1}{n}\\right] \\ge 1 - \\frac{\\varepsilon}{2^{n+1}}\\] for all $0&lt;\\delta&lt;\\delta_n$ and $P\\in \\mathcal{L}$. Then $K_\\varepsilon := B \\cap \\bigcap_{n=1}^\\infty B_n$ is relatively compact by Arzela-Ascoli and closed, and&nbsp; \\[P(K^{c}) \\le \\sum_{n=1}^\\infty \\frac{\\varepsilon}{2^{n+1}} + \\frac{\\varepsilon}{2} = \\varepsilon\\\\\\] for all $P\\in \\mathcal{L}$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of entropy number $\\mathcal{N}(X,\\varepsilon,T)$ (and $\\delta(X,\\varepsilon,T)$, and stopping times $(T_j)$) for continuous process $(X_t)$. Upper bound on $\\mathbb{P}\\left( w_T(X,\\delta) &gt; \\varepsilon \\right) $ in terms of $\\delta(X,\\varepsilon,T)$?",
    "back": "<div><b>Definition.</b> Let $(X_t)$ be a $C(\\mathbb{R}^d)$-valued stochastic process. Fix $\\varepsilon &gt; 0$. Define stopping times $T_0 := 0$ and \\[&nbsp;&nbsp;&nbsp; T_{j+1}:= \\inf \\left\\{ t &gt; T_j\\colon \\left| X(t) - X(T_j) \\right| \\ge \\frac{\\varepsilon}{4} \\right\\} , \\quad j\\in \\mathbb{N}_0.\\] For $T &gt; 0$, define <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathcal{N}(X,\\varepsilon,T) &amp;:= \\sup \\left\\{ j\\in \\mathbb{N}\\colon T_j \\le T \\right\\} &lt; \\infty, \\\\&nbsp;&nbsp;&nbsp; \\delta(X,\\varepsilon,T) &amp;:= \\min \\left\\{ T_j - T_{j-1}\\colon 1\\le j \\le \\mathcal{N}(X,\\varepsilon,T) \\right\\} &gt; 0 ,\\\\\\end{align*}[/$$] called the <i>entropy number</i> and the <i>minimal gap</i>, respectively.<br><br><b>Lemma.</b> In this situation, we have $w_T(X,\\delta(X,\\varepsilon,T)) \\le \\varepsilon$ and thus \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( w_T(X,\\delta) &gt; \\varepsilon&nbsp; \\right) \\le \\mathbb{P}\\left( \\delta(X,\\varepsilon,T) &lt; \\delta \\right) \\] for all $\\delta &gt; 0$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $t,s\\le T$ and $\\left| t-s \\right| &lt; \\delta(X,\\varepsilon,T)$, then at most one $T_j$ lies between $s$ and $t$, and the variation on each interval $[T_{j-1},T_j]$ is bounded by $\\varepsilon / 2$. In particular, $\\delta \\le \\delta(X,\\varepsilon,T)$ implies $w_T(X,\\delta) \\le w_T(X,\\delta(X,\\varepsilon,T)) \\le \\varepsilon$.</i> </p> </div><div></div>"
  },
  {
    "front": "Entropy: Bounds on $\\mathbb{E} \\left[ \\boldsymbol{1}_{\\left\\{ T_j - T_{j-1} \\le \\delta, T_{j-1}&lt;\\infty \\right\\} } \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}}\\right]$ and $\\mathbb{P}\\left( \\mathcal{N}(X,\\varepsilon,T)\\ge k \\right) $ for continuous semimartingale $X$ (under suitable assumption).",
    "back": "<div><b>Lemma.</b> Suppose $X = X_0 + M + A$ is a $d$-dimensional continuous semimartingale such that there exists $C &gt; 0$ with \\[&nbsp;&nbsp;&nbsp; \\left| \\mathop{}\\!\\mathrm{d} A_t \\right| \\le C\\mathop{}\\!\\mathrm{d} t,\\quad \\sum_{i,j}\\left| \\mathop{}\\!\\mathrm{d} \\left&lt;M^i,M^j \\right&gt;&nbsp;&nbsp; \\right| \\le C \\mathop{}\\!\\mathrm{d} t.\\] Then for any $\\varepsilon &gt; 0$, there exist constants $K &gt; 0$ and $\\alpha \\in (0,1)$, depending only on $\\varepsilon$ and $C$, such that the following hold.<br><ol>  <li>If $T_0 = 0, T_j = \\inf \\left\\{ t &gt; T_{j-1}\\colon \\left| X(t) - X(T_{j-1}) \\right| \\ge \\varepsilon / 4 \\right\\} $, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(T_j - T_{j-1} \\le \\delta, T_{j-1} &lt; \\infty \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}}\\right) \\le K \\delta\\\\&nbsp;&nbsp;&nbsp; \\] for all $\\delta &gt; 0$.</li>  <li>&nbsp; $\\mathbb{P} \\left( \\mathcal{N}(X,\\varepsilon,T) \\ge k \\right) \\le \\mathrm{e}^T \\alpha ^k$ for any $T &gt; 0$ and $k\\in \\mathbb{N}_0$.</li></ol><br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $f \\in C_c^\\infty(\\mathbb{R}^d,\\mathbb{R})$ non-negative with $f(x) = 1$ if $\\left| x \\right| = \\varepsilon / 4$ and $f(0) = 0$. Applying Ito to $(X_{T_{j-1} + t} - X_{T_{j-1}})_{t\\ge 0}$ on the probability space $\\left\\{ T_{j-1}&lt; \\infty \\right\\} $ yields&nbsp;&nbsp;&nbsp; \\begin{equation*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(X_{T_j \\wedge (T_{j-1} + \\delta)} - X_{T_{j-1}})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\int_{T_{j-1}}^{T_j \\wedge (T_{j-1} + \\delta)} \\nabla f \\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2} \\sum_{i,j} \\int \\nabla ^2 f \\mathop{}\\!\\mathrm{d} \\left&lt;M^i,M^j \\right&gt; _s&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{equation*} on $\\left\\{ T_{j-1} &lt; \\infty \\right\\} $, and so (since $M$ is a true martingale), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[&nbsp; \\boldsymbol{1}_{\\left\\{T_j - T_{j-1}&lt; \\delta, T_{j-1}&lt; \\infty\\right\\}} \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}} \\right] \\le \\mathbb{E} \\left[f(X_{T_j \\wedge (T_{j-1} + \\delta)} - X_{T_{j-1}}) \\boldsymbol{1}_{\\left\\{T_{j-1}&lt; \\infty\\right\\}} \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}} \\right] \\le K \\delta,\\\\&nbsp;&nbsp;&nbsp; \\] with $K$ depending on $C$ and $f$.</li>  <li>For $\\beta &gt; 0$ and $j\\in \\mathbb{N}$ we have&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[\\mathrm{e}^{-(T_{j}-T_{j-1})} \\boldsymbol{1}_{\\left\\{ T_{j-1}&lt; \\infty \\right\\} } \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}}\\right]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathrm{e}^{-\\beta} + (1-\\mathrm{e}^{-\\beta}) \\mathbb{P} \\left(T_{j} - T_{j-1} \\le \\beta, T_{j-1}&lt;\\infty \\,\\middle\\vert\\,\\mathcal{F}_{T_{j-1}} \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathrm{e}^{-\\beta} + (1-\\mathrm{e}^{-\\beta})K\\beta =: \\alpha &lt; 1\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] if $ 0 &lt; K\\beta &lt; 1 / 2$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[\\mathrm{e}^{-T_j} \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}}\\right] = \\mathbb{E} \\left[\\mathrm{e}^{-(T_j - T_{j-1})}\\mathrm{e}^{T_j} \\boldsymbol{1}_{\\left\\{ T_{j-1} &lt; \\infty \\right\\} } \\,\\middle\\vert\\, \\mathcal{F}_{T_{j-1}}\\right] \\le \\alpha \\mathrm{e}^{-T_{j-1}}&nbsp;&nbsp;&nbsp; \\] for all $j\\in \\mathbb{N}$, so $\\mathbb{E} \\left[ \\mathrm{e}^{-T_j} \\right] \\le \\alpha^j, \\, j\\in \\mathbb{N}_0$. We conclude \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\mathcal{N}(X,\\varepsilon,T) \\ge k \\right) = \\mathbb{P}\\left( T_k \\le T \\right) \\le \\mathbb{E} \\left[ \\mathrm{e}^{T - T_k} \\right] \\le \\mathrm{e}^T \\alpha^k\\\\&nbsp;&nbsp;&nbsp; \\] for $k\\in \\mathbb{N}$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Tightness criterion for sequence $(X^{(n)})$ of continuous semimartingales.",
    "back": "<div><b>Theorem.</b> Let $(X^{(n)})$ be a sequence of $d$-dimensional continuous semimartingales such that $\\limsup_{n\\to \\infty} \\mathbb{P}\\left( \\left| X^{(n)}_0 \\right| \\ge L \\right) \\to 0$ as $L\\to \\infty$ and&nbsp; \\[&nbsp;&nbsp;&nbsp; \\left| \\mathop{}\\!\\mathrm{d} A^{(n)}_t \\right| \\le C \\mathop{}\\!\\mathrm{d} t, \\quad \\sum_{i,j}\\left| \\mathop{}\\!\\mathrm{d} \\left&lt;M^{(n)}_i, M^{(n)}_j \\right&gt;_t&nbsp; \\right|&nbsp; \\le C\\mathop{}\\!\\mathrm{d} t,\\qquad n\\in \\mathbb{N},\\\\\\] for some constant $C &gt; 0$. Then $(X^{(n)})$ is tight.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $T &gt; 0$, $\\varepsilon&gt; 0$, we have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( w_T(X^{(n)},\\delta) &gt; \\varepsilon \\right)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{P}\\left( \\delta(X^{(n)},\\varepsilon,T) &lt; \\delta \\right)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{P}\\left( \\mathcal{N}(X^{(n)},\\varepsilon,T) &gt; k \\right) + \\mathbb{P}\\left( \\inf_{1\\le j\\le k} (T_j - T_{j-1}) &lt; \\delta \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathrm{e}^T \\alpha^k + k K \\delta\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] Thus \\[&nbsp;&nbsp;&nbsp; \\limsup_{\\delta \\to 0} \\sup_{n\\in \\mathbb{N}} \\mathbb{P}\\left( w_T(X^{(n)},\\delta) \\ge \\varepsilon \\right) \\le \\mathrm{e}^T \\alpha^k \\downarrow&nbsp; 0 \\quad (k\\to \\infty)&nbsp;&nbsp;&nbsp; \\] for all $k\\in \\mathbb{N}$, which finishes the proof.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of $\\mathcal{P}_p(E)$ and Wasserstein metric $W_p$ for Polish space $E$, and characterisation of convergence in $W_p$ (without proof).",
    "back": "<div><b>Definition.</b> Let $(E,\\rho)$ be a Polish space and $p\\ge 1$, and put \\[&nbsp;&nbsp;&nbsp; \\mathcal{P}_p(E) := \\left\\{ P\\in M_1(E)\\colon \\int_E \\rho(x,y)^p P(\\mathop{}\\!\\mathrm{d} x) &lt; \\infty \\text{ for all (any) $y\\in E$} \\right\\} .\\] If $P,Q\\in M_1(E)$, let $\\Pi(P,Q) \\subset M_1(E\\times E)$ be the set of couplings of $P$ and $Q$. Then the Wasserstein metric on $\\mathcal{P}_p(E)$ is defined by \\[W_p(P,Q) := \\left[ \\inf_{\\Theta \\in \\Pi(P,Q)} \\int_{E\\times E} \\rho(x,y)^p \\Theta(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y) \\right] ^{1 / p},\\quad P,Q\\in \\mathcal{P}_p(E).\\] <br><br><b>Fact.</b> If $P_n,P\\in \\mathcal{P}_p(E),\\, n\\in \\mathbb{N},$ then $W(P_n,P) \\to 0$ if and only if $P_n(f) \\to P(f)$ for all $f\\in C(E)$ with \\[&nbsp;&nbsp;&nbsp; \\left| f(x)&nbsp; \\right| \\le C \\left( 1 + \\rho(x,y)^p \\right) ,\\quad x\\in E,\\\\\\] for some constant $C &gt; 0$ and for any (thus all) $y\\in E$. In particular, $P_n \\to P$ weakly.</div><div></div>"
  },
  {
    "front": "Dual representation of Wasserstein metric $W_1$ using Lipschitz functions (without proof).",
    "back": "<div><b>Proposition.</b> If $(E,\\rho)$ is a Polish space, then \\[&nbsp;&nbsp;&nbsp; W_1(P,Q) = \\sup_{\\substack{f\\in \\text{Lip}(E)\\\\ \\left\\|f\\right\\|_L \\le 1}} \\left( \\int_E f \\mathop{}\\!\\mathrm{d} P - \\int_E f \\mathop{}\\!\\mathrm{d} Q \\right) ,\\quad P,Q\\in \\mathcal{P}_1(E),\\\\\\] where $\\left\\|f\\right\\|_L = \\inf \\left\\{ L &gt; 0\\colon \\left| f(x) - f(y) \\right| \\le L\\rho(x,y) \\forall x,y\\in E \\right\\} $ for a Lipschitz function $f\\in C(E)$.</div><div></div>"
  },
  {
    "front": "Four equivalent conditions under which weak convergence implies $W_p$-convergence. Thus briefly put, what is the connection between weak and $W_p$-convergence?",
    "back": "<div><b>Lemma.</b> Suppose $(E,\\rho)$ is a Polish space, $p\\ge 1$ and $P_n,P\\in \\mathcal{P}_p(E),\\,n\\in \\mathbb{N},$ and $P_n\\to P$ weakly. Then the following are equivalent.<br><ol>  <li>For some (all) $y\\in E$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_E \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x) \\stackrel{  }{\\longrightarrow}&nbsp; \\int_E \\rho(x,y)^p P(\\mathop{}\\!\\mathrm{d} x),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>For some (all) $y\\in E$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty}\\int_E \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x) \\le \\int_E \\rho(x,y)^p P(\\mathop{}\\!\\mathrm{d} x),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>For some (all) $y\\in E$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\int\\limits_{\\left\\{ y\\colon \\rho(x,y) &gt; R \\right\\} } \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x) \\stackrel{  }{\\longrightarrow}&nbsp; 0 \\quad (R\\to \\infty),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>$W_p(P_n,P) \\stackrel{  }{\\longrightarrow} 0$.</li></ol><br>Thus, convergence in $W_p$ is equivalent to weak convergence plus convergence of $p$'th moments.<br><br><p><i><b>Proof.</b>&nbsp; &nbsp; \\textbf{\\underline{(i)$\\iff$(ii):}} We have $\\rho(\\cdot ,y)^p \\in C(E)_+$, so $\\liminf_{n\\to \\infty} \\int \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x) \\ge \\int \\rho(x,y)^p P(\\mathop{}\\!\\mathrm{d} x)$ holds by $P_n \\stackrel{ d }{\\longrightarrow} P$.<br><br>\\textbf{\\underline{(ii)$\\iff$(iii):}} We have $1 = \\boldsymbol{1}_{\\left\\{x\\colon \\rho(x,y) \\le R\\right\\}} + \\boldsymbol{1}_{\\left\\{ x\\colon \\rho(x,y) &gt; R \\right\\} }$, so by $P_n \\stackrel{ d }{\\longrightarrow} P$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\int \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x) = \\int\\limits_{\\left\\{ \\rho(x,y)\\le R \\right\\} }\\rho(x,y)^p P(\\mathop{}\\!\\mathrm{d} x) + \\limsup_{n\\to \\infty} \\int\\limits_{\\left\\{ \\rho(x,y) &gt; R \\right\\} }\\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\\underline{$\\exists \\implies \\forall $ in (iii):}} Suppose (iii) holds for $y\\in E$. Let $z\\in E$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int\\limits_{\\left\\{\\rho(x,z) &gt; R\\right\\} } \\rho(x,z)^p P_n(\\mathop{}\\!\\mathrm{d} x) \\le K \\rho(y,z)^p P_n(\\rho(x,z) &gt; R) + K \\int\\limits_{\\left\\{\\rho(x,y) &gt; R - \\rho(x,z)\\right\\} } \\rho(x,y)^p P_n(\\mathop{}\\!\\mathrm{d} x),\\\\&nbsp;&nbsp;&nbsp; \\] for some $K &gt; 0$, and $\\limsup_{n\\to \\infty} \\text{RHS} \\to 0$ as $R\\to \\infty$.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\\underline{(iii)$\\implies$(iv):}} Let $f\\in C(E)$ with $\\left| f(x) \\right| \\le C(1 + \\rho(x,y)^p)$ for some $C &gt; 0$ and $y\\in E$, WLOG $f\\ge 0$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int f \\mathop{}\\!\\mathrm{d} P_n \\le \\int\\limits_{\\left\\{ \\rho(x,y) \\le R \\right\\} } f(x) P_n(\\mathop{}\\!\\mathrm{d} x) + C\\int\\limits_{\\left\\{ \\rho(x,y) &gt; R \\right\\} } ( 1 + \\rho(x,y)^p) P_n(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp; \\] and $\\limsup_{n\\to \\infty} \\text{RHS} \\stackrel{  }{\\longrightarrow} \\int f \\mathop{}\\!\\mathrm{d} P + 0$ as $R \\to \\infty$, so $\\limsup_{n\\to \\infty} \\int f \\mathop{}\\!\\mathrm{d} P_n \\le \\int f \\mathop{}\\!\\mathrm{d} P$, and $\\liminf_{n\\to \\infty} \\int f \\mathop{}\\!\\mathrm{d} P_n \\ge \\int f \\mathop{}\\!\\mathrm{d} P$ follows from weak convergence.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\\underline{(iv)$\\implies$(i):}} Clear.</i> </p> </div><div></div>"
  },
  {
    "front": "Criterion for tightness of sequences $(P_n)\\in \\mathcal{P}_1(E)^\\mathbb{N}$ involving the Wasserstein metric.",
    "back": "<div><b>Theorem.</b> Let $(E,\\rho)$ be a Polish space. Then any $W_1$-Cauchy sequence $(P_n)\\in \\mathcal{P}_1(E)^{\\mathbb{N}}$ is tight.</div><div><br><div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\varepsilon &gt; 0$. For $l\\in \\mathbb{N}$, there exists $n_l \\in \\mathbb{N}$ such that $W_1(P_n,P_m) \\le \\frac{\\varepsilon}{l 2^{l+1}}$ for all $n,m\\ge n_l$. Let $K_l \\subset E$ be compact with $P_j(K_l) \\ge 1 - \\varepsilon / 2^{l+1}$ for $j\\le n_l$. $K_l$ is totally bounded, so let $G_l$ be a cover of $K_l$ with $1 / l$-balls, and let $U_l = \\left\\{ \\rho(\\cdot ,G_l) &lt; \\frac{1}{l} \\right\\} $. Define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\phi_l := 0 \\vee \\left( 1 - l\\rho(\\cdot ,G_l) \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\] so that $\\left\\|\\phi_l\\right\\|_L \\le l$ and $ \\boldsymbol{1}_{G_l}\\le \\phi_l \\le \\boldsymbol{1}_{U_l}$. Then for $n &gt; n_l$,<br>&nbsp;&nbsp;&nbsp; \\begin{equation*}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P_n(U_l) \\ge \\int \\phi_l \\mathop{}\\!\\mathrm{d} P_n \\ge \\int \\phi_l \\mathop{}\\!\\mathrm{d} P_{n_l} - \\left\\|\\phi_l\\right\\|_L W_1(P_n,P_{n_l}) \\ge P_{n_l}(G_l) - l \\frac{\\varepsilon}{l 2^{l+1}} \\ge 1 - \\frac{\\varepsilon}{2^{l}},<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{equation*} and the same holds for $n\\le n_l$ by assumption, so with $A := \\bigcap_{l=1}^\\infty U_l$ totally bounded ($U_l$ is $2 / l$-coverable), so relatively compact, we have \\[&nbsp;&nbsp;&nbsp; P_n(A^{c}) \\le \\sum_{l=1}^\\infty \\frac{\\varepsilon}{2^{l}} = \\varepsilon,\\quad n\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div></div>"
  },
  {
    "front": "If $P_n \\stackrel{ d }{\\longrightarrow} P$ on a metric space $S$ and $f\\in C(S)_+$, then $\\int f \\mathop{}\\!\\mathrm{d} P \\le \\liminf \\int f \\mathop{}\\!\\mathrm{d} P_n$.",
    "back": "<div><b>Proposition.</b> Let $S$ be a metric space and $P_n \\stackrel{ d }{\\longrightarrow} P$ on $S$. Then for all non-negative $f\\in C(S)$ we have \\[\\int f \\mathop{}\\!\\mathrm{d} P \\le \\liminf_{n\\to \\infty} \\int f\\mathop{}\\!\\mathrm{d} P_n.\\]<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For any $R &gt; 0$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int (f\\wedge R) \\mathop{}\\!\\mathrm{d} P = \\lim_{n\\to \\infty} \\int (f\\wedge R) \\mathop{}\\!\\mathrm{d} P_n \\le \\liminf_{n\\to \\infty} \\int f \\mathop{}\\!\\mathrm{d} P_n,\\\\&nbsp;&nbsp;&nbsp; \\] and LHS converges to $\\int f \\mathop{}\\!\\mathrm{d} P$ as $R \\to \\infty$ by MCT.</i> </p> </div><div></div>"
  },
  {
    "front": "Expected number of $r$-cycles in $\\mathcal{G}(n,p)$.",
    "back": "\"<div><b>Claim.</b> $\\displaystyle \\mathbb{E} \\left[ \\#\\text{cycles} \\right] = \\frac{n!}{2r (n-r)!}p^r$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The number of $r$-tuples of vertices with no duplicates is $r! \\binom{n}{r} $, and every cycle corresponds to exactly $2r$ such tuples (freedom in direction and starting vertex), so number of cycles is that number divided by $2r$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\#\\text{cycles} \\right] = \\sum_{\\text{cycles } C} \\mathbb{P}\\left( C \\subset E(G) \\right) = \\frac{n!}{2r(n-r)!} p^r.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>\""
  },
  {
    "front": "There exist graphs with arbitrarily large girth and chromatic number.",
    "back": "<div><b>Theorem.</b> Let $k,l\\ge 1$. Then there exists a graph $G$ with $g(G) \\ge l$ and $\\chi(G) \\ge k$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $k,l\\ge 3$. For $n\\in \\mathbb{N}$ put $p = p(n) = n ^{1 / l - 1} \\in [0,1]$. Then let $X$ be the number of cycles in $G\\sim \\mathcal{G}(n,p)$ of length less than $l$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X \\right] = \\sum_{r=3}^{l-1} \\frac{n(n-1)\\cdot \\ldots \\cdot (n-r+1)}{2r} p^r \\le \\sum_{r=3}^{l-1} (np)^r = O((np)^{l-1}) = O(n^{1 - 1/l}) = o(n),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $\\mathbb{P}\\left( X \\ge n / 2 \\right) \\le 2 \\mathbb{E} \\left[ X&nbsp; \\right] / n \\to 0$. Now let $m = m(n) = \\left\\lfloor&nbsp; n^{1 - 1 / (2l)} \\right\\rfloor$ and $Y$ the number of independent sets of size $m$ in $G$, so \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ Y \\right] = \\binom{n}{m} (1-p)^{\\binom{m}{2} } \\le \\left( \\frac{\\mathrm{e} n}{m} \\right) ^m \\mathrm{e}^{-p \\binom{m}{2} } = \\left( \\frac{\\mathrm{e} n}{m} \\mathrm{e}^{-p(m-1) / 2} \\right) ^m \\stackrel{  }{\\longrightarrow} 0\\\\\\] because $mp \\sim n^{1 / l - 1} n^{1 - 1 / (2l)} = n^{1 / (2l)}$. Thus $\\mathbb{P}(Y\\ge 1) = \\mathbb{P}(\\alpha(G) \\ge m) \\to 0$, so \\[\\mathbb{P}(\\alpha(G) &lt; m \\text{ and } X &lt; n / 2)&nbsp; \\stackrel{  }{\\longrightarrow} 1,\\\\\\] so there exists $G$ with arbitrarily large $n$, fewer than $n / 2$ short cycles and $\\alpha(G) &lt; m$. Delete one vertex from each short cycle to obtain $G'$ with no short cycles, so $g(G') \\ge l$, and \\[\\chi(G') \\ge \\frac{\\left| G' \\right| }{\\alpha(G')} \\ge \\frac{n}{2 \\alpha(G)} \\ge \\frac{n^{1 / (2l)}}{2} &gt; k.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Lower bound on $\\mathbb{P}(X\\neq 0)$ involving $\\mathbb{E} \\left[ X \\right] $ and $\\mathbb{E} \\left[ X^2 \\right] $ for real random variable $X$.",
    "back": "<div><b>Lemma.</b> If $X$ is a real random variable, then&nbsp; \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(X \\neq 0) \\ge \\frac{\\mathbb{E} \\left[ X \\right]^2 }{\\mathbb{E} \\left[ X^2 \\right]},\\\\\\] that is, $\\mathbb{E} \\left[ X \\right] ^2 \\le \\mathbb{P}(X \\neq 0) \\mathbb{E} \\left[ X^2 \\right] $.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\mathbb{E} \\left[ X \\right]^2 = \\mathbb{P}(X\\neq 0)^2 \\mathbb{E} \\left[X \\,\\middle\\vert\\, X\\neq 0\\right]^2 $ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[X \\,\\middle\\vert\\, X\\neq 0\\right]^2 \\le \\mathbb{E} \\left[X^2 \\,\\middle\\vert\\, X\\neq 0\\right] = \\frac{\\mathbb{E} \\left[ X^2 \\,\\boldsymbol{1}\\!\\left\\{X\\neq 0\\right\\} \\right] }{\\mathbb{P}(X\\neq 0)} = \\frac{\\mathbb{E} \\left[ X^2 \\right] }{\\mathbb{P}(X\\neq 0)}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Condition under which $\\mathbb{P}(X_n &gt; 0) \\stackrel{  }{\\longrightarrow} 1$ for counting random variables $(X_n)$ (involving only variance and expectation).",
    "back": "<div><b>Lemma.</b> If $(X_n)$ are counting random variables with $\\mathbb{V}(X_n) = o(\\mu_n^2)$, then $\\mathbb{P}(X_n = 0) \\stackrel{  }{\\longrightarrow} 0$. In fact, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( (1-\\varepsilon)\\mu_n &lt; X_n &lt; (1+\\varepsilon) \\mu_n \\right) \\stackrel{  }{\\longrightarrow} 1,\\quad n\\to \\infty,\\\\\\] for all $\\varepsilon &gt; 0$. An equivalent condition is $\\mathbb{E} \\left[ X_n^2 \\right] \\sim \\mu_n^2$ (that is, $\\mathbb{E} \\left[ X_n^2 \\right] = (1+o(1)) \\mu_n^2$).<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Cheybysheff, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| X_n - \\mu_n \\right| \\ge \\varepsilon \\mu_n \\right) \\le \\frac{\\mathbb{V}(X_n)}{\\varepsilon^2 \\mu_n^2} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; \\] Equivalence of the conditions follows from $\\mathbb{V}(X_n) = \\mathbb{E} \\left[ X_n^2 \\right]&nbsp; - \\mu_n^2 \\in o(\\mu_n^2)$ iff $\\mathbb{V}(X_n) / \\mu_n^2 = \\mathbb{E} \\left[ X_n^2 \\right] / \\mu_n^2 - 1 \\stackrel{  }{\\longrightarrow} 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of threshold function (for $\\mathcal{G}(n,p)$).",
    "back": "<div>If $\\mathcal{P}$ is a graph property, then $p^*(n)$ is called a <i>threshold function for $\\mathcal{P}$ (in the model $\\mathcal{G}(n,p)$)</i> if<br><ol>  <li>$p(n) / p^*(n) \\to 0$ implies $\\mathbb{P}(\\text{$\\mathcal{G}(n,p(n))$ has property $\\mathcal{P}$}) \\to 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$p(n) / p^*(n) \\to \\infty$ implies $\\mathbb{P}(\\text{$\\mathcal{G}(n,p(n))$ has property $\\mathcal{P}$}) \\to 1$.</li></ol></div><div></div>"
  },
  {
    "front": "Threshold function for $\\mathcal{G}(n,p)$ to contain a $K_4$.",
    "back": "<div><b>Proposition.</b> $p^*(n) = n^{-2 / 3}$ is a threshold function for $\\mathcal{G}(n,p)$ to contain a $K_4$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $X$ be the number of $K_4$'s in $G \\sim \\mathcal{G}(n,p)$, so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu = \\binom{n}{4} p^6 \\sim \\frac{n^4}{4!} p^6 = \\frac{(n^{2 / 3}p)^6}{4!},\\\\&nbsp;&nbsp;&nbsp; \\] which already finishes the first part by Markov. Now if $n^{2 / 3}p \\to \\infty$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X^2 \\right] &amp;= \\sum_{S,T \\in \\binom{V}{4} } \\mathbb{P}(S,T\\subset E(G)) = \\sum_{k=0}^4 \\sum_{\\left| S\\cap T \\right| = k} \\mathbb{P}(S,T\\subset E(G))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\underbrace{\\mu + \\sum_{k=1}^3(\\ldots )}_{= o(\\mu^2)} + \\underbrace{\\binom{n}{4} \\binom{n-4}{4} p^{12}}_{\\sim\\mu^2} \\sim \\mu^2,\\\\\\end{align*}[/$$] so $\\mathbb{P}(X &gt; 0) \\stackrel{  }{\\longrightarrow} 1$.</i> </p> </div><div></div>"
  },
  {
    "front": "Condition on counting random variables $X = \\sum_{i} \\boldsymbol{1}_{A_i}$ to satisfy $\\mathbb{V}(X) = o(\\mu^2)$, and thus $\\mathbb{P}(X &gt; 0) \\to 1$.",
    "back": "<div><b>Lemma.</b> If $\\mu \\to \\infty$ and $\\Delta := \\sum_i \\sum_{j\\sim i}\\mathbb{P}(A_i\\cap A_j) = o(\\mu^2)$, then $\\mathbb{V}(X) = o(\\mu^2)$, in particular $\\mathbb{P}(X &gt; 0) \\stackrel{  }{\\longrightarrow} 1$. Here $i\\sim j \\iff i\\neq j$ and $A_i$ and $A_j$ are independent.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{V}(X) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{i,j} \\operatorname{Cov} \\left( \\boldsymbol{1}_{A_i}, \\boldsymbol{1}_{A_j} \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_i \\left( \\mathbb{P}(A_i) - \\mathbb{P}(A_i)^2 \\right) + \\sum_i \\sum_{j\\sim i} \\left( \\mathbb{P}(A_i \\cap A_j) - \\mathbb{P}(A_i) \\mathbb{P}(A_j) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\sum_i \\mathbb{P}(A_i) + \\Delta = \\mu + \\Delta.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Thus, $\\mathbb{V}(X) / \\mu^2 = 1 / \\mu + \\Delta / \\mu^2 \\to 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition ((uniformly) elliptic) second order differential operator and maximum principle.",
    "back": "<div><b>Definition.</b> A <i>second order differential operator</i> is of the form \\[&nbsp;&nbsp;&nbsp; L u = \\sum_{i,j=1}^d a_{ij}(\\cdot ) \\frac{\\partial^2 u}{\\partial x_i \\partial x_j} + \\sum_{i=1}^d b_i(\\cdot ) \\frac{\\partial u}{\\partial x_i} - c(\\cdot ) u.\\] It is called <i>elliptic</i> if $A(x) := \\left( a_{ij}(x) \\right) _{i,j=1}^d$ is positive definite for all $x\\in \\mathbb{R}^d$, so, putting $\\mu(x) := \\min_{\\left\\|z\\right\\|=1}z^\\top A(x) z &gt; 0$, \\[&nbsp;&nbsp;&nbsp; z^\\top A(x) z \\ge \\mu(x) \\left\\|z\\right\\|^2\\\\\\] for all $z\\in \\mathbb{R}^d$. It is called <i>uniformly elliptic</i> if $\\mu := \\inf_{x\\in \\mathbb{R}^d} \\mu(x) &gt; 0$, so if the above holds with a uniform $\\mu &gt; 0$.<br><br><b>Lemma.</b> If $L$ is elliptic with $c(\\cdot ) \\ge 0$ and $Lu \\ge 0$ on some domain $E\\subset \\mathbb{R}^d$, and $u$ takes a non-negative maximum in the interior of $E$, then $u$ is constant on $E$.</div><div></div>"
  },
  {
    "front": "Number of copies of a graph $H$ in $K_n$, thus expected number of copies in $G(n,p)$ and suggested threshold function. Why is this not the correct threshold function in general?",
    "back": "<div>Denote by $\\text{aut}(H)$ the number of automorphisms of $H$ (f.ex. $\\text{aut}(C_n) = 2n$). Then the number of copies is $\\frac{n(n-1)\\cdot \\ldots \\cdot (n-v+1)}{\\text{aut}(H)}$. Thus \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X \\right] = \\frac{n(n-1)\\cdot \\ldots \\cdot (n-v+1)}{\\text{aut}(H)}p^e = \\Theta(n^vp^e).\\] However $p^*(n) = n^{-v / e}$ is not a threshold function in general. For $H$ a $K_4$ plus one edge we have $n^{- 5 / 7} \\ll n^{- 2/3}$, but $H$ can only appear if $K_4$ appears. The problem is that in the regime between, it is extremely unlikely that $K_4$ appears, but if it does, then a shitton of copies of $H$ are there, so the expectation explodes even still.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Every injection $\\phi \\colon V(H) \\to [n]$ (so any labelled $v$-element subset) is a copy of $H$, and for every copy there exist exaclty $\\text{aut}(H)$ injections (injections for fixed copy $H'$ are precisely automorphisms from $H$ to $H'$, of which there are $\\text{aut}(H)$).</i> </p> </div><div></div>"
  },
  {
    "front": "Definition edge density of a graph, and (strictly) balanced graphs.",
    "back": "<div><b>Definition.</b> For a graph $H$, the <i>edge density</i> is $d(H) := \\frac{e(H)}{\\left| H \\right| }$, which is just half the average degree. $H$ is called <i>balanced</i> if $d(H') \\le d(H)$ for all $H' \\le H$, and <i>strictly balanced</i> if $d(H') &lt; d(H)$ for all $H' &lt; H$.</div><div></div>"
  },
  {
    "front": "Threshold function for a copy of a balanced graph $H$ to appear in $\\mathcal{G}(n,p)$. What if $H$ is not balanced?",
    "back": "<div><b>Theorem.</b> Let $H$ be a fixed, balanced graph. Then $p^*(n) = n^{-v / e} = n^{-1/d(H)}$ is a threshold function for a copy of $H$ to appear in $\\mathcal{G}(n,p)$.<br><br><b>Remark.</b> For general $H$, the threshold function is $p^*(n) = n^{-1/d(H')}$ with $H'$ the densest subgraph of $H$, essentially by the same proof.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $G\\sim \\mathcal{G}(n,p)$, and $X$ the number of copies of $H$ in $G$. Then we already know that $\\mu := \\mathbb{E} \\left[ X \\right] = \\Theta(n^vp^e)$. Thus if $p / p^* \\to 0$, we are done by Markov. Now assume that $p / p^* \\to \\infty$, so $\\mu \\to \\infty$. Denote by $A_i$ the event that the $i$'th copy $H_i$ of $H$ in $K_n$ appears in $G$, and put $\\Delta := \\sum_i \\sum_{j\\sim i}\\mathbb{P}(A_i\\cap A_j)$, so that we have to show $\\Delta = o(\\mu^2)$. Then $\\mathbb{P}(A_i\\cap A_j) = p^{e(H_i\\cup H_j)}$, so \\[&nbsp;&nbsp;&nbsp; \\Delta = \\sum_{r=2}^v \\sum_{s=1}^e N_{r,s} p^{2e - s} = \\sum_{r,s} \\Delta_{rs},\\\\\\] where $N_{rs}$ is the number of pairs $(i,j)$ such that $\\left| H_i\\cap H_j \\right| = v$ and $e(H_i\\cap H_j) = s$. Given $V(H_1\\cup H_2)$, there is only a finite (independent of $n$) number of ways to place to copies of $H$ in there (say $|V|^{v} \\cdot |V|^v$), so $N_{rs} = O(n^{2v - r})$, so \\[\\Delta_{rs} = O\\left(\\frac{\\mu^2}{n^rp^s}\\right) = o(\\mu^2),\\\\\\] because $s / r \\le e / v$ by balancedness, so $n^rp^s \\ge n^r p^{e r / v}= (n^v p^e)^{r / v} \\to \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Unique solution to martingale problem [$](A,x)[/$] if [$]E[/$] is separable and compact and [$]\\bar{A}[/$] is a Markov generator.<div></div><div>Uniqueness.</div>",
    "back": "<div><b>Theorem.</b> If $E$ is separable and complete and $A$ is a Markov pregenerator on $C(E)$ such that $\\overline{A}$ is a Markov generator, then the unique solutions $(\\mathbb{P}^x)_{x\\in E}$ to the martingale problem $(A,x)$ for $x\\in E$ are given by the unique Feller process associated with $\\overline{A}$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <i>Uniqueness</i>. Suppose $X$ is a solution to $(A,x)$, and thus (!) to $(\\overline{A},x)$. Let $g\\in C(E)$, $\\lambda &gt; 0$ and $f\\in D(\\overline{A})$ with $(\\lambda - \\overline{A})f = g$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[f(X_t) - \\int_r^t \\overline{A} f(X_s) \\mathop{}\\!\\mathrm{d} s \\,\\middle\\vert\\, \\mathcal{F}_r\\right] = f(X_r),\\quad 0\\le r\\le t.&nbsp;&nbsp;&nbsp; \\] Applying $\\int_r^\\infty \\lambda \\mathrm{e}^{-\\lambda t} \\, \\cdot \\, \\mathop{}\\!\\mathrm{d} t$ to both sides yields \\[\\int_r^\\infty \\mathrm{e}^{-\\lambda t} \\mathbb{E} \\left[g(X_t)\\,\\middle\\vert\\, \\mathcal{F}_r\\right] \\mathop{}\\!\\mathrm{d} t&nbsp; = \\mathrm{e}^{-\\lambda r} f(X_r).\\] Putting $r=0$ fixes the Laplace transform of $t\\mapsto \\mathbb{E} \\left[ g(X_t) \\right] $ for all $g\\in C(E)$ and thus the one-dimensional distributions of $(X_t)$. Now let $n\\in \\mathbb{N}$ and $h_i \\in C(E)$, $\\lambda_i \\ge 0$. Then<br>\\begin{multline*}<br>\\int_0^\\infty \\mathop{}\\!\\mathrm{d} s_1 \\int_{s_1}^\\infty \\mathop{}\\!\\mathrm{d} s_2 \\ldots \\int_{s_n}^\\infty \\mathop{}\\!\\mathrm{d} s_{n+1} \\exp \\left( -\\sum_{i=1}^{n+1}\\lambda_i s_i \\right) \\mathbb{E} \\left[ \\prod_{i=1}^{n+1}h_i(X_{s_i}) \\right] <br>= \\int_0^\\infty \\mathop{}\\!\\mathrm{d} s_1 \\ldots \\int_{s_{n-1}}^\\infty \\mathop{}\\!\\mathrm{d} s_n \\exp \\left( -\\sum_{i=1}^n \\lambda_i s_i - \\lambda_{n+1}s_n \\right) \\mathbb{E} \\left[ h_{n+1}(X_{s_n}) \\prod_{i=1}^n h_i(X_{s_i}) \\right] ,<br>\\end{multline*} <br>which fixes all fidis by induction.</i> </p> </div><div></div><div></div><div></div><div>[Also Stochastic Analysis and PDEs, Page 5]</div>"
  },
  {
    "front": "When can integration and differentiation be exchanged?",
    "back": "<div><b>Theorem.</b> Suppose $(\\Omega,\\mathcal{A},\\mu)$ is a measure space, $O\\subset \\mathbb{R}$ is open, and $f\\colon O \\times \\Omega \\to \\mathbb{R}$ is measurable such that<br><ol>  <li>$f(x,\\cdot )$ is integrable for all $x\\in O$,&nbsp;&nbsp;&nbsp;</li>  <li>$f(\\cdot ,\\omega)$ is differentiable for $\\mu$-almost all $\\omega\\in \\Omega$,&nbsp;&nbsp;&nbsp;</li>  <li>There is $g \\in L^1(\\mu)$ such that $\\left| \\frac{\\partial f}{\\partial x}(x,\\cdot ) \\right| \\le g$ $\\mu$-a.e.</li></ol><br>Then, \\[&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\int_\\Omega f(x,\\omega) \\mu(\\mathop{}\\!\\mathrm{d} \\omega) = \\int_\\Omega \\frac{\\partial f}{\\partial x}(x,\\omega) \\mu(\\mathop{}\\!\\mathrm{d} \\omega).\\]</div><div></div>"
  },
  {
    "front": "Definition of dependency digraph and Lovász local lemma ",
    "back": "<div><b>Definition.</b> Let $A_1,\\ldots ,A_n$ be events in a probability space $(\\Omega, \\mathcal{A}, \\mathbb{P})$. Then a <i>dependency digraph</i> for $(A_i)$ is a directed graph on $[n]$ such that $A_i \\perp \\!\\!\\! \\perp (A_j\\colon i\\not\\to j)$ for every $i\\in [n]$.<br><br><b>Theorem (local lemma).</b> If $A_1,\\ldots ,A_n$ are events with given dependency digraph, and there are $x_i \\in [0,1)$ such that \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(A_i) \\le x_i \\prod_{j\\colon i\\to j} (1-x_j),\\quad i\\in [n],\\\\\\] then $\\mathbb{P}\\left( \\bigcap_{i=1}^n A_i^{c} \\right) \\ge \\prod_{i=1}^n (1-x_i) &gt; 0$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Claim that for any $S \\subset [n]$, and $i\\in [n]$ it holds that $\\mathbb{P}\\left(\\bigcap_{j\\in S} A_j^{c}\\right) &gt; 0$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(A_i \\,\\middle\\vert\\, \\bigcap_{j\\in S} A_j^{c}\\right) \\le x_i,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] which implies the claim with $S = [n]$. The case where $\\left| S \\right| =0$ is trivial. Now consider general $i,S$ (wlog $i\\not\\in S$), then by induction hypothesis \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left(\\bigcap_{j\\in S} A_j ^{c}\\right) = \\prod_{j\\in S} \\mathbb{P}\\left( A_j^{c} \\,\\middle\\vert\\, \\bigcap_{S\\ni l &lt; j} A_l^{c} \\right) \\ge \\prod_{j\\in S} (1- x_j) &gt; 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] and put $B := \\bigcap_{\\substack{j\\in S\\\\ i\\to j}} A_j^{c}$, $C := \\bigcap_{\\substack{j\\in S \\\\ i\\not\\to j} } A_j^{c}&nbsp; $. Then, \\[\\mathbb{P} \\left(B \\,\\middle\\vert\\, C\\right) = \\prod_{\\substack{j\\in S \\\\ i\\to j}} \\mathbb{P} \\left( A_j^{c}\\,\\middle\\vert\\, C\\cap \\bigcap_{l&lt;j} A_l^{c} \\right) \\ge \\prod_{j\\colon j\\to i} (1-x_j).\\] Thus \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(A_i \\,\\middle\\vert\\, B\\cap C\\right) = \\frac{\\mathbb{P} \\left(A_i \\cap B \\,\\middle\\vert\\, C\\right)}{\\mathbb{P} \\left(B \\,\\middle\\vert\\, C\\right)} \\le \\frac{\\mathbb{P} \\left(A_i \\,\\middle\\vert\\, C\\right)}{\\mathbb{P} \\left(B \\,\\middle\\vert\\, C\\right)} = \\frac{\\mathbb{P}(A_i)}{\\mathbb{P} \\left(B \\,\\middle\\vert\\, C\\right)} \\le x_i.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Natural dependency digraph in the case where $A_1,\\ldots ,A_n \\in \\sigma\\left( X_\\alpha\\colon \\alpha \\in J \\right) $, with $(X_\\alpha)_{\\alpha \\in J}$ an independent family of random variables.",
    "back": "<div><b>Fact.</b> If $(X_\\alpha)_{\\alpha \\in J}$ is a family of independent random variables, and $A_i \\in \\sigma\\left( X_\\alpha \\colon \\alpha \\in F_i \\right)$ for $i\\in [n]$, then putting $i\\leftrightarrow j$ if $F_i \\cap F_j \\neq \\varnothing$ and $i\\neq j$ gives a (symmetric) dependency digraph.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $i\\in [n]$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A_i \\in \\sigma\\left( X_\\alpha\\colon \\alpha \\in F_i \\right) \\perp \\!\\!\\! \\perp \\sigma \\left( X_\\alpha\\colon \\alpha \\in \\bigcup_{j\\colon i\\to j} F_j \\right) \\supset \\sigma\\left( A_j\\colon i\\to j \\right).&nbsp;&nbsp;&nbsp; \\]</i> </p> </div><div></div>"
  },
  {
    "front": "Symmetric version of&nbsp;Lovász local lemma.",
    "back": "<div><b>Lemma (Local lemma, symmetric version).</b> If $A_1,\\ldots ,A_n$ and a dependency digraph are given, and \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(A_i) \\le \\frac{1}{\\mathrm{e} (d+1)}, \\quad i\\in [n],\\\\\\] where $d = \\max_{i\\in [n]} \\text{deg}(i)$, then $\\mathbb{P}\\left( \\bigcap_{i=1} ^n A_i ^{c} \\right) &gt; 0$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Put $p = \\max_i \\mathbb{P}(A_i)$, and $x_i = \\frac{1}{d+1}$ for all $i$ (note that this maximizes $x_i(1-x_i)^{d}$). Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x_i \\prod_{j\\colon i\\to j} (1-x_i) \\ge \\frac{1}{d+1}\\left( \\frac{d}{d+1} \\right) ^d \\ge \\frac{1}{\\mathrm{e} (d+1)} \\ge p \\ge \\mathbb{P}(A_i),\\quad i\\in [n],\\\\&nbsp;&nbsp;&nbsp; \\] so the local lemma applies.</i> </p> <br><br><b>Remark.</b> This can't be true without the $\\mathrm{e}$, just take $\\sum_{i=1}^{d+1}A_i = \\Omega$ and $\\mathbb{P}(A_i) = \\frac{1}{d+1}$ for all $i$, then $\\mathbb{P}\\left( \\bigcap_{i=1} ^{d+1}A_i^{c} \\right) = 0$.</div><div></div>"
  },
  {
    "front": "Condition for $2$-colourability of $r$-uniform hypergraph (by Local lemma).",
    "back": "<div><b>Proposition.</b> Let $H$ be an $r$-uniform hypergraph such that every edge meets at most $d$ other edges, and $\\mathrm{e} (d+1) \\le 2^{r-1}$. Then $H$ is $2$-colourable.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Colour vertices randomly and build a dependency digraph for the events $A_e := \\left\\{ \\text{$e$ is monochromatic} \\right\\} ,\\,e\\in E(H),$ by $A_e \\leftrightarrow A_f$ iff $e\\cap f\\neq \\varnothing$. Then every event has at most $d$ neighbours, and $\\mathbb{P}(A_e) =: p = 2^{1-r}$ for all $e\\in E$. The symmetric version of the local lemma gives $\\mathbb{P}\\left( \\bigcap_{e\\in E} A_e^{c} \\right)&nbsp; &gt; 0$, so there exists a good colouring.</i> </p> </div><div></div>"
  },
  {
    "front": "Lower bound on Ramsey-number $R(k,k)$ by Local lemma.",
    "back": "<div><b>Proposition.</b> For $n,k\\ge 3$, if $\\mathrm{e} \\binom{k}{2} \\binom{n}{k-2} 2^{1-\\binom{k}{2} }\\le 1$, then $R(k,k) &gt; n$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Have to show that there exists a good colouring of $K_n$. Consider a random colouring of, and consider events $A_S := \\left\\{\\text{$S$ is monochromatic}\\right\\} $ for $S\\in V^{(k)} $. Build a dependency digraph by joining $A_S \\leftrightarrow A_T$ iff they share an edge, so if $\\left| S\\cap T \\right| \\ge 2$. Thus, every $A_S$ has at most, roughly, $d := \\binom{k}{2} \\binom{n}{k-2} - 1$ neighbours, and $p := \\mathbb{P}(A_S) = 2^{1-\\binom{k}{2} }$. Then, by assumption, $\\mathrm{e} (d+1) p \\le 1$, so $\\mathbb{P}\\left( \\bigcap_{S} A_S^{c} \\right) &gt; 0$ by the Local lemma.</i> </p> <br><br><b>Corollary.</b> $R(k,k) \\ge \\sqrt{2} \\frac{k}{\\mathrm{e}} 2^{k / 2} (1 + o(1))$.</div><div></div>"
  },
  {
    "front": "Definition $D(S)$ for metric space $S$, modulus of continuity $\\omega_D$, and connection with ordinary modulus of continuity.",
    "back": "<div><b>Definition.</b> Let $(S,d)$ be a metric space, and denote by $D(S)$ the space of c\\`adl\\`ag paths from $[0,\\infty$ to $S$. Then, if $\\delta ,T &gt; 0$, and $x\\colon [0,\\infty)\\to S$, define \\[&nbsp;&nbsp;&nbsp; \\omega_D (x,\\delta,T) := \\inf_{(t_i) \\text{ $\\delta$-thin}} \\max_{j=1,\\ldots ,k} \\omega\\left( x, [t_{j-1},t_j) \\right) ,\\\\\\] where the infimum is taken over all finite partitions $0 = t_0 &lt; t_1 &lt; \\ldots &lt; t_k = T$ with $t_j - t_{j-1} &gt; \\delta$ for all $1\\le j\\le k-1$ (no restriction on $\\left| T - t_{k-1} \\right| $).<br><br><b>Lemma.</b> For $\\delta,T &gt; 0$, and any $x\\colon [0,\\infty) \\to S$, we have \\[&nbsp;&nbsp;&nbsp; \\omega_D(x,\\delta,T) \\le \\omega (x,2\\delta,T),\\\\\\] where $2$ may be replaced by any constant larger than one. If $x\\in D(S)$, we also have \\[&nbsp;&nbsp;&nbsp; \\omega(x,\\delta,T) \\le 2 \\omega_D(x,\\delta,T) + j(x,T),\\\\\\] where $j(x,T) := \\sup_{t\\le T} d(x(t),x(t-))$ is the maximum jump size of $x$ on $[0,T]$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Take any finite partition of $[0,T]$ with $\\left| t_j - t_{j-1} \\right| \\in (\\delta,2\\delta)$ for all $j\\le k-1$, and $\\left| t_{k-1}-T \\right| &lt; 2\\delta$. Then<br>&nbsp;&nbsp;&nbsp; \\begin{equation*}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\omega_D(x,\\delta,T) \\le \\max_{j\\le k} \\omega(x,[ t_{j-1},t_j)) \\le \\omega(x,2\\delta,T).<br>&nbsp;&nbsp;&nbsp; \\end{equation*}<br>&nbsp;&nbsp;&nbsp; For the second claim, note that for any $\\delta$-thin partition $(t_j)$, and any $0\\le s,t\\le T$ with $\\left| s - t \\right| &lt; \\delta$, we have at most one of the $t_j$ in $[s,t]$, in which case<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d(x(s),x(t)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le d(x(s),x(t_j-)) + d(x(t_j-),x(t_j)) + d(x(t_j),x(t)) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\max_{j\\le k} \\omega(x,[t_{j-1}, t_j)) + j(x,T).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] </i> </p> </div><div></div>"
  },
  {
    "front": "If $x\\in D(\\mathbb{R}_+,S)$, then $\\omega_D(x,\\delta,T) \\to 0$ as $\\delta \\to 0$. When does the converse hold? Corollary on local boundedness, measurability, and number of discontinuities of c\\`adl\\`ag paths.",
    "back": "<div><b>Lemma.</b> If $x\\in D(\\mathbb{R}_+,S)$ and $T &gt; 0$, then $\\omega_D(x,\\delta,T) \\stackrel{  }{\\longrightarrow} 0$ as $\\delta \\to 0$. If $S$ is complete, the converse holds.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\varepsilon &gt; 0$. Denote by $\\tau \\le T$ be the supremum of all $t\\in [0,T]$ for which there exists a finite partition $0 = t_0 &lt; \\ldots &lt; t_k = \\tau$ with $\\omega(x,[t_{j-1},t_j)) &lt; \\varepsilon$ for all $j\\le k$. Then $\\tau &gt; 0$ by right-continuity of $x$. Next, observe that $\\tau$ is, in face, a maximum. Indeed, we find $\\tau' &lt; \\tau$ with oscillation at most $\\varepsilon$ on $[\\tau',\\tau)$ (because left-limits exist), and thus obtain a valid partition $0 = t_0 &lt; \\ldots &lt; t_k = \\tau' &lt; \\tau$. Now assume that $\\tau &lt; T$. Then we can extend a valid partition of $[0,\\tau]$ to $[0,\\tau'']$ for some $\\tau'' \\in (\\tau,T]$ by right-continuity of $x$, a contradiction.<br><br>&nbsp;&nbsp;&nbsp; Now assume that $S$ is complete, and $x\\colon [0,\\infty) \\to S$ satisfies $\\omega_D(x,\\delta,T) \\to 0$ as $\\delta \\to 0$. Fix $t\\in [0,T)$ and let $\\varepsilon &gt; 0$. Then the existence of a valid partition implies that $x$ oscillates by at most $\\varepsilon$ on some interval $[t,t+\\delta')$. For left-continuity, take $t\\in (0,T]$ any $t_n \\uparrow t$. Then for any $\\varepsilon &gt; 0$, there exists $\\delta'$ such that $x$ oscillates by at most $\\varepsilon$ on $(t-\\delta,t)$, so $(x(t_n))$ is a Cauchy-sequence.</i> </p> <br><br><b>Corollary.</b> $x\\in D(S)$ is measurable, locally bounded and has at most countably many discontinuities.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Measurability follows because $x$ is uniform limit of step functions. For every $\\varepsilon &gt; 0$, there are only finitely many jumps of size $\\varepsilon &gt; 0$, so at most countably many positive jumps. Local boundedness follows from $\\omega_D(x,\\delta,T) &lt; \\infty$ for any $\\delta &gt; 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Kolmogorov's criterion for tightness of sequence of $\\mathbb{R}^d$-valued <b>continuous</b> stochastic processes (without proof).",
    "back": "<div><b>Theorem.</b> If $(X^{(n)})$ is a sequence of $\\mathbb{R}^d$-valued continuous stochastic processes such that $(X^{(n)}(0))_n$ is tight and there exist constants $C &gt; 0$ and $a,b&gt; 0$ with \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left| X^{(n)}_t - X^{(n)}_s \\right| ^a \\right] \\le C \\left| t-s \\right| ^{1 + b},\\\\\\] for all $n\\in \\mathbb{N}$ and $s,t\\ge 0$, then $(X^{(n)})$ is tight.</div><div></div>"
  },
  {
    "front": "Definition of $\\Lambda \\subset \\mathbb{R}_+^{\\mathbb{R}_+}$, definition of $\\gamma(\\lambda)$ for $\\lambda\\in \\Lambda$. Given $\\gamma(\\lambda)$, what about $\\gamma(\\lambda^{-1})$, $\\gamma(\\lambda\\circ \\lambda')$, $\\left\\|\\lambda-\\operatorname{id}\\right\\|_T$?<br><br>Definition of Skorokhod topology on $D(S)$ for metric space $S$. What if $S$ is separable or Polish. Generator of Borel $\\sigma$-algebra?",
    "back": "<div><b>Definition.</b> By $\\Lambda$ denote the set of bijective, increasing functions $\\lambda\\colon [0,\\infty)\\to [0,\\infty)$. Such functions are necessarily continuous, strictly increasing, and satisfy $\\lambda(0) = 0$ and $\\lambda(\\infty) = \\infty$. Define \\[&nbsp;&nbsp;&nbsp; \\gamma(\\lambda) := \\sup_{0\\le s &lt; t} \\left| \\log \\frac{\\lambda(t) - \\lambda(s)}{t - s} \\right| ,\\quad \\lambda\\in \\Lambda,\\\\\\] and put $\\Lambda_0 := \\left\\{ \\lambda\\in \\Lambda\\colon \\gamma(\\lambda) &lt; \\infty \\right\\} $. Then<br><ol>  <li>For all $\\lambda\\in \\Lambda$, $\\lambda^{-1}\\in \\Lambda$ exists and $\\gamma(\\lambda) = \\gamma(\\lambda^{-1})$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\gamma(\\lambda_1\\circ \\lambda_2) \\le \\gamma(\\lambda_1) + \\gamma(\\lambda_2)$ for all $\\lambda_1,\\lambda_2\\in \\Lambda_0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_{t\\le T} \\left| \\lambda(t) - t \\right| \\le T \\left( \\mathrm{e}^{\\gamma(\\lambda)} - 1 \\right) $ for all $\\lambda\\in \\Lambda_0$ and $T &gt; 0$.</li></ol><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>\\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{s &lt; t} \\left| \\log \\frac{\\lambda^{-1}(t) - \\lambda^{-1}(s)}{t-s} \\right| = \\sup_{s &lt; t} \\left| \\log \\frac{\\lambda^{-1}(\\lambda(t)) - \\lambda^{-1}(\\lambda(s))}{\\lambda(t) - \\lambda(s)} \\right| .&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>Expand by $\\frac{\\lambda_2(t) - \\lambda_2(s)}{t - s}$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><b>Definition.</b> Let $(S,d)$ be a metric space. On $D(S)$ define the <i>Skorokhod metric</i> \\[&nbsp;&nbsp;&nbsp; \\rho(x,y) := \\inf_{\\lambda\\in \\Lambda} \\left[ \\gamma(\\lambda) \\vee \\int_0^\\infty \\left( 1 \\wedge \\left\\|d(x^s,y^s\\circ \\lambda)\\right\\|_\\infty \\right)\\mathrm{e}^{-s}\\mathop{}\\!\\mathrm{d} s&nbsp; \\right] ,\\quad x,y\\in D(S),\\\\\\] where $\\left\\|d(x,y)\\right\\|_T:= \\sup_{0\\le t\\le T} d(x(t),y(t))$ for $x,y\\in D(S)$. This defines a metric, and if $S$ is separable or Polish, so is $(D(S),\\rho)$. Furthermore, if $S$ is Polish, $\\mathcal{B}(D(S)) = \\sigma\\left( \\pi_t\\colon t\\ge 0 \\right)$.</div><div></div>"
  },
  {
    "front": "Characterisations of convergence w.r.t. Skorokhod topology in $D(S)$ for metric space $(S,d)$ (one local and one global).",
    "back": "<div><b>Lemma.</b> Let $(S,d)$ be a metric space, let $\\rho$ denote the Skorokhod topology on $D(S)$, and let $x^{(n)},x\\in D(S),\\,n\\in \\mathbb{N}$. Then $\\rho(x^{(n)},x) \\stackrel{  }{\\longrightarrow} 0$ if and only if either (both) of the following hold.<br><ol>  <li>There exists $(\\lambda_n) \\in \\Lambda^{\\mathbb{N}}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\gamma(\\lambda_n) \\stackrel{  }{\\longrightarrow}&nbsp; 0,\\qquad \\forall T &gt; 0\\colon \\left\\| d(x^{(n)}, x\\circ \\lambda_n)\\right\\|_T\\stackrel{  }{\\longrightarrow}&nbsp; 0.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>For every $T &gt; 0$, there exists $(\\lambda_n^T)_n\\in \\Lambda_0^\\mathbb{N}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\lambda_n^T - \\operatorname{id}\\right\\|_T \\stackrel{  }{\\longrightarrow} 0,\\qquad \\left\\|d(x^{(n)},x\\circ \\lambda_n^T)\\right\\|_T \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]</li></ol><br>In particular, if this is the case, then $x^{(n)}(t) \\stackrel{  }{\\longrightarrow} x(t)$ for all $t\\in \\mathcal{C}(x)$, and, if $x$ is continuous, this is equivalent to locally uniform convergence.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $\\rho(x^{(n)},x) \\stackrel{  }{\\longrightarrow} 0$, so (ii) holds. Then, for $t \\ge 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d(x^{(n)}(t),x(t)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le d(x^{(n)}(t),x(\\lambda_n^T(t))) + d(x(\\lambda_n^T(t)),x(t))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\rho(x^{(n)},x) + d(x(\\lambda_n^T(t)),x(t)).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $t\\in \\mathcal{C}(x)$, this goes to zero. If $x$ is continuous, the second expression is at most $\\omega(x, \\left\\|\\lambda_n^T - \\operatorname{id}\\right\\|, T + 1)$, so the bound is uniform in $t\\le T$. On the other hand, locally uniform convergence always implies convergence in $\\rho$ (just take $\\lambda_n = 0$).</i> </p> </div><div></div>"
  },
  {
    "front": "Arzel\\`a-Ascoli theorem for $D(S)$ with Skorokhod topology for Polish space $S$, and characterisation of relative compactness of $\\mathcal{L}\\subset M_1(D(\\mathbb{R}^d))$.",
    "back": "<div><b>Theorem (Arz\\`ela-Ascoli).</b> If $(S,d)$ is a Polish space, then a subset $A\\subset D(S)$ is relatively compact w.r.t. the Skorokhod topology if and only if<br><ol>  <li>&nbsp; There exists a dense set $I \\subset [0,\\infty)$ such that $\\left\\{ x(t)\\colon x\\in A \\right\\} \\subset S$ is relatively compact for all $t\\in I$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_{x\\in A} \\omega_D(x,\\delta,T) \\stackrel{  }{\\longrightarrow} 0$ as $\\delta \\to 0$ for all $T &gt; 0$.</li></ol><br><br><b>Corollary.</b> A set $\\mathcal{L}\\subset M_1(D(\\mathbb{R}^d))$ is relatively compact if and only if the following hold.<br><ol>  <li>For all $T &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{P\\in \\mathcal{L}} P\\left( x\\colon \\left\\|x\\right\\|_T \\ge L \\right) \\stackrel{  }{\\longrightarrow} 0,\\quad L \\to \\infty,\\\\&nbsp;&nbsp;&nbsp; \\]</li>  <li>For all $T &gt; 0$ and $\\varepsilon &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{P\\in \\mathcal{L}} P\\left( x\\colon \\omega_D(x,\\delta,T) \\ge \\varepsilon \\right) \\stackrel{  }{\\longrightarrow} 0,\\quad \\delta \\to 0.\\]</li></ol></div><div></div>"
  },
  {
    "front": "Lower bound on Ramsey number $R(3,k)$ for large $k\\in \\mathbb{N}$.",
    "back": "<div><b>Theorem.</b> There exists $C &gt; 0$ such that $R(3,k) \\ge C \\left( \\frac{k}{\\log k} \\right) ^2$ for large $k$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For fixed $n\\in \\mathbb{N}$, colour each edge in $K_n$ red with probability $p$, blue otherwise, independently. For $S \\in [n]^{(3)}$, let $A_S := \\left\\{ \\text{$S$ is red} \\right\\} $, and for $T \\in [n]^{(k)}$, put $B_T := \\left\\{ \\text{$T$ is blue} \\right\\} $. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(A_S) = p^3,\\quad \\mathbb{P}(B_T) = (1-p)^{\\binom{k}{2} }.&nbsp;&nbsp;&nbsp; \\] As digraph, join events if they overlap in at least two vertices (i.e. at least one edge). We wish to apply the Local lemma with $x_i = x$ for $A$ events and $x_i = y$ for $B$ events. To estimate degrees, each $A_S$ connects to at most $3n$ other $A$ events and at most $n^k$ $B$ events. $B$ events connect to at most $n\\binom{k}{2} $ $A$ and at most $n^k$ $B$ events. We thus have to satisfy the bounds<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p^3 &amp;\\le x (1-x)^{3n} (1-y)^{n^k},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1-p)^{\\binom{k}{2} } &amp;\\le y (1-x)^{n\\binom{k}{2} } (1-y)^{n^k}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; This turns out to work with $k\\sim 30 \\sqrt{n} \\log n$ and suitable choices for $p,x,y$ as functions of $n$, which gives $R(3,k) &gt; n$ in that case. Turning around the formula for $k$ gives $n\\sim \\left( \\frac{k}{60 \\log k} \\right) ^2$.</i> </p> </div><div></div>"
  },
  {
    "front": "Chernoff bound on $\\mathbb{P}(X\\ge nx)$ and $\\mathbb{P}(X\\le nx)$ for $X\\sim \\text{Bin}(n,p)$ (raw form).",
    "back": "<div><b>Proposition.</b> Let $n\\in \\mathbb{N}$, $p\\in (0,1)$, and $X\\sim \\text{Bin}(n,p)$. Then, if $x \\ge p$,\\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( X\\ge nx \\right) \\le \\left[ \\left( \\frac{p}{x} \\right) ^x \\left( \\frac{1-p}{1-x} \\right) ^{1-x} \\right] ^n.\\] The same bound holds on $\\mathbb{P}(X\\le nx)$ if $x\\le p$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $x\\ge p$. Then for $t &gt; 0$ we have&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( X\\ge nx \\right) = \\mathbb{P}\\left( \\mathrm{e}^{tX}\\ge \\mathrm{e}^{tnx} \\right) \\le \\mathrm{e}^{-tnx} \\mathbb{E} \\left[ \\mathrm{e}^{tX} \\right] = \\left[ \\mathrm{e}^{-tx} \\left( p\\mathrm{e}^t + (1-p) \\right)&nbsp; \\right] ^n.&nbsp;&nbsp;&nbsp; \\] Minimising over $t &gt; 0$ gives the bound. The other bound follows from $n - X \\sim \\text{Bin}(n,1-p)$ and $\\mathbb{P}(X\\le nx) = \\mathbb{P}(n-X \\ge n(1-x))$.</i> </p> </div><div></div>"
  },
  {
    "front": "Chernoff bounds on $\\mathbb{P}(\\text{Bin}(n,p) \\ge np + t)$ and $\\mathbb{P}\\left( \\text{Bin}(n,p) \\ge (1+\\varepsilon)np \\right) $ for $t &gt; 0$ and $0&lt;\\varepsilon \\le 1$ (and other way around).",
    "back": "<div><b>Proposition.</b> Let $X\\sim \\text{Bin}(n,p)$. Then, for $t &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(X\\ge np + t) \\le \\mathrm{e}^{-2t^2 / n},\\qquad \\mathbb{P}(X \\le np -t) \\le \\mathrm{e}^{-2t^2 / n}.\\] and for $0&lt; \\varepsilon\\le 1$, \\[\\mathbb{P}\\left( X \\ge (1+\\varepsilon) np \\right) \\le \\mathrm{e}^{-\\varepsilon^2 np / 4},\\qquad \\mathbb{P}\\left( X \\le (1-\\varepsilon) np \\right) \\le \\mathrm{e}^{-\\varepsilon^2 np / 2}.\\]</div><div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We know that, if $x &gt; p$ or $x &lt; p$, then $\\mathbb{P}(X \\ge (\\le) nx) \\le \\mathrm{e}^{-f(x) n}$, where&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(x) = x\\log \\left( \\frac{p}{x} \\right) + (1-x) \\log \\left(\\frac{1-p}{1-x}\\right).&nbsp;&nbsp;&nbsp; \\] Then $f(p) = f'(p) = 0$, and $f''(x) = \\frac{1}{x} + \\frac{1}{1-x}$. For the first bound, note that $f''(x) \\ge 4$, so $f(p\\pm h) \\ge 2h^2$. For the second, note that for $p\\le x \\le (1+\\varepsilon)p\\le 2p$, $f''(x) \\ge \\frac{1}{x} \\ge \\frac{1}{2p} $, so $f((1+\\varepsilon)p) \\ge \\varepsilon^2 p / 4$. for the final bound, note that for $(1-\\varepsilon) p \\le x \\le p$ we have $f''(x) \\ge \\frac{1}{x}\\ge \\frac{1}{p}$, so $f((1-\\varepsilon)p) \\ge \\varepsilon^2 p / 2$.</i> </p> </div><div></div>"
  },
  {
    "front": "Asymptotic bound on maximal degree in $\\mathcal{G}(n,p)$ of the form $\\mathbb{P}(\\Delta \\ge ?) \\stackrel{  }{\\longrightarrow} 0$.",
    "back": "<div><b>Proposition.</b> If $\\Delta$ is the maximum degree in $\\mathcal{G}(n,p)$, and $np \\ge 10 \\log n$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\Delta \\ge np + 3\\sqrt{np \\log n}&nbsp; \\right) \\stackrel{  }{\\longrightarrow} 0\\\\\\] as $n\\to \\infty$. That is, no vertex has degree $3\\sqrt{\\log n} $ standard deviations above its mean.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\mathbb{P}\\left( \\Delta \\ge d \\right) \\le n \\mathbb{P}\\left( \\text{Bin}(n,p) \\ge d \\right)$. Put $\\varepsilon := 3 \\sqrt{\\frac{\\log n}{np}} \\le 1$, so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\text{Bin}(n,p) \\ge np (1+\\varepsilon) \\right) \\le \\exp \\left( -9 / 4 \\log n \\right) = n^{-9 / 4} = o\\left(\\frac{1}{n}\\right).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of convergence $y_n \\stackrel{  }{\\longrightarrow} y$ in Skorokhod topology on $D([0,\\infty),S)$ involving limiting behaviour of $y_n(t_n)$ if $t_n \\stackrel{  }{\\longrightarrow} t\\in [0,\\infty)$.",
    "back": "<div><b>Proposition.</b> Let $(S,d)$ be a metric space. Then $y_n \\stackrel{  }{\\longrightarrow} y$ in $D(S)$ with respect to the Skorokhod topology if and only if, whenever $t_n \\to t$ in $[0,\\infty)$, the following hold.<br><ol>  <li>$d(y_n(t_n),y(t)) \\wedge d(y_n(t_n),y(t-)) \\to 0$,&nbsp;&nbsp;&nbsp;</li>  <li>If $d(y_n(t_n),y(t)) \\to 0$ and $s_n \\ge t_n$, $s_n \\to t$, then also $d(y_n(s_n),y(t)) \\to 0$,&nbsp;&nbsp;&nbsp;</li>  <li>If $d(y_n(t_n),y(t-)) \\to 0$ and $s_n\\le t_n$, $s_n \\to t$, then also $d(y_n(s_n),y(t-)) \\to 0$.</li></ol></div><div></div>"
  },
  {
    "front": "Characterisation of tightness of a sequence of $D([0,\\infty),S)$-valued random variables for Polish $S$ (directly from Arzel\\`a Ascoli).",
    "back": "<div><b>Proposition.</b> Let $(X^{n})_{n\\in \\mathbb{N}}$ be a sequence of $D([0,\\infty),S)$-valued processes with Polish $S$. Then $(X^{n})$ is tight if and only if the following hold.<br><ol>  <li>There exists $I\\subset [0,\\infty)$ dense such that $(X^n(t))_n$ is tight in $S$ for all $t\\in I$,&nbsp;&nbsp;&nbsp;</li>  <li>For all $\\varepsilon,T &gt; 0$, $\\limsup_{n\\to \\infty} \\mathbb{P}\\left( \\omega_D(X^n,\\delta,T) \\ge \\varepsilon \\right) \\to 0$ as $\\delta \\to 0$.</li></ol></div><div></div>"
  },
  {
    "front": "Criterion for tightness of sequence of $D([0,\\infty),\\mathbb{R})$-valued stochastic processes (Aldous), and in the case of semimartingales (Aldous-Rebolledo).",
    "back": "<div><b>Proposition.</b> A sequence $(Y^n)$ of $D(\\mathbb{R})$-valued stochastic processes is tight if<br><ol>  <li>$(Y^n(t))_n$ is tight for all $t\\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>For any bounded sequence $(\\tau_n)$ of stopping times and $\\varepsilon &gt; 0$,\\[&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\sup_{\\theta &lt; \\delta} \\mathbb{P}\\left(\\left| Y^n_{\\tau_n + \\theta} - Y^n_{\\tau_n} \\right|\\ge \\varepsilon\\right) \\stackrel{  }{\\longrightarrow} 0,\\quad \\delta \\to 0.&nbsp;&nbsp;&nbsp; \\]</li></ol><br>If all $Y^n$ are semimartingales, it suffices if the second condition holds with $Y$ replaced by $A$ and $\\left&lt;Y \\right&gt; $ (the finite variation part and the quadratic variation).</div><div></div>"
  },
  {
    "front": "Large deviations: Definition and basic properties of $M_\\mu(\\lambda)$ and $I_\\mu(x)$ for $\\mu\\in M_1(\\mathbb{R})$ (with existing exponential moments).",
    "back": "\"<div><b>Definition/Proposition.</b> Suppose $\\xi\\sim\\mu\\in M_1(\\mathbb{R})$ integrable with mean $a\\in \\mathbb{R}$, and existing exponential moments. Then put<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; M_\\mu(\\lambda) &amp;:= \\int\\mathrm{e}^{\\lambda z}\\mu(\\mathop{}\\!\\mathrm{d} z),\\quad \\lambda \\in \\mathbb{R},\\\\&nbsp;&nbsp;&nbsp; I_\\mu(x) &amp;:= \\sup_{\\lambda \\in \\mathbb{R}} \\left( \\lambda x - \\log M_\\mu(\\lambda) \\right) ,\\quad x\\in \\mathbb{R}.\\end{align*}[/$$] <br>Then we have<br><ol>  <li>$M_\\mu\\colon \\mathbb{R} \\to (0,\\infty)$ is $C^\\infty$ and log-convex, and $M_\\mu(\\lambda)\\ge \\mathrm{e}^{\\lambda a}$,&nbsp;&nbsp;&nbsp;</li>  <li>$I_\\mu\\colon \\mathbb{R} \\to [0,\\infty)$ is convex, <i>good</i> in the sense that $\\left\\{ I_\\mu \\le c \\right\\} \\subset \\mathbb{R}$ is compact for all $c\\in \\mathbb{R}$,&nbsp;&nbsp;&nbsp;</li>  <li>$I_\\mu(a) = 0$ (attained at $\\lambda = 0$), and if $x\\ge (\\le) a$, we only have to consider $\\lambda \\ge (\\le) 0$ in the supremum.</li></ol><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>$M_\\mu(\\lambda) \\ge \\mathrm{e}^{\\lambda a}$ follows from Jensen's inequality, smoothness is clear, log-convexity follows from H\\\"\"older. Indeed, if $\\alpha,\\beta \\in (0,1)$ with $\\alpha + \\beta = 1$, then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M_\\mu(\\alpha \\lambda_1 + \\beta \\lambda_2) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\left( \\mathrm{e}^{\\lambda_1 z} \\right) ^\\alpha \\left( \\mathrm{e}^{\\lambda_2 z} \\right) ^\\beta \\mu(\\mathop{}\\!\\mathrm{d} z) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left( \\int\\mathrm{e}^{\\lambda_1 z}\\mu(\\mathop{}\\!\\mathrm{d} z) \\right) ^\\alpha \\left( \\int\\mathrm{e}^{\\lambda_2 z}\\mu(\\mathop{}\\!\\mathrm{d} z) \\right) ^\\beta\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= M_\\mu(\\lambda_1)^\\alpha M_\\mu(\\lambda_2)^\\beta.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$I$ is a supremum of linear (so convex) functions and thus convex. $I \\ge 0$ follows because $\\lambda = 0$ yields $0$. $I$ is continuous (because it's convex), so $\\left\\{ I_\\mu\\le c \\right\\} $ is closed. Boundedness follows because whenever $I_\\mu(x) \\le c$, then (choosing $\\lambda = \\pm 1$), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\pm x - \\log M_\\mu(\\pm 1) \\le I_\\mu(x) \\le c \\implies \\left| x \\right| \\le c + \\log M_\\mu(1) + \\log M_\\mu(-1).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>From (i), we know that $\\lambda x - \\log M_\\mu(\\lambda) \\le \\lambda(x-a)$, so we can forget $\\lambda &lt; 0$ if $x \\ge a$ and vice-versa. This also implies $I_\\mu(a) = 0$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>\""
  },
  {
    "front": "Natural upper bound for $\\limsup_{n\\to \\infty} \\frac{1}{n}\\log(a_n+b_n)$ for sequences $(a_n),(b_n)\\in [0,\\infty)^{\\mathbb{N}}$.",
    "back": "<div><b>Lemma.</b> If $a_n,b_n\\ge 0,\\,n\\in \\mathbb{N},$ Then \\[&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty}\\frac{1}{n}\\log(a_n+b_n) \\le \\limsup_{n\\to \\infty} \\frac{1}{n}\\log a_n \\vee \\limsup_{n\\to \\infty} \\frac{1}{n} \\log b_n.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\log(a_n+b_n) \\le \\log (a_n\\vee b_n) + \\log 2$, so LHS is bounded by $\\limsup_{n\\to \\infty} \\frac{1}{n}\\log (a_n\\vee b_n)$, which is equal to RHS because $\\log(a_n \\vee b_n) = \\log a_n \\vee \\log b_n$.</i> </p> </div><div></div>"
  },
  {
    "front": "Cram\\'ers theorem on large deviations (proof of upper bound).",
    "back": "<div><b>Cram\\'ers theorem.</b> If $(\\xi_i)_{i=1}^\\infty$ are i.i.d. with existing exponential moments, and $\\mu_n := \\mathbb{P}\\left( \\frac{1}{n}\\sum_{i=1}^n\\xi_i \\in \\cdot&nbsp; \\right) ,\\, n\\in\\mathbb{N}$, then $(\\mu_n)$ satisfies the large deviation prinicple (LDP) with rate function $I_\\mu$. That is,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\frac{1}{n}\\log\\mu_n(F) &amp;\\le -\\inf_{F} I_\\mu,\\\\&nbsp;&nbsp;&nbsp; \\liminf_{n\\to \\infty}\\frac{1}{n}\\log \\mu_n(G) &amp;\\ge -\\inf_G I_\\mu,\\\\\\end{align*}[/$$]<br>whenever $F\\subset \\mathbb{R}$ is closed and $G\\subset \\mathbb{R}$ is open.<br><br><p><i><b>Proof.</b>[Proof of upper bound]<br>&nbsp;&nbsp;&nbsp; If $F = \\varnothing$ or $a\\in F$, the bound is trivial. If $F \\subset (a,\\infty)$, so $F \\subset [y,\\infty)$ with $y = \\min F$, then $-\\inf_F I = -I(y)$, and for any $\\lambda &gt; 0$, we have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu_n(F)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mu_n([y,\\infty)) \\le \\mathrm{e}^{-\\lambda y} \\mathbb{E} \\left[ \\exp \\left( \\frac{\\lambda}{n} \\sum_{i=1}^n \\xi_i \\right) \\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-\\lambda y} \\left( \\int \\mathrm{e}^{\\lambda z / n}\\mu(\\mathop{}\\!\\mathrm{d} z) \\right) ^n\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-\\lambda y} M_\\mu\\left( \\frac{\\lambda}{n} \\right) ^n.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Rearranging yields \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{n}\\log \\mu_n(F) \\le - \\left( \\frac{\\lambda}{n}y - M_\\mu\\left( \\frac{\\lambda}{n} \\right)&nbsp; \\right) ,\\quad \\lambda &gt; 0,\\\\&nbsp;&nbsp;&nbsp; \\] where the supremum over $\\lambda &gt; 0$ of the RHS is $-I_\\mu(y)$. This implies the claim for $F \\subset (a,\\infty)$, and $F \\subset (-\\infty,a)$. For general $a\\not\\in F \\neq \\varnothing$, we have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\frac{1}{n}\\log \\mu_n(F) &nbsp;&nbsp;&nbsp; &amp;= \\limsup_{n\\to \\infty} \\frac{1}{n}\\log \\mu_n(F_1+ F_2) \\\\&nbsp;&nbsp;&nbsp; &amp;\\le \\left( -\\inf_{F_1}I_\\mu \\right) \\vee \\left( -\\inf_{F_2}I_\\mu \\right) \\\\&nbsp;&nbsp;&nbsp; &amp;= -\\inf_F I_\\mu.\\end{align*}[/$$] </i> </p> </div><div></div>"
  },
  {
    "front": "Cram\\'ers theorem on large deviations (proof of lower bound).",
    "back": "<div><b>Cram\\'ers theorem.</b> If $(\\xi_i)_{i=1}^\\infty$ are i.i.d. with existing exponential moments, and $\\mu_n := \\mathbb{P}\\left( \\frac{1}{n}\\sum_{i=1}^n\\xi_i \\in \\cdot&nbsp; \\right) ,\\, n\\in\\mathbb{N}$, then $(\\mu_n)$ satisfies the large deviation prinicple (LDP) with rate function $I_\\mu$. That is,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\limsup_{n\\to \\infty} \\frac{1}{n}\\log\\mu_n(F) &amp;\\le -\\inf_{F} I_\\mu,\\\\&nbsp;&nbsp;&nbsp; \\liminf_{n\\to \\infty}\\frac{1}{n}\\log \\mu_n(G) &amp;\\ge -\\inf_G I_\\mu,\\\\\\end{align*}[/$$]<br>whenever $F\\subset \\mathbb{R}$ is closed and $G\\subset \\mathbb{R}$ is open.</div><div></div><div><p><i><b>Proof.</b>[Proof of lower bound]<br>&nbsp;&nbsp;&nbsp; Let $G\\subset \\mathbb{R}$ be open. We have to show that $\\liminf_{n\\to \\infty} (\\ldots ) \\ge -I(x)$ for all $x\\in G$, so let $x\\in G$, WLOG $x \\ge a$. Suppose first that the minimum is attained, so $I(x) = \\lambda x - \\log M(\\lambda)$ for some $\\lambda \\ge 0$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 \\stackrel{!}{=} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} \\eta} \\left. \\left( \\eta x - \\log M(\\eta) \\right) \\right\\vert_{\\eta = \\lambda} = x - \\frac{\\int z \\mathrm{e}^{\\lambda z}\\mu(\\mathop{}\\!\\mathrm{d} z)}{M(\\lambda)}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Thus if we define $\\nu\\in M_1(\\mathbb{R})$ by $\\nu(\\mathop{}\\!\\mathrm{d} z) = \\frac{\\mathrm{e}^{\\lambda z}}{M(\\lambda)}\\mu(\\mathop{}\\!\\mathrm{d} z)$, then $\\nu$ has mean $x$. Now let $\\delta &gt; 0$ such that $(x-\\delta,x+\\delta) \\subset G$, so that<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu_n(G)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\mu_n((x-\\delta,x+\\delta)) = \\mathbb{P}\\left( \\left| \\frac{1}{n}\\sum_{i=1}^n \\xi_i - x \\right| &lt; \\delta&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathrm{e}^{-n\\lambda (x+\\delta)} \\mathbb{E} \\left[ \\mathrm{e}^{\\lambda \\sum_{i=1}^n \\xi_i} \\colon \\left| \\ldots&nbsp; \\right| &lt; \\delta \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-n\\lambda (x+\\delta)} M(\\lambda)^n \\mathbb{P}\\left( \\left| \\frac{1}{n}\\sum_{i=1}^n Y_i - x \\right| &lt; \\delta \\right) ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where $Y_i \\sim \\nu$ i.i.d. This implies<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{n}\\log \\mu_n(G)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge -\\lambda \\delta - \\lambda x + \\log M(\\lambda) + o(1)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= -\\lambda \\delta - I(x) + o(1),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; so $\\limsup_{n\\to \\infty}\\frac{1}{n}\\log \\mu_n(G) \\ge -\\lambda \\delta - I(x)$ for all $\\delta &gt; 0$.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If $I(x)$ is not attained, we find $0\\le \\lambda_n \\uparrow \\infty$ such that $\\lambda_n x - \\log M(\\lambda_n) \\to I(x)$. We have<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{n\\to \\infty}&amp; \\int_{(-\\infty,x)} \\mathrm{e}^{\\lambda_n(z-x)}\\mu(\\mathop{}\\!\\mathrm{d} z) = 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{n\\to \\infty}&amp; \\int_\\mathbb{R} \\mathrm{e}^{\\lambda_n(z-x)} \\mu(\\mathop{}\\!\\mathrm{d} z) = \\lim_{n\\to \\infty}\\mathrm{e}^{ -\\lambda_nx + \\log M(\\lambda_n) } = \\mathrm{e}^{-I(x)},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Furthermore, for $\\delta &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu([x+\\delta,\\infty)) \\le \\mathrm{e}^{-\\delta \\lambda} \\int_{[x+\\delta,\\infty)} \\mathrm{e}^{\\lambda_n(z-x)}\\mu(\\mathop{}\\!\\mathrm{d} z) \\le \\mathrm{e}^{-\\delta \\lambda_n} \\int_\\mathbb{R} (\\ldots ) \\stackrel{  }{\\longrightarrow} 0 \\cdot&nbsp; \\mathrm{e}^{-I(x)} = 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $\\mu((x,\\infty)) = 0$. Hence, $\\mu(\\left\\{ x \\right\\} ) = \\int_\\mathbb{R} \\mathrm{e}^{\\lambda_n(z - x)}\\mu(\\mathop{}\\!\\mathrm{d} z) \\to \\mathrm{e}^{-I(x)}$, and finally,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\begin{equation*}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu_n(G) \\ge \\mu_n(\\left\\{ x \\right\\} ) \\ge \\mathbb{P}\\left( \\xi_1= \\ldots =\\xi_n = x \\right) = \\mu(\\left\\{ x \\right\\} )^n = \\mathrm{e}^{-n I(x)}.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{equation*}</i> </p> </div><div></div>"
  },
  {
    "front": "GW branching processes: Connection of $\\eta_n = \\mathbb{P}(Z_n = 0)$ and $\\eta = \\mathbb{P}(\\text{process dies out})$ with pgf $f$ of $Z$.",
    "back": "<div><b>Proposition.</b> Let $(Z_n)$ be the GW branching process with offspring distribution $Z$, and put $\\eta_n = \\mathbb{P}(Z_n = 0)$, and $\\eta = \\lim_{n\\to \\infty} \\eta_n = \\mathbb{P}\\left( \\text{process dies} \\right) $. Then $\\eta_{n+1} = f(\\eta_n)$, and $\\eta$ is the smallest solution to $f(x) = x$ in $[0,1]$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\eta_0 = 0$, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\eta_{n+1} = \\sum_{k=0}^\\infty \\mathbb{P}(Z_1 = k) \\mathbb{P} \\left(Z_{n+1} = 0 \\,\\middle\\vert\\, Z_1 = k\\right) = \\sum_{k=0}^\\infty p_k \\eta_n^k = f(\\eta_n).&nbsp;&nbsp;&nbsp; \\] By continuity of $f$, we have $\\eta \\leftarrow \\eta_{n+1} = f(\\eta_n) \\rightarrow f(\\eta)$. Now if $f(x_0) = x_0$, then $\\eta_0 = 0 \\le x_0$, so $\\eta_{n+1} = f(\\eta_n) \\le f(x_0) = x_0$ for all $n$, so $\\eta \\le x_0$.</i> </p> </div><div></div>"
  },
  {
    "front": "When does a GW branching process almost surely die out?",
    "back": "<div><b>Proposition.</b> If $(Z_n)$ is the GW process with offspring distribution $Z$, $\\lambda := \\mathbb{E} \\left[ Z \\right] $, and $\\eta = \\mathbb{P}(\\text{process dies})$, then<br><ol>  <li>If $\\lambda &lt; 1$, then $\\eta = 1$,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\lambda = 1$ and $\\mathbb{P}(Z = 1) &lt; 1$, then $\\eta = 1$,&nbsp;&nbsp;&nbsp;</li>  <li>If $\\lambda &gt; 1$, then $\\eta &lt; 1$.</li></ol><br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\lambda := \\mathbb{E} \\left[ Z&nbsp; \\right] &lt; 1 $, this follows from $\\mathbb{E} \\left[ Z_n \\right] = \\lambda^n \\to 0$. Now we know that $\\eta &lt; 1$ if and only if $f(x) = x$ has a solution in $[0,1)$. If $f'(1) = \\lambda &gt; 1$, then $f(0) \\ge 0$ implies the existence of such a solution. If $f'(1) = \\lambda = 1$ and $\\mathbb{P}(Z\\ge 2) &gt; 0$, then $f$ is strictly convex, so $f'(x) &lt; 1$ for all $x\\in [0,1)$, and $f(1) = 1$, so $f(x) &gt; x$ for all $x\\in (0,1]$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of the GW- and the $G(n,p)$-exploration processes $(Y_t ^{bp})$ and $(Y_t)$, and connection with size of the component of start vertex $v$.",
    "back": "<div><b>Definition.</b> For the branching process, let $(Z_t)_{t\\in \\mathbb{N}}$ be i.i.d. with offspring distribution, and put $Y_0^{bp} = 1$, and \\[Y_{t}^{bp} = \\begin{cases}&nbsp;&nbsp;&nbsp; Y_{t-1} ^{bp} + Z_t - 1 &amp;, \\,Y_{t-1} ^{bp} &gt; 0,\\\\&nbsp;&nbsp;&nbsp; 0 &amp;, \\, \\text{otherwise}.\\end{cases}\\] Then $\\min \\left\\{ t\\in \\mathbb{N} \\colon Y^{bp}_t = 0 \\right\\} $ is the total size of the branching process.<br><br><b>Definition.</b> For $G(n,p)$, take $v\\in [n]$ fixed start vertex and do DFS. After step $t$, there are $Y_t$ vertices in the buffer ($Y_0 = 1$), $t$ processed vertices, and $U_t := n - t - Y_t$ undiscovered vertices. Then pick the buffered vertex with the smallest label, remove it, and denote by $R_t$ the number of new vertices discovered from it, so $R_{t}\\sim \\text{Bin}(U_{t-1},p)$ (given $Y_{t-1}$), and \\[Y_{t} = \\begin{cases}&nbsp;&nbsp;&nbsp; Y_{t-1} + R_{t} - 1 &amp;,\\, Y_{t-1} &gt; 0,\\\\&nbsp;&nbsp;&nbsp; 0 &amp;,\\, \\text{otherwise}.\\end{cases}\\] Then $\\left| C_v \\right| = \\min \\left\\{ t\\in \\mathbb{N}\\colon Y_t = 0 \\right\\} $.</div><div></div>"
  },
  {
    "front": "Asymptotic probability of the component of a fixed vertex in $G(n,p)$ having size $k\\in \\mathbb{N}$ if $np \\stackrel{  }{\\longrightarrow} c &gt; 0$. Corollary on expected number of $k$-components.",
    "back": "<div><b>Proposition.</b> Denote by $\\rho_k(c)$ for $c &gt; 0$ the probability that a $\\text{Po}(c)$-GW process has size exactly $k\\in \\mathbb{N}$. Suppose that $np \\stackrel{  }{\\longrightarrow} c \\in (0,\\infty)$, and let $v \\in \\mathbb{N}$. Then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left( \\left| C_v \\right| = k \\right) \\stackrel{  }{\\longrightarrow} \\rho_k(c).\\] In particular, if $N_k(G)$ denotes the number of vertices in a component of size $k$, so $k$ times the number of such components, then \\[\\mathbb{E} \\left[ N_k(G(n,p)) \\right] \\sim n \\rho_k(c).\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; LHS is the probability that the exploration process $(Y_t)$ hits zero after exactly $k$ steps, RHS is the same for $(Y_t ^{bp})$. Since both RW's can only take steps of size at least $-1$, there is a fixed finite set $S \\subset (\\mathbb{Z}_{\\ge -1})^k$ of possible steps the random walks can take, so have to show that, for fixed $s=(y_1,\\ldots ,y_k = 0)\\in S$, the probability that $Y$ takes that path converges to the probability that $Y^{bp}$ takes that path. We have, with $r_t := y_t - y_{t-1} + 1$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(Y) = \\prod_{t=1}^k \\underbrace{\\mathbb{P}\\left( \\text{Bin}(n - (t-1) - y_{t-1},p) = r_t \\right) }_{\\stackrel{  }{\\longrightarrow} \\mathbb{P} (\\text{Po}(c) = r_t)} \\stackrel{  }{\\longrightarrow} \\mathbb{P}(Y^{bp}).&nbsp;&nbsp;&nbsp; \\]<br><br>&nbsp;&nbsp;&nbsp; The second statement follows from \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ N_k(G(n,p)) \\right] = \\sum_{v\\in G(n,p)} \\mathbb{P}(\\left| C_v \\right| = k) = n \\mathbb{P}(\\left| C_1 \\right| = k).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Definition (good) rate function $I\\colon E \\to [0,\\infty]$ for Polish $E$, and (weak) large deviation principle. In that case, what do we know about $P_\\varepsilon(B)$ asymptotically for $B\\in \\mathcal{B}(E)$?",
    "back": "<div><b>Definition.</b> Let $E$ be a Polish space. Then a lower semi-continuous function $I\\colon E\\to [0,\\infty]$ with $\\inf I &lt; \\infty$ is called a <i>rate function</i>. It is a <i>good</i> rate function if $I_c := \\left\\{ I \\le&nbsp; c \\right\\} \\subset E$ is compact for all $c \\ge 0$.<br><br><b>Definition.</b> If $I$ is a rate function, then a family $(P_\\varepsilon \\colon \\varepsilon &gt; 0)$ of probabilities on $E$ satisfies the <i>large deviation principle (LDP) with rate $I$</i> if<br><ol>  <li>$\\limsup_{\\varepsilon \\to 0} \\varepsilon \\log P_\\varepsilon(F) \\le -\\inf_F I$ for all $F\\subset E$ closed,&nbsp;&nbsp;&nbsp;</li>  <li>$\\liminf_{\\varepsilon\\to o} \\varepsilon\\log P_\\varepsilon(G) \\ge -\\inf_G I$ for all $G\\subset E$ open.</li></ol><br>It satisfies the <i>weak LDP</i> if (i) holds only for $K\\subset E$ compact.<br><br>If LDP is satisfied, and $B\\in \\mathcal{B}(E)$ with $\\inf_{\\overline{B}}I = \\inf_{B^\\circ} I$, then $\\lim_{\\varepsilon\\to 0} \\varepsilon\\log P_\\varepsilon(B) = -\\inf_B I$, so \\[&nbsp;&nbsp;&nbsp; P_\\varepsilon(B) \\approx \\exp \\left( -\\frac{\\inf_B I}{\\varepsilon} \\right) ,\\quad \\varepsilon &gt; 0.\\]</div><div></div>"
  },
  {
    "front": "Definition (and heuristic meaning) of exponential tightness of a family $(P_\\varepsilon\\colon \\varepsilon &gt; 0)$ on a Polish space $E$.",
    "back": "<div><b>Definition.</b> Let $E$ be a Polish space. Then a family $(P_\\varepsilon\\colon \\varepsilon&gt; 0)$ of probabilities on $E$ is called <i>exponentially tight</i> if, for every $c &gt; 0$, there exists a compact set $K_c \\subset E$ such that \\[&nbsp;&nbsp;&nbsp; \\limsup_{\\varepsilon \\to 0} \\varepsilon \\log P_\\varepsilon \\left( E \\setminus K_c \\right) \\le -c,\\\\\\] that is, $P_\\varepsilon(E\\setminus K_c) \\lesssim \\mathrm{e}^{-c / \\varepsilon}$.</div><div></div>"
  },
  {
    "front": "Under which assumption does weak LDP with a rate function $I\\colon E\\to [0,\\infty]$ imply that $I$ is good and (strong) LDP holds?",
    "back": "<div><b>Theorem.</b> Let $E$ be a Polish space, and suppose that $(P_\\varepsilon\\colon \\varepsilon &gt; 0)$ satisfies the weak LDP with a rate function $I\\colon E\\to [0,\\infty]$, and is exponentially tight. Then<br><ol>  <li>$I$ is a good rate function,&nbsp;&nbsp;&nbsp;</li>  <li>$(P_\\varepsilon)$ satisfies the LDP with rate $I$.</li></ol><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $c &gt; 0$ and choose $K_c \\subset E$ according to exponential tightness. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -c \\ge \\limsup_{\\varepsilon\\to 0} \\varepsilon \\log P_\\varepsilon(E\\setminus K_c) \\ge \\liminf_{\\varepsilon \\to 0}\\varepsilon \\log P_\\varepsilon(E\\setminus K_c) \\ge -\\inf_{E\\setminus K_c} I,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $\\inf_{E\\setminus K_c} \\ge c$ and thus $I_c \\subset K_{c+1}$ is compact.&nbsp;&nbsp;&nbsp;</li>  <li>Have to prove the upper bound, so let $S\\subset E$ be closed. For any $c &gt; 0$, we have&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\limsup_{\\varepsilon\\to 0} \\varepsilon \\log P_\\varepsilon(S)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\limsup_{\\varepsilon\\to 0} \\varepsilon\\log P_\\varepsilon(S\\cap K_c) \\vee \\limsup_{\\varepsilon\\to 0}\\varepsilon\\log P_\\varepsilon(E\\setminus K_c)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le (-\\inf_{S\\cap K_c} I) \\vee (-c)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le - (\\inf_S I \\wedge c),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp; which implies the claim with $c\\to \\infty$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "If $I\\colon E\\to [0,\\infty]$ is a good rate function, then it achieves its minimum on closed sets. More generally, $\\Phi - I$ achieves its supremum for what kind of $\\Phi$?",
    "back": "<div><b>Proposition.</b> Let $E$ be Polish and $I\\colon E\\to [0,\\infty]$ a good rate function. Then, if $\\Phi \\colon E \\to [-\\infty,\\infty)$ is upper semi-continuous and $S\\subset E$ is closed with $\\sup_S \\Phi &lt; \\infty$, then \\[&nbsp;&nbsp;&nbsp; \\Phi(s_0) - I(s_0) = \\sup_S (\\Phi - I)\\] for some $s_0 \\in S$. In particular, $I$ achieves minima on closed sets.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\sup_S(\\Phi - I) = -\\infty$, we are done. Otherwise $\\sup_S(\\Phi - I) \\in (-\\infty,\\infty)$, so for any $n\\in \\mathbb{N}$ there exists $s_n \\in S$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_S(\\Phi - I) - \\frac{1}{n} \\le \\Phi(s_n) - I(s_n) \\le \\sup_S(\\Phi - I),\\quad n\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp; \\] Then $I(s_n) \\le |\\sup_S(\\Phi - I)| + \\sup_S \\Phi + 1$ for all $n\\in \\mathbb{N}$, so WLOG $s_n \\to s_0 \\in S$ because $I$ is good. Then \\[&nbsp;&nbsp;&nbsp; \\sup_S(\\Phi - I) = \\limsup_{n\\to \\infty} \\left( \\Phi(s_n) - I(s_n) \\right) \\le \\Phi(s_0) - I(s_0),\\\\&nbsp;&nbsp;&nbsp; \\] because $\\Phi - I$ is upper semi-contiuous.</i> </p> </div><div></div>"
  },
  {
    "front": "If $I\\colon E\\to [0,\\infty]$ is a good rate function, then $\\inf_{S^\\delta} I \\uparrow \\inf_S I$ for any closed $S\\subset E$.",
    "back": "<div><b>Proposition.</b> Let $E$ be Polish and $I\\colon E\\to [0,\\infty]$ a good rate function. Then if $S\\subset E$ is closed, \\[\\inf_{S^\\delta} I \\uparrow \\inf_S I.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\inf_{S^\\delta} I \\uparrow l \\le \\inf_S I$. Now take $s_n \\in S^{1 / n}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\inf_n I := \\inf_{S^{1 / n}}I \\le I(s_n) \\le \\inf_n I + \\frac{1}{n},\\quad n\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp; \\] Then $I(s_n) \\le \\inf_S I + 1$, so WLOG $s_n \\to s_0\\in S$, so \\[&nbsp;&nbsp;&nbsp; \\liminf_{n\\to \\infty} (\\inf_n I) \\ge \\liminf_{n\\to \\infty} I(s_n) \\ge I(s_0) \\ge \\inf_S I.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><div></div></div>"
  },
  {
    "front": "Varadhan's theorem on functional integration and LDPs, lower bound.",
    "back": "<div><b>Proposition (Varadhan, lower bound).</b> Let $E$ be a Polish space, and suppose $(P_\\varepsilon\\colon \\varepsilon&gt;0)$ satisfies the weak LDP with rate function $I\\colon E\\to [0,\\infty]$. Then, if $\\Phi\\colon E \\to (-\\infty,\\infty]$ is lower semi-continuous, \\[&nbsp;&nbsp;&nbsp; \\varliminf_{\\varepsilon\\to 0} \\varepsilon \\log \\int \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\ge \\sup_{\\Phi \\wedge I &lt; \\infty} (\\Phi - I).\\] <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $s\\in E$ such that $\\Phi(s) \\wedge I(s) &lt; \\infty$. Then, for any $\\delta &gt; 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varliminf_{\\varepsilon \\to 0} \\varepsilon \\log \\int \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\varliminf_{\\varepsilon \\to 0}\\varepsilon\\log \\int_{B(s,\\delta)} \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\inf_{B(s,\\delta)} \\Phi + \\varliminf_{\\varepsilon \\to 0} \\varepsilon \\log P_\\varepsilon(B(s,\\delta))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\inf_{B(s,\\delta)}\\Phi - \\inf_{B(s,\\delta)} I\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\inf_{B(s,\\delta)}\\Phi - I(s).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Since $\\Phi$ is lower semi-continuous, we have $\\inf_{B(s,\\delta)}\\Phi \\uparrow \\Phi(s)$ as $\\delta \\to 0$, which finishes the proof. Indeed, we may choose $s_n \\to s$ and $\\Phi(s_n) \\le \\inf_{B(s,1 / n)}\\Phi + \\frac{1}{n}$, and $\\varliminf_n \\Phi(s_n) \\ge \\Phi(s)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Varadhan's theorem on functional integration and LDPs, upper bound.",
    "back": "<div><b>Proposition (Varadhan, upper bound).</b> Let $ E$ be a Polish space and suppose that $(P_\\varepsilon\\colon \\varepsilon &gt; 0)$ satisfies the LDP with a good rate function $I\\colon E\\to [0,\\infty]$, such that \\[\\varlimsup_{\\varepsilon \\to 0} \\varepsilon \\log \\int_{\\left\\{ \\Phi \\ge c \\right\\} } \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\stackrel{  }{\\longrightarrow} -\\infty,\\quad c \\to \\infty.\\] Then, if $\\Phi\\colon E\\to [-\\infty,\\infty)$ is upper semi-continuous, \\[\\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\int \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\le \\sup(\\Phi- I).\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume first that $\\Phi \\le M$ for some $M &gt; 0$. Then, for any $c &gt; 0$, and any $\\delta &gt; 0$, we may cover $I_c = \\left\\{ I \\le c \\right\\} $ with finitely many balls $B_i := B(s_i,r_i)$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{\\overline{B_i}} \\Phi \\le \\Phi(s_i) +&nbsp; \\delta,\\qquad \\inf_{\\overline{B_i}}I \\ge I(s_i) -&nbsp; \\delta.&nbsp;&nbsp;&nbsp; \\] Then put $G:= \\bigcup_{i} B_i \\supset I_c$, and<br>&nbsp; &nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\int\\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}}\\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\varlimsup_{\\varepsilon\\to 0} \\varepsilon \\log \\int_{E\\setminus G} \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\vee \\max_i \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\int_{B_i} \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\Big(M+ \\varlimsup_{\\varepsilon\\to 0} \\varepsilon \\log P_\\varepsilon(E\\setminus G)\\Big) \\vee \\max_i \\Big(\\sup_{\\overline{B_i}} \\Phi + \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log P_\\varepsilon(\\overline{B_i})\\Big)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\Big( M - \\inf_{E\\setminus G} I\\Big) \\vee \\max_i \\Big(\\Phi(s_i) - I(s_i) + 2\\delta\\Big)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\big(M - c\\Big) \\vee \\sup(\\Phi - I) + 2\\delta.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Letting $\\delta \\to 0$ and $c \\to \\infty$ finishes the proof in this case.<br><br>&nbsp;&nbsp;&nbsp; Now let $\\Phi$ be general. Then, for any $c &gt; 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\int \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\varlimsup_{\\varepsilon\\to 0} \\varepsilon \\log \\int \\mathrm{e}^{\\frac{\\Phi \\wedge c}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\vee \\varlimsup_{\\varepsilon\\to 0}\\varepsilon \\log \\int_{\\left\\{ \\Phi \\ge c \\right\\} }\\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}}\\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\sup(\\Phi \\wedge c - I) \\vee X\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\sup(\\Phi - I) \\vee X,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and $X\\to -\\infty$ as $c \\to \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Varadhan's theorem on functional integration and LDPs, and sufficient criterion for technical assumption.",
    "back": "<div><b>Theorem (Varadhan).</b> If $E$ is Polish, and $(P_\\varepsilon\\colon \\varepsilon&gt; 0)$ satisfies LDP with good rate function $I\\colon E\\to [0,\\infty]$, and $\\Phi\\colon E\\to \\mathbb{R}$ is continuous with<br>\\begin{equation}\\label{eq:tech}<br>\\varlimsup_{\\varepsilon\\to 0}\\varepsilon \\log \\int_{\\left\\{ \\Phi\\ge c \\right\\} } \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon \\stackrel{  }{\\longrightarrow} -\\infty,\\quad c \\to \\infty<br>\\end{equation} then \\[\\lim_{\\varepsilon\\to 0}\\varepsilon\\log \\int \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}}\\mathop{}\\!\\mathrm{d} P_\\varepsilon = \\sup(\\Phi - I).\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Follows from the proofs of upper and lower bound.</i> </p> <br><br><b>Lemma.</b> If, in the above context, there exists $\\gamma &gt; 0$ with \\[&nbsp;&nbsp;&nbsp; \\sup_{0&lt;\\varepsilon\\le 1} \\left( \\int \\mathrm{e}^{(1+\\gamma) \\frac{\\Phi}{\\varepsilon}}\\mathop{}\\!\\mathrm{d} P_\\varepsilon \\right) ^\\varepsilon &lt; \\infty,\\\\\\] then the technical assumption (1) holds.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $ \\boldsymbol{1}_{\\left\\{ \\Phi \\ge c \\right\\} } \\le \\mathrm{e}^{\\gamma (\\Phi - c) / \\varepsilon}$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varepsilon \\log \\int_{\\left\\{ \\Phi\\ge c \\right\\} } \\mathrm{e}^{\\frac{\\Phi}{\\varepsilon}} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\varepsilon \\log \\int \\mathrm{e}^{(1+\\gamma) \\frac{\\Phi}{\\varepsilon}} \\mathrm{e}^{-\\gamma c / \\varepsilon} \\mathop{}\\!\\mathrm{d} P_\\varepsilon\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= -\\gamma c + \\log \\left( \\int \\mathrm{e}^{(1+\\gamma) \\frac{\\Phi}{\\varepsilon}}\\mathop{}\\!\\mathrm{d} P_\\varepsilon \\right) ^{\\varepsilon}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le -\\gamma c + \\log M,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; which tends to zero as $c\\to \\infty$, independent of $\\varepsilon$.</i> </p> </div><div></div>"
  },
  {
    "front": "Main theorem on diffusion approximation with discrete-time Markov chains.",
    "back": "<div><b>Theorem.</b> Suppose that $a\\colon \\mathbb{R} ^d\\to \\mathbb{R}^{d\\times d}$, and $b\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ are locally bounded and measurable, and such that the martingale problem $M(a,b)$ is well-posed. Now suppose $(Y^h\\colon h&gt;0)$ is a family of discrete-time Markov chains with state space $E_h\\subset \\mathbb{R}^d$, such that, if<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; a^h&amp;\\colon E_h \\to \\mathbb{R} ^{d\\times d} ;\\, x\\mapsto \\frac{1}{h} \\mathbb{E} \\left[(Y_1^h - x) (Y_1^h-x)^\\top \\boldsymbol{1}_{\\left\\{ \\left| Y^h_1 - x \\right| \\le 1 \\right\\} } \\,\\middle\\vert\\, Y^h_0 = x\\right],\\\\&nbsp;&nbsp;&nbsp; b^h&amp;\\colon E_h\\to \\mathbb{R}^d ;\\, x\\mapsto \\frac{1}{h} \\mathbb{E} \\left[\\left( Y^h_1 - x \\right) \\boldsymbol{1}_{\\left\\{ \\left| Y^h_1 - x \\right| \\le 1 \\right\\} } \\,\\middle\\vert\\, Y^h_0 = x\\right],\\\\&nbsp;&nbsp;&nbsp; \\Delta^h_\\varepsilon&amp;\\colon E_h \\to [0,\\infty) ;\\, x\\mapsto \\frac{1}{h} \\mathbb{P}\\left( \\left| Y^h_1 - x \\right| \\ge \\varepsilon \\,\\middle\\vert\\, Y^h_0 = x \\right) ,\\\\\\end{align*}[/$$]<br>then $a^h\\to a$, $b^h \\to b$, and $\\Delta^h_\\varepsilon \\to 0$ ($\\forall \\varepsilon&gt;0$) locally uniformly, and $Y^h_0 \\equiv x^h_0 \\to x_0$, then the chains $X^h := (Y^h_{\\left\\lfloor \\frac{t}{h} \\right\\rfloor })_{0\\le t \\le 1}$ converge in distribution to the unique solution of $M(a,b)$.</div><div></div>"
  },
  {
    "front": "Main theorem on diffusion approximation with continuous-time Markov processes.",
    "back": "<div><b>Theorem.</b> Suppose that $a\\colon \\mathbb{R} ^d\\to \\mathbb{R}^{d\\times d}$, and $b\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ are locally bounded and measurable, and such that the martingale problem $M(a,b)$ is well-posed. Now suppose $(X^h\\colon h&gt;0)$ is a family of continuous-time Markov processes with state space $E_h\\subset \\mathbb{R}$, such that, putting \\[&nbsp;&nbsp;&nbsp; K_h(x,\\cdot ) := \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} P^h_t(x,\\cdot ) = \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{P}^x \\left( X^h_t \\in \\cdot&nbsp; \\right) ,\\\\\\] we have $\\sup_{x\\in E_h \\cap K} K_h(x,\\mathbb{R}^d\\setminus \\left\\{ x \\right\\} ) &lt; \\infty$ for every compact set $K\\subset \\mathbb{R}^d$, and<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; a^h &amp;\\colon E_h\\to \\mathbb{R} ^{d\\times d};\\, x\\mapsto \\int_{\\left\\{ \\left| y-x \\right| \\le 1 \\right\\} } (y-x) (y-x)^\\top K_h(x,\\mathop{}\\!\\mathrm{d} y),\\\\&nbsp;&nbsp;&nbsp; b^h &amp;\\colon E_h \\to \\mathbb{R}^d;\\, x \\mapsto \\int_{\\left\\{ \\left| y-x \\right| \\le 1 \\right\\} } (y-x) K_h(x,\\mathop{}\\!\\mathrm{d} y),\\\\&nbsp;&nbsp;&nbsp; \\Delta^h_\\varepsilon&amp;\\colon E_h \\to \\mathbb{R}^d;\\, x\\mapsto K_h(x, B(x,\\varepsilon)^{c}),\\\\\\end{align*}[/$$]<br>then $a^h\\to a$, $b^h \\to b$, and $\\Delta^h_\\varepsilon \\to 0$ ($\\forall \\varepsilon&gt;0$) locally uniformly, and $X^h_0 \\equiv x^h_0 \\to x_0$, then the chains $(X^h_t)_{0\\le t \\le 1}$ converge in distribution to the unique solution of $M(a,b)$.</div><div></div>"
  },
  {
    "front": "Asymptotic behaviour of number $N_k$ of vertices in a component of size exactly $k$ in $G(n,p)$, if $np \\to c &gt; 0$.",
    "back": "<div><b>Proposition.</b> Let $k\\in \\mathbb{N}$, and $np \\stackrel{  }{\\longrightarrow} c &gt; 0$, and denote by $N_k$ the number of vertices in $G(n,p)$ whose component has size $k$. Then \\[&nbsp;&nbsp;&nbsp; \\frac{N_k}{n} \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\rho_k(c),\\\\\\] which is the probability that a $\\text{Poi}(c)$-GW process has size exactly $k$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We already know that $\\mathbb{E} \\left[ N_k \\right] \\sim n \\rho_k(c)$, so expectation converges and it suffices to show that $\\mathbb{E} \\left[ \\left( \\frac{N_k}{n} \\right) ^2 \\right] \\stackrel{  }{\\longrightarrow} \\rho_k(c)^2$. Putting $I_v := \\left\\{ \\left| C_v \\right| =k \\right\\} $, we have \\[&nbsp;&nbsp;&nbsp; N_k = \\sum_{v,w} I_v I_w = \\sum_{v,w} I_v I_w \\boldsymbol{1}_{\\left\\{ C_v = C_w \\right\\} } + \\sum_{v,w} \\boldsymbol{1}_{\\left\\{ C_v \\neq C_w \\right\\} } I_v I_w = A + B.&nbsp;&nbsp;&nbsp; \\] Then \\[&nbsp;&nbsp;&nbsp; A = \\sum_v I_v \\sum_w I_w \\boldsymbol{1}_{\\left\\{ C_v = C_w \\right\\} } = \\sum_v I_v k = k N_k \\le kn,\\\\&nbsp;&nbsp;&nbsp; \\] and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ B \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= n \\mathbb{E} \\left[ I_1 \\sum_w I_w \\boldsymbol{1}_{\\left\\{ C_1 \\neq C_w \\right\\} } \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= n \\mathbb{P}\\left( \\left| C_1 = k \\right|&nbsp; \\right) \\mathbb{E} \\left[N_k(\\mathcal{G} - C_1) \\,\\middle\\vert\\, \\left| C_1 \\right| = k\\right].&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now if $1 \\in C\\subset \\mathbb{N}$ with $\\left| C \\right| = k$ is fixed, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ N_k(\\mathcal{G}-C) \\,\\middle\\vert\\, C_1 = C \\right] = \\mathbb{E} \\left[ N_k(\\mathcal{G}-C) \\right] = \\mathbb{E} \\left[ N_k(\\mathcal{G}(n-k,p)) \\right] \\sim n \\rho_k(c),\\\\&nbsp;&nbsp;&nbsp; \\] so \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ B \\right] \\sim n \\rho_k(c) \\cdot n \\rho_k(c) = n^2\\rho_k(c)^2,\\\\\\] and so $\\mathbb{E} \\left[ N_k^2 \\right] \\sim n^2 \\rho_k(c)^2$ as required.</i> </p> </div><div></div>"
  },
  {
    "front": "How does LDP of $(Z^\\varepsilon\\colon \\varepsilon&gt;0)$ on $E$ with good rate $I\\colon E\\to [0,\\infty]$ transfer to $(f(Z^\\varepsilon)\\colon \\varepsilon&gt;0)$ for $f\\colon E\\to E'$ continuous?",
    "back": "<div><b>Proposition.</b> Suppose $E,E'$ are Polish spaces, $f\\colon E\\to E'$ is continuous, and $I\\colon E\\to [0,\\infty]$ is a good rate function. Then \\[&nbsp;&nbsp;&nbsp; I'(s') := \\inf \\left\\{ I(s)\\colon s\\in E, f(s) = s' \\right\\} = \\inf_{f^{-1}(s')} I\\\\\\] defines a good rate function. If $(Z^\\varepsilon\\colon \\varepsilon&gt;0)$ are $E$-valued random variables that satisfy LDP with rate $I$, then $(X^\\varepsilon:= f(Z^\\varepsilon)\\colon \\varepsilon &gt; 0)$ satisfy LDP with rate $I'$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First observe that the infinmum is always attained because $I$ is good and $f^{-1}(s')$ closed for all $s'\\in E'$. If $A\\subset E'$, then<br>&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\inf_A I'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\inf_{s\\in A} \\inf _{f^{-1}(s)}I = \\inf_{f^{-1}(A)}I.&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp; Thus, $\\inf I' = \\inf I &lt; \\infty$, $I\\ge 0$. Now, if $(s_n') \\in {I'_c}^{\\mathbb{N}}$ for some $c \\ge 0$, take $s_n \\in E$ with $f(s_n) = s_n'$ and $I(s_n) = I'(s_n') \\le c$, so WLOG $s_n \\to s\\in E$, so $s_n' = f(s_n) \\to f(s) =: s'$, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I'(s') \\le I(s) \\le \\varliminf_{n\\to \\infty} I(s_n) \\le c,\\\\&nbsp;&nbsp; \\] so $s'\\in I'_c$. Thus $I'_c$ is compact, so $I'$ is lower semi-continuous and good. Now, if $F\\subset E'$ is closed, so is $f^{-1}(F)$, thus \\[&nbsp;&nbsp; \\varliminf_{\\varepsilon\\to 0} \\varepsilon\\log \\mathbb{P}\\left( X^\\varepsilon \\in F \\right) = \\varliminf_{\\varepsilon\\to 0} \\varepsilon\\log \\mathbb{P}(Z^\\varepsilon \\in f^{-1}(F) \\le -\\inf_{f^{-1}(F)}I = -\\inf_F I',\\\\&nbsp;&nbsp; \\] and similarly for $G\\subset E'$ open.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of exponential convergence $X^\\varepsilon_n \\stackrel{  }{\\longrightarrow} X^\\varepsilon$ in the context of LDPs. In that case, if $(X^\\varepsilon_n\\colon \\varepsilon&gt;0)$ satisfies LDP with good rate $I_n$, what can you say about $\\varlimsup_{\\varepsilon\\to 0}\\varepsilon\\log\\mathbb{P}(X^\\varepsilon\\in S)$ for $S\\subset E$ closed, and analogously with $G\\subset E$ open?",
    "back": "<div><b>Definition/Lemma.</b> If $(X^\\varepsilon_n\\colon \\varepsilon&gt;0),\\,n\\in \\mathbb{N},$ and $(X^\\varepsilon\\colon \\varepsilon&gt;0)$ are families of random variables with values in a Polish space $E$, we say that $X^\\varepsilon_n$ goes to $X^\\varepsilon$ <i>exponentially</i> if, for every $\\delta &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\lim_{n\\to \\infty} \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\mathbb{P}\\left( \\rho(X^\\varepsilon_n,X^\\varepsilon) \\ge \\delta \\right) = -\\infty.\\] In that case, if $(X^\\varepsilon_n\\colon \\varepsilon&gt;0)$ satisfies LDP with good rate $I_n$, then \\[\\varlimsup_{\\varepsilon\\to 0}\\varepsilon\\log \\mathbb{P}(X^\\varepsilon\\in S) \\le -\\lim_{\\delta \\to 0} \\varlimsup_{n\\to \\infty} \\inf_{s\\in \\overline{S^\\delta}}I_n(s),\\\\\\] for $S\\subset E$ closed, and, for $G\\subset E$ open, \\[\\varliminf_{\\varepsilon\\to 0} \\varepsilon\\log \\mathbb{P}(X^\\varepsilon\\in G) \\ge - \\varliminf_{n\\to \\infty} \\inf_{B_s(\\delta)} I_n,\\\\\\] for every $s\\in G$ and $\\delta &gt; 0$ such that $B_s(2\\delta) \\subset G$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $S\\subset E$ closed. Then, for any $\\varepsilon&gt; 0$ and $\\delta &gt; 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(X^\\varepsilon\\in S) \\le \\mathbb{P}(X^\\varepsilon_n \\in S^\\delta) + \\mathbb{P}(\\rho(X^\\varepsilon_n,X^\\varepsilon) \\ge \\delta),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0} \\varepsilon \\log \\mathbb{P}(X^\\varepsilon\\in S)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le (-\\inf_{s\\in \\overline{S^\\delta}} I_n(s) )\\vee \\varlimsup_{\\varepsilon\\to 0}\\varepsilon\\log \\mathbb{P}(\\rho(X^\\varepsilon_n,X^\\varepsilon) \\ge \\delta),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; which yields the claim by passing to $\\varliminf_{n\\to \\infty}$ on the RHS, and then $\\delta \\to 0$. Now let $G\\subset E$ be open and $B(s,2\\delta) \\subset G$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(X^\\varepsilon_n \\in B(s,\\delta)) \\le \\mathbb{P}(X^\\varepsilon\\in G) + \\mathbb{P}(\\rho(X^\\varepsilon_n,X^\\varepsilon) \\ge \\delta),\\\\&nbsp;&nbsp;&nbsp; \\] and thus \\[&nbsp;&nbsp;&nbsp; -\\inf_{B_s(\\delta)} I_n \\le \\varliminf_{\\varepsilon\\to 0}\\varepsilon\\log\\mathbb{P}(X^\\varepsilon\\in G) \\vee \\varliminf_{\\varepsilon\\to 0}\\varepsilon\\log\\mathbb{P}(\\rho(X^\\varepsilon_n,X^\\varepsilon)\\ge \\delta),\\\\&nbsp;&nbsp;&nbsp; \\] yielding the claim with $n\\to \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of two families $(X^\\varepsilon\\colon \\varepsilon&gt;0)$ and $(Y^\\varepsilon\\colon \\varepsilon&gt;0)$ being exponentially close, and how LDP with good rate transfers in this case.",
    "back": "<div><b>Proposition.</b> If $(X^\\varepsilon\\colon \\varepsilon&gt; 0)$, and $(Y^\\varepsilon\\colon \\varepsilon&gt;0)$ are families of random variables taking values in a Polish space $E$, that are <i>exponentially close</i> in the sense that \\[&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log\\mathbb{P}(\\rho(X^\\varepsilon,Y^\\varepsilon)\\ge \\delta) = -\\infty\\\\\\] for all $\\delta &gt; 0$, then, if $(Y^\\varepsilon\\colon \\varepsilon&gt;0)$ satisfies LDP with a good rate $I$, so does $(X^\\varepsilon\\colon \\varepsilon&gt;0)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have that $Y^\\varepsilon$ goes to $X^\\varepsilon$ exponentially. Let $S\\subset E$ be closed. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0}\\varepsilon\\log \\mathbb{P}(X^\\varepsilon\\in S) \\le - \\lim_{\\delta \\to 0} \\inf_{\\overline{S^\\delta}} I = - \\inf_S I.&nbsp;&nbsp;&nbsp; \\] If $G\\subset E$ is open, and $B(s,2\\delta)\\subset G$, then \\[&nbsp;&nbsp;&nbsp; \\varliminf_{\\varepsilon\\to 0} \\varepsilon\\log\\mathbb{P}(X^\\varepsilon\\in G) \\ge -\\inf_{B(s,\\delta)} I \\ge -I(s).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "General contraction principle for LDPs.",
    "back": "<div><b>Theorem.</b> Suppose $E,E'$ are Polish spaces, $I$ is a good rate on $E$, $f_n\\colon E\\to E',\\,n\\in \\mathbb{N},$ are continuous and converge uniformly on sets $\\left\\{ I\\le c \\right\\} $ to a function $f\\colon \\left\\{ I&lt;\\infty \\right\\} \\to E'$. Put<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; I_n'(s') &amp;:= \\inf \\left\\{ I(s)\\colon s\\in E, f_n(s) = s' \\right\\} \\\\&nbsp;&nbsp;&nbsp; I'(s') &amp;:= \\inf \\left\\{ I(s)\\colon s\\in E, I(s)&lt;\\infty, f(s) = s' \\right\\} ,\\\\\\end{align*}[/$$]<br>which are good rates on $E'$. Suppose that $(X^\\varepsilon\\colon \\varepsilon&gt;0)$ satisfy LDP with $I_n'$ for $n\\in \\mathbb{N}$, and that $X^\\varepsilon_n \\stackrel{  }{\\longrightarrow} X^\\varepsilon$ exponentially. Then $(X^\\varepsilon\\colon \\varepsilon &gt; 0)$ satisfies LDP with rate $I'$.<br><p><i><b>Proof.</b>We already know that $I_n'$ are good rates. By a similar argument, $I$ is as well, and both infimums are always attained (For $I'$, note that if $I(s_n) \\to I'(s')$ with $s_n \\in \\left\\{ I &lt; \\infty \\right\\} \\cap f^{-1}(s')$, then $s_n \\in \\left\\{ I\\le I'(s') + 1 \\right\\} $, so $s_{k(n)}\\to s\\in \\left\\{ I&lt;\\infty \\right\\} $ and $f(s) = s'$, and $I(s) = I'(s')$).<br><br>Let $G\\subset E'$ be open, and $s'\\in G$. Choose $\\delta &gt; 0$ with $B(s',2\\delta) \\subset G$, so that \\[&nbsp;&nbsp;&nbsp; \\varliminf_{\\varepsilon\\to 0}\\varepsilon\\log\\mathbb{P}(X^\\varepsilon\\in G) \\ge -\\varliminf_{n\\to \\infty} \\inf_{B(s',\\delta)} I_n' \\stackrel{!}{\\ge} -I'(s').&nbsp;&nbsp;&nbsp; \\] Indeed, this is clear if $I'(s') = \\infty$. Otherwise, let $s\\in \\left\\{ I&lt;\\infty \\right\\} $ with $f(s) = s'$ and $I(s) = I'(s')$. Then $f_n(s) \\in B(s',\\delta)$ for large $n$ because $f_n(s) \\to f(s) = s'$, so \\[&nbsp;&nbsp;&nbsp; \\inf_{B(s',\\delta)}I_n' = \\inf_{f_n^{-1}(B(s',\\delta))} I \\le I(s) = I'(s')\\] for large $n$.<br><br>Now let $S\\subset G$ be closed We know that \\[&nbsp;&nbsp;&nbsp; \\varlimsup_{\\varepsilon\\to 0} \\varepsilon\\log \\mathbb{P}(X^\\varepsilon\\in S) \\le -\\lim_{\\delta \\to 0} \\varlimsup_{n\\to \\infty} \\inf_{\\overline{S^\\delta}}I_n' =: -l,\\\\\\] so need to show that $l \\ge \\inf_{S} I'$. For $n\\in \\mathbb{N}$, choose $s_n' \\in S^{1 / n}$ with $I_n'(s_n') \\le l + \\frac{1}{n}$, and take $s_n \\in E$ with $f(s_n) = s_n'$ and $I(s_n) = I'_n(s_n')$. So $s_n \\in I_{l+1}$, so $s_n \\to s$ in $I_{l+1}$, so $s_n' = f_n(s_n) \\to f(s) =: s' \\in S$. Then, \\[\\inf_S I' \\le I'(s') \\le I(s) \\le \\varliminf_{n\\to \\infty} I(s_n) = l.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Asymptotic behaviour of number of vertices in components of size at most $\\omega(n)\\uparrow \\infty$ in $\\mathcal{G}(n,\\frac{c}{n})$, if $\\omega(n)$ grows sufficiently slowly. (Sketch of proof; coupling of $\\text{Po}(c)$ and $\\text{Bin}(n,\\frac{c}{n})$).",
    "back": "<div><b>Lemma.</b> If $\\omega(n) \\to \\infty$, and $\\omega(n) \\le n^{1 / 4}$, and $G\\sim \\mathcal{G}(n,\\frac{c}{n})$, then \\[&nbsp;&nbsp;&nbsp; \\frac{N_{\\le \\omega(n)}(G)}{n} \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\eta(c).\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Firstly, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{k=1}^{\\omega(n)}\\rho_k(c) \\stackrel{  }{\\longrightarrow} \\sum_{k=1}^\\infty \\rho_k(c) = \\mathbb{P}\\left( \\left| \\boldsymbol X_{\\text{Po}(c)} \\right| &lt; \\infty \\right) = \\eta(c).&nbsp;&nbsp;&nbsp; \\] The claim then follows similar to how we proved that $N_k(G) / n \\stackrel{ \\mathbb{P} }{\\longrightarrow} \\rho_k(c)$ for fixed $k\\in\\mathbb{N}$, by considering error bounds more carefully. For that purpose, the following fact is useful.</i> </p> <br><br><b>Fact.</b> One can clearly couple $\\text{Po}(p)$ and $\\text{Bin}(1,p)$ to disagree with probability $O(p^2)$. Thus, one can couple $\\text{Po}(np)$ and $\\text{Bin}(n,p)$ to disagree with probability $O(np^2)$, and thus $\\text{Po}(c)$ and $\\text{Bin}(n,\\frac{c}{n})$ to disagree with probability $O(\\frac{1}{n})$.</div><div></div>"
  },
  {
    "front": "What is (whp) the size of the largest component in $\\mathcal{G}(n,\\frac{c}{n})$ at most, if $c \\in (0,1)$ (Phase transition in the subcritical case).",
    "back": "<div><b>Theorem.</b> Let $c \\in (0,1)$. Then there exists $A = A(c) &gt; 0$ such that with high probability, <i>every</i> vertex in $\\mathcal{G}(n,\\frac{c}{n})$ has component size at most $A\\log n$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For fixed $v\\in \\mathbb{N}$, and $k\\in \\mathbb{N}$, we have $\\mathbb{P}\\left( \\left| C_v \\right| \\ge k \\right) = \\mathbb{P}(Y_k &gt; 0)$, where $(Y_t)_{t\\ge 0}$ is the component exploration process started at $v$. By a simple coupling, this is at most $\\mathbb{P}(Y_k^+ &gt; 0)$, where $(Y_t^+)_{t\\ge 0}$ is a random walk started at $1$ with i.i.d. increments $\\sim \\text{Bin}(n,p)-1$. Thus,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| C_v \\right| &gt; k \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( Y_k &gt; 0 \\right) \\le \\mathbb{P}(Y_k^+ &gt; 0)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( 1 - k + \\text{Bin}(kn,p) &gt; 0 \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( \\text{Bin}(kn,p) \\ge k\\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( \\text{Bin}(kn,p) \\ge kc\\left(1 + \\frac{1-c}{c}\\right) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\exp \\left( -\\varepsilon^2 kc / 4 \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where $\\varepsilon = \\frac{1-c}{c}\\wedge 1\\in (0,1]$. Putting $A:= \\frac{8}{\\varepsilon^2 c}$, this gives $\\mathbb{P}\\left( \\left| C_v \\right| &gt; A \\log n \\right) \\le \\exp \\left( -2 \\log n \\right) = n^{-2}$, so a union bound yields \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\exists v \\in [n]\\colon \\left| C_v \\right| &gt; A \\log n&nbsp; \\right) \\le \\frac{1}{n} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Phase transition in $\\mathcal{G}(n,\\frac{c}{n})$ for $c &gt; 1$ (i.e. number of vertices in large and small components).",
    "back": "<div><b>Theorem.</b> Let $c &gt; 1$, and denote by $L_i(G)$ the size of the $i$'th largest component in $G\\sim \\mathcal{G}(n,\\frac{c}{n})$. Then there exists $A &gt; 0$ such that $L_2(G) \\le A\\log n$ whp, and \\[&nbsp;&nbsp;&nbsp; \\frac{L_1(G)}{n}\\stackrel{ \\mathbb{P} }{\\longrightarrow} 1 - \\eta(c).\\] Thus, whp, there are $\\sim n\\rho_k(c)$ vertices in components of size $k = O(\\log n)$, and there is one big component of size $\\sim n(1-\\eta(c))$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\delta &gt; 0$, and $(Y_t)$ be the exploration process of $G$. Then the drift (expected jump size) in step $t$ is, given $Y_{t-1}$, exactly $\\mathbb{E} \\left[ \\text{Bin}(U_{t-1},p) \\right] -1 = U_{t-1} \\frac{c}{n} - 1$, which is larger than $c\\delta$ iff \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; U_{t-1} \\ge n \\left( \\frac{1}{c} + \\delta \\right) \\iff n- U_{t-1} \\le n\\left( 1 - \\frac{1}{c} - \\delta \\right) =: k^+.&nbsp;&nbsp;&nbsp; \\] We define a random walk $(Y_t^-)$ with i.i.d. increments $\\sim \\text{Bin}(n - k^+,p) - 1$, so that $R_t^- \\le R_t$ as long as $U_{t-1} \\ge n - k^+$. Then, for $k\\le k^+$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\left| C_v \\right| = k \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( Y_1,\\ldots ,Y_{k-1} &gt; 0, Y_k = 0 \\right) \\le \\mathbb{P}(Y_k^- \\le 0),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; since, in the event of the LHS, $U_k = n - k \\ge n - k^+$. Now we can use Chernoff bounds on $Y_k^- \\sim 1 - k + \\text{Bin}(k(n-k^+),p)$ to obtain, for $k^- \\le k \\le k^+$, where $k^- := \\frac{6}{\\varepsilon^2}\\log n$, $\\mathbb{P}\\left( \\left| C_v \\right| = k \\right) \\le \\frac{1}{n^3}$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\exists v\\in V\\colon k^- \\le \\left| C_v \\right| \\le k^+&nbsp; \\right) \\le \\frac{1}{n} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp; \\] Thus, whp, all vertices are in small components, or big components, and there are whp $\\eta(c)n$ vertices in small components. It only remains to show that there is only one big component, which would follow from the existence of $\\delta &gt; 0$ with $2k^+ &gt; (1-\\eta(c))n$, i.e. from $1-\\frac{1}{c} &gt; \\frac{1-\\eta}{2}$, i.e. $\\eta &gt; \\frac{2}{c} - 1 =: x$. This is the case iff $\\mathrm{e}^{-c(1-x)}&gt;x$, which is true.</i> </p> </div><div></div>"
  },
  {
    "front": "In the context of diffusion approximation by Markov chains, the sequence $(X^h_t = Y^h_{\\lfloor t / h \\rfloor}))$ in $D([0,1],\\mathbb{R}^d)$ is tight.",
    "back": "<div>by Markov chains, the sequence $(X^h_t = Y^h_{\\lfloor t / h \\rfloor}))$ in $D([0,1],\\mathbb{R}^d)$ is tight.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By localisation, we may assume all locally uniform convergences are globally uniform. Tightness of initial values is clear because they are deterministic. Fix $\\varepsilon &gt; 0$, and define entropy times $(\\tau_n)$, $\\mathcal{N}(X^h,\\varepsilon)$, and $\\delta(X^h,\\varepsilon)$ as usual (where $T = 1$), as well as $\\theta := \\max_{0\\le t \\le 1} \\left| X^h(t) - X^h(t-) \\right| $. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall \\delta \\le \\delta(X^h,\\varepsilon)\\colon \\omega(X^h,\\delta) \\le 3\\varepsilon + \\theta.&nbsp;&nbsp;&nbsp; \\] Thus, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\omega(X^h,\\delta) &gt; 4\\varepsilon \\right) \\le \\mathbb{P}(\\theta &gt; \\varepsilon) + \\mathbb{P}(\\delta(X^h,\\varepsilon) &lt; \\delta)\\] Now the first term is easy, since \\[\\mathbb{P}(\\theta &gt; \\varepsilon) \\le \\frac{1}{h} \\sup_{y\\in E^h} \\Pi_h(y,B(y,\\varepsilon)^{c}) = \\sup_y \\Delta^h_\\varepsilon(y) \\stackrel{  }{\\longrightarrow} 0.\\] For the second term, it suffices to show that there exists $C_\\varepsilon &gt; 0$ such that $\\mathbb{P}(T_j - T_{j-1} \\le \\delta) = \\mathbb{P}(T_1\\le \\delta) \\le C_\\varepsilon \\delta$. Then, by standard entropy arguments, there exists $\\alpha \\in (0,1)$ such that \\[\\mathbb{P}\\left( \\delta(X^h,\\varepsilon) \\le \\delta \\right) \\le \\mathbb{P}\\left( \\mathcal{N}(X^h,\\varepsilon) &gt; k \\right) + \\mathbb{P}\\left( \\min_{j\\le k}(T_j - T_{j-1}) \\le \\delta \\right) \\le \\mathrm{e} \\alpha^k + kC_\\varepsilon\\delta,\\\\\\] which yields the claim. Putting $\\tau := \\inf \\left\\{ k\\ge 1\\colon \\left| Y^h_k - x_0^h \\right| &gt; \\varepsilon \\right\\} $, we have $T_1 = \\tau / h$. Let $f\\in C^2(\\mathbb{R}^d)$ with $f(0) = 1$, and $f(x) = 0$ if $\\left\\|x\\right\\|\\ge \\varepsilon$, and $\\left\\|f\\right\\|\\le 1$. Then, with $L^h f(x) := \\int (f(y) - f(x) ) K_h(x,\\mathop{}\\!\\mathrm{d} y)$, we have that $(f(Y^h_n) - \\sum_{k=0}^{n-1} h L^hf (Y^h_k))_{n\\ge 0}$ is a martingale, and since $\\left\\|L^h f\\right\\|\\le C_\\varepsilon$ independent of $h$(!), \\[(f(Y^h_k) + C_\\varepsilon h k)_{k\\ge 0}\\] is a submartingale(!), so <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{P}(T_1 \\le \\delta) = \\mathbb{P}(\\tau \\le \\delta / h) \\le \\mathbb{E} \\left[ 1 - f(Y^h_{\\tau \\wedge (\\delta / h)}) \\right] \\le hC_\\varepsilon \\delta / h = C_\\varepsilon \\delta.\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "In the context of diffusion approximation by Markov chains, the weak limit of any subsequence of $(X^h_t = Y^h_{\\left\\lfloor t / h \\right\\rfloor })$ in $D([0,1],\\mathbb{R}^d)$ is a solution to $M(a,b)$.",
    "back": "<div><b>Proposition.</b> In the context of diffusion approximation by Markov chains, the weak limit of any subsequence of $(X^h)$ in $D([0,1],\\mathbb{R}^d)$ is a solution to $M(a,b)$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; L f(x) := \\frac{1}{2}\\sum_{i,j=1}^d a_{ij}(x) \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x) + \\sum_{i=1}^d b_i(x) \\frac{\\partial f}{\\partial x_i}(x).&nbsp;&nbsp;&nbsp; \\] Then (!) $L^hf \\stackrel{  }{\\longrightarrow} Lf$ uniformly on $\\mathbb{R}^d$ for all $f\\in C^2_c(\\mathbb{R}^d)$. Now suppose that $h_n \\stackrel{  }{\\longrightarrow} 0$ and $X^{h_n}\\stackrel{ d }{\\longrightarrow} X$ for a $C([0,1],\\mathbb{R}^d)$-valued process $X$. STS that $X$ solves $(L,x_0)$. We know that \\[&nbsp;&nbsp;&nbsp; \\left( f(X^{h_n}_{h_n k}) - \\sum_{j=0}^{k-1} h_n L^{h_n} f(X^{h_n}_{jh_n}) \\right) _{k\\ge 0}&nbsp;&nbsp;&nbsp; \\] is a martingale, so whenever $0\\le s \\le t \\le 1$, put $l_n := \\left\\lfloor \\frac{t}{h_n} \\right\\rfloor $ and $k_n := \\left\\lfloor \\frac{s}{h_n} \\right\\rfloor $, so that, for any bounded, continuous, $\\mathcal{F}_s$-measurable $F\\colon D\\to \\mathbb{R}$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ F(X^{h_n}) \\left( f(X^{h_n}_{h_n l_n}) - f(X^{h_n}_{h_n k_n}) - \\sum_{j=k_n}^{l_n - 1} h_n L^{h_n}f(X^{h_n}_{jh_n}) \\right)&nbsp; \\right] = 0.&nbsp;&nbsp;&nbsp; \\] By Skorokhod, WLOG we have $X^{h_n}\\stackrel{ \\text{a.s.} }{\\longrightarrow} X$, so the above converges to \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ F(X) \\left( f(X_t) - f(X_s) - \\int_s^t L f(X_r) \\mathop{}\\!\\mathrm{d} r \\right)&nbsp; \\right] .\\] Thus, $\\left( f(X_t) - \\int_0^t Lf(X_s)\\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0}$ is a martingale, which finishes the proof.</i> </p> </div><div></div>"
  },
  {
    "front": "Method of duality. What do the conditions mean in the case of Markov processes, and how can it be generalised (Ethier \\&amp; Kurtz).",
    "back": "<div><b>Theorem.</b> Suppose $E_1,E_2$ are metric spaces and $\\mathbb{P}$ and $\\mathbb{Q}$ are probability measures on $D([0,\\infty),E_1)$ and $D([0,\\infty),E_2)$, respectively. Suppose that there exist bounded functions $f,g\\colon E_1\\times E_2\\to \\mathbb{R}$ that are continuous in both arguments, and such that the following holds.<br><ol>  <li>$\\left( f(X_t,y) - \\int_0^t g(X_s,y)\\mathop{}\\!\\mathrm{d} s \\right) _{t\\ge 0}$ defines a $\\mathbb{P}$-martingale for all $y\\in E_2$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\left( f(x,Y_t) - \\int_0^t g(x,Y_s)\\mathop{}\\!\\mathrm{d} s \\right)_{t\\ge 0} $ defines a $\\mathbb{Q}$-martingale for all $x\\in E_1$.</li></ol><br>Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E}^{\\mathbb{P}\\otimes \\mathbb{Q}} \\left[ f(X_t,Y_0) \\right] = \\mathbb{E}^{\\mathbb{P}\\otimes \\mathbb{Q}}\\left[ f(X_0, Y_t) \\right] .\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{P} \\left( f(X_t,y) \\right) &amp;= \\mathbb{P} \\left( g(X_t,y) \\right),\\quad y\\in E_2,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{Q}\\left( f(x,Y_t) \\right) &amp;= \\mathbb{Q} \\left( g(x,Y_t) \\right) ,\\quad x\\in E_1,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so, putting $h(r,s) := \\mathbb{E} \\left[ f(X_r,Y_s) \\right] $, $h_r(r,s) = h_s(r,s) = \\mathbb{E} \\left[ g(X_r,Y_s) \\right] =: h'(r,s) $, so $h$ is continuously differentiable and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} s} \\mathbb{E} \\left[ f(X_s,Y_{t-s}) \\right] = \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} s}h(s,t-s) = (1, -1) \\begin{pmatrix} h'(s,t-s) \\\\ h'(s,t-s) \\end{pmatrix} = 0,\\\\&nbsp;&nbsp;&nbsp; \\] for all $0\\le s \\le t$.</i> </p> <br><br><b>Remark.</b> If $X$ and $Y$ are Markov processes, the assertion holds if $f$ is such that $f(\\cdot,y) \\in D(A_X)$ and $f(x,\\cdot)\\in D(A_Y)$ for all $x,y$ and $A_X f(\\cdot ,y)(x)= A_Y f(x,\\cdot )(y)$ for $x\\in E_1$, $y\\in E_2$. More generally, if $\\alpha \\colon E_1\\to \\mathbb{R}$ and $\\beta \\colon E_2\\to \\mathbb{R}$ are continuous and such that \\[&nbsp;&nbsp;&nbsp; \\int_0^t \\left| \\alpha(X_s) \\right| \\mathop{}\\!\\mathrm{d} s &lt; \\infty,\\quad \\int_0^t \\left| \\beta(Y_s) \\right| \\mathop{}\\!\\mathrm{d} s &lt; \\infty\\\\\\] hold almost-surely, and \\[A_X f(\\cdot ,y)(x) + \\alpha(x) f(x,y) = A_Y f(x,\\cdot )(y) + \\beta(y) f(x,y),\\quad x\\in E_1,y\\in E_2,\\\\\\] then \\[\\mathbb{E} \\left[ f(X_t,Y_0) \\exp \\left( \\int_0^t \\alpha(X_s)\\mathop{}\\!\\mathrm{d} s \\right)&nbsp; \\right] = \\mathbb{E} \\left[ f(X_0,Y_t) \\exp \\left( \\int_0^t \\beta(Y_s)\\mathop{}\\!\\mathrm{d} s \\right)&nbsp; \\right] ,\\\\\\] given both integrands are integrable.</div><div></div>"
  },
  {
    "front": "How uniqueness of one-dimensional distributions to $(A,\\mu)$ implies uniqueness. What does this imply for using duality as a means to proving uniqueness in distribution of a martingale problem?",
    "back": "<div><b>Theorem.</b> Suppose that $A$ is a Markov pregenerator on a compact and separable space $E$. Then, if, for any $\\mu \\in M_1(E)$, any two solutions to $(A,\\mu)$ have the same one-dimensional distributions, then, for any $\\mu\\in M_1(E)$, any two solutions to $(A,\\mu)$ have the same distributions (i.e. fidis).<br><br><b>Remark.</b> Thus if we can find a process $(Y_t)$ that is dual to any solution of a martingale problem for a sufficiently large class of functions, then uniqueness of one-dimensional distributions and thus uniqueness in distribution of the martingale problem follows.</div><div></div>"
  },
  {
    "front": "Harris's lemma on correlation of up-sets $\\mathcal{A},\\mathcal{B}\\subset \\mathcal{P}(X)$. (Careful with $\\mathbb{P}^{[n]}$ and $\\mathbb{P}^{[n-1]}$!)",
    "back": "<div>Let $X$ be a finite set, and $X_p \\subset X$ the random subset that contains each element independently with probability $p\\in [0,1]$. Denote its distribution by $\\mathbb{P}_p$.<br><br><b>Lemma (Harris).</b> Let $\\mathcal{A},\\mathcal{B}\\subset \\mathcal{P}(X)$ be up-sets. Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}_p(\\mathcal{A}\\cap \\mathcal{B}) \\ge \\mathbb{P}_p(\\mathcal{A}) \\mathbb{P}_p(\\mathcal{B}).\\] It follows that down-sets are also positively correlated, and up- and down-sets are negatively correlated.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Use induction, $n=0$ is trivial. Define<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{A}^- &amp;:= \\left\\{ A \\in \\mathcal{A}\\colon n \\not\\in A \\right\\} \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{A}^+ &amp;:= \\left\\{ A \\setminus n \\colon A\\in \\mathcal{A}, n\\in A \\right\\} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then $\\mathcal{A}^+$ and $\\mathcal{A}^-$ are upsets, $\\mathcal{A}^- \\subset \\mathcal{A}^+$, and $\\mathbb{P}_p(\\mathcal{A}) = p\\mathbb{P}_p(\\mathcal{A}^+) + (1-p) \\mathbb{P}_p(\\mathcal{A}^-)$. Put $a^\\pm := \\mathbb{P}_p(\\mathcal{A}^\\pm)$, same for $\\mathcal{B}$. Then $(\\mathcal{A}\\cap \\mathcal{B})^\\pm = \\mathcal{A}^\\pm \\cap \\mathcal{B}^\\pm$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}_p(\\mathcal{A}\\cap \\mathcal{B})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= p \\mathbb{P}_p(\\mathcal{A}^+ \\cap \\mathcal{B}^+) + (1-p) \\mathbb{P}_p(\\mathcal{A}^- \\cap \\mathcal{A}^+)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge p a^+ b^+ + (1-p) a^- b^- =: x,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and $\\mathbb{P}_p(\\mathcal{A}) \\mathbb{P}_p(\\mathcal{B}) = (pa^+ + (1-p) a^-) (p b^+ + (1-p) b^-) =: y$, and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x-y\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= p(1-p) \\big( a^+ b^+ - a^+ b^- - a^- b^+ + a^-b^-\\big)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= p(1-p) (a^+ - a^-)(b^+ - b^-)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Janson's inequality I.",
    "back": "<div>Suppose that $X\\neq \\varnothing$, and $X_p$ is the random subset of $X$ that contains each element with probability $p\\in [0,1]$, independently. Let $n\\in \\mathbb{N}$, and $E_1,\\ldots ,E_n \\subset X$, and put $A_i := \\left\\{ E_i \\subset X_p \\right\\} $ for $i\\in [n]$, $Z := \\sum_{i=1}^n \\boldsymbol{1}_{A_i}$, $\\mu = \\mathbb{E} \\left[ Z \\right] $, and $\\Delta = \\sum_{i} \\sum_{j\\sim i} \\mathbb{P}(A_i\\cap A_j)$.<br><br><b>Proposition (Janson I).</b> In the above context, we have \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(Z = 0) \\le \\mathrm{e}^{-\\mu + \\Delta / 2}.\\] Together with Janson II, we have $\\mathbb{P}(Z = 0) \\le \\exp \\left( - \\frac{\\mu}{2}\\wedge \\frac{\\mu^2}{2\\Delta} \\right) $ (so the worse bound of the two, because either bound is only true in one of the cases $\\Delta &gt; (\\le) \\mu$).<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have, putting $r_i := \\mathbb{P} \\left(A_i \\,\\middle\\vert\\, \\bigcap_{\\substack{j&lt;i \\\\ j\\sim i} } A_j^{c}\\right)$, that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(Z = 0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\prod_{i=1}^n (1-r_i) \\le \\prod_{i=1}^n \\mathrm{e}^{-r_i} = \\mathrm{e}^{-\\sum_{i} r_i}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now let $i\\in [n]$, and $D_0 := \\bigcap_{\\substack{j &lt; i \\\\ j\\not\\sim i} } A_j^{c}$, $D_1 := \\bigcap_{\\substack{j &lt; i \\\\ j\\sim i} } A_j^{c}$, so that $A_i \\perp \\!\\!\\! \\perp D_0$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(A_i \\,\\middle\\vert\\, D_0 \\cap D_1\\right)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{\\mathbb{P}(A_i \\cap D_0 \\cap D_1)}{\\mathbb{P}(D_0 \\cap D_1)} \\ge \\frac{\\mathbb{P}(A_i \\cap D_0\\cap D_1)}{\\mathbb{P}(D_0)}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P} \\left(A_i \\cap D_1 \\,\\middle\\vert\\, D_0\\right) = \\mathbb{P}(A_i) - \\mathbb{P} \\left(A_i \\cap D_1^{c} \\,\\middle\\vert\\, D_0\\right)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\mathbb{P}(A_i) - \\mathbb{P}(A_i \\cap D_1^{c}),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used Harris' inequality and that $A_i$ is an upset, and $D_0,D_1$ are downsets. Now<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( A_i \\cap D_1^{c} \\right) = \\mathbb{P}\\left( A_i \\cap \\bigcup_{\\substack{j &lt; i \\\\ j\\sim i} A_j}&nbsp; \\right) \\le \\sum_{\\substack{j &lt; i \\\\ j\\sim i} } \\mathbb{P}(A_i \\cap A_j),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{i=1}^n r_i \\ge \\sum_{i=1}^n \\mathbb{P}(A_i) - \\sum_{i=1}^n \\sum_{\\substack{j &lt; i \\\\ j\\sim i} }\\mathbb{P}(A_i \\cap A_j) = \\mu - \\frac{\\Delta}{2}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Janson's inequality II.",
    "back": "<div>Suppose that $X\\neq \\varnothing$, and $X_p$ is the random subset of $X$ that contains each element with probability $p\\in [0,1]$, independently. Let $n\\in \\mathbb{N}$, and $E_1,\\ldots ,E_n \\subset X$, and put $A_i := \\left\\{ E_i \\subset X_p \\right\\} $ for $i\\in [n]$, $Z := \\sum_{i=1}^n \\boldsymbol{1}_{A_i}$, $\\mu = \\mathbb{E} \\left[ Z \\right] $, and $\\Delta = \\sum_{i} \\sum_{j\\sim i} \\mathbb{P}(A_i\\cap A_j)$.<br><br><b>Proposition (Janson II).</b> In the above context, if $\\Delta \\ge \\mu$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(Z = 0) \\le \\mathrm{e}^{-\\frac{\\mu^2}{2\\Delta}}.\\] Together with Janson I, we have $\\mathbb{P}(Z = 0) \\le \\exp \\left( - \\frac{\\mu}{2}\\wedge \\frac{\\mu^2}{2\\Delta} \\right) $ (so the worse bound of the two, because either bound is only true in one of the cases $\\Delta &gt; (\\le) \\mu$).<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For any $S\\subset [n]$, by Janson I, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(Z = 0) = \\mathbb{P}\\left( \\bigcap_{i=1} ^n A_i ^{c} \\right) \\le \\mathbb{P}\\left( \\bigcap_{i\\in S} A_i ^{c} \\right) \\le \\mathrm{e}^{-\\mu_S + \\Delta_S / 2}.&nbsp;&nbsp;&nbsp; \\] Let $S$ be the random subset of $[n]$ that contains each element with probability $r\\in [0,1]$, independently of each other. Then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widetilde{\\mathbb{E}} \\left[ \\mu_S \\right] &amp;= \\sum_{i=1}^n \\widetilde{\\mathbb{P}}(i\\in S) \\mathbb{P}(A_i) = r\\mu,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widetilde{\\mathbb{E}} \\left[ \\Delta_S \\right] &amp;= \\sum_{i=1}^n \\sum_{j\\sim i} \\widetilde{\\mathbb{P}}(i,j\\in S) \\mathbb{P}(A_i \\cap A_j) = r^2\\Delta.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Thus, $\\widetilde{\\mathbb{P}}(\\mu_S - \\Delta_S / 2 \\ge r\\mu - r^2\\Delta / 2) &gt; 0$, so there exists $S\\subset [n]$ with $\\mu_S - \\Delta_S / 2 \\ge r\\mu - r^2\\Delta / 2$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(Z = 0) \\le \\mathrm{e}^{-r\\mu + r^2\\Delta / 2},\\quad 0\\le r \\le 1.&nbsp;&nbsp;&nbsp; \\] The choice $r = \\frac{\\mu}{\\Delta}$ is valid if $\\Delta \\ge \\mu$, in which case it gives the proposed bound.</i> </p> </div><div></div>"
  },
  {
    "front": "Theory of speed and scale: How does the generator of a diffusion $X$ on $[a,b]$ change under a change of time $\\tau(t) = \\int_0^t \\beta(X_s) \\mathop{}\\!\\mathrm{d} s$, or a change of space via $S\\colon [a,b]\\to [S(a),S(b)]$?",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.<br><br><b>Proposition.</b> If $\\beta\\colon \\mathbb{R}\\to (0,\\infty)$ is continuous, $\\tau(t) := \\int_0^t \\beta(X_s) \\mathop{}\\!\\mathrm{d} s$, and $Z_t := X_{\\tau(t)}$, then $Z$ is a diffusion with generator&nbsp; \\[&nbsp;&nbsp;&nbsp; \\mathcal{L}_Z f (x) = \\beta(x) \\mathcal{L}_X f(x), \\quad x \\in [a,b].\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ X_t - x \\right] = \\mu(x) t + o(t),\\quad \\mathbb{E}^x \\left[ \\left( X_t - x \\right) ^2 \\right] = \\sigma(x)^2 t + o(t),\\\\&nbsp;&nbsp;&nbsp; \\] so $\\mathbb{E}^x\\left[ Z_t - x \\right] = \\mathbb{E}^x \\left[ X_{\\tau(t)}-x \\right] = \\mu(x) \\beta(x) t + o(t)$, and similarly for infinitesimal variance, so $Z$ is a $(\\beta \\mu,\\beta\\sigma)$-diffusion.</i> </p> <br><br><b>Proposition.</b> If $S\\colon [a,b]\\to \\mathbb{R}$ is continuous and strictly increasing, and $Z_t := S(X_t)$, then $Z$ is a diffusion with generator \\[&nbsp;&nbsp;&nbsp; \\mathcal{L}_Z f(z) = \\frac{1}{2} \\sigma^2(x) S'(x)^2 f''(z) + \\mathcal{L}_X S (x) f'(z),\\\\\\] where $x := S^{-1}(z)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{L}_Z f(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{E} \\left[ f(Z_t) \\,\\middle\\vert\\, Z_0 = z\\right] = \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{E} \\left[f(S(X_t)) \\,\\middle\\vert\\, X_0 = x\\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathcal{L}_X (f\\circ S) (x),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; the rest is a straight-forward calculation.</i> </p> </div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Definition of scale function, speed density, and speed measure of a diffusion $X$ on $[a,b]$. How can the generator be expressed in terms of those?",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.</div><div></div><div><b>Definition.</b> The <i>scale function</i> of $X$ is given by \\[&nbsp;&nbsp;&nbsp; S(x) := \\int_{x_0}^x \\exp \\left( -\\int_\\eta^y \\frac{2\\mu(z)}{\\sigma^2(z)}\\mathop{}\\!\\mathrm{d} z \\right) \\mathop{}\\!\\mathrm{d} y,\\quad x\\in [a,b],\\\\\\] where $x_0,\\eta \\in (a,b)$ are fixed but arbitrary. $X$ is said to be on <i>natural scale</i> if $S$ can be taken to be linear. The <i>speed density</i> and <i>speed measure</i> of $X$ are given by \\[m(x) := \\frac{1}{\\sigma^2(x) S'(x)},\\quad M(x) := \\int_{x_0}^x m(x') \\mathop{}\\!\\mathrm{d} x',\\quad x\\in [a,b].\\] With these definitions, we have $\\mathcal{L}_X S = 0$ and \\[\\mathcal{L}_X f = \\frac{1}{2} \\frac{1}{\\mathop{}\\!\\mathrm{d} M / \\mathop{}\\!\\mathrm{d} S} \\frac{\\mathop{}\\!\\mathrm{d} ^2 f}{\\mathop{}\\!\\mathrm{d} S^2} = \\frac{1}{2} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} M} \\left( \\frac{\\mathop{}\\!\\mathrm{d} f}{\\mathop{}\\!\\mathrm{d} S}&nbsp; \\right) .\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $\\mathcal{L}_X S = 0$ by definition of $S$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{2} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} M} \\left( \\frac{\\mathop{}\\!\\mathrm{d} f}{\\mathop{}\\!\\mathrm{d} S} \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2} \\frac{1}{m(x)} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\frac{1}{S'(x)} \\frac{\\mathop{}\\!\\mathrm{d}&nbsp; f }{\\mathop{}\\!\\mathrm{d} x} \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2} \\sigma^2(x) S'(x) \\left( \\frac{1}{S'(x)} \\frac{\\mathop{}\\!\\mathrm{d} ^2f}{\\mathop{}\\!\\mathrm{d} x^2} - \\frac{S''(x)}{S'(x)^2} \\frac{\\mathop{}\\!\\mathrm{d} f}{\\mathop{}\\!\\mathrm{d} x} \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2}\\sigma^2(x) \\frac{\\mathop{}\\!\\mathrm{d} ^2 f}{\\mathop{}\\!\\mathrm{d} x^2} + \\mu(x) \\frac{\\mathop{}\\!\\mathrm{d} f}{\\mathop{}\\!\\mathrm{d} x}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Probabilities $\\mathbb{P} \\left(T_x &lt; T_y \\,\\middle\\vert\\, T_x \\wedge T_y &lt; \\infty\\right)$ (in terms of scale function).",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.<br><br><b>Proposition.</b> If $a &lt; x &lt; y &lt; b$, and $x_0 \\in [x,y]$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(T_x &lt; T_y \\,\\middle\\vert\\, T_x \\wedge T_y &lt; \\infty\\right) = \\frac{S(y) - S(x_0)}{S(y) - S(x)}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $S(X_t)$ has no drift, so it is a time-changed Brownian motion, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^{x_0} \\left(T_x &lt; T_y \\,\\middle\\vert\\, T_x \\wedge T_y &lt; \\infty\\right) = \\mathbb{P}_B^{S(x_0)} \\left( T_{S(x)} &lt; T_{S(y)}\\right) = \\frac{S(y) - S(x_0)}{S(y) - S(x)}.&nbsp;&nbsp;&nbsp; \\] Note that the choice of $x_0$ cancels in the difference, and the choice of $\\eta$ in the ratio.</i> </p> </div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Feller boundary classification.",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.</div><div></div><div><b>Definition.</b> If $M$ and $S$ are the speed measure and the scale function, then define \\[&nbsp;&nbsp;&nbsp; u(x) = \\int_{x_0}^x M \\mathop{}\\!\\mathrm{d} S,\\quad v(x) = \\int_{x_0}^x S \\mathop{}\\!\\mathrm{d} M.\\] The boundary at $b$ is said to be<br><ol>  <li><i>regular</i> if $u(b) &lt;&nbsp; \\infty$, $v(b) &lt; \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li><i>exit</i> if $u(b) &lt; \\infty$, $v(b) = \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li><i>entrance</i> if $u(b) = \\infty$, $v(b) &lt; \\infty$,&nbsp;&nbsp;&nbsp;</li>  <li><i>natural</i> if $u(b) = \\infty$, $v(b) = \\infty$.</li></ol><br>The former two are called <i>accessible</i>, the latter two <i>inaccessible</i>.<br><br><b>Theorem.</b> $\\mathcal{L}:= \\frac 12 \\sigma(\\cdot)^2 f''(\\cdot) + \\mu(\\cdot) f'(\\cdot)$ defines a Markov generator with domain all $f\\in C([a,b]) \\cap C^2((a,b))$ with the following additional conditions (for $b$ analogously).<br><ol>  <li>If $a$ is inaccessible, no further restrictions,&nbsp;&nbsp;&nbsp;</li>  <li>If $ a$ is exit, then $\\mathcal{L}f(x) \\stackrel{ ! }{\\longrightarrow} 0$ as $x \\to a$, &nbsp;&nbsp;&nbsp;</li>  <li>If $a$ is regular, then for every $q\\in [0,1]$ there is a Feller semigroup associated with $\\mathcal{L}_q$ for the domain on which the additional assumption \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q\\lim_{x\\to a}\\mathcal{L}f(x) = (1-q) \\lim_{x\\to a}\\frac{1}{S'(x)}f'(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] is satisfied (for $b$ a minus on RHS).</li></ol></div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Green's function of $(X_t)$ and how it can be used to calculate $\\mathbb{E}^x \\left[ \\int_0^T g(X_s) \\mathop{}\\!\\mathrm{d} s \\right] $ with exit time $T = T_a \\wedge T_b$.",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.<br><br><b>Theorem.</b> Suppose $T:= T_a\\wedge T_b$, and $g\\colon [a,b]\\to \\mathbb{R}$ is continuous. Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\int_0^T g(X_s)\\mathop{}\\!\\mathrm{d} s \\right] = \\int_a^b G(x,\\xi) g(\\xi) \\mathop{}\\!\\mathrm{d} \\xi,\\\\&nbsp;&nbsp;&nbsp; \\] where the <i>Green's function</i> $G\\colon (a,b)\\times (a,b) \\to \\mathbb{R}$ is defined by \\[&nbsp;&nbsp;&nbsp; G(x,\\xi) := \\frac{2m(\\xi)}{S(b) - S(a)} \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (S(b) - S(x)) (S(\\xi) - S(a)) &amp;, a &lt; \\xi \\le x\\\\&nbsp; &nbsp; &nbsp; &nbsp; (S(b) - S(\\xi)) (S(x) - S(a)) &amp;, x &lt; \\xi &lt; b&nbsp;&nbsp;&nbsp; \\end{cases}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Note first that, for $h &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; w(x) = \\mathbb{E}^x \\left[ \\int_0^h g(X_s) \\mathop{}\\!\\mathrm{d} s \\right] + \\mathbb{E}^x \\left[ \\int_h^T g(X_s) \\mathop{}\\!\\mathrm{d} s \\right] .&nbsp;&nbsp;&nbsp; \\] Now,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\int_h^T g(X_s) \\mathop{}\\!\\mathrm{d} s \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} } \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^x \\left[ \\mathbb{E}^x \\left[ \\int_h^T g(X_s) \\mathop{}\\!\\mathrm{d} s \\,\\middle\\vert\\, \\mathcal{F}_h \\right] \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} } \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^x \\left[ \\mathbb{E}^{X_h} \\left[ \\int_0^T g(X_s) \\mathop{}\\!\\mathrm{d} s \\right] \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} } \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^x \\left[ w(X_h) \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} } \\right] ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used that $ \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} }T = \\boldsymbol{1}_{\\left\\{ T &gt; h \\right\\} } (h + \\inf \\left\\{ t \\ge 0\\colon X_{h+t} \\in \\left\\{ a,b \\right\\}&nbsp; \\right\\} )$. Now note that, putting $c := (b-x) \\wedge (x - a) &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^x(T &lt; h) \\le \\mathbb{P}^x \\left( \\left| X_t - x \\right| \\ge c \\right) \\le c^{-3} \\mathbb{E} \\left[ \\left| X_t - x \\right| ^3 \\right] = o(h),\\\\&nbsp;&nbsp;&nbsp; \\] so, since $\\mathbb{E}^x \\left[ w(X_h) \\right] = T_h w (x)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{h\\to 0}\\frac{1}{h} \\left( \\mathbb{E} \\left[ \\int_h^t g(X_s)\\mathop{}\\!\\mathrm{d} s \\right] - w(x) \\right)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{h\\to 0} \\frac{T_h w(x) - w(x)}{h}= \\mathcal{L} w (x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; The limit exists and $w\\in D(\\mathcal{L})$ because \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\frac{1}{h}\\int_0^h g(X_s) \\mathop{}\\!\\mathrm{d} s \\right] \\stackrel{  }{\\longrightarrow} g(x) \\quad (h\\to 0),\\\\&nbsp;&nbsp;&nbsp; \\] so we conclude \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{L} w = -g &amp;,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; w(a) = w(b) = 0.&nbsp;&nbsp;&nbsp; \\end{cases}\\] Now use that $\\mathcal{L}w(x) = \\frac{1}{2} \\frac{1}{m(x)} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\frac{w'(x)}{S'(x)} \\right)&nbsp; $ and integrate twice.</i> </p> </div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Definition of stationary distribution and reversible stationary distribution for the diffusion $X$, and motivation for the definition of the latter.",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.<br><br><b>Definition.</b> $\\nu \\in M_1([a,b])$ is called a <i>stationary distribution for $(X_t)$</i> if $X_0 \\sim \\nu$ implies $X_t \\sim \\nu$ for all $t &gt; 0$. It is called <i>reversible</i>, or $X$ is called <i>reversible w.r.t. $\\nu$</i> if, for all $f,g\\in D(\\mathcal{L})$, \\[\\int f \\mathcal{L}g \\mathop{}\\!\\mathrm{d} \\nu = \\int g \\mathcal{L}f \\mathop{}\\!\\mathrm{d} \\nu.\\] <br><b>Motivation.</b> Suppose $\\nu(\\mathop{}\\!\\mathrm{d} x) = \\nu(x) \\mathop{}\\!\\mathrm{d} x$, and $P_t(x,\\mathop{}\\!\\mathrm{d} y) = p_t(x,y) \\mathop{}\\!\\mathrm{d} y$. Then we might want to define reversability by requiring $\\nu(x) p_t(x,y) = \\nu(y) p_t(y,x)$. Multiplying by $f(x)g(y)$ and integrating on both sides gives<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\int \\mathop{}\\!\\mathrm{d} x\\, \\nu(x) f(x) T_t g(x) = \\int \\mathop{}\\!\\mathrm{d} y\\, \\nu(y) g(y) T_t f (y),\\\\\\end{align*}[/$$]<br>and differentiating after $t$ (heuristically) gives the equation we required.</div><div></div>"
  },
  {
    "front": "Theory of speed and scale: Definition of regularity of $X$, and under what circumstances $\\nu(\\mathop{}\\!\\mathrm{d} x) \\sim m(x) \\mathop{}\\!\\mathrm{d} x$ defines a (reversible?) stationary distribution, and when the stationary distribution is unique.",
    "back": "<div>Suppose $a &lt; b$, and $\\mu,\\sigma \\colon [a,b] \\to \\mathbb{R}$ are locally Lipschitz, bounded, and $\\sigma^2 &gt; 0$ on $(a,b)$, and $X$ is a $(\\mu,\\sigma)$-diffusion.\\\\</div><div><br><b>Definition.</b> $X$ is called <i>regular</i> if $\\mathbb{P}^x(T_y &lt; \\infty) &gt; 0$ for all $x\\in (a,b)$ and $y\\in [a,b]$.<br><br><b>Theorem.</b> If $\\int_a^b m(x) \\mathop{}\\!\\mathrm{d} x =: I&nbsp; &lt; \\infty$, then \\[&nbsp;&nbsp;&nbsp; \\nu(\\mathop{}\\!\\mathrm{d} x) := \\frac{m(x)\\mathop{}\\!\\mathrm{d} x}{I}\\] defines a reversible stationary distribution for $(X_t)$. If $X$ is regular, on natural scale, and without absorbing boundaries, then a stationary distribution exists iff $I &lt; \\infty$, in which case the above is the unique stationary distribution.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly, $\\nu(x)$ as defined above is non-negative and integrates to $1$, so defines a $\\nu \\in M_1([a,b])$. We have <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2} \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} (S'(x) \\sigma^2(x) \\nu(x))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2} S''(x) \\sigma^2(x) \\nu(x) + \\frac{1}{2} S'(x) \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\sigma^2(x) \\nu(x) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= -S'(x) \\mu(x) \\nu(x) + \\frac{1}{2} S'(x) \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\sigma^2(x) \\nu(x) \\right), \\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $\\frac{1}{2} \\frac{\\mathop{}\\!\\mathrm{d} ^2}{\\mathop{}\\!\\mathrm{d} x^2} \\left( \\sigma^2(x) \\nu(x) \\right) - \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\mu(x) \\nu(x) \\right) = 0$, so, if $f\\in D(\\mathcal{L})$ with $f,f'$ vanishing on the boundary,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{E}^\\nu \\left[ f(X_t) \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\mathcal{L}f(x) \\nu(\\mathop{}\\!\\mathrm{d} x)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\left( \\frac{1}{2} \\sigma^2(x) \\nu(x) f''(x) + \\mu(x) \\nu(x) f'(x) \\right) \\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int f(x) \\left( \\frac{1}{2}\\frac{\\mathop{}\\!\\mathrm{d}^2}{\\mathop{}\\!\\mathrm{d} x^2}\\left(\\sigma^2(x) \\nu(x)\\right) - \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x} \\left( \\mu(x) \\nu(x) \\right)&nbsp; \\right) \\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Clique number of $G(n,p)$: Definition of $X_k$, $\\mu_k$, thus $\\mu_{k+1}/\\mu_k$. Definition and asymptotic behaviour of $k_0$. What are $(1 / p)^k$ and $\\mu_{k+1}/\\mu_k$ if $k\\sim k_0$?",
    "back": "<div><b>Definition/Lemma.</b> Let $0 &lt; p &lt; 1$ fixed, and $G \\sim \\mathcal{G}(n,p)$. Let $X_k$ be the number of copies of $K_k$ in $G$, and put \\[\\mu_k := \\mathbb{E} \\left[ X_k \\right] = \\binom{n}{k} p^{\\binom{k}{2} },\\quad \\frac{\\mu_{k+1}}{\\mu_k} = \\frac{n-k}{k+1} p^k.\\] Then the ratio is decreasing in $k$, so $\\mu_k$ first increases ($\\mu_0 = 1,\\mu_1 = n,\\ldots $), then decreases ($\\mu_n = p^{\\binom{n}{2} } \\ll 1$). Define \\[k_0 := \\min \\left\\{ k\\ge 1\\colon \\mu_k &lt; 1 \\right\\} .\\] Then, $k_0 \\sim 2 \\frac{\\log n}{\\log (1 / p)}$. In particular, if $k\\sim k_0$, then \\[\\left( \\frac{1}{p} \\right) ^k = n^{2 + o(1)},\\quad \\frac{\\mu_{k+1}}{\\mu_k} = n^{-1 + o(1)}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left( \\frac{n}{k} \\right) ^k p^{\\binom{k}{2} }\\le \\mu_k \\le \\left( \\frac{\\mathrm{e} n}{k} \\right) ^k p^{\\binom{k}{2} },\\\\&nbsp;&nbsp;&nbsp; \\] so $\\mu_k^{1 / k}= \\Theta \\left( \\frac{n}{k} p^{k / 2} \\right) $. Now, if $k \\le (1-\\varepsilon) 2 \\log_{1 / p}(n)$, then $(1 / p)^{k / 2} \\le n^{1-\\varepsilon}$, so \\[&nbsp;&nbsp;&nbsp; \\frac{n}{k} p^{k / 2} \\ge n^{\\varepsilon} / k \\gg 1,\\\\\\] so $k_0 \\ge (1-\\varepsilon) 2\\log_{1 / p} (n)$ for large $n$. Other direction similarly.</i> </p> </div><div></div>"
  },
  {
    "front": "Clique number of $G(n,p)$: Asymptotic bound on $\\Delta_k / \\mu_k^2$ (if $k\\sim k_0$).",
    "back": "<div><b>Lemma.</b> If $k\\sim k_0$, then&nbsp; \\[&nbsp;&nbsp;&nbsp; \\frac{\\Delta_k}{\\mu_k^2} \\le \\max \\left\\{ n^{-2 + o(1)}, \\frac{n^{-1 + o(1)}}{\\mu_k} \\right\\} .\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By definition, if $A_S = \\left\\{ \\text{all edges in $S$ are present} \\right\\} $ for $S\\in [n]^{(k)}$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Delta_k &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{S} \\sum_{T\\sim S} \\mathbb{P}\\left( A_S \\cap A_T \\right) = \\sum_{s=2}^{k-1} \\binom{n}{k} \\binom{k}{s} \\binom{n-k}{k-s} p^{2 \\binom{k}{2} - \\binom{s}{2}},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\Delta_k}{\\mu_k^2} = \\sum_{s=2}^{k-1}\\alpha_s,\\qquad \\alpha_s = \\frac{\\binom{k}{s} \\binom{n-k}{k-s}&nbsp; }{\\binom{n}{k} } p^{\\binom{s}{2} }\\, (0\\le s \\le k).&nbsp;&nbsp;&nbsp; \\] By examining ratios, one can show that $\\alpha_s$ first decreases, then increases, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\frac{\\Delta_k}{\\mu_k^2} &nbsp;&nbsp;&nbsp; &amp;\\le k \\max_{2\\le s \\le k-1}\\alpha_s\\\\&nbsp;&nbsp;&nbsp; &amp;\\le n^{o(1)} \\max \\left\\{ \\alpha_2,\\alpha_{k-1} \\right\\} \\\\&nbsp;&nbsp;&nbsp; &amp;\\le \\max \\left\\{ n^{-2 + o(1)}, \\frac{n^{-1 + o(1)}}{\\mu_k} \\right\\} .\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "What is whp the clique number of $\\mathcal{G}(n,p)$ for $0 &lt; p &lt; 1$ fixed? (Using the asymptotic bound on $\\Delta_k / \\mu_k^2$ for the lower bound).",
    "back": "<div><b>Theorem.</b> Let $0 &lt; p &lt; 1$ be fixed. Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left( k_0-2 \\le \\omega(G) \\le k_0 \\right) \\stackrel{  }{\\longrightarrow} 1.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For the upper bound, just note that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\omega(G) &gt; k_0 \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( X_{k_0 + 1} \\ge 1 \\right) \\le \\mu_{k_0 + 1} = \\mu_{k_0} \\frac{\\mu_{k_0+1}}{\\mu_{k_0}}\\le n^{-1 + o(1)} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For the lower bound, it suffices to show that $\\mathbb{P}\\left( X_k \\ge 1 \\right) \\to 1$ if $k = k_0 - 2$. Indeed, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu_k = \\frac{\\mu_k}{\\mu_{k+1}} \\mu_{k+1} \\ge n^{1 + o(1)}\\stackrel{  }{\\longrightarrow} \\infty,\\\\&nbsp;&nbsp;&nbsp; \\] since $k\\sim k_0$ and $k+1 = k_0 - 1$, so $\\mu_{k+1} \\ge 1$. Finally, \\[&nbsp;&nbsp;&nbsp; \\frac{\\Delta_k}{\\mu_k^2} \\le \\max \\left\\{ n^{-2 + o(1)}, \\frac{n^{-1 + o(1)}}{\\mu_k} \\right\\} \\le n^{-2 + o(1)} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Clique number of $G(n,p)$: Strong upper bound on $\\mathbb{P}\\left( \\omega(G) &lt; k_0 - 3 \\right) $ (by Janson's inequality).",
    "back": "<div><b>Theorem.</b> Let $0 &lt; p &lt; 1$ be fixed. Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left( \\omega(G) &lt; k_0 - 3 \\right) \\le \\mathrm{e}^{-n ^{2 - o(1)}}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have, putting $k := k_0 - 3$ \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu_k = \\mu_{k_0 - 1} \\frac{\\mu_{k_0 - 2}}{\\mu_{k_0-1}} \\frac{\\mu_{k_0 - 3}}{\\mu_{k_0 - 2}} \\ge n^{2 + o(1)},\\\\&nbsp;&nbsp;&nbsp; \\] and \\[&nbsp;&nbsp;&nbsp; \\frac{\\Delta_k}{\\mu_k^2} \\le \\max \\left\\{ n^{-2 + o(1)}, \\frac{n^{-1 + o(1)}}{\\mu_k} \\right\\} \\le n^{-2 + o(1)},\\\\&nbsp;&nbsp;&nbsp; \\] so Janson's inequality yields the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Asymptotic chromatic number of $\\mathcal{G}(n,p)$, for fixed $0 &lt; p &lt; 1$.",
    "back": "<div><b>Theorem.</b> Let $0 &lt; p &lt; 1$ fixed, and $G \\sim \\mathcal{G}(n,p)$. Then, for every $\\varepsilon &gt; 0$, \\[&nbsp;&nbsp;&nbsp; (1-\\varepsilon) \\frac{n}{2\\log_b n} \\le \\chi(G) \\le (1+\\varepsilon) \\frac{n}{2\\log_b n}\\] whp, where $b = 1 / (1 - p)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We know that $\\chi(G) \\ge n / \\alpha(G)$, and $\\alpha(G) = \\omega(G^{c}) \\le k_0(n,1-p)$ whp, where $k_0(n,1-p) \\sim 2 \\log_b n$.<br><br>&nbsp;&nbsp;&nbsp; For the upper bound, let $m = \\left\\lfloor n / (\\log n)^2 \\right\\rfloor $ (so that $m =n^{1 + o(1)}$, but $m = o(\\text{proposed bound})$). For $W \\subset \\mathbb{N}$, $\\left| W \\right| = m$, let $E_W$ be the event that $G[W] \\sim \\mathcal{G}(m,p)$ contains an independent set of size at least \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k := k_0(m,1-p) - 3 \\sim 2 \\log_b m \\sim 2 \\log_b n.&nbsp;&nbsp;&nbsp; \\] Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(E_W^{c}) = \\mathbb{P}\\left( \\alpha(G[W]) &lt; k \\right) \\le \\mathrm{e}^{-m^{2 - o(1)}} = \\mathrm{e}^{-n ^{2 - o(1)}},\\\\&nbsp;&nbsp;&nbsp; \\] so, putting $E := \\bigcap_{W} E_W$, $\\mathbb{P}(E^{c}) \\le 2^n \\mathrm{e}^{-n^{2 - o(1)}} \\stackrel{  }{\\longrightarrow} 0$. Now suppose that $G$ is a fixed graph on $n$ vertices for which $E$ holds. Then pick independent sets of size $k$ and give them some colour each, until $&lt;m$ vertices are left, give them all their own colours, so \\[&nbsp;&nbsp;&nbsp; \\chi(G) \\le \\frac{n}{k} + m \\sim \\frac{n}{2\\log_b n}.\\] In particular, $\\chi(G) \\le (1+\\varepsilon) n / (2\\log_b n)$ for large $n$.</i> </p> </div><div></div>"
  },
  {
    "front": "Suppose $U\\subset \\mathbb{R}^d$ is open and bounded. Then all moments of the exit time $T:= \\inf \\left\\{ t\\ge 0\\colon B_t \\not\\in U \\right\\} $ are finite.",
    "back": "<div><b>Lemma.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded, and put $T:= \\inf \\left\\{ t\\ge 0\\colon B_t\\not\\in U \\right\\} $. Then, \\[\\sup_{x\\in U} \\mathbb{E}^x \\left[ T^p \\right] &lt; \\infty, \\quad p &gt; 0.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ T^p \\right] = \\int_0^\\infty \\mathbb{P}^x \\left( T^p \\ge x \\right) \\mathop{}\\!\\mathrm{d} s = \\int_0^\\infty p s^{p-1} \\mathbb{P}^x \\left( T \\ge s \\right) \\mathop{}\\!\\mathrm{d} s,\\\\&nbsp;&nbsp;&nbsp; \\] so it suffices to show that $\\sup_x \\mathbb{P}^x \\left( T \\ge k \\right) \\le q^k$ for all $k\\in \\mathbb{N}$ for some $q \\in (0,1)$. We have \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^x(T \\ge 1) \\le \\mathbb{P}^x( B_1 \\in \\overline{U} ) \\le \\mathbb{P}^x \\left( \\left| B_1-x \\right| \\le K \\right) = \\mathbb{P}^0 \\left( \\left| B_1 \\right| \\le K \\right)&nbsp; =: q \\in (0,1),\\\\&nbsp;&nbsp;&nbsp; \\] where $K := \\text{diam}\\, U$. For $k \\ge 2$, we have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^x \\left( T \\ge k \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}^x \\left( T \\ge k, B_1 \\in U \\right) = \\mathbb{E}^x \\left[ \\boldsymbol{1}_{\\left\\{ B_1 \\in U \\right\\} } \\mathbb{P}^{B_1}\\left( T\\ge k-1 \\right)&nbsp;&nbsp; \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le q^{k-1} \\mathbb{P}^x \\left( B_1 \\in U \\right) \\le q^k.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of harmonic function $u\\colon \\overline{U}\\to \\mathbb{R}$. In that case, what do you know about $(u(B_t))_{0\\le t\\le T}$, and how can $u(x),x\\in \\overline{U}$, be expressed in terms of Brownian motion?",
    "back": "<div><b>Definition/Theorem.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded. Then a function $u\\in C(\\overline{U},\\mathbb{R}) \\cap C^2(U,\\mathbb{R})$ is called <i>harmonic</i> if $\\Delta u = 0$ on $U$, in which case it is said to solve the <i>Dirichlet problem</i> with boundary $\\phi := u \\!\\!\\restriction_{\\partial U} \\in C(\\partial U,\\mathbb{R})$. Then, $(u(B_t))_{t\\ge 0}$ is a local martingale, and \\[&nbsp;&nbsp;&nbsp; u(x) = \\mathbb{E}^x \\left[ u(B_T) \\right] = \\mathbb{E}^x \\left[ \\phi(B_T) \\right] ,\\quad x\\in \\overline{U},\\\\\\] where $T = \\inf \\left\\{ t\\ge 0\\colon B_t \\not\\in U \\right\\} $. In particular, a solution to the Dirichlet problem with fixed boundary value is, if it exists, unique and given by the above formula.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Ito's formula, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(B_t) = u(x) + \\sum_{i=1}^d \\int_0^t D_i u(B_s) \\mathop{}\\!\\mathrm{d} B^j_s,\\quad 0\\le t\\le T,\\\\&nbsp;&nbsp;&nbsp; \\] so there exists a random time-change $\\gamma \\colon [0,\\infty) \\to [0,T]$ such that $X_t := u(B_{\\gamma(t)}),\\,t\\ge 0,$ defines a bounded martingale (take $\\gamma(t) := \\inf \\left\\{ s\\ge 0\\colon \\left&lt;M \\right&gt; _s &gt; t \\right\\} \\wedge T\\uparrow T$). We thus have $X_t \\stackrel{  }{\\longrightarrow} X_\\infty = u(B_T)$ a.s. and in $L^2$ ($T &lt; \\infty$ a.s., and $u$ is continuous), so, for any $x\\in \\overline{U}$, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(x) = u(B_0) = X_0 = \\mathbb{E} \\left[ X_0 \\right] = \\mathbb{E} \\left[ X_\\infty \\right] = \\mathbb{E}^x \\left[ u(B_T) \\right] .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definition recurrence, neighbourhood recurrence, and transience for an $\\mathbb{R}^d$-valued Markov process.",
    "back": "<div><b>Definition.</b> An $\\mathbb{R}^d$-valued Markov process is said to be<br><ol>  <li><i>recurrent</i> if it visits every point $x\\in \\mathbb{R}^d$ infinitely often,&nbsp;&nbsp;&nbsp;</li>  <li><i>neighbourhood recurrent</i> if it visits every open set infinitely often,&nbsp;&nbsp;&nbsp;</li>  <li><i>transient</i> if it goes to $\\infty$ almost-surely,</li></ol><br>where doing something infinitely often means doing it for arbitrarily large times almost-surely.</div><div></div>"
  },
  {
    "front": "$\\mathbb{P}^x \\left( T_r &lt; T_R \\right) $ for $0 &lt; r &lt; \\left| x \\right| &lt; R$ for Brownian motion, and connection with Bessel process.",
    "back": "<div><b>Theorem.</b> Let $0 &lt; r &lt; \\left| x \\right| &lt;R$. Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^x \\left( T_r &lt; T_R \\right) = \\frac{\\psi(R) - \\psi(\\left| x \\right| )}{\\psi(R) - \\psi(r)},\\\\\\] where $\\psi$ is a non-vanishing solution to $\\psi''(\\rho) + \\frac{d-1}{\\rho}\\psi'(\\rho) = 0$, so \\[\\psi(\\rho) = \\begin{cases}&nbsp;&nbsp;&nbsp; c_1 \\rho + c_2 &amp;, d = 1,\\\\&nbsp;&nbsp;&nbsp; c_2 \\log \\rho + c_2 &amp;, d=2,\\\\&nbsp;&nbsp;&nbsp; c_1 \\rho ^{2 - d} +c_2 &amp;, d\\ge 3.\\end{cases}\\] In particular, those are the 1d-exit times of a diffusion with generator $\\frac{1}{2} \\frac{\\mathop{}\\!\\mathrm{d} ^2}{\\mathop{}\\!\\mathrm{d} x^2} + \\frac{d-1}{2x}\\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} x}$, so the Bessel process of dimension $d$, i.e. the modulus of $d$-dimensional Brownian motion.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $A := \\left\\{ x\\in \\mathbb{R}^d\\colon r &lt; \\left| x \\right| &lt; R \\right\\} $, open and bounded. Then the Ansatz $u(x) = \\psi(\\left| x \\right| )$ gives a harmonic function (in fact, every harmonic function on $A$ has to be radially symmetric) iff $\\psi$ solves the above equation, to which the general solution is as given above. Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\psi(\\left| x \\right| )&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= u(x) = \\mathbb{E}^x \\left[ u(B_T) \\right] = \\mathbb{P}^x\\left( T_r &lt; T_R \\right) \\psi(r) + \\mathbb{P}^x \\left( T_R &lt; T_r \\right) \\psi(R),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; which implies the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Brownian motion is neighbourhood recurrent in $d = 2$.",
    "back": "<b>Proposition.</b> In $d=2$, Brownian motion is neighbourhood recurrent.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Because BM is a strong Markov process, it suffices to show that it hits every ball with probability one. Let&nbsp; $\\varepsilon &gt; 0$. Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^x \\left( T_\\varepsilon &lt; \\infty \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}^x \\left( \\bigcup_{R \\in \\mathbb{N}} \\left\\{ T_\\varepsilon &lt; T_R \\right\\}&nbsp; \\right) = \\lim_{R\\to \\infty} \\mathbb{P}^x \\left( T_\\varepsilon &lt; T_R \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{R\\to \\infty} \\frac{\\log R - \\log \\left| x \\right| }{\\log R - \\log \\varepsilon} = 1.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><div></div>"
  },
  {
    "front": "Definition of polar sets, singletons are polar in $d\\ge 2$.",
    "back": "<div><b>Definition.</b> A compact set $K\\subset \\mathbb{R}^d$ is called <i>polar</i> if $\\mathbb{P}^x \\left( B_t \\in K \\text{ for some $t &gt; 0$} \\right) = 0$ for all $x\\in \\mathbb{R}^d$.<br><br><b>Proposition.</b> Singletons are polar if $d\\ge 2$. In particular, Brownian motion is not recurrent if $d\\ge 2$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $T := \\inf \\left\\{ t &gt; 0\\colon B_t = 0 \\right\\} $. Then, for $x\\neq 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^x\\left( T &lt; \\infty \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{R \\to \\infty} \\mathbb{P}^x\\left( T &lt; T_R \\right) \\le \\lim_{R\\to \\infty}\\varliminf_{\\varepsilon\\to 0} \\mathbb{P}^x \\left( T_\\varepsilon &lt; T_R \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{R\\to \\infty} 0 = 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now, if $x = 0$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^0 \\left( \\text{$B_t = 0$ for some $t\\ge \\varepsilon$} \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}^0\\left( \\mathbb{P}^{B_\\varepsilon} \\left( T &lt; \\infty \\right)&nbsp; \\right) = 0\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for all $\\varepsilon &gt; 0$, and LHS converges to $\\mathbb{P}^0(T &lt; \\infty)$ as $\\varepsilon \\to 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Brownian motion is transient in $d \\ge 3$.",
    "back": "<div><b>Proposition.</b> Brownian motion is transient in $d\\ge 3$. That is, $\\lim_{t\\to \\infty}\\left| B_t \\right| = \\infty$ almost-surely.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $n\\in \\mathbb{N}$, put \\[&nbsp;&nbsp;&nbsp; A_n := \\left\\{ \\text{$\\left| B_t \\right| \\le \\sqrt{n} $ for some $t\\ge T_n$} \\right\\} .&nbsp;&nbsp;&nbsp; \\] Then, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^x(A_n) = \\mathbb{E}^x \\left[ \\mathbb{P}^{B_{T_n}}\\left( T_{\\sqrt{n} }&lt;\\infty \\right)&nbsp; \\right] = \\left( \\frac{1}{\\sqrt{n} } \\right) ^{d-2},\\\\&nbsp;&nbsp;&nbsp; \\] where we used that $\\mathbb{P}^x \\left( T_r &lt; \\infty \\right) = \\lim_{R\\to \\infty}\\frac{R^{2-d}-\\left| x \\right| ^{2-d}}{R^{2-d} - r^{2-d}}$, so \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^x\\left( \\varlimsup_{n\\to \\infty}A_n^{c} \\right) = \\lim_{n\\to \\infty} \\mathbb{P}^x \\left( \\bigcup_{k\\ge n} A_k^{c} \\right) \\ge \\lim_{n\\to \\infty} \\mathbb{P}^x \\left( A_n^{c} \\right) = 1,\\\\\\] so $(B_t)$ almost-surely eventually leaves $B(0,\\sqrt{n})&nbsp;$ for infinitely many $n\\in \\mathbb{N}$.</div><div></i> </p> </div><div></div>"
  },
  {
    "front": "Definition harmonic measure on $\\partial U$ for $U\\subset \\mathbb{R}^d$ open and bounded, and Poisson formula for harmonic measure on unit sphere.",
    "back": "<div><b>Definition/Proposition.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded, and $T:= \\inf \\left\\{ t\\ge 0\\colon B_t\\not\\in U \\right\\} $. Then $\\mu(x,\\cdot ):= \\mathbb{P}^x \\left( B_T \\in \\cdot&nbsp; \\right) $ for $x\\in U$ is called the <i>harmonic measure</i> on $\\partial U$. If $U$ is the unit sphere, then \\[&nbsp;&nbsp;&nbsp; \\mu(x,A) = \\mathbb{P}^x \\left( B_T \\in A \\right) = \\int_A \\frac{1 - \\left| x \\right| ^2}{\\left| x-y \\right| ^d} \\pi(\\mathop{}\\!\\mathrm{d} y),\\quad A\\in \\mathcal{B}(\\partial U),\\\\\\] for $x\\in U$, where $\\pi$ is the uniform measure on $\\partial U$.<br><p><i><b>Proof.</b>&nbsp; &nbsp; It suffices to show that, for any $\\phi\\in C( \\partial U)$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\phi(B_T) \\right] = \\int_{\\partial U} \\frac{1-\\left| x \\right| ^2}{\\left| x-y \\right| ^d}\\phi(y) \\pi(\\mathop{}\\!\\mathrm{d} y) =: u(x),\\quad x\\in U.&nbsp;&nbsp;&nbsp; \\] Thus it is sufficient to show that $u$ is harmonic and can be continuously extended to $\\overline{U}$ such that $u\\!\\!\\restriction_{\\partial U} = \\phi$. It is a straight-forward calculation that $\\Delta u \\equiv 0$. Now consider $\\phi\\equiv 1$. In that case, $u$ is rotationally symmetric and $u(0) = 1$, so for any $x\\in U$, $\\left| x \\right| \\in (0,1)$, \\[&nbsp;&nbsp;&nbsp; 1 = u(0) = \\mathbb{E}^0 \\left[ u(B_{T_{\\left| x \\right| }}) \\right] = \\mathbb{E}^0 \\left[ u(x) \\right] = u(x),\\\\\\] so $u\\equiv 1$. Now, let $z\\in \\partial U$, $\\delta &gt; 0$, $D_0 := \\partial U \\cap B(y,\\delta)$, and $D_1 = \\partial U \\setminus D_0$. Then, for $x\\in U$, $\\left| x-z \\right| &lt; \\delta$,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left| \\phi(z) - u(x) \\right| &nbsp;&nbsp;&nbsp; &amp;\\le \\int_{\\partial U} \\frac{1-\\left| x \\right| ^2}{\\left| x-y \\right| ^d} \\left| \\phi(z) - \\phi(y) \\right| \\pi(\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\left\\|\\phi\\right\\| \\underbrace{\\int_{D_1} \\frac{1-\\left| x \\right| ^2}{\\left| x-y \\right| ^d}\\pi(\\mathop{}\\!\\mathrm{d} y)}_{\\stackrel{  }{\\longrightarrow} 0 \\, (x\\to z)} + \\sup_{y\\in D_0} \\left| \\phi(z) - \\phi(y) \\right| \\end{align*}[/$$]<br>Thus, for any $\\delta &gt; 0$, \\[&nbsp;&nbsp;&nbsp; \\varlimsup_{\\substack{x\\to z \\\\ x\\in U} } \\left| \\phi(z) - u(x) \\right| \\le \\sup_{\\substack{\\left| y-z \\right| &lt; \\delta \\\\ y\\in \\partial U} } \\left| \\phi(z) - \\phi(y) \\right| \\stackrel{  }{\\longrightarrow} 0,\\quad \\delta \\to 0.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Poisson problem for $g\\in C(U)$ for $U\\subset \\mathbb{R}^d$ bounded and open, and connection with expected occupation time measure of Brownian motion and $G_U$.",
    "back": "<div><b>Definition/Proposition.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded, and $g\\in C(U)$. Then $u\\in C(\\overline{U}) \\cap C^2(U)$ is called a solution to the <i>Poisson problem for $g$</i> if \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\frac{1}{2}\\Delta u = -g &amp; \\text{on $U$},\\\\&nbsp;&nbsp;&nbsp; u = 0 &amp; \\text{on $\\partial U$}.\\end{cases}\\] In that case, if $T:= \\inf \\left\\{ t\\ge 0\\colon B_t\\not\\in U \\right\\} $, then \\[u(x) = \\mathbb{E}^x \\left[ \\int_0^T g(B_s) \\mathop{}\\!\\mathrm{d} s \\right] = \\int_U G_U(x,y) g(y) \\mathop{}\\!\\mathrm{d} y ,\\quad x\\in U,\\\\\\] if $G_U$ is Green's function of Brownian motion on $U$. In particular, solutions to the Poisson problem are unique (if they exist).<br><div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Ito's formula,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(B_{t\\wedge \\tau}) = u(x) + \\text{(clm)} - \\int_0^{t\\wedge \\tau} g(B_s) \\mathop{}\\!\\mathrm{d} s,\\quad 0\\le t &lt; T,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $M_t := u(B_{t\\wedge \\tau}) + \\int_0^{t\\wedge \\tau} g(B_s) \\mathop{}\\!\\mathrm{d} s$ defines a local martingale which is uniformly bounded by $\\left\\|u\\right\\| + T \\left\\|g\\right\\|$, so $L^2$-bounded, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(x)= \\mathbb{E}^x \\left[ M_0 \\right] = \\mathbb{E}^x \\left[ M_{\\infty} \\right]&nbsp; = \\mathbb{E}^x \\left[ \\int_0^T g(B_s) \\mathop{}\\!\\mathrm{d} s \\right] .&nbsp;&nbsp;&nbsp; \\] </i> </p> </div></div><div></div>"
  },
  {
    "front": "(In-)Finiteness of occupation time $\\int_0^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s$ of Brownian motion for open, bounded $U\\subset \\mathbb{R}^d$.",
    "back": "<div><b>Proposition.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded. Then, for any $x\\in \\mathbb{R}^d$, \\[&nbsp;&nbsp;&nbsp; \\int_0^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s = \\infty,\\quad \\text{$\\mathbb{P}^x$-a.s.},\\\\\\] if $d = 2$, and \\[\\mathbb{E}^x \\left[ \\int_0^\\infty \\boldsymbol{1}_{U}(B_s)\\mathop{}\\!\\mathrm{d} s \\right] &lt; \\infty,\\\\\\] if $d \\ge 3$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $d = 2$ and assume $U = B(0,r)$. Then put $T_0 := 0$ and, for $n\\in \\mathbb{N}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S_k &amp;:= \\inf \\left\\{ t &gt; T_{k-1} \\colon \\left| B_t \\right| &lt; r \\right\\} ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; T_k &amp;:= \\inf \\left\\{ t &gt; S_k\\colon \\left| B_t \\right| &gt; 2r \\right\\} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Since $B$ is neighbourhood-recurrent, this defines a sequence of finite stopping times. Now, if $k\\in \\mathbb{N}$ and $z \\ge 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}^x \\left(\\int_{S_k}^{T_k} \\boldsymbol{1}_U(B_s) \\mathop{}\\!\\mathrm{d} s \\ge z \\,\\middle\\vert\\, \\mathcal{F}_{S_k}\\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P} ^{B_{S_k}} \\left( \\int_0^{T_1} \\boldsymbol{1}_U(B_s) \\mathop{}\\!\\mathrm{d} s \\ge z \\right) \\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; is constant (because the RHS only depends on $\\left| B_{S_k} \\right| = r$) and does not depend on $k$, so $\\int_{S_k}^{T_k} \\boldsymbol{1}_U(B_s) \\mathop{}\\!\\mathrm{d} s$ is independent of $\\mathcal{F}_{S_k}$ and its non-zero distribution does not depend on $k$. Thus, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^\\infty \\boldsymbol{1}_U(B_s) \\mathop{}\\!\\mathrm{d} s = \\sum_{k=1}^\\infty \\int_{S_k}^{T_k} \\boldsymbol{1}_U(B_s) \\mathop{}\\!\\mathrm{d} s = \\infty&nbsp;&nbsp;&nbsp; \\] $\\mathbb{P}^x$-a.s. by the strong law of large numbers.<br><br>&nbsp;&nbsp;&nbsp; Now, if $d\\ge 3$, and $x = 0$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^0 \\left[ \\int_0^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_{U} G(0,y) \\mathop{}\\!\\mathrm{d} y = c(d) \\int_0^r s^{d-1} s^{2-d} \\mathop{}\\!\\mathrm{d} s = c(d) \\frac{r^2}{2}&lt; \\infty,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and, if $x \\neq 0$, and $T := T_{\\left| x \\right| }$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\int_0^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^0 \\left[ \\int_T^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s \\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{E}^0 \\left[ \\int_0^\\infty \\boldsymbol{1}_{U}(B_s) \\mathop{}\\!\\mathrm{d} s \\right] &lt; \\infty.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Green's function of Brownian motion in $\\mathbb{R}^d$ for $d\\ge 3$ and connection with expected occupation time measure.",
    "back": "<div><b>Definition/Proposition.</b> Let $d\\ge 3$ and put \\[&nbsp;&nbsp;&nbsp; G(x,y) := \\int_0^\\infty p_t(x,y) \\mathop{}\\!\\mathrm{d} t = \\frac{\\Gamma \\left( d / 2 - 1 \\right) }{2 \\pi^{d / 2}} \\left| x-y \\right| ^{2-d}.\\] Then, for $f\\colon \\mathbb{R}^d\\to \\mathbb{R}$ measurable and bounded or non-negative, \\[\\mathbb{E}^x \\left[ \\int_0^\\infty f(B_s) \\mathop{}\\!\\mathrm{d} s \\right] = \\int_{\\mathbb{R}^d} G(x,y) f(y) \\mathop{}\\!\\mathrm{d} y,\\quad x\\in \\mathbb{R}^d.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $f$ is bounded, then, by Fubini,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\int_0^\\infty f(B_s) \\mathop{}\\!\\mathrm{d} s \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^\\infty \\mathbb{E}^x \\left[ f(B_s) \\right] \\mathop{}\\!\\mathrm{d} s = \\int_0^\\infty \\int_{\\mathbb{R}^d} p_t(x,y) f(y) \\mathop{}\\!\\mathrm{d} y \\mathop{}\\!\\mathrm{d} s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_{\\mathbb{R}^d} G(x,y) f(y)\\mathop{}\\!\\mathrm{d} y,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for any $x\\in \\mathbb{R}^d$. If $f$ is non-negative, the claim thus follows with monotone convergence.</i> </p> </div><div></div>"
  },
  {
    "front": "Green's function $G_U$ of a bounded domain $U\\subset \\mathbb{R}^d$ of Brownian motion in $d\\ge 3$, and connection with expected occupation time measure.",
    "back": "<div><b>Definition/Proposition.</b> Let $U\\subset \\mathbb{R}^d$ be open and bounded, and put \\[&nbsp;&nbsp;&nbsp; G_U(x,y) := G(x,y) - \\int_{\\partial U} G(z,y) \\mu(x,\\mathop{}\\!\\mathrm{d} z),\\quad x,y\\in U,\\, x\\neq y,\\\\\\] where $\\mu(x,\\cdot ) = \\mathbb{P}^x \\left( B_T \\in \\cdot&nbsp; \\right) $ is the harmonic measure on $\\partial U$. Then, for $x\\in U$ and $f\\colon U \\to \\mathbb{R}$ measurable and bounded or non-negative, \\[\\mathbb{E}^x \\left[ \\int_0^T f(B_s) \\mathop{}\\!\\mathrm{d} s \\right] = \\int_U G_U(x,y) f(y) \\mathop{}\\!\\mathrm{d} y,\\\\\\] so $G_U(x,\\cdot )$ is the density of the expected occupation time measure.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $f\\colon U\\to \\mathbb{R}$ be measurable and bounded, and $x\\in U$. Put $f\\equiv 0$ on $U^{c}$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; w(x) := \\mathbb{E}^x \\left[ \\int_0^\\infty f(B_s) \\mathop{}\\!\\mathrm{d} s \\right] = \\int_{U} G(x,y) f(y) \\mathop{}\\!\\mathrm{d} y,\\\\&nbsp;&nbsp;&nbsp; \\] but also \\[&nbsp;&nbsp;&nbsp; w(x) = \\mathbb{E}^x \\left[ \\int_0^T f(B_s)\\mathop{}\\!\\mathrm{d} s \\right] + \\mathbb{E}^x \\left[ w(B_T) \\right] .&nbsp;&nbsp;&nbsp; \\] Thus,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^x \\left[ \\int_0^T f(B_s) \\mathop{}\\!\\mathrm{d} s \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_U G(x,y) f(y) \\mathop{}\\!\\mathrm{d} y - \\mathbb{E}^x \\left[ \\int_U G(B_T,y) f(y) \\mathop{}\\!\\mathrm{d} y \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_U G(x,y) f(y) \\mathop{}\\!\\mathrm{d} y - \\int_U \\left( \\int_{\\partial U} G(z,y) \\mu(x,\\mathop{}\\!\\mathrm{d} z) \\right) f(y) \\mathop{}\\!\\mathrm{d} y\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_U G_U(x,y) f(y) \\mathop{}\\!\\mathrm{d} y.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; It follows from this equation that $G_U$ must be non-negative, so the statement for non-negative $f$ follows by monotone convergence.</i> </p> </div><div></div>"
  },
  {
    "front": "Feynman-Kac formula for heat equation with dissipation rate in $\\mathbb{R}^d$.",
    "back": "<div><b>Definition/Proposition.</b> A continuous function $u\\colon [0,\\infty) \\times \\mathbb{R}^d\\to \\mathbb{R}$ that is $C^2$ on $(0,\\infty)\\times \\mathbb{R}^d$ is said to satisfy the <i>heat equation with heat dissipation rate $c\\colon (0,\\infty)\\times \\mathbb{R}^d\\to \\mathbb{R}$ and initial condition $f\\colon \\mathbb{R}^d\\to \\mathbb{R}$</i> if \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\partial u}{\\partial t}(t,x) = \\frac{1}{2}\\Delta u(t,x) - c(t,x)u(t,x) &amp; \\text{on $(0,\\infty)\\times \\mathbb{R}^d$},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(0,\\cdot ) = f(\\cdot ).&nbsp;&nbsp;&nbsp; \\end{cases}\\] In that case, and if $u$ is bounded on $[0,t]\\times \\mathbb{R}^d$ for all $t &gt; 0$ and $c$ is bounded, then \\[u(t,x) = \\mathbb{E}^x \\left[ f(B_t) \\exp \\left( - \\int_0^t c(t-r,B_r)\\mathop{}\\!\\mathrm{d} r \\right)&nbsp; \\right] ,\\quad t\\ge 0,x\\in \\mathbb{R}^d.\\] In particular, the heat equation has a unique (locally bounded) solution in this case.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Define, for fixed $t\\ge 0$ and $s\\in [0,t)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M_s := u(t-s,B_s) \\exp \\left( -\\int_0^s c(t-r,B_r)\\mathop{}\\!\\mathrm{d} r \\right) .&nbsp;&nbsp;&nbsp; \\] Then applying Ito's formula to the csm $(t-s,B^1_s,\\ldots ,B^d_s,\\int_0^s c(t-r,B_r)\\mathop{}\\!\\mathrm{d} r)_{s\\ge 0}$ gives that $M$ is a local martingale. It is bounded by assumption, so a uniformly integrable martingale, and $M_s \\stackrel{ \\text{a.s.} }{\\longrightarrow} f(B_t) \\exp \\left( -\\int_0^t c(t-r,B_r)\\mathop{}\\!\\mathrm{d} r \\right) =: M_t$, so convergence holds in $L^1$ and \\[&nbsp;&nbsp;&nbsp; u(t,x) = M_0 = \\mathbb{E}^x\\left[ M_0 \\right] = \\mathbb{E}^x \\left[ M_t \\right] &nbsp;&nbsp;&nbsp; \\] for $x\\in\\mathbb{R}^d$.</i> </p> </div><div></div>"
  },
  {
    "front": "Feynman-Kac formula for one-dimensional parabolic equation (and generalisation without proof).",
    "back": "<div><b>Theorem.</b> Suppose that $F$ solves \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\frac{\\partial F}{\\partial t}(t,x) +\\mu(t,x) \\frac{\\partial F}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2(t,x) \\frac{\\partial^2 F}{\\partial x^2} = 0 &amp;, 0\\le t \\le T,\\\\&nbsp;&nbsp;&nbsp; F(T,\\cdot ) = \\Phi(\\cdot ).\\end{cases}\\] Then, if $(X_t)_{0\\le t \\le T}$ solves $\\mathop{}\\!\\mathrm{d} X_t = \\mu(t,X_t)\\mathop{}\\!\\mathrm{d} t + \\sigma(t,X_t) \\mathop{}\\!\\mathrm{d} B_t$, and \\[\\int_0^T \\mathbb{E} \\left[ \\left( \\sigma(s,X_s) \\frac{\\partial F}{\\partial x}(s,X_s) \\right) ^2&nbsp; \\right] \\mathop{}\\!\\mathrm{d} s &lt; \\infty,\\\\\\] then $F(t,x) = \\mathbb{E} \\left[\\Phi(X_T) \\,\\middle\\vert\\, X_t = x\\right]$.<br><br><b>Remark.</b> There is also a formula in the case where there are additional terms $V(t,x) F(t,x) + f(t,x)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Applying Ito's formula to $(F(s,X_s))_{t\\le s \\le T}$ for some fixed $t$ gives<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Phi(X_T)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= F(T,X_T) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= F(t,X_t) + \\int_t^T \\left( \\frac{\\partial F}{\\partial s}(s,X_s) \\mu(s,X_s) \\frac{\\partial F}{\\partial x}(s,X_s) + \\frac{1}{2}\\sigma(s,X_s)^2 \\frac{\\partial^2 F}{\\partial x^2}(s,X_s)&nbsp; \\right) \\mathop{}\\!\\mathrm{d} s \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\phantom{=} + \\int_t^T \\sigma(s,X_s) \\frac{\\partial F}{\\partial x}(s,X_s) \\mathop{}\\!\\mathrm{d} B_s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= F(t,X_t) + \\int_t^T \\sigma(s,X_s) \\frac{\\partial F}{\\partial x}(s,X_s) \\mathop{}\\!\\mathrm{d} B_s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; By assumption, this local martingale is a true martingale, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[\\Phi(X_T) \\,\\middle\\vert\\, X_t = x\\right] = F(t,x).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Schilder's theorem on LDP of Brownian motion and sketch of proof.",
    "back": "<div><b>Theorem.</b> The family $(\\sqrt{\\varepsilon} B\\colon 0&lt;\\varepsilon&lt;1)$ of $E=C_0([0,1])$-valued random variables satisfies LDP with good rate function \\[&nbsp;&nbsp;&nbsp; I\\colon E\\to [0,\\infty]; \\, w\\mapsto \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{2} \\int_0^1 \\left| \\dot{w}(t) \\right| ^2 \\mathop{}\\!\\mathrm{d} t &amp;, w\\in H^{1,2},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &amp;, \\text{else}.&nbsp;&nbsp;&nbsp; \\end{cases}\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Uses the generalised contraction principle. Let $E_n = \\mathbb{R} ^{2^n}$, $\\pi_n\\colon E\\to E_n$ the evaluation at the $n$-th dyadics, $g_n\\colon E_n \\to E$ the natural linearly interpolating map, and $f_n := g_n \\circ \\pi_n\\colon E\\to E$, which is continuous. Put $X^\\varepsilon := \\sqrt{\\varepsilon} B$ and $X^\\varepsilon_n := f_n(X^\\varepsilon) = g_n(\\pi_n(X^\\varepsilon))$, where $\\pi_n(X^\\varepsilon)$ is just Gaussian and has LDP with strong rate \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I_n\\colon E_n \\to E_n; a \\mapsto \\frac{1}{2} \\sum_{k=1}^{2^n} 2^n \\left( a_k - a_{k-1} \\right) ^2,\\\\&nbsp;&nbsp;&nbsp; \\] so $X_n^\\varepsilon$ has good rate <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I_n'(w) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\inf \\left\\{ I_n(a)\\colon g_n(a) = w \\right\\} = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I(w) &amp;, w \\in g_n(E_n),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &amp;, \\text{else}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\inf \\left\\{ I(x)\\colon x\\in E, f_n(x) = w \\right\\} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; It thus suffices to show that $I$ is a good rate, that $X_n^\\varepsilon \\stackrel{  }{\\longrightarrow} X^\\varepsilon$ exponentially, and that $f_n \\stackrel{  }{\\longrightarrow} \\id_E$ uniformly on $\\left\\{ I \\le c \\right\\} $ for $c &gt; 0$.<br><br>&nbsp;&nbsp;&nbsp; Let $c &gt; 0$ and $w\\in I_c$. Then, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| w_t - w_s \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left| \\int_s^t \\dot{w}_r \\mathop{}\\!\\mathrm{d} r \\right| \\le \\sqrt{t-s} \\sqrt{\\int_0^t \\left| \\dot{w}_r \\right| ^2 \\mathop{}\\!\\mathrm{d} r} \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\sqrt{2c(t-s)} ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $I_c$ is equicontinuous and thus relatively compact. It remains to show closedness, which is not too hard.<br><br>&nbsp;&nbsp;&nbsp; Exponential convergence is quite straight-forward as well, and if $w\\in I_c$ for $c &gt; 0$, then, for $n\\in \\mathbb{N}$ and $t\\in [t^n_{k-1},t^n_k]$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|w - f_n(w)\\right\\|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\max_{1\\le k \\le 2^n} \\sup_{t\\in J_k} \\left| w_t - f_n(w)_t \\right| \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\max_k \\sup_{s\\le 2^{-n}} \\left| w_{t_k^n + s} - w_{t_k^n} \\right| \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\max_k \\int_{t_k^n}^{t_{k+1}^n} \\left| \\dot{w}_s \\right| \\mathop{}\\!\\mathrm{d} s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\sqrt{2^{-n}} \\sqrt{2c} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Exponential distribution (pdf, cdf, expectation, variance, mgf, cf).",
    "back": "<div>Let $X\\sim \\text{Exp}(\\lambda)$, $\\lambda &gt; 0$.<br><ol>  <li>$f(x) = \\lambda \\mathrm{e} ^{-\\lambda x}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{P}(X\\le x) = 1 - \\mathrm{e}^{-\\lambda x}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ X \\right] = \\frac{1}{\\lambda}$,&nbsp;&nbsp;&nbsp;</li>  <li>&nbsp; $\\mathbb{V}(X) = \\frac{1}{\\lambda^2}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ \\mathrm{e} ^{aX} \\right] = \\frac{\\lambda}{\\lambda - a}$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ \\mathrm{e} ^{\\mathrm{i} t X} \\right] = \\frac{\\lambda}{\\lambda - \\mathrm{i} t}$.</li></ol></div><div></div>"
  },
  {
    "front": "Poisson distribution (expectation, variance, pgf, mgf, cf)",
    "back": "<div>Let $X\\sim \\text{Poi}(\\lambda)$, $\\lambda &gt; 0$.<br><ol>  <li>$p_k = \\mathrm{e}^{-\\lambda} \\frac{\\lambda^k}{k!}, k\\in&nbsp; \\mathbb{N}_0$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ X \\right] = \\lambda$,&nbsp;&nbsp;&nbsp;</li>  <li>&nbsp; $\\mathbb{V}(X) = \\lambda$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ z^X \\right] = \\exp \\left( \\lambda(z-1) \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ \\mathrm{e} ^{aX} \\right] = \\exp \\left( \\lambda (\\mathrm{e}^a - 1) \\right) $,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{E} \\left[ \\mathrm{e}^{i t X} \\right] = \\exp \\left( \\lambda (\\mathrm{e}^{\\mathrm{i} t}-1) \\right) $.</li></ol></div><div></div>"
  },
  {
    "front": "Geometric distribution (both versions): Expectation, Variance, PGF/MGF/CF",
    "back": "<div>Let $X\\sim \\text{Geo}(p)$ (including first success), $Y = X-1$ (only failures), $p\\in (0,1]$.<br>\\begin{center}<br>&nbsp;&nbsp;&nbsp; \\begin{tabular}{c | c | c | c}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; $X$ &amp; $Y$ &amp; $Y$ where $p = a / (a+b)$ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\hline<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\rule{0pt}{3ex} $\\mathbb{E} $ &amp; $\\frac{1}{p}$ &amp; $\\frac{1-p}{p}$ &amp; $\\frac{b}{a}$ \\\\[5pt]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\hline\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\rule{0pt}{3ex}$\\mathbb{V}$ &amp; $\\frac{1-p}{p^2}$ &amp;&nbsp; &amp; $\\frac{b}{a}\\left( 1+\\frac{b}{a} \\right) $ \\\\[5pt]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\hline\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\rule{0pt}{4ex}$\\mathbb{E} \\left[ \\mathrm{e}^{a \\cdot } \\right] $ &amp;&nbsp; $\\frac{p \\mathrm{e}^a}{1-(1-p)\\mathrm{e}^a}$ &amp; $\\frac{p}{1-(1-p)\\mathrm{e}^a}$ &amp; $\\left( 1- \\frac{(\\mathrm{e}^a - 1)b}{a} \\right) ^{-1}$<br>&nbsp;&nbsp;&nbsp; \\end{tabular}<br>\\end{center}</div><div></div>"
  },
  {
    "front": "Probability generating function and how to obtain (factorial) moments and [$]p_k[/$] from it.",
    "back": "<div>For $\\mathbb{N}_0$-valued random variable $X$, $g(s) := g_X(s) := \\sum_{k=0}^\\infty p_k s^k$. Then,<br><ol>  <li>$p_k = \\frac{1}{k!} g^{(k)}(0)$,&nbsp;&nbsp;&nbsp;</li>  <li>$g ^{(k)}(1-)$ is $k$-th factorial moment, in particular $\\mathbb{E} \\left[ Z \\right] = g'(1-)$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{V}(X) = g''(1-) + g'(1-) - g'(1-)^2$.</li></ol></div><div>All proofs are trivial.</div><div></div>"
  },
  {
    "front": "Solution to recurrence $p_n = a_n p_{n-1} + b_n$ started at $p_s, \\, s\\in \\mathbb{Z}$ (and simplification if $p_s = b_s$).",
    "back": "<div>If $p_n = a_n p_{n-1} + b_n$, started at $p_s$ for some $s\\in \\mathbb{Z}$, then \\[p_n = \\sum_{l=s+1}^n \\left[ b_l \\prod_{m=l+1}^n a_m \\right] + p_s \\prod_{m=s+1}^n a_m.\\] <br>If $p_s = b_s$, this simplifies to $p_n = \\sum_{l=s}^n \\left[ b_l \\prod_{m = l+1}^n a_m \\right] $.</div><div></div>"
  },
  {
    "front": "Lagrange Multipliers.",
    "back": "<div>Let $f,g_1, \\ldots ,g_s \\in C^1(U)$ and $U\\subset \\mathbb{R}^d$ open, such that $\\left\\{ \\nabla g_i(x)\\colon i\\in [s] \\right\\} \\subset \\mathbb{R}^d$ is linearly independent for all $x\\in U$. Put \\[&nbsp;&nbsp;&nbsp; \\Lambda\\colon U \\times \\mathbb{R}^s \\to \\mathbb{R}; \\, (x,\\lambda) \\mapsto f(x) - \\sum_{k=1}^s \\lambda_k g_k(x).\\] Then, if $x_\\star \\in U$ is a local extremum of $f$ under the condition $g_i(x_\\star) = 0\\forall i\\in [s]$, then $\\frac{\\partial \\Lambda}{\\partial x_i} (x_\\star)= 0$ for all $i\\in [d] $. (Together with the constraints, this gives $d + s$ equations for $x$ and $\\lambda$, and the solutions are all possible <i>candidates</i> for local extrema).</div><div></div>"
  },
  {
    "front": "Stirling formula, standard bounds for binomial coefficients (with proof), and sum formula for $\\sum_{m=k}^n \\binom{m}{k}$.",
    "back": "<div>Stirling formula is $n! \\sim \\sqrt{2\\pi n} \\left( \\frac{n}{\\mathrm{e}} \\right) ^n$ (and $\\ge$). \\[&nbsp;&nbsp;&nbsp; \\left( \\frac{n}{k} \\right) ^k \\le \\binom{n}{k} \\le \\frac{n^k}{k!}\\le \\left( \\frac{\\mathrm{e} n}{k} \\right) ^k.\\] The upper bound follows from $k! \\ge (k/\\mathrm{e})^k$, which follows directly from series expansion of $\\mathrm{e}^k$.&nbsp; \\[\\sum_{m=k}^n \\binom{m}{k}&nbsp; = \\binom{n+1}{k+1} .\\]</div><div></div>"
  },
  {
    "front": "A continuous local martingale $M$ with $M_0 = 0$ converges a.s. on $\\left\\{ \\left&lt;M \\right&gt; _\\infty &lt; \\infty \\right\\} $.",
    "back": "<div><b>Proposition.</b> Let $M$ be a clm with $M_0 = 0$. Then&nbsp; there exists $M_\\infty$ such that $M_t \\stackrel{  }{\\longrightarrow} M_\\infty$ a.s. on $\\left\\{ \\left&lt;M \\right&gt; _\\infty&lt;\\infty \\right\\} $.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $n\\in \\mathbb{N}$, put $\\tau_n := \\inf \\left\\{ t\\ge 0\\colon \\left&lt;M \\right&gt; _t &gt;n \\right\\} $. Then $M^{\\tau_n}$ is an $L^2$-bounded martingale, so \\[&nbsp;&nbsp;&nbsp; M^{\\tau_n}_t \\stackrel{  }{\\longrightarrow} M^{n}_\\infty\\\\&nbsp;&nbsp;&nbsp; \\] a.s. Let $N$ be a $\\mathbb{P}$-null-set such that $M^{\\tau_n}(\\omega) \\stackrel{  }{\\longrightarrow} M^n_\\infty(\\omega)$ for all $n\\in \\mathbb{N}$ and $\\omega\\not\\in N$. Then, if $\\omega\\in \\left\\{ \\left&lt;M \\right&gt; _\\infty\\le n \\right\\}\\setminus N $ and $n\\le m$, $\\tau_n(\\omega) = \\tau_m(\\omega) = \\infty$ and \\[&nbsp;&nbsp;&nbsp; M^n_\\infty(\\omega) \\longleftarrow M^{\\tau_n}_t(\\omega) = M_t(\\omega) = M^{\\tau_m}_t(\\omega) \\stackrel{  }{\\longrightarrow} M^m_\\infty(\\omega).\\] Thus, \\[M_\\infty(\\omega) := \\begin{cases}&nbsp;&nbsp;&nbsp; M^n_\\infty(\\omega) &amp;, \\left&lt;M \\right&gt; _\\infty(\\omega) \\le n,\\\\&nbsp;&nbsp;&nbsp; 0 &amp;, \\left&lt;M \\right&gt; _\\infty(\\omega) = \\infty,\\\\\\end{cases}\\] is well-defined, and $M_t(\\omega) \\stackrel{  }{\\longrightarrow} M_\\infty(\\omega)$ for all $\\omega\\in \\left\\{ \\left&lt;M \\right&gt; _\\infty&lt;\\infty \\right\\}\\setminus N $.</i> </p> </div><div></div>"
  },
  {
    "front": "Distribution of $\\int_0^t A(s) \\mathop{}\\!\\mathrm{d} B_s$ for $A\\in C([0,\\infty),\\mathbb{R} ^{n\\times m})$ and $m$-dimensional Brownian motion $B$.",
    "back": "<div><b>Proposition.</b> If $A\\colon [0,\\infty) \\to \\mathbb{R} ^{n\\times m}$ has continuous, and $B$ is an $m$-dimensional Brownian motion, then&nbsp; \\[&nbsp;&nbsp;&nbsp; \\int_0^t A(s) \\mathop{}\\!\\mathrm{d} B_s \\sim \\mathcal{N}_n \\left( 0, \\int_0^t A(s) A(s)^\\top \\mathop{}\\!\\mathrm{d} s \\right) .\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Fix $t\\ge 0$, and take $\\Delta_n$ with $\\left| \\Delta_n \\right| \\to 0$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{t_i \\in \\Delta_n} A(t_i) (B_{t_{i+1}}- B_{t_i}) \\stackrel{ \\text{a.s.} }{\\longrightarrow} \\int_0^t A(s) \\mathop{}\\!\\mathrm{d} B_s.&nbsp;&nbsp;&nbsp; \\] The LHS has distribution<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{t_i\\in \\Delta_n} A(t_i) \\mathcal{N}_m\\left( 0, (t_{i+1}-t_i) I_m \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\stackrel{d}{=} \\sum_{t_i\\in \\Delta_n} \\mathcal{N}_n \\left( 0, (t_{i+1}-t_i) A(t_i)A(t_i)^\\top \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\stackrel{d}{=} \\mathcal{N}_n\\left( 0, \\sum_{t_i\\in \\Delta_n} (t_{i+1}-t_i) A(t_i)A(t_i)^\\top \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\stackrel{ d }{\\longrightarrow} \\mathcal{N}_n\\left( 0, \\int_0^t A(s)A(s)^\\top \\mathop{}\\!\\mathrm{d} s \\right) .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Transformation formula for densities.",
    "back": "<div>Let $U,V\\subset \\mathbb{R}^d$ open, $\\phi$ a $C^1$-diffeomorphism from $U$ to $V$ with non-singular Jacobian on $U$. If $X$ is a $U$-valued random vector with density $f_X\\colon U\\to [0,\\infty)$, then $Y$ has density \\[&nbsp;&nbsp;&nbsp; f_Y(y) = f_X(\\phi^{-1}(y)) \\left| \\det J_{\\phi^{-1}}(y) \\right| ,\\quad y\\in V.\\] Alternatively, $\\left|\\det J_{\\phi^{-1}}(y) \\right|= \\left| \\det J_\\phi \\left( \\phi^{-1}(y) \\right)&nbsp; \\right| ^{-1}$.</div><div></div>"
  },
  {
    "front": "Version of Ito's formula for $f\\in C^2(U,\\mathbb{R})$ with $U\\subset \\mathbb{R}^d$ open.",
    "back": "<div><b>Proposition.</b> If $f\\in C^2(U,\\mathbb{R})$ with $U\\subset \\mathbb{R}$ open, and $X$ an $\\mathbb{R}$-valued csm, then for any stopping time $\\tau$ such that $X^\\tau$ is $A$-valued for some closed $A\\subset U$, \\[&nbsp;&nbsp;&nbsp; f(X_{t\\wedge \\tau}) = f(X_0) + \\int_0 ^{t\\wedge \\tau} f'(X_s)\\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2} \\int_0 ^{t\\wedge \\tau} f''(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s,\\quad t\\ge 0,\\\\\\] a.s. Analogously for $d &gt; 1$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $g\\in C^2(\\mathbb{R})$ with $g = f$ on $A$. Then Ito applied to $g$ and $X^\\tau$ gives<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(X_{t\\wedge \\tau}) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= g(X^\\tau_t) = g(X_0) + \\int_0^t g'(X_{s\\wedge \\tau}) \\mathop{}\\!\\mathrm{d} \\left( X^\\tau \\right) _s + \\frac{1}{2} \\int_0^t g''(X_{s\\wedge \\tau}) \\mathop{}\\!\\mathrm{d} \\left&lt;X^\\tau \\right&gt; _s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= f(X_0) + \\int_0 ^{t\\wedge \\tau} g'(X_{s\\wedge \\tau}) \\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2}\\int_0^{t\\wedge \\tau} g''(X_{s\\wedge \\tau}) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= f(X_0) + \\int_0^{t\\wedge \\tau} f'(X_s) \\mathop{}\\!\\mathrm{d} X_s + \\frac{1}{2}\\int_0^{t\\wedge \\tau}f''(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "If $M$ is a clm, $M_0 = x$, $a &lt; x &lt; b$, and $\\tau_a\\wedge \\tau_b &lt; \\infty$ a.s., what is $\\mathbb{P}(\\tau_a &lt; \\tau_b)$.",
    "back": "<div><b>Proposition.</b> $\\mathbb{P}(\\tau_a &lt; \\tau_b) = \\frac{b-x}{b-a}$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Set $\\tau := \\tau_a\\wedge \\tau_b$. Then $M^\\tau$ is a bounded martingale, and $\\tau &lt; \\infty$ a.s., so by OST for finite stopping times, $p := \\mathbb{P}(\\tau_a &lt; \\tau_b)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = \\mathbb{E} \\left[ M^\\tau_0 \\right] = \\mathbb{E} \\left[ M^\\tau_\\tau \\right] = \\mathbb{E} \\left[ M_\\tau \\right] = pa + (1-p)b.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Skorokhod's lemma ($z = y + a$, without proof)",
    "back": "<div>If $y\\colon [0,\\infty) \\to \\mathbb{R}$ is continuous with $y(0) \\ge 0$, then there exist unique continuous functions $a,z\\colon [0,\\infty)\\to \\mathbb{R}$ such that<br><ol>  <li>$z = y+a$,&nbsp;&nbsp;&nbsp;</li>  <li>$z \\ge 0$,&nbsp;&nbsp;&nbsp;</li>  <li>$a$ is increasing, $a(0) = 0$, and $\\mathop{}\\!\\mathrm{d} a$ is supported on $\\left\\{ z = 0 \\right\\} $.</li></ol><br>In that case, \\[&nbsp;&nbsp;&nbsp; a(t) = 0\\vee \\left[ \\sup_{s\\le t}(-y(s)) \\right] .\\]</div><div></div>"
  },
  {
    "front": "Let $M$ be a clm, $M_0 = 0$, $S_t := \\sup_{s\\le t}M_s$. Connection between processes $(L^0_t)$, $(S_t)$, and $(\\left| M_t \\right|) $, and also $(L^x_t)$.",
    "back": "<div><b>Proposition.</b> As processes, \\[&nbsp;&nbsp;&nbsp; (S_t-M_t,S_t) \\stackrel{d}{=} (\\left| M_t \\right| ,L^0_t),\\qquad (L^x_t) \\stackrel{d}{=} \\big((S_t-\\left| x \\right| )\\vee 0\\big).\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $X_t := -\\int_0^t \\operatorname{sgn}(M_s)\\mathop{}\\!\\mathrm{d} M_s $ defines a clm, $X_0 = 0$, and $\\left&lt;X \\right&gt; = \\left&lt;M \\right&gt; $, so $(X_t = B_{\\left&lt;X \\right&gt; _t}= B_{\\left&lt;M \\right&gt; _t}) \\stackrel{d}{=} (\\widetilde{B}_{\\left&lt;M \\right&gt; _t} = M_t)$. Then, $\\left| M_t \\right|&nbsp; = -X_t + L_t$, so $L_t = \\sup_{s\\le t} X_s$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left( \\left| M_t \\right| ,L_t \\right) = \\left( L_t - X_t,L_t \\right) \\stackrel{d}{=} \\left( S_t - M_t,S_t \\right) .&nbsp;&nbsp;&nbsp; \\] Finally, $\\left| M_t - x \\right| = \\left| x \\right|&nbsp; - X_s + L^x_t$, so&nbsp; \\[&nbsp;&nbsp;&nbsp; L^x_t = \\sup_{s\\le t} \\left( X_s - \\left| x \\right|&nbsp; \\right) \\vee 0 = \\left( \\sup_{s\\le t }X_s - \\left| x \\right|&nbsp; \\right) \\vee 0.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "For a clm $X$, under what assumption on $\\left&lt;X \\right&gt; $ can you conclude $\\frac{X_t}{t^\\alpha} \\stackrel{  }{\\longrightarrow} 0$ a.s. as $t\\to \\infty$ for $\\alpha &gt; 0$.",
    "back": "<div><b>Proposition.</b> If $X$ is a clm and $\\left&lt;X \\right&gt; _t \\le C t^\\beta$ a.s. for all $t&gt; 0$ for some $c,\\beta &gt; 0$, then \\[&nbsp;&nbsp;&nbsp; \\frac{X_t}{t^\\alpha}\\stackrel{  }{\\longrightarrow} 0,\\quad t\\to \\infty,\\\\\\] a.s. for all $\\alpha &gt; \\beta / 2$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By BDG, $\\mathbb{E} \\left[ (X_t^\\star)^p \\right] \\le c_p \\mathbb{E} \\left[ \\left&lt;X \\right&gt;_t ^{p / 2}&nbsp; \\right] \\le c t ^{\\beta p / 2} $ for any $p &gt; 0$, so, for $\\varepsilon &gt; 0$, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( X_n^\\star \\ge \\varepsilon n^\\alpha \\right) \\le (\\varepsilon n^\\alpha)^{-p} c n ^{\\beta p / 2} = c n ^{p \\left( \\beta / 2 - \\alpha \\right) }.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; This is summable for large $p$, so $X_n^\\star / n^\\alpha \\le \\varepsilon$ eventually a.s., so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{t\\to \\infty} \\frac{|X_t|}{t^\\alpha} \\le \\varlimsup_{n\\to \\infty} \\frac{X_n^\\star}{(n-1)^\\alpha} = \\varlimsup_{n\\to \\infty} \\frac{X_n^\\star}{n^\\alpha} \\le \\varepsilon,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; a.s. for any $\\varepsilon &gt; 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "If $f\\in C^2([s,\\infty),\\mathbb{R})$ and $X$ is a csm, what is $\\mathop{}\\!\\mathrm{d} (f(\\cdot )X)_t$ and $\\left&lt;f(\\cdot )X \\right&gt; _t$.",
    "back": "<div><b>Proposition.</b> $\\mathop{}\\!\\mathrm{d} (f(\\cdot )X)_t = f'(t) X_t \\mathop{}\\!\\mathrm{d} t + f(t) \\mathop{}\\!\\mathrm{d} X_t$ and \\[&nbsp;&nbsp;&nbsp; \\left&lt;f(\\cdot )X \\right&gt; _t = \\int_s^t f(s)^2 \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s,\\\\\\] for $t\\ge s$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\tilde{f}\\in C^2(\\mathbb{R},\\mathbb{R})$ such that $\\tilde{f} = f$ on $[s,\\infty)$, and put $g(t,x) = \\tilde{f}(t) x$ for $x,t\\in \\mathbb{R}$, so Ito applied to $g$ and $(t,X_t)_{t\\ge s}$ gives<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(t) X_t = g(t,X_t) = f(s) X_s + \\int_s^t f'(r) X_r \\mathop{}\\!\\mathrm{d} r + \\int_s^t f(r) \\mathop{}\\!\\mathrm{d} X_r,\\quad t\\ge s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Local time of a clm in terms of Brownian local time. More specifically, local time of $(g(t) B_{f(t)})$.",
    "back": "<div><b>Proposition.</b> If $B$ is a BM, $f,g$ continuous, $f(0) = 0$ increasing, $g &gt; 0$ of finite variation, and $Y_t := g(t) B_{f(t)}$, $t\\ge 0$, then \\[&nbsp;&nbsp;&nbsp; L^0_t(Y) = \\int_0^t g(s) \\mathop{}\\!\\mathrm{d} L^0_{f(s)}(B) = \\int_0 ^{f(t) } g (f^{-1}(s)) \\mathop{}\\!\\mathrm{d} L_s^0(B),\\\\\\] where the second equality holds if $f$ is strictly increasing. In particular, if $M$ is a clm, \\[L^0_t(M) = L^0_{\\left&lt;M \\right&gt; _t}(W)\\] for a BM $W$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Write $L_t^Y := L_t^0(Y), L_t = L_t^0(B)$. Recall that $\\mathop{}\\!\\mathrm{d} \\left| B_f(s) \\right| = \\operatorname{sgn} (B_{f(s)}) \\mathop{}\\!\\mathrm{d} B_{f(s)} + \\mathop{}\\!\\mathrm{d} L_{f(s)}$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| Y_t \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= g(t) \\left| B_{f(t)} \\right| = \\int_0^tg(s) \\mathop{}\\!\\mathrm{d} \\left| B_f(s) \\right| + \\int_0^t \\left| B_{f(s)} \\right| \\mathop{}\\!\\mathrm{d} g(s)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^t g(s) \\operatorname{sgn}(B_{f(s)}) \\mathop{}\\!\\mathrm{d} B_{f(s)} + \\int_0^t g(s) \\mathop{}\\!\\mathrm{d} L_{f(s)} + \\int_0^t \\left| B_{f(s)} \\right| \\mathop{}\\!\\mathrm{d} g(s),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; but also, since $\\mathop{}\\!\\mathrm{d} Y_s = B_{f(s)}\\mathop{}\\!\\mathrm{d} g(s) + g(s) \\mathop{}\\!\\mathrm{d} B_{f(s)}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| Y_t \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^t \\operatorname{sgn}(Y_s) \\mathop{}\\!\\mathrm{d} Y_s + L^Y_t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^t \\operatorname{sgn}(B_{f(s)}) B_{f(s)}\\mathop{}\\!\\mathrm{d} g(s) + \\int_0^t \\operatorname{sgn}(B_{f(s)}) g(s) \\mathop{}\\!\\mathrm{d} B_{f(s)} + L^Y_t\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$],<br>&nbsp;&nbsp;&nbsp; so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; L^Y_t = \\int_0^t g(s) \\mathop{}\\!\\mathrm{d} L_{f(s)} .&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Local time of a csm as an almost-sure limit.",
    "back": "<div><b>Proposition.</b> If $X$ is a csm, $x\\in \\mathbb{R}$, $t\\ge 0$, then a.s. \\[&nbsp;&nbsp;&nbsp; L^a_t(X) = \\lim_{\\varepsilon\\to 0} \\frac{1}{\\varepsilon}\\int_0^t \\boldsymbol{1}_{[a,a+\\varepsilon)}(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By the occupation time formula, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{\\varepsilon}\\int_0^t \\boldsymbol{1}_{[a,a+\\varepsilon)}(X_s) \\mathop{}\\!\\mathrm{d} \\left&lt;X \\right&gt; _s = \\frac{1}{\\varepsilon} \\int_a^{a + \\varepsilon} L^x_t \\mathop{}\\!\\mathrm{d} x ,\\\\&nbsp;&nbsp;&nbsp; \\] and the claim follows from right-continuity of $L$.</i> </p> </div><div></div>"
  },
  {
    "front": "Dubins-Schwarz for general clm's (with idea of proof).",
    "back": "<div><b>Theorem.</b> If $M$ is a clm, $M_0 = 0$, then there is an enlarged probability space with a BM $\\beta \\perp \\!\\!\\! \\perp M$ such that \\[B_t = \\begin{cases}&nbsp;&nbsp;&nbsp; M_{\\tau_t} &amp;, t &lt; \\left&lt;M \\right&gt; _\\infty,\\\\&nbsp;&nbsp;&nbsp; M_\\infty + \\beta_{t - \\left&lt;M \\right&gt; _\\infty} &amp;, t\\ge \\left&lt;M \\right&gt; _\\infty\\\\\\end{cases}\\] is a BM.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(\\Omega',(\\mathcal{F}_t'),\\mathbb{P}')$ host a BM $\\beta$ and consider $(\\Omega\\times \\Omega', (\\mathcal{F}_{\\tau_t}\\otimes \\mathcal{F}_t),\\mathbb{P}\\otimes \\mathbb{P}')$, and put \\[&nbsp;&nbsp;&nbsp; B_t = M_{\\tau_t} + \\int_0^t \\boldsymbol{1}_{\\left\\{s &gt; \\left&lt;M \\right&gt; _\\infty\\right\\}} \\mathop{}\\!\\mathrm{d} \\beta'_s,\\quad t\\ge 0.\\] Then $(B_t)$ is adapted, a clm, and a BM by Levy.</i> </p> </div><div></div>"
  },
  {
    "front": "Glauber dynamics (definition of Markov chain, stationary distribution).",
    "back": "[PGL, page 10]"
  },
  {
    "front": "Mixing time of Glauber dynamics.",
    "back": "[PGL page 11]"
  },
  {
    "front": "Critical percolation probability [$]p^{(x)}_c[/$] in infinite, connected graph does not depend on origin vertex [$]x[/$].",
    "back": "[PGL Page 5]"
  },
  {
    "front": "Third proof of Sauer-Selah.",
    "back": "[Combinatorics, Page 5]"
  },
  {
    "front": "If $p\\in \\mathbb{P}$, $\\mathcal{A}\\subset [4p]^{(2p)}$ such that $\\left| A\\cap B \\right| \\neq p$ for all $A,B\\in \\mathcal{A}$, then $\\left| \\mathcal{A} \\right| \\le \\binom{4p}{p} $.",
    "back": "[Combinatorics, Page 6]"
  },
  {
    "front": "If $H_1, \\ldots ,H_m$ are hyperplanes in $\\mathbb{R}^n$ that cover all but one vertex in $\\left\\{ 0,1 \\right\\} ^n$, then $m\\ge n$.",
    "back": "[Combinatorics, Page 6]"
  },
  {
    "front": "Definition of pseudo-Poisson process, semigroup and generator.",
    "back": "<div><b>Proposition.</b> Let $E$ be a measurable space, $\\alpha &gt; 0$ and $\\mu\\colon E\\times \\mathcal{E}\\to [0,1]$ a stochastic kernel. Then the associated <i>pseudo-Poisson process</i> on $E$ is $X = Y \\circ N$, where $N$, $Y$ are independent, respectively a Poisson process with rate $\\alpha$ on $\\mathbb{N}_0$ and a discrete-time Markov chain on $E$ with transition kernel $\\mu$. Then $T_t = \\mathrm{e}^{t A}$ (defined on the Banach space $B_b(E)$) where $A = \\alpha(G-I)$, that is, \\[&nbsp;&nbsp;&nbsp; (Af)(x) = \\alpha \\int_E (f(y) - f(x)) \\mu(x,\\mathop{}\\!\\mathrm{d} y),\\quad f\\in C(E), x\\in E.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; T_t f(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=0}^\\infty \\mathbb{P}(N_t = n) \\mathbb{E}^x \\left[ Y_n \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=0}^\\infty \\mathrm{e}^{-\\alpha t} \\frac{(\\alpha t)^n}{n!} G^n f(x)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{\\alpha (G-I) t} f (x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Borsuk's conjecture.",
    "back": "[Combinatorics, Page 6]"
  },
  {
    "front": "Dual of Dilworth.",
    "back": "<div><b>Proposition.</b> In a finite poset, the size of a largest chain is the minimal size of a covering by antichains.</div><div></div><div><p><i><b>Proof.</b></div><div>One direction is clear. For the other, suppose that every chain has size at most $k\\in \\mathbb{N}$. For $x\\in P$ put $x$ in $\\mathcal{F}_l$ if $l\\in[k]$ is the size of a largest chain with maximal element $x$. Then $\\mathcal{F}_1,\\ldots,\\mathcal{F}_k$ is an antichain-covering of $P$.</div><div></i> </p> </div><div><br><div></div><div>[Combinatorics, Sheet 1, Exercise 3]</div></div>"
  },
  {
    "front": "Characterisation of $A &lt;_\\text{colex} B$ for $A,B\\subset [n]$.",
    "back": "<div><b>Claim.</b> $A &lt;_\\text{colex} B$ iff $\\left\\|A\\right\\| := \\sum_{i\\in A} 2^i &lt; \\sum_{i\\in B} 2^i$.</div><div></div><div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clear if $A\\subset B$ or $B\\subset A$, so suppose not. By definition, $A &lt;_\\text{colex}B$ iff $\\max (A\\setminus B) &lt; \\max(B\\setminus A)$, so HTS that $\\max U &lt; \\max V$ iff $\\left\\|U\\right\\|&lt; \\left\\|V\\right\\|$ for disjoint sets $\\varnothing \\neq U,V\\subset [n]$. Suppose the former holds. Then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|U\\right\\| \\le \\sum_{i=1}^{\\max U}2^i = 2^{\\max U+1} - 1 \\le 2^{\\max V}-1 &lt; 2^{\\max V} \\le \\left\\|V\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; By the same argument, if $\\max V &lt; \\max U$, then $\\left\\|V\\right\\|&lt; \\left\\|U\\right\\|$.</i> </p> </div><div></div>"
  },
  {
    "front": "Exponential bound on $\\mathbb{P}\\left( \\sup_{s\\le t}\\left| X_s \\right|\\ge x&nbsp; \\right) $ for a clm $X$ with $X_0 = 0$ and deterministic bound on $\\left&lt;X \\right&gt; _t$.",
    "back": "<div><b>Proposition.</b> If $X$ is a continuous local martingale with a deterministic bound $\\left&lt;X \\right&gt; _t \\le S_t$ for all $t &gt; 0$, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{s\\le t} \\left| X_t \\right| \\ge x \\right) \\le 2 \\exp \\left( -\\frac{x^2}{2S_t} \\right) ,\\quad t &gt; 0.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Novikov, the stochastic exponential is a true martingale. For any $\\lambda &gt; 0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{s\\le t} X_s \\ge x \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( \\mathrm{e}^{\\lambda \\sup_{s\\le t}X_s -\\lambda^2 \\frac{\\left&lt;X \\right&gt; _t}{2} } \\ge \\mathrm{e}^{\\lambda x - \\lambda^2 \\frac{\\left&lt;X \\right&gt; _t}{2} }&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{P}\\left( \\sup_{s\\le t} \\mathrm{e}^{\\lambda X_s - \\lambda ^2\\frac{\\left&lt;X \\right&gt; _s}{2}} \\ge \\mathrm{e}^{\\lambda x - \\lambda^2 \\frac{S _t}{2}} \\right)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathrm{e}^{\\lambda^2 S_t / 2 - \\lambda x} \\\\&nbsp; &nbsp; &nbsp; &nbsp; &amp;= \\exp \\left(-\\frac{x^2}{2 S_t}\\right),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp; &nbsp; where we chose $\\lambda = x / S_t$. Finally, $\\left&lt;-X \\right&gt; = \\left&lt;X \\right&gt;&nbsp; $, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\sup_{s\\le t} \\left| X_s \\right| \\ge x \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{P}\\left( \\sup_{s\\le t}X_s \\ge x \\right) + \\mathbb{P}\\left( \\sup_{s\\le t}(-X_s) \\ge x \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 2 \\exp \\left( -\\frac{x^2}{2 S_t} \\right) .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Regularity of local times.",
    "back": "<div><b>Proposition.</b> Let $X$ be a continuous semimartingale with local times $(\\widetilde{L}^a_t)$. Then there exists a $\\mathcal{B}([0,\\infty))\\otimes \\mathcal{P}$-measurable map $(a,t,\\omega) \\mapsto L^a_t(\\omega)$ such that $(\\widetilde{L}^a_t)_{t\\ge 0}$ and $(L^a_t)_{t\\ge 0}$ are indistinguishable for all $a\\in \\mathbb{R}$ and such that $L$ is continuous in $t$ and cadlag in $a$, and, a.s., \\[L^a_t - L^{a-}_t = 2\\int_0^t \\boldsymbol{1}_{\\left\\{ X_s = a \\right\\} }\\mathop{}\\!\\mathrm{d} X_s = 2\\int_0^t \\boldsymbol{1}_{\\left\\{ X_s = a \\right\\} }\\mathop{}\\!\\mathrm{d} A_s,\\\\\\] for $t\\ge 0$ and $a\\in \\mathbb{R}$. In particular, $L$ is continuous in $a$ if $X$ is a local martingale.</div><div></div>"
  },
  {
    "front": "Connection between a process $(M_t)$ on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$ and its law on the canonical path space being *martingales.",
    "back": "<div><b>Proposition.</b> If $M = (M_t)$ is a *martingale on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t),\\mathbb{P})$, then the canonical process $X$ on $(\\mathbb{R} ^{[0,\\infty)}, \\mathcal{F},\\mathbb{P}^M)$ is a *martingale w.r.t. the natural filtration $\\mathcal{F}^X_t = \\sigma(X_s\\colon s\\le t)$. The converse holds if $(\\mathcal{F}_t) = (\\mathcal{F}^M_t)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $0\\le s \\le t$. Then the set \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{D}:= \\left\\{ A\\in \\mathcal{F}^X_s\\colon \\mathbb{P}^M(X_t \\boldsymbol{1}_{A}) \\phantom{i}^\\star\\!\\!= \\mathbb{P}^M(X_s \\boldsymbol{1}_{A}) \\right\\} &nbsp;&nbsp;&nbsp; \\] is a Dynkin system and contains cylinders, since, if $A = \\left\\{ X_{t_i}\\in B_i\\forall i\\in [n] \\right\\} $, then \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}^M(X_t \\boldsymbol{1}_{A}) = \\mathbb{P}\\left( M_t \\boldsymbol{1}_{\\left\\{ M_{t_i} \\in B_i \\forall i\\in [n] \\right\\} } \\right) \\phantom{i}^\\star\\!\\!= \\mathbb{P}(M_s \\boldsymbol{1}) = \\mathbb{P}^M(X_s \\boldsymbol{1}_{A}).\\] The same argument shows that, assuming the latter, that $M$ is a *martingale w.r.t. $(\\mathcal{F}^M_t)$</i> </p> </div><div></div>"
  },
  {
    "front": "Is a martingale $M$ on $(\\Omega,(\\mathcal{F}_t),\\mathbb{P})$ also a martingale w.r.t. the right-continuous completion of $(\\mathcal{F}_t)$?",
    "back": "<div><b>Proposition.</b> If $M = (M_t)$ is a <b>rightcontinuous</b> martingale on $(\\Omega,\\mathcal{F},(\\mathcal{F}_t), \\mathbb{P})$, then so is it w.r.t. the right-continuous completion of $(\\mathcal{F}_t)$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Completion is clear. Now let $0\\le s &lt; t$ and $A\\in \\mathcal{F}_{s+}$. Let $t &gt; s_n \\downarrow s$, so $X_{s_n} \\stackrel{ \\text{a.s.} }{\\longrightarrow} X_s$, and $X_{s_n}= \\mathbb{E} \\left[X_t \\,\\middle\\vert\\, \\mathcal{F}_{s_n}\\right]$, so uib, so $X_{s_n}\\stackrel{ L^1 }{\\longrightarrow} X_s$. Also $A\\in \\mathcal{F}_{s_n}$ for all $n\\in \\mathbb{N}$, so \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ X_t \\boldsymbol{1}_{A} \\right] = \\mathbb{E} \\left[ X_{s_n} \\boldsymbol{1}_{A} \\right] \\stackrel{  }{\\longrightarrow} \\mathbb{E} \\left[ X_s \\boldsymbol{1}_{A} \\right] .&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "ML and moment estimators based on number $K$ of haplotypes.",
    "back": "<div>Both are the same, so that $\\widehat{\\theta}&gt; 0$ for which \\[&nbsp;&nbsp;&nbsp; k = \\sum_{j=1}^n \\frac{\\widehat{\\theta}}{j - 1 + \\widehat{\\theta}} \\left( = \\mathbb{E}_{\\widehat{\\theta}}[K] \\right) .\\]</div><div></div>"
  },
  {
    "front": "Expected number of recombination events in full ARG.",
    "back": "<div><b>Proposition.</b> If $E_j$ is expected number when $j$ lineages remain, then \\[\\begin{cases}&nbsp;&nbsp;&nbsp; E_j = \\frac{j-1}{\\rho + j-1} E_{j-1} + \\frac{\\rho}{\\rho + j-1} E_{j+1} + \\frac{\\rho}{\\rho + j-1}&amp;,j\\ge 2,\\\\&nbsp;&nbsp;&nbsp; E_1 = 0.\\end{cases}\\] The unique solution is $E_j = \\rho \\int_0^1 \\frac{1-(1-x)^{n-1}}{x}\\mathrm{e}^{\\rho x}\\mathop{}\\!\\mathrm{d} x$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The recursive equation is clear, and equivalent to \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; E_{j+1} - E_j = \\frac{j-1}{\\rho} (E_j - E_{j-1}) - 1,\\quad j\\ge 2.&nbsp;&nbsp;&nbsp; \\] Write $E_j$ for the integral. Then $E_1 = 0$ and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; E_{j+1}- E_j &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\rho \\int_0^1 (1-x)^{j-1} \\mathrm{e}^{\\rho x} \\mathop{}\\!\\mathrm{d} x \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= (1-x)^{j-1} \\mathrm{e}^{\\rho x} \\Big\\vert_{x=0}^1 + (j-1) \\int_0^t (1-x)^{j-2} \\mathrm{e}^{\\rho x} \\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= -1 + \\frac{j-1}{\\rho} (E_j - E_{j-1}).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Feynman-Kac forwards/backwards",
    "back": ""
  },
  {
    "front": "Given $Q$-matrix, how to get $R_\\lambda$ and from there $p_{ij}(t)$<br>",
    "back": ""
  },
  {
    "front": " How to prove that $\\mathcal{R}(1-\\lambda A)$ is dense? \\\\<div></div><div>(Given concrete $A$, say $Af = \\frac 12 x(1-x) f''(x)$ on $E = [0,1]$).</div>",
    "back": ""
  },
  {
    "front": "Diffusion-approximation: How to show something <i>doesn't</i> converge?",
    "back": ""
  },
  {
    "front": "If $u\\in C^2(U)$ is harmonic, $U \\subset \\mathbb{R}^d$ a domain and $u$ achieves a maximum, then $u$ is constant.",
    "back": ""
  },
  {
    "front": "Kolmogorov forward- and backward equations.",
    "back": ""
  },
  {
    "front": "How to count elements in lex/colex.",
    "back": "<div>\\textbf{colex (in $\\mathbb{N}^{(k)}$).} The first $\\binom{m}{k} $ elements are $[m]^{(k)}$.<br><br>\\textbf{lex (in $[n]^{(k)}$).} The first $\\binom{n-l}{k-l} $ elements ($l\\in [k-1]$) are $\\left\\{ [l] \\cup A\\colon A\\in \\left\\{ l+1, \\ldots ,n \\right\\} ^{(k-l)}\\right\\} $.</div><div></div>"
  },
  {
    "front": "Given Markov generator $A$, how to prove a specific representation of $R_\\lambda$. What is this for Brownian motion.",
    "back": "<div><b>Proposition.</b> Denote the proposed bounded linear operator $S_\\lambda$. Since $\\mathcal{R}(\\lambda - A) = C(E)$, it suffices to show that $S_\\lambda (\\lambda - A) f = f$ for all $f\\in D$ for a core $D$ of $A$ ($\\mathcal{R}(\\lambda - A \\!\\!\\restriction_{D} )$ is dense). For BM, one gets \\[&nbsp;&nbsp;&nbsp; R_\\lambda f (x) = \\int_\\mathbb{R} r_\\lambda(x,y) f(y) \\mathop{}\\!\\mathrm{d} y,\\qquad r_\\lambda(x,y) = \\frac{1}{\\sqrt{2\\lambda} } \\mathrm{e}^{-\\sqrt{2\\lambda}&nbsp; \\left| x-y \\right| }.\\]<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $g\\in C(E)$, so that $(\\lambda - A) f = g$ for some $f\\in \\mathcal{D}(A)$. Then take $D \\ni f_n \\stackrel{  }{\\longrightarrow} f$ with $A f_n \\stackrel{  }{\\longrightarrow} A f$, so that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S_\\lambda g = S_\\lambda (\\lambda - A) f\\longleftarrow S_\\lambda (\\lambda - A) f_n = f_n \\stackrel{  }{\\longrightarrow} f = R_\\lambda (\\lambda - A) f = R_\\lambda g.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For BM take $D = C_c^\\infty(\\mathbb{R})$ and use distributional derivative of $\\left| \\cdot&nbsp; \\right| $ to do integration by parts.</i> </p> </div><div></div>"
  },
  {
    "front": "Semigroup and generator of pure jump process.",
    "back": "[Cram, Page 2]"
  },
  {
    "front": "Duality absorbing and reflecting BM on $[0,\\infty)$ and corollary.",
    "back": "<div>If $g\\in C^2_b(\\mathbb{R})$ odd, then $X:= B^a$ and $Y:= B^r$ are dual w.r.t. $F(x,y) := g(x+y) + g(x-y)$. By taking $g$ a mollification of the signum function (times $\\frac 12$), one gets for $X_0,Y_0&gt;0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{P}(X_t &gt; Y_0) = \\mathbb{P}(X_0 &gt; Y_t).\\]</div><div></div>"
  },
  {
    "front": "PDE for Laplace transform $F(t,x) = \\mathbb{E}^x \\left[\\mathrm{e}^{-\\theta A_t}\\right]$ of occupation time of a diffusion, and PDE for the Laplace transform $\\widehat{F}(\\lambda,x)$ of $F$. What do you get for Brownian motion?",
    "back": "[Cram, Page 2]"
  },
  {
    "front": "$G_\\mathcal{U}$ for BM in $d\\ge 3$ with $\\mathcal{U}$ unit ball.",
    "back": "[Cram, Page 3]"
  },
  {
    "front": "\"Threshold function for \"\"[$]\\delta(G) \\ge 1[/$]\"\" and \"\"[$]G[/$] is connected\"\".\"",
    "back": "[Sheetscribbles]"
  },
  {
    "front": "Strictly balanced: [$]K_n, K_{nm}[/$], trees, cycles, and connected regular.",
    "back": "[Sheetscribbles]"
  },
  {
    "front": "Threshold for cycle of specific or any length to appear in $\\mathcal{G}(n,p)$",
    "back": "\"<div><b>Proposition.</b> The threshold function for a specific cycle as well as for any cycle to appear in $\\mathcal{G}(n,p)$ is $1 / n$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For fixed cycle $C$, the threshold function is $n^{- v / e} = 1&nbsp; /n$. It remains to show that $\\mathbb{P}(\\text{$G$ has a cycle}) \\stackrel{  }{\\longrightarrow} 0$ if $np \\stackrel{   }{\\longrightarrow}0 $. Indeed, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\text{\\#cycles}\\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{r=3}^n \\frac{n(n-1)\\cdot \\ldots \\cdot&nbsp; (n-r+1)}{2r} p^r \\le \\sum_{r=3}^n (np)^r\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le (np)^3 \\frac{1}{1- np} \\stackrel{  }{\\longrightarrow} 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>\""
  },
  {
    "front": "First term in expansion of survival probability [$]\\rho(1+\\varepsilon)[/$] of Poisson-GW process and how to get further terms.",
    "back": "[Sheetscribbles]"
  },
  {
    "front": " $\\boldsymbol X_{\\text{Po}(c)}$ conditioned on extinction.",
    "back": "<div><b>Proposition.</b><br><ol>  <li>For every $c \\in (1,\\infty)$ exists a unique $d \\in (0,1)$ with $c\\mathrm{e}^{-c} = d\\mathrm{e}^{-d}$,&nbsp;&nbsp;&nbsp;</li>  <li>If $c &gt; 1$ and $\\eta &lt; 1$ is the extinction probability of $\\boldsymbol X_{\\text{Po}(c)}$, then $d = \\eta c$,&nbsp;&nbsp;&nbsp;</li>  <li>$\\boldsymbol X_{\\text{Po}(c)}$ conditioned on extinction is $\\boldsymbol X_{\\text{Po}(d)}$.</li></ol><br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>$x \\mapsto x \\mathrm{e}^{-x}$ is a bijection $(0,1) \\to (0,\\mathrm{e})$ and $(1,\\infty) \\mapsto (0,\\mathrm{e})$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>We know that $\\eta = f_{\\text{Po}(c)}(\\eta) = \\mathrm{e}^{-c(1-\\eta)}$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\eta c = c\\mathrm{e}^{-c} \\mathrm{e}^{\\eta c}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left(Z_1 = k \\,\\middle\\vert\\, \\text{extinction}\\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{\\eta^k \\mathbb{P}(Z_1 = k)}{\\eta} = \\mathrm{e}^{c(1-\\eta)} \\eta^k \\mathrm{e}^{-c} \\frac{c^k}{k!}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-\\eta c} \\frac{(\\eta c)^k}{k!},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and the conditioning applies also to subsequent generations.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "\"\"\"Independent upsets ignore each other in Harris inequality\"\"\"",
    "back": "[Sheetscribbles]"
  },
  {
    "front": "Characterisation of increasing events (upsets) being independent.",
    "back": "<div><b>Proposition.</b> Two upsets $A,B \\subset \\mathcal{P}(X) $ ($\\varnothing \\neq X$ finite) are independent w.r.t. $\\mathbb{P}_p \\in \\mathcal{M}_1(\\mathcal{P}(X))$ iff they depend on disjoint sets, that is, iff there are disjoint $I,J\\subset X$ such that \\[&nbsp;&nbsp;&nbsp; \\forall \\omega\\in \\mathcal{P}(X)\\colon \\omega\\in A \\iff \\omega\\cap I \\in A,\\\\\\] and the same for $B$ and $J$. That is, $A \\in \\sigma \\left( x \\colon x\\in I \\right) $ in the obvious sense.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x\\in X$. Define<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A^+ &amp;:= \\left\\{ \\omega \\setminus \\left\\{ x \\right\\} \\colon \\omega\\in A \\right\\} ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A^- &amp;:= \\left\\{ \\omega\\subset X\\setminus \\left\\{ x \\right\\} \\colon \\omega \\in A \\right\\} ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] the upper and lower face of $A$ w.r.t. $x$. Then as in the proof of Harris' inequality we see that, putting $a^+ := \\mathbb{P}_p^{(X\\setminus \\left\\{ x \\right\\} )}(A^+)$, $a^-$ and $b^{\\pm}$ similarly, \\[&nbsp;&nbsp;&nbsp; 0 = \\mathbb{P}_p(A\\cap B) - \\mathbb{P}_p(A)\\mathbb{P}_p(B) \\ge p(1-p) (a^+ - a^-) (b^+ - b^-) \\ge 0,\\\\&nbsp;&nbsp;&nbsp; \\] so in fact equality holds everywhere, say $a^+ = a^-$. This means that $A^- = A^+$ and hence that $\\omega\\in A \\iff \\omega \\cup \\left\\{ x \\right\\} \\in A$ for all $\\omega\\subset X$, so $A$ does not depend on $x$.</i> </p> </div><div></div>"
  },
  {
    "front": "\"Bound of the form [$]\\mathbb{P}(\\# \\text{of copies of } H = 0) \\le \\mathrm{e}^{-c n^\\beta}[/$].\"",
    "back": "[Sheetscribbles]"
  },
  {
    "front": "Recipe to show [$]\\chi(G) \\sim \\frac{n}{f(n)}[/$].",
    "back": ""
  },
  {
    "front": "\"Lower bound of the form [$]\\mathbb{P}(\\# \\text{ copies of $H$ in }\\mathcal{G}(n,p) = 0) \\ge \\mathrm{e}^{-\\mu + o(1)})[/$] (under certain assumption).<br>\"",
    "back": ""
  },
  {
    "front": "Chernoff for general $S_n = \\sum_{i=1}^{n} X_i$?",
    "back": "<div>Same bounds hold if $0\\le X_i \\le 1$ are independent with $p = \\sum_i \\mathbb{E}[X_i] / n$.<div></div></div>"
  },
  {
    "front": "[$]\\int_0^1 \\frac{1-(1-x)^{n-1}}{x} \\mathop{}\\!\\mathrm{d} x = ?[/$]",
    "back": "\"[Front page, \"\"Facts\"\"]\""
  },
  {
    "front": "[$]M_n \\sim \\text{Poi} \\left(\\frac{\\theta}{2} \\int_0^1 \\mathcal{G}(x) \\mathop{}\\!\\mathrm{d} x\\right)[/$].",
    "back": ""
  },
  {
    "front": "If prob. of choosing A-gene given current proportion is [$]x[/$] is [$]p(x) = x + \\frac{b(x)}{2N} + o(1 / N)[/$], what does [$]Y_t = X_{\\lfloor 2Nt\\rfloor}[/$] converge to?",
    "back": ""
  },
  {
    "front": "\"How to get [$]\\mathbb{E}[\\# \\text{ number of recombination events with property P}][/$].\"",
    "back": ""
  },
  {
    "front": "Example where [$]R_H = 1, H_M \\ge 2[/$].",
    "back": ""
  },
  {
    "front": "[$]k[/$]-progressions: [$]N = |\\text{Pr}|, \\mu,\\Delta[/$], threshold function.",
    "back": ""
  },
  {
    "front": "How to deal with [$]-\\text{Bin}(n,p)[/$] or [$]\\text{Bin}(n,1/2) - \\text{Bin}(n,1/2)[/$]?",
    "back": ""
  },
  {
    "front": "When use which version of 2nd moment.",
    "back": ""
  },
  {
    "front": "Convenient exponential lower bound on $1-x$ for $x\\in [0,1)$.",
    "back": "<div><b>Lemma.</b> If $x\\in [0,1)$, \\[&nbsp;&nbsp;&nbsp; 1-x \\ge \\mathrm{e}^{-\\frac{x}{1-x}} \\ge \\mathrm{e}^{-2x},\\\\\\] where the second part holds if $x \\le \\frac{1}{2}$.<br>&nbsp;&nbsp;&nbsp; <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; STS that $f(x) := \\log (1-x) + \\frac{x}{1-x}\\ge 0$ for all $x\\in [0,1)$, but $f(0) = 0$ and $f'(x) = \\frac{x}{(1-x)^2}\\ge 0$ for all $x\\in[0,1)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Density of first hitting time $T_a$ of BM (in $d = 1$).",
    "back": "<div><b>Lemma.</b> If $B$ is a BM and $T_a := \\inf \\left\\{ t\\ge 0\\colon B_t \\ge a \\right\\} $, then $\\mathbb{P}(T_a \\le t) = 2 \\mathbb{P}(B_t \\ge a)$ and $T_a$ has density \\[&nbsp;&nbsp;&nbsp; f_a(t) = \\frac{a}{\\sqrt{2\\pi t^3} } \\mathrm{e}^{- \\frac{a^2}{2t}}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By reflection principle,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(T_a \\le t) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}(B_t^\\star \\ge a) = \\mathbb{P}(B_t \\ge a) + \\mathbb{P}(B_t^\\star \\ge a, B_t &lt; a)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}(B_t\\ge a) + \\mathbb{P}(B_t &gt; a) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= 2\\mathbb{P}(B_t\\ge a).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Hence,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_a(t) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\mathbb{P}(T_a \\le t) = 2 \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\int_a^\\infty \\frac{1}{\\sqrt{2\\pi t} } \\mathrm{e}^{-x^2 / (2t)}\\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\ldots = \\frac{a}{\\sqrt{2\\pi t^3} } \\mathrm{e}^{-\\frac{a^2}{2t}}&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; which can be confirmed directly or obtained via IBP.</i> </p> </div><div></div>"
  },
  {
    "front": "Suppose $B$ is a BM on $(\\Omega,\\mathcal{G},(\\mathcal{G}_t),\\mathbb{P})$ and $(A_t)$ is a continuous increasing process independent of $B$ with $A_0 = 0$. Put $X_t := B_{A_t}$.<br><ol>  <li>Show that $X$ is a $\\mathcal{F}:= \\mathcal{G}^X$-local martingale,&nbsp;&nbsp;&nbsp;</li>  <li>Find $\\left&lt;X \\right&gt; $,&nbsp;&nbsp;&nbsp;</li>  <li>Find characterisation of $X$ being a martingale.</li></ol>",
    "back": "<div><b>Proposition.</b> $\\left&lt;X \\right&gt; _t = A_t$ and $X$ is a martingale iff $\\mathbb{E} \\left[ \\sqrt{A_t}&nbsp; \\right] &lt; \\infty$ for all $t\\ge 0$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Put $\\tau_n := \\inf \\left\\{ t\\ge 0\\colon A_t \\ge n \\right\\} \\uparrow \\infty$, and note $A_{t \\wedge \\tau_n}= A_t \\wedge n$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>First of all, $X$ is adapted, and for $t\\ge 0$ and $n\\in \\mathbb{N}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ |X_{\\tau_n \\wedge t}| \\right] = \\mathbb{E} \\left[ |B_{A_t\\wedge n}| \\right] = \\mathbb{E} \\left[ \\sqrt{A_t \\wedge n}&nbsp; \\right] \\mathbb{E} \\left[ \\left| \\mathcal{N} \\right|&nbsp; \\right] &lt; \\infty.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Let $0\\le s \\le t$, $0\\le s_1\\le\\ldots \\le s_k \\le s$, $C\\in \\mathcal{B}(\\mathbb{R})$, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A:= \\left\\{ (B_{A_{s_1}}, \\ldots ,B_{A_{s_k}}) \\in C \\right\\} .&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] HTS $\\mathbb{E} \\left[ (X_{\\tau_n \\wedge t} - X_{\\tau_n \\wedge s}) \\boldsymbol{1}_{A} \\right] = 0$. Indeed, by independence of $A $ and $B$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{LHS}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int\\limits_{C[0,\\infty)} \\underbrace{\\mathbb{E} \\left[ (B_{a(t) \\wedge n}- B_{a(s) \\wedge n}) \\boldsymbol{1}_{\\left\\{ \\left(B_{a(s_1)}, \\ldots ,B_{a(s_k)}\\right) \\in C \\right\\} } \\right] }_{= 0} \\mathbb{P}^A(\\mathop{}\\!\\mathrm{d} a) = 0.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Let $t &gt; 0$ and $(\\pi_n)$ a sequence of partitions with $\\left| \\pi_n \\right| \\to 0$. Then, for $\\varepsilon &gt; 0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}&amp;\\left( \\left| QV^{\\pi_n}_t(X) - A_t \\right| \\ge \\varepsilon&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int\\limits_{C[0,\\infty)} \\underbrace{\\mathbb{P} \\left( \\left| \\sum_{t_i\\in \\pi_n} \\left( B_{a(t_{i+1})}- B_{a(t_i)} \\right) ^2 - a(t) \\right| \\ge \\varepsilon \\right)}_{= \\mathbb{P}\\left( \\left| QV^{a(\\pi_n)}_{a(t)}(B) - a(t) \\right| \\ge \\varepsilon \\right) \\to 0} \\mathbb{P}^A(\\mathop{}\\!\\mathrm{d} a) \\stackrel{  }{\\longrightarrow} 0\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; by dominated convergence. In particular, $QV_t ^{\\pi_n}(X) \\stackrel{ \\text{a.s.} }{\\longrightarrow} A_t$ for some subsequence, so $(A_t)$ is $\\mathcal{F}^X_t$-measurable and $\\left&lt;X \\right&gt; _t = A_t$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For $t\\ge 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ |X_t| \\right] = \\mathbb{E} \\left[ \\left| B_{A_t} \\right|&nbsp; \\right] = \\mathbb{E} \\left[ \\sqrt{A_t}&nbsp; \\right] \\mathbb{E} \\left[ \\left| \\mathcal{N} \\right|&nbsp; \\right] ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $X$ is integrable iff $\\mathbb{E} \\left[ \\sqrt{A_t}&nbsp; \\right] &lt;\\infty$ for all $t\\ge 0$. In that case, $X$ is a true martingale by the same argument as before.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "$\\sigma,b\\colon \\mathbb{R} \\to \\mathbb{R}$ bounded mb, $\\sigma \\ge \\varepsilon$. Then uniqueness in law of $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t$ implies uniqueness in law of $\\mathop{}\\!\\mathrm{d} X_t = b(X_t) \\mathop{}\\!\\mathrm{d} t + \\sigma(X_t) \\mathop{}\\!\\mathrm{d} t$ and vice versa.",
    "back": "<div><b>Proposition.</b> Let $\\sigma, b \\colon \\mathbb{R}\\to \\mathbb{R}$ be bounded and measurable, $\\sigma \\ge \\varepsilon$ for some $\\varepsilon &gt; 0$. Then uniqueness in law of $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t$ implies uniqueness in law of $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t)\\mathop{}\\!\\mathrm{d} B_t + b(X_t) \\mathop{}\\!\\mathrm{d} t$ and vice versa.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose that $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} \\overline{B}_t + b(X_t) \\mathop{}\\!\\mathrm{d} t$ and $X_0 = x\\in \\mathbb{R}$ w.r.t. $\\mathbb{P}$. Put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\overline{L}_t := - \\int_0^t b(X_s) \\sigma(X_s)^{-1} \\mathop{}\\!\\mathrm{d} \\overline{B}_s,\\quad t\\ge 0.&nbsp;&nbsp;&nbsp; \\] Then $\\mathcal{E}(\\overline{L})$ is a martingale by Novikov, so let $\\mathbb{Q}$ with $\\mathop{}\\!\\mathrm{d} \\mathbb{Q}_t = \\mathcal{E}(\\overline{L})_t \\mathop{}\\!\\mathrm{d} \\mathbb{P}_t$, so that $\\mathop{}\\!\\mathrm{d} X_t = \\sigma(X_t) \\mathop{}\\!\\mathrm{d} B_t$ where $B =&nbsp; \\overline{B} - \\left&lt;\\overline{B},\\overline{L} \\right&gt; $ is a $\\mathbb{Q}$-Brownian motion. Hence if $F\\colon C[0,\\infty)\\to \\mathbb{R}$ is measurable and $t\\ge 0$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left( F(X^t) \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{Q}\\left( \\mathcal{E}(\\overline{L})^{-1} F(X^t) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{Q} \\left( \\mathcal{E}(-L) F(X^t) \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where $L_t = \\overline{L}_t - \\left&lt;\\overline{L}_t \\right&gt; =- \\int_0^t b(X_s) \\sigma(X_s)^{-1} \\mathop{}\\!\\mathrm{d} B_s $, so the RHS depends only on $\\mathbb{Q}^{(X,B)}$, which is unique. The other way works the same way.</i> </p> </div><div></div>"
  },
  {
    "front": "Kazamaki proof outline",
    "back": ""
  },
  {
    "front": "$M_0 = 0$ clm, $\\alpha \\in (0,1)$ such that $\\left| M \\right| ^\\alpha$ csm, then $M \\equiv 0$.",
    "back": ""
  },
  {
    "front": "Representation $\\mathop{}\\!\\mathrm{d} L^0_t = ? \\mathop{}\\!\\mathrm{d} \\left| M \\right| _t$ for clm $M$.",
    "back": ""
  },
  {
    "front": "For clm $M_0 = 0$, $\\inf \\left\\{ t \\ge 0\\colon L^0_t &gt; 0 \\right\\} = \\inf \\left\\{ t\\ge 0\\colon \\left&lt;M \\right&gt; _t &gt; 0 \\right\\} $.",
    "back": "<div><b>Proposition.</b> If $M$ is a continuous local martingale and $M_0 = 0$, then a.s. \\[\\inf \\left\\{ t\\ge 0\\colon \\left&lt;M \\right&gt; _t &gt; 0 \\right\\}&nbsp; = \\inf \\left\\{ t \\ge 0\\colon L^0_t &gt; 0 \\right\\} .\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Set $\\tau := \\text{LHS}$ and $N := M^\\tau$. Then $\\left&lt;N \\right&gt; = \\left&lt;M \\right&gt; ^\\tau \\equiv 0$, so $N\\equiv 0$, so $L^0_{t\\wedge \\tau}(M) = L^0_t(N) = 0$ for all $t\\ge 0$, so $\\tau_\\text{RHS} \\ge \\tau$ a.s. Similarly it suffices to show that $L^0(N) \\equiv 0$ implies $N\\equiv 0$. Indeed, $\\left| N_t \\right| = \\int_0^t \\operatorname{sgn} (N_s) \\mathop{}\\!\\mathrm{d} N_s$ would then be a non-negative local martingale, so a supermartingale, so $\\mathbb{E} \\left[ \\left| N_t \\right|&nbsp; \\right] \\le \\mathbb{E} \\left[ \\left| N_0 \\right|&nbsp; \\right] = 0 $, so $\\left| N_t \\right| = 0$ for all $t\\ge 0 $.</i> </p> </div><div></div>"
  },
  {
    "front": "$\\mathbb{P}^0(L^a_\\infty = \\infty) = 1$ $\\forall a \\in \\mathbb{R}$ for BM.",
    "back": ""
  },
  {
    "front": "Characterisation when a non-negative supermartingale (or clm) is a uib martingale.",
    "back": "<div>If $(M_t)$ is a non-negative supermartingale (or clm), then it is a uib martingale iff $\\mathbb{E} \\left[ M_\\infty \\right] = \\mathbb{E} \\left[ M_0 \\right] $.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; One direction is clear. For the other, let $t \\ge 0$. Then $M_t \\ge \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_t\\right]$ by Fatou, so $\\mathbb{E} \\left[ M_0 \\right] \\ge \\mathbb{E} \\left[ M_t \\right] \\ge \\mathbb{E} \\left[ M_\\infty \\right] = \\mathbb{E} \\left[ M_0 \\right] $ and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left| M_t - \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_t\\right] \\right|&nbsp; \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ M_t - \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_t\\right] \\right] = \\mathbb{E} \\left[ M_0 \\right] - \\mathbb{E} \\left[ M_0 \\right] = 0,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so in fact $M_t = \\mathbb{E} \\left[M_\\infty \\,\\middle\\vert\\, \\mathcal{F}_t\\right]$, where $M_\\infty\\in L^1$.</i> </p> </div><div></div>"
  },
  {
    "front": "A rate function $I\\colon E\\to [0,\\infty]$ for (strong) LDP is unique (whether $(P_\\varepsilon)$ or $(\\nu_n)$).",
    "back": ""
  },
  {
    "front": "Rate function of $\\mathcal{N}(a,\\Sigma)$.",
    "back": "<div><b>Proposition.</b> The rate function of $\\mathcal{N}(a,\\Sigma) \\in \\mathcal{M}_1(\\mathbb{R}^d)$ is \\[&nbsp;&nbsp;&nbsp; I(x) = \\frac{1}{2} (x-a)^\\top \\Sigma ^{-1} (x-a),\\quad x\\in \\mathbb{R}^d.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First consider $a = 0$ and $\\Sigma = I_d$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M(\\lambda) = \\mathbb{E} \\left[ \\mathrm{e}^{\\lambda X} \\right] = \\mathrm{e}^{\\lambda^\\top \\lambda / 2},\\quad \\lambda \\in \\mathbb{R},\\\\&nbsp;&nbsp;&nbsp; \\] so, for $x\\in \\mathbb{R}^d$, \\[&nbsp;&nbsp;&nbsp; I(x) = \\sup_{\\lambda \\in \\mathbb{R}} \\left[ \\lambda^\\top \\left( x - \\frac{\\lambda}{2} \\right)&nbsp; \\right] = \\frac{x^\\top x}{2},\\\\\\] because the supremum must be attained locally, and $\\nabla_\\lambda \\left[ \\cdot&nbsp; \\right] = x - \\lambda$. Now let $\\Sigma = A A^\\top$, so that $Z:= f(X) \\sim \\mathcal{N}(a,\\Sigma)$, where $f(x) := Ax + b$. Then $f$ is bijective, so by the contraction principle and because $X_n := \\frac{1}{n} \\sum_{i=1}^n X^{(i)}$ satisfies LDP with good rate $I$, it follows that $f(X_n) \\sim Z_n$ satisfies LDP with good rate \\[I'(x) = \\inf_{f^{-1}(x)} I = I(f^{-1}(x)) = I(A^{-1}(x-a)) = \\frac{1}{2} (x-a)^\\top \\Sigma^{-1}(x-a).\\] Since good rates are unique, $I'$ must be <i>the</i> rate function of $\\mathcal{N}(a,\\Sigma)$ (in particular the one we would obtain from Cramer's definition).</i> </p> </div><div></div>"
  },
  {
    "front": "Suppose $\\mu \\in \\mathcal{M}_1(\\mathbb{R}^d)$, $\\nu_\\varepsilon = \\mu \\circ \\Gamma_\\varepsilon^{-1}$ (where $\\Gamma_\\varepsilon(x) = \\varepsilon^\\alpha x$ for some $\\alpha &gt; 0$). Then if $(\\nu_{1/n})$ satisifies LDP with good rate $I$ (say by Cramer), then so does $(\\nu_\\varepsilon)$.",
    "back": ""
  },
  {
    "front": "If breakpoints and values of a step function in [$]D([0,\\infty),S)[/$], converge, then the step function converges in the Skorokhod metric.",
    "back": ""
  },
  {
    "front": "If $b\\colon R\\to \\mathbb{R}$ Lipschitz, and $f\\colon C_0[0,1] \\to C_0[0,1]$ sends $w$ to the solution of \\[&nbsp;&nbsp;&nbsp; X_t = w(t) + \\int_0^t b(X_s) \\mathop{}\\!\\mathrm{d} s,\\\\\\] then<br><ol>  <li>$f$ is well-defined and Lipschitz,&nbsp;&nbsp;&nbsp;</li>  <li>$(X^\\varepsilon)$ where $X^\\varepsilon_t = \\sqrt{\\varepsilon} B_t + \\int_0^t b(X_s^\\varepsilon) \\mathop{}\\!\\mathrm{d} s$ satisfies LDP with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I(X) = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{2} \\int_0^1 | \\dot{X}_t - b(X_t) |^2 \\mathop{}\\!\\mathrm{d} t &amp;, X \\in&nbsp; \\mathcal{R}(f) \\cap H^{1,2},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &amp;, \\text{else}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\]</li></ol>",
    "back": ""
  },
  {
    "front": "[$$]\\mathcal{B}(C([0,\\infty),\\mathbb{R}^d)) = \\sigma\\left(X_t \\colon t\\ge 0\\right)[/$$]",
    "back": ""
  },
  {
    "front": "Bounds on $\\sup_{t\\ge 0} \\frac{\\lambda(t)}{t}, \\left\\| \\lambda - \\operatorname{id} \\right\\|_T, \\sup_{t\\le T} \\lambda_1(|\\lambda_2(t) - t|)$",
    "back": ""
  },
  {
    "front": " Convenient upper bound for $\\varliminf_{n\\to \\infty} a_n \\vee b_n$.",
    "back": "<div><b>Proposition.</b> If $(a_n),(b_n) \\in \\mathbb{R}^\\mathbb{N}$, then \\[&nbsp;&nbsp;&nbsp; \\varliminf_{n\\to \\infty} (a_n \\vee b_n) \\le \\varlimsup_{n\\to \\infty} a_n \\vee \\varliminf_{n\\to \\infty}b_n.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varliminf_{n\\to \\infty} (a_n \\vee b_n)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{n\\to \\infty} \\inf_{k\\ge n} (\\underbrace{a_k}_{\\le \\sup_{l\\ge n} a_l}&nbsp; \\vee b_k)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\lim_{n\\to \\infty} \\left( \\sup_{l\\ge n}a_l \\vee \\inf_{k\\ge n} b_k \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\varlimsup_{n\\to \\infty} a_n \\vee \\varliminf_{n\\to \\infty}b_n.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Rate function and Varadhan's theorem for single $P\\in \\mathcal{M}_1(E)$ ($E$ Polish).",
    "back": "<div><b>Proposition.</b> Single $P\\in \\mathcal{M}_1(E)$ with $S := \\operatorname{supp}(P)$ has rate function $I(x) = \\infty \\boldsymbol{1}_{S}$, and if $\\Phi\\colon E\\to \\mathbb{R}$ is continuous, then \\[\\lim_{\\varepsilon \\to 0} \\varepsilon \\log \\int \\mathrm{e}^{\\Phi / \\varepsilon} \\mathop{}\\!\\mathrm{d} P = \\sup_S \\Phi.\\] <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>For any $A\\in \\mathcal{B}(E)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{\\varepsilon \\to 0} \\varepsilon \\log P(A) = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -\\infty &amp;, P(A) = 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, P(A) &gt; 0.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] If $F\\subset E$ is closed, then $\\inf_F I = \\infty \\boldsymbol{1}_{\\left\\{ F\\subset S^{c} \\right\\} }$. Hence if $F\\cap S \\neq \\varnothing$, there is nothing to show, otherwise $P(F) = 0$ and we are also done. If $G\\subset E$ is open, then $\\inf_G I = \\infty \\boldsymbol{1}_{\\left\\{ G \\subset S^{c} \\right\\} } = \\infty \\boldsymbol{1}_{\\left\\{ P(G) = 0 \\right\\} }$, and we are also done.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>First note that $\\sup (\\Phi - I) = \\sup_S \\Phi$. If $\\sup_S \\Phi = \\infty$, then by lower bound of Varadhan \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varliminf_{\\varepsilon \\to 0} \\varepsilon \\log \\int \\mathrm{e}^{\\Phi / \\varepsilon}\\mathop{}\\!\\mathrm{d} P \\ge \\sup_S \\Phi = \\infty.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Otherwise, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{\\varepsilon \\le 1} \\left( \\int \\mathrm{e}^{2\\Phi / \\varepsilon} \\mathop{}\\!\\mathrm{d} P \\right) ^\\varepsilon \\le \\mathrm{e}^{2 \\sup_S \\Phi} &lt; \\infty,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so Varadhan's full theorem gives the claim.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": " WUSF $\\neq$ FUSF on $T^{(2)}_\\infty$",
    "back": ""
  },
  {
    "front": "If $T\\sim \\text{UST}(\\mathbb{Z}^2)$, then $\\mathbb{P}(e\\in E(T)) = \\frac 12$.",
    "back": ""
  },
  {
    "front": "$|V(\\Lambda(n))|$ and $|E(\\Lambda(n))|$.",
    "back": ""
  },
  {
    "front": "If $A_1,\\ldots,A_n \\subset \\mathcal{P}(X)$ are (measurable) upsets, then $\\max_i \\mathbb{P}_p(A_i) \\ge ?$",
    "back": "<div><b>Proposition.</b> If $X\\neq \\varnothing$, and $A_i \\subset \\mathcal{P}(X),i\\in \\mathbb{N},$ are (measurable if $\\left| X \\right| = \\infty$) upsets, then \\[&nbsp;&nbsp;&nbsp; \\max_{i=1, \\ldots ,n} \\mathbb{P}_p(A_i) \\ge 1 - \\left( 1 - \\mathbb{P}_p\\left( \\bigcup_{i=1} ^n A_i \\right)&nbsp; \\right) ^{1 / n}.\\] <br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Harris' inequality,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - \\mathbb{P} \\left( \\bigcup_{i=1} ^n A_i \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( \\bigcap_{i=1} ^n A_i^{c} \\right) \\ge \\prod_{i=1}^n (1 - \\mathbb{P}(A_i))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\left( 1 - \\max_{i=1, \\ldots ,n}\\mathbb{P}(A_i) \\right) ^n.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Fekete's lemma on subadditive sequences",
    "back": "<div><b>Proposition.</b> If $(x_n) \\in \\mathbb{R}^\\mathbb{N}$ such that $x_{n+m}\\le x_n + x_m$ for all $n,m\\in\\mathbb{N}$, then \\[&nbsp; &nbsp; \\frac{x_n}{n} \\stackrel{  }{\\longrightarrow} l \\in [-\\infty,\\infty),\\\\\\] where $l = \\inf_{n\\in \\mathbb{N}}\\frac{x_n}{n}$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $x &gt; -\\infty$, the argument is the same. Let $\\varepsilon &gt; 0$ and choose $m\\in \\mathbb{N}$ with $\\frac{x_m}{m} \\le x + \\varepsilon$. Then for any $n\\ge m$ we may write $n = k m + r$ with $r &lt; m$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{x_n}{n}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\frac{k x_m}{k m + r} + \\frac{x_r}{n} \\le \\frac{x_m}{m} + \\frac{\\sup_{r\\le m}x_r}{n} \\le x + 2\\varepsilon\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; as soon as $n\\ge n_0 = n_0(m,\\varepsilon) = n_0(\\varepsilon)$ is so large that $\\sup_{r\\le m} x_r / n \\le \\varepsilon$.</i> </p> </div><div></div>"
  },
  {
    "front": "Percolation on $T_d$. $p_c = ?$, $\\mathbb{P}(\\exists v\\colon v \\leftrightarrow \\infty) = ?$.",
    "back": ""
  },
  {
    "front": "[$]p_c^\\text{bond} \\le p_c^\\text{site} \\le 1 - (1 - p_c^\\text{bond})^{\\Delta - 1}[/$].",
    "back": ""
  },
  {
    "front": "$\\theta$ for any locally finite, connected graph is right-continuous on $[0,1)$ and continuous on $(p_c,1]$.",
    "back": "<div><b>Proposition.</b> Let $G$ be a locally finite, connected graph. Then $\\theta\\colon [0,1] \\to [0,1]$ is right-continuous on $[0,1)$ and left-continuous on $(p_c,1]$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\theta(p) = \\mathbb{P}_p(x \\leftrightarrow \\infty) = \\mathbb{P}_p(\\forall n\\in \\mathbb{N}\\colon \\text{SAW}_n) = \\lim_{n\\to \\infty} f_n(p),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] where \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_n(p) = \\mathbb{P}_p(\\text{SAW}_n) = \\mathbb{P}_p(\\text{$\\exists$ self-avoiding walk of length $n$ starting at $x$})&nbsp;&nbsp;&nbsp; \\] is a polynomial (it only depends on edges with distance at most $n$ from $x$). This is a decreasing limit, and $f_n$ is increasing in $p$, so we may exchange two decreasing limits in \\[&nbsp;&nbsp;&nbsp; \\lim_{q \\downarrow p} \\theta(q) = \\lim_{q\\downarrow p} \\lim_{n\\to \\infty} f_n(q) = \\lim_{n\\to \\infty} \\lim_{q\\downarrow p} f_n(q) = \\lim_{n\\to \\infty} f_n(p) = \\theta(p).&nbsp;&nbsp;&nbsp; \\]</li>  <li>Let $p\\in (p_c,1)$, and $p_c &lt; p_n \\uparrow p$.&nbsp; We may host $(\\mathbb{P}_q)_{q\\in [0,1]}$ on one probability space by taking i.i.d. $U(0,1)$-variables $(U_e)_{e\\in E}$ and opening $e$ w.r.t. $\\mathbb{P}_p$ iff $U_e &lt; p$. Then&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{n\\to \\infty} \\theta(p_n) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{n\\to \\infty} \\mathbb{P} \\left( x \\stackrel{p_n}{\\longleftrightarrow } \\infty \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( \\exists n \\in \\mathbb{N}\\colon x \\stackrel{p_n}{\\longleftrightarrow} \\infty \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{P}(x \\stackrel{p}{\\longleftrightarrow} \\infty) = \\theta(p).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp; Now a.s. there exists a unique infinite component $C_n$ for every $p_n$ and $C$ for $p$, so $C_1 \\subset C_2 \\subset \\ldots \\subset C$. Then $ x \\stackrel{p}{\\longleftrightarrow} \\infty $ implies that there is a finite path from $x$ to $C_1$ consisting of edges with $U_e &lt; p$, so all of these edges have weight $&lt; p_n$ for some $n\\in \\mathbb{N}$, so $x \\stackrel{p_n}{\\longleftrightarrow} \\infty $ for some $n\\in \\mathbb{N}$ already, so we actually have equality above.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "How to quickly argue time-inverse chains.",
    "back": ""
  },
  {
    "front": "Proof of Minkowski's inequality.",
    "back": "<div>Clear if $p\\in \\left\\{ 1,\\infty \\right\\} $. Otherwise,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left\\|f+g\\right\\|_p^p &nbsp;&nbsp;&nbsp; &amp;= \\int_E \\left| f+g \\right| ^p \\mathop{}\\!\\mathrm{d} \\mu \\le&nbsp; \\int_E \\left| f \\right| \\left| f+g \\right| ^{p-1}\\mathop{}\\!\\mathrm{d} \\mu + \\int_E \\left| g \\right| \\left| f+g \\right| ^{p-1}\\mathop{}\\!\\mathrm{d} \\mu \\\\&nbsp;&nbsp;&nbsp; &amp;\\le \\left( \\left\\|f\\right\\|_p + \\left\\|g\\right\\|_p \\right) \\left\\|f+g\\right\\|_p^{p - 1}.\\end{align*}[/$$]</div><div></div>"
  },
  {
    "front": "If $(\\Omega,\\mathcal{A},\\mu)$ is a measure space and $p\\in [1,\\infty]$, then $L^p(\\Omega,\\mathcal{A},\\mu)$ is a Banach space.",
    "back": "<div><p><i><b>Proof.</b>Only have to show completeness, so let $(f_k) \\subset L^p$ be Cauchy. If $p = \\infty$, then $f_n(x)$ is Cauchy, so $f_n \\to f$ pointwise, and $f\\in L^\\infty$, and<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left| f_n(x) - f(x) \\right| = \\lim_{m\\to \\infty} \\left| f_n(x) - f_m(x) \\right| \\le \\sup_{m\\ge n} \\left\\|f_n - f_m\\right\\|_\\infty \\stackrel{ n\\to \\infty }{\\longrightarrow} 0.\\end{align*}[/$$]<br>If $p \\in [1,\\infty)$, then we can pass to a subsequence $(g_j := f_{k_j})$ with $\\mu(x\\colon \\left| g_j - g_{j+1} \\right| \\ge 2^{-j}) \\le 2^{-j} $, so that $g_j$ is pointwise Cauchy $\\mu$-a.e., with measurable limit $f:= g:= \\varliminf_j g_j$. Then $f\\in L^p$ by Fatou and<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left\\|f_n - f\\right\\|_p^p &nbsp;&nbsp;&nbsp; &amp;= \\int_E \\lim_{j\\to \\infty} \\left| f_n(x) - f_{k_j}(x) \\right|^p \\mu(\\mathop{}\\!\\mathrm{d} x) \\le \\varliminf_{j\\to \\infty} \\left\\|f_n - f_{k_j}\\right\\|_p^p\\\\&nbsp;&nbsp;&nbsp; &amp;\\le \\sup_{m\\ge n} \\left\\|f_n - f_m\\right\\|_p^p \\stackrel{  }{\\longrightarrow} 0.\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Dual of Hölder's inequality and dual space of $L^p(\\Omega,\\mathcal{A},\\mu)$",
    "back": "<div><b>Proposition.</b> If $(\\Omega,\\mathcal{A},\\mu)$ is a measure space, and $p\\in [1,\\infty]$, then \\[&nbsp;&nbsp;&nbsp; \\left\\|f\\right\\|_p = \\sup \\left\\{ \\int fg \\mathop{}\\!\\mathrm{d} \\mu\\colon \\left\\|g\\right\\|_{q} \\le (=) 1 \\right\\} .\\] (If $p = \\infty$ then $\\nu$ must be $\\sigma$-finite). If $p\\in [1,\\infty)$, then the dual space of $L^p$ is $L^q$. The dual of $L^\\infty$ is <i>not</i> $L^1$.<br><br><p><i><b>Proof.</b>Clear if $\\left\\|f\\right\\|_p = 0$, assume $\\in (0,\\infty)$. If $p\\in [1,\\infty)$, put $g:= (\\operatorname{sgn} f) \\left| f \\right| ^{p-1} / \\left\\|f\\right\\|_p^{p-1}$. If $p = \\infty$, choose for $\\varepsilon &gt; 0$ a set $\\Omega_\\varepsilon$ with $\\mu(\\Omega_\\varepsilon) \\in (0,\\infty)$ and $f\\ge \\left\\|f\\right\\|_\\infty - \\varepsilon$ on $\\Omega_\\varepsilon$, and put $g := (\\operatorname{sgn} f) \\boldsymbol{1}_{\\Omega_\\varepsilon} / \\mu(\\Omega_\\varepsilon)$. Then $\\mu(fg) \\ge \\left\\|f\\right\\|_\\infty - \\varepsilon$.<br><br>If $\\left\\|f\\right\\|_p = \\infty$, WLOG $f \\ge 0$, take $f_k := f \\wedge k$ and $\\left\\|g_k\\right\\|= 1$ with $\\mu( f_k g_k) \\ge \\left\\|f_k\\right\\|_p - 1$ Then $\\text{RHS} \\ge \\mu(f g_k) \\ge \\left\\|f_k\\right\\|_p - 1\\uparrow \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Weak/weak* sequential compactness (Alouglu and Helly), and corollary on $L^p$ for $p\\in (1,\\infty]$.",
    "back": "<div><p><b>Theorem.</b> <i><ol>  <li>Every bounded sequence in a reflexive Banach space has a weakly convergent subsequence.&nbsp;&nbsp;&nbsp;</li>  <li>Every bounded sequence in the dual of a separable Banach space has a weak* convergent subsequence.</li></ol></i> </p> <br><br><p><b>Corollary.</b> <i>&nbsp;&nbsp;&nbsp; If $(f_n)\\subset L^p(\\Omega,\\mathcal{A},\\mu)$ for $p\\in (1,\\infty]$ (and $\\mu$ $\\sigma$-finite if $p = \\infty$) is bounded (w.r.t. $\\left\\|\\cdot \\right\\|_p$), then there exists a subsequence $f_{k(n)}$ and $f\\in L^p$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall g\\in L^{q}\\colon \\int_E f_{k(n)} g \\mathop{}\\!\\mathrm{d} \\mu&nbsp; \\stackrel{  }{\\longrightarrow} \\int_E f g \\mathop{}\\!\\mathrm{d} \\mu.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Follows from (i) if $p\\in (1,\\infty)$, and from (ii) if $p = \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Dense subsets of $L^p(\\Omega,\\mathcal{A},\\mu)$, in particular when is it separable.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $p\\in [1,\\infty)$, then the set of $p$-integrable simple functions is dense in $L^p(\\Omega,\\mathcal{A},\\mu)$. If<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\Omega$ is topological space, $\\mu$ $\\sigma$-finite Borel measure and $\\mathcal{N}\\subset \\mathcal{A}_\\nu$ is countable and produce open sets through unions, or&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\mu$ is finite and $\\mathcal{N}\\subset \\mathcal{A}$ is countable and produces a good generator through unions,&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; then the set of rational linear combinations of indicators of $\\mathcal{N}$ is dense. In particular $L^p$ is separable if $\\mu$ is $\\sigma$-finite Borel measure on second-countable space, or if $\\mu$ is finite and $\\mathcal{A}$ countably generated.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $f\\in L^p$, WLOG $f\\ge 0$. Then there is a sequence $0\\le f_n \\uparrow f$ of simple functions (hence also integrable), and $\\left\\|f_n - f\\right\\|_p \\to 0$ by DCT (dominated by $2f$).<br><br>&nbsp;&nbsp;&nbsp; In (i) we may assume that $\\mu$ is finite (for general $f$ take open set $O$ of finite measure such that $f$ has small mass outside $O$ and approximate $f$ on $O$, where we replace $\\mathcal{N}$ by those members that lie in $O$), so that we are in the case of (i). Denote by $\\mathcal{F}$ the $\\left\\|\\cdot \\right\\|_p$-closure of (rational=real) linear combinations of indicators of $\\mathcal{N}$. Then $\\mathcal{F}$ is a vector space, and \\[&nbsp;&nbsp;&nbsp; \\mathcal{D} := \\left\\{ A\\in \\mathcal{A}\\colon \\boldsymbol{1}_{A}\\in \\mathcal{F} \\right\\} \\] contains $\\mathcal{M}$ and is a Dynkin-system ($1 = \\boldsymbol{1}_{\\Omega}$ because $\\mathcal{M}$ is good, $1 - \\boldsymbol{1}_{A}$ because $\\mathcal{F}$ vector space, rest because of MCT), so $\\mathcal{D} = \\mathcal{A}$, so we are done.</i> </p> </div><div></div>"
  },
  {
    "front": "If $f\\in L^p(\\mathbb{R}^d)$ ($p\\in [1,\\infty)$) and $f_\\lambda(\\cdot) = f(\\lambda \\cdot)$ and $f^y(\\cdot) = f(\\cdot - y)$, then the maps<div><ol>  <li>$\\mathbb{R}^d\\to L^p; \\, y\\to f^y$,</div><div></li>  <li>$(0,\\infty)\\to L^p;\\, \\lambda \\to f_\\lambda$</div><div></li></ol></div><div>are continuous.</div>",
    "back": "<div><p><i><b>Proof.</b>&nbsp; &nbsp; It suffices to show that $\\left\\| f^y - f\\right\\| \\stackrel{ y\\to 0 }{\\longrightarrow} 0$ and $\\left\\| f_\\lambda - f\\right\\|\\stackrel{ \\lambda\\to 1 }{\\longrightarrow} 0$. The set of functions for which this holds is a vector space, and closed in $L^p$. Indeed, if $f_k\\to f$ and it holds for all $f_k$, then for any $\\varepsilon &gt; 0$ take $k\\in \\mathbb{N}$ with $\\left\\|f_k - f\\right\\|_p &lt; \\varepsilon$, so that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{n\\to \\infty} \\left\\|f^y - f\\right\\|\\le 2\\varepsilon + \\varlimsup_{n\\to \\infty} \\left\\|f_k^y - f_k\\right\\| = 2\\varepsilon,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</div><div>&nbsp; &nbsp; similarly for the $f_\\lambda$ case. Hence it suffices to show that it holds for indicators of cubes, which is obvious in both cases.</div><div></i> </p> </div><div></div>"
  },
  {
    "front": "Young's convolution inequality, and if $f\\in L^p(\\mathbb{R}^d)$, $g\\in C_c^k(\\mathbb{R}^d)$, then $f\\star g\\in ?$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $p,q,r\\in [1,\\infty]$ such that $\\frac{1}{p} + \\frac{1}{q} = \\frac{1}{r} + 1$, and $f\\in L^p(\\mathbb{R}^d), g\\in L^q(\\mathbb{R}^d)$, then $f\\star g\\in L^r(\\mathbb{R}^d)$ and \\[&nbsp;&nbsp;&nbsp; \\left\\|f \\star g\\right\\|_r \\le \\left\\|f\\right\\|_p \\left\\|g\\right\\|_q\\\\&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Bunch of Hölder.</i> </p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\in L^p(\\mathbb{R}^d)$ with $p\\in [1,\\infty]$, and $g\\in C^k_c(\\mathbb{R}^d)$, then $f\\star g \\in C^k(\\mathbb{R}^d) \\cap L^p(\\mathbb{R}^d)$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\partial_\\alpha (f\\star g) = f \\star (\\partial_\\alpha g)&nbsp;&nbsp;&nbsp; \\] for every multi-index $g$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $g\\in L^1(\\mathbb{R}^d)$, so $f\\star g\\in L^p$ by Young's inequality.<br><br>&nbsp;&nbsp;&nbsp; For $k = 0$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left|(f\\star g)(x+z) - (f\\star g)(x)\\right| \\le \\left\\|f\\right\\|_p \\left\\|g^z - g\\right\\|_q.&nbsp;&nbsp;&nbsp; \\] This goes to zero because $g$ is uniformly continuous. If $k = 1$, one can show with DCT that $f\\star g$ has partial derivatives and $\\partial (f\\star g) = f \\star (\\partial g)$, and by case $k = 0$ this is continuous.</i> </p> </div><div></div>"
  },
  {
    "front": "Approximation of identity (version for $f\\in C(\\mathbb{R}^d)$ and $f\\in L^p(\\mathbb{R}^d)$).",
    "back": "<div>For $\\rho \\colon \\mathbb{R}^d\\to \\mathbb{R}$, put $\\rho_\\varepsilon(\\cdot ) := \\frac{1}{\\varepsilon^d} \\rho\\left( \\frac{\\cdot}{\\varepsilon} \\right) $ for $\\varepsilon &gt; 0$.<br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $\\rho \\ge 0$ measurable, $\\int \\rho \\mathop{}\\!\\mathrm{d} x = 1$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $\\rho$ has compact support and $f\\in C(\\mathbb{R}^d)$, $f \\star \\rho_\\varepsilon \\stackrel{  }{\\longrightarrow} f$ uniformly on compacts.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $f\\in L^p(\\mathbb{R}^d)$, $p\\in [1,\\infty)$, then $\\left\\|f\\star \\rho_\\varepsilon - f\\right\\|_p \\stackrel{  }{\\longrightarrow} 0$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $K\\subset \\mathbb{R}^d$ be compact, $K'$ its $1$-fattening. WLOG $\\operatorname{supp}(\\rho) \\subset B(0,1)$, so $\\operatorname{supp}(\\rho_\\varepsilon) = \\varepsilon\\operatorname{supp}(\\rho)&nbsp; \\subset B(0,\\varepsilon)$. Then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{x\\in K} \\left| (f\\star \\rho_\\varepsilon)(x) - f(x) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\sup_{x\\in K}\\underbrace{ \\left| f(x-y) - f(x) \\right|}_{\\text{here $x,x-y \\in K'$}}&nbsp; \\rho_\\varepsilon(y) \\mathop{}\\!\\mathrm{d} y\\le \\omega\\left( f\\!\\!\\restriction_{K'} ,\\varepsilon \\right) \\stackrel{  }{\\longrightarrow} 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; because $f$ is uniformly continuous on $K'$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For $x\\in \\mathbb{R}^d$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| (f\\star \\rho_\\varepsilon)(x) - f(x) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\left| f(x-y) - f(x) \\right| \\rho_\\varepsilon(y)^{1 / p} \\rho_\\varepsilon(y)^{1 / q} \\mathop{}\\!\\mathrm{d} y\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left(\\int \\left| f(x-y) - f(x) \\right|^p \\rho_\\varepsilon(y) \\mathop{}\\!\\mathrm{d} y\\right)^{1 / p} \\underbrace{\\left(\\int \\rho_\\varepsilon(y) \\mathop{}\\!\\mathrm{d} y\\right)^{1 / q}}_{ = 1},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; so, taking $r &gt; 0$ with $\\left\\|f^y - f\\right\\|\\le \\delta$ for $\\left| y \\right| \\le r$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}\\left\\|f\\star \\rho_\\varepsilon - f\\right\\|_p^p\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\left\\|f^y - f\\right\\|_p^p \\rho_\\varepsilon(y)\\mathop{}\\!\\mathrm{d} y \\le \\delta + (2\\left\\|f\\right\\|_p)^p \\int_{\\left| y \\right| \\ge r} \\rho_\\varepsilon(y)\\mathop{}\\!\\mathrm{d} y \\stackrel{ \\varepsilon \\to 0 }{\\longrightarrow} \\delta,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and this holds for every $\\delta&gt; 0$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Denseness of $C_c^\\infty$ in $L^p$ (for $\\mathbb{R}^d$ and $\\Omega\\subset \\mathbb{R}^d$).",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $p\\in [1,\\infty)$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$C_c^\\infty(\\mathbb{R}^d)$ is dense in $L^p(\\mathbb{R}^d)$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\Omega \\in \\mathcal{B}(\\mathbb{R}^d)$ satisfies $\\lambda(\\Omega\\setminus \\Omega^o) = 0$, then $C_c^\\infty(\\Omega) := \\left\\{ f\\in C^\\infty(\\Omega)\\colon \\operatorname{supp}(f) \\subset \\Omega^o \\right\\} $ is dense in $L^p(\\Omega)$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Take $\\rho\\in C_c^\\infty(\\mathbb{R}^d)$ with $\\int \\rho\\mathop{}\\!\\mathrm{d} x = 1$ and $\\operatorname{supp}(\\rho)\\subset B(0,1)$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>Take $f\\in L^p$. Since $\\left\\|f \\boldsymbol{1}_{\\mathbb{R}^d \\setminus B(0,N)}\\right\\| \\to 0$ as $N\\to \\infty$, we may assume that $f$ has compact support. Then, $f_\\varepsilon := f\\star \\rho_\\varepsilon \\stackrel{ L^p }{\\longrightarrow} f$ and $f_\\varepsilon$ is in $C^\\infty$ and compactly supported (because $\\rho_\\varepsilon$ and $f$ are).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Let $f\\in L^p(\\Omega)$, we may assume it has compact (i.e. bounded) support. For $k\\in \\mathbb{N}$, put $\\Omega_k := \\left\\{ x\\in \\Omega\\colon B(x, 1/ k) \\subset \\Omega \\right\\} $ and $f_k := f \\boldsymbol{1}_{\\Omega_k}$. Then $f_k \\star \\rho_\\varepsilon \\in C^\\infty_c(\\Omega)$ for $\\varepsilon &lt; 1 / k$ and&nbsp; approximates $f_k$ as $\\varepsilon \\to 0$. Finally, $f - f_k \\downarrow f \\boldsymbol{1}_{\\Omega\\setminus \\Omega^o}$, which is $\\lambda$-a.e. zero, so $\\left\\|f - f_k\\right\\|_p \\to 0$ by DCT.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "A function $f\\colon X\\to [0,\\infty]$ on a metric space $X$ is lower semi-continuous iff it is the increasing limit (or supremum) of non-negative continuous functions.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\impliedby$\":} Suppose $f_n \\uparrow f$ and $x_m \\to x$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_n(x) = \\lim_{m\\to \\infty} f_n(x_m) \\le \\varliminf_{m\\to \\infty} f(x_m),\\\\&nbsp;&nbsp;&nbsp; \\] and the LHS goes to $f(x)$.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\implies $\":} If $f\\equiv \\infty$ it is clear, assume otherwise. Put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_n(\\cdot ) := \\inf_{y\\in X} \\left( f(y) + n \\rho(\\cdot ,y) \\right),\\quad n\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Then, $f_n\\colon X\\to [0,\\infty)$ is increasing and bounded above by $f$ (and everywhere finite because $\\inf f &lt; \\infty$). Then $f_n$ is $n$-Lipschitz. Indeed, WLOG $n = 1$, and if $x,x'\\in X$, choose $y'\\in X$ with $f_1(x') \\le f(y') + \\rho(x',y') \\le f_1(x') + \\varepsilon$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_1(x) \\le f(y') + \\rho(x, y') \\le f(y) + \\varepsilon + \\rho(x,y).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Now let $x\\in X$, then $\\varlimsup f_n(x) \\le f(x)$, and if $y_n\\in X$ with $f(y_n) + n\\rho(x,y_n) \\le f_n(x) + \\varepsilon(\\le f(x) + \\varepsilon)$, then either $\\rho(x,y_n) \\to 0$ and hence \\[f(x) \\le \\varliminf_{n\\to \\infty} f(y_n) \\le \\varliminf_{n\\to \\infty} \\left( f(y_n) + n\\rho(x,y_n) \\right) \\le \\varliminf_{n\\to \\infty} f_n(x) + \\varepsilon,\\\\\\] or $\\rho(x,y_n)\\not\\to 0$, in which case $f_{k(n)}(x) \\uparrow \\infty$ for a subsequence and hence $f_n(x) \\uparrow \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "(DDSDE) Assumptions $(A^{1.1})$ on $\\sigma,b$ for singular SDEs, definition of $L^p_q$ and $\\mathcal{K}$",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; For $p,q &gt; 0$ and $s &lt; t$, $L^p_q$ consists of those $f\\colon [s,t]\\times \\mathbb{R}^d\\to \\mathbb{R}$ for which \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|f\\right\\|_{L^p_q} := \\sup_{z\\in \\mathbb{R}^d} \\left( \\int_s^t \\left\\| \\boldsymbol{1}_{B(z,1)} f_r\\right\\|_{L^p}^q \\mathop{}\\!\\mathrm{d} r\\right) ^{ 1/q} &lt; \\infty.&nbsp;&nbsp;&nbsp; \\] Put \\[&nbsp;&nbsp;&nbsp; \\mathcal{K}:= \\left\\{ (p,q)\\colon p,q &gt; 2, \\frac{d}{p} + \\frac{2}{q} &lt; 1 \\right\\} .&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><b>Assumption.</b>[$A^{1.1}$] Let $\\sigma\\colon [0,T]\\times&nbsp; \\mathbb{R}^d \\to \\mathbb{R} ^{d\\times m}$ and $b = b^{(0)}+ b^{(1)}\\colon [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d$ measurable such that there are $(p_i,q_i)\\in \\mathcal{K},i = 0,1$ with<br><ol>  <li>$a:= \\sigma \\sigma^\\top$ is invertible with $\\left\\|a\\right\\|_\\infty+ \\left\\|a^{-1}\\right\\|_\\infty &lt; \\infty$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{t\\in [0,T]} \\omega(a_t,\\varepsilon) \\stackrel{  }{\\longrightarrow} 0,\\quad \\varepsilon \\to 0,\\\\&nbsp;&nbsp;&nbsp; \\] and $\\left\\|\\nabla \\sigma\\right\\|\\in L^{p_1}_{q_1}$</li>  <li>$\\left| b^{(0)} \\right| \\in L_{q_0}^{p_0}$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{t\\in [0,T],x\\neq y} \\left( \\big| b^{(1)}_t(0)\\big| + \\frac{\\big| b^{(1)}_t(x) - b^{(1)}_t(y) \\big| }{\\big| x-y \\big| }&nbsp;&nbsp; \\right)&nbsp; &lt;\\infty.\\]</li></ol></p> </div><div></div>"
  },
  {
    "front": "Stochastic Grönwall inequality.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $(\\xi_t)$ is cadlag, $(\\eta_t)$ progressively measurable both non-negative, and $(A_t)$ continuous adapted increasing with $A_0 = 0$, $M$ a local martingale with $M_0 = 0$, and \\[&nbsp;&nbsp;&nbsp; \\xi_t \\le \\xi_0 + \\int_0^t \\eta_s \\mathop{}\\!\\mathrm{d} s + \\int_0^t \\xi_s \\mathop{}\\!\\mathrm{d} A_s + M_t, \\quad t\\ge 0,\\\\&nbsp;&nbsp;&nbsp; \\] then for any $0 &lt; q &lt; p &lt; 1$ and $t \\ge 0$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\sup_{s\\le t} \\left| \\xi_s \\right| ^q \\right] ^{1 / q} \\le \\left( \\frac{p}{p-q} \\right) ^{1 / q}&nbsp; \\mathbb{E} \\left[ \\mathrm{e}^{\\frac{pA_t}{1 - p}} \\right] ^{\\frac{1-p}{p}} \\mathbb{E} \\left[ \\xi_0 + \\int_0^t \\eta_s \\mathop{}\\!\\mathrm{d} s \\right] .&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Bound on $|f(x)-f(y)|$ in terms of maximal function $\\mathcal{M}|\\nabla f|$.",
    "back": "<div>For $f\\colon \\mathbb{R}^d\\to [0,\\infty)$ measurable, \\[&nbsp;&nbsp;&nbsp; (Mf)(x) := \\sup_{0 &lt; r &lt; 1} \\frac{1}{\\left| B(0,r) \\right| } \\int_{B(0,r)} f(x+y)\\mathop{}\\!\\mathrm{d} y,\\quad x\\in \\mathbb{R}^d.\\] <br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; There is a $c &gt; 0$ such that<br>&nbsp;&nbsp;&nbsp; <ol>  <li>for any $f\\colon \\mathbb{R}^d\\to \\mathbb{R}$ with $\\left| \\nabla f \\right| \\in L^1_\\text{loc}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| f(x) - f(y) \\right| \\le c \\left| x-y \\right| \\left( M \\left| \\nabla f \\right| (x) + M \\left| \\nabla f \\right| (y) + \\left\\|f\\right\\|_\\infty&nbsp; \\right) ,\\quad \\text{$\\lambda^d$-a.e. $x,y\\in \\mathbb{R}^d$},\\\\&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>for any $f\\colon [0,T]\\times \\mathbb{R}^d\\to [0,\\infty)$ measurable, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|M f\\right\\|_{L^p_q} \\le c \\left\\|f\\right\\|_{L^p_q},\\quad p,q\\ge 1.&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "(DDSDE) Lemma (PDE) and Zvonkin's transform.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Assume $(A^{1.1})$ with generator $L_t$, $p,q&gt;1$. Then for any $f\\in L^{p}_q$, the PDE \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (\\partial_t + L_t) u^{\\lambda}_t = \\lambda u^{\\lambda}_t + f_t,\\quad u^\\lambda_T = 0\\\\&nbsp;&nbsp;&nbsp; \\] has a unique solution $u^\\lambda \\in H_q^{2,p}$. Furthermore, if $(2p,2q)\\in \\mathcal{K}$, there are $c , \\varepsilon,\\lambda_0 &gt; 0$ (independent of $f$) such that<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\left\\|u^\\lambda\\right\\|_\\infty + \\left\\|\\nabla u^{\\lambda}\\right\\|_\\infty \\le c \\lambda^{-\\varepsilon} \\left\\|f\\right\\|_{L^p_q}$ for $\\lambda\\ge\\lambda_0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\left\\|(\\partial_t + b^{(1)} \\nabla )u^\\lambda\\right\\|_{L^p_q} + \\left\\|\\nabla ^2 u^\\lambda\\right\\|_{L^p_q} \\le c \\left\\|f\\right\\|_{L^p_q} $ for $\\lambda \\ge \\lambda_0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $(p,q)\\in \\mathcal{K}$, then for all $0 &lt; \\alpha &lt; 1 - \\frac{d}{p} - \\frac{2}{q}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{t\\in [0,T],x\\neq y} \\frac{\\left| \\nabla u_t(x) - \\nabla u_t(y) \\right| }{\\left| x-y \\right| ^\\alpha} &lt; \\infty,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div><div>Hence, \\[&nbsp; &nbsp; \\mathop{}\\!\\mathrm{d} u_t(X_t) = (\\lambda u_t + f_t) (X_t)\\mathop{}\\!\\mathrm{d} t + ((\\nabla u_t) \\sigma_t)(X_t) \\mathop{}\\!\\mathrm{d} W_t.\\] Choosing $f_t = -b^{(0)}_t$, $Y_t := X_t +u_t(X_t)$ then solves a regular SDE, and the transformation is a diffeomorphism for large $\\lambda$ so that $\\left\\|\\nabla u\\right\\|_\\infty &lt; 1$.</div><div></div>"
  },
  {
    "front": "(DDSDE) Krylov and Kasminskii estimates.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Assume $(A^{1.1})$ holds and $X$ solves the SDE, and $(2p,2q)\\in \\mathcal{K}$. Then there exist $c,k &gt; 1$ such that for any $f\\in L^p_q(s,t)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{\\bf (Krylov)} &amp;\\qquad \\mathbb{E} \\left[\\int_{s}^t \\left| f_r(X_r) \\right| \\mathop{}\\!\\mathrm{d} r \\,\\middle\\vert\\, \\mathcal{F}_s\\right] \\le c \\left\\|f\\right\\|_{L^p_q(s,t)},\\\\[8pt]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{\\bf (Kasminskii)} &amp; \\qquad \\mathbb{E} \\left[\\mathrm{e}^{\\int_s^t \\left| f_r(X_r) \\right| \\mathop{}\\!\\mathrm{d} r} \\,\\middle\\vert\\, \\mathcal{F}_s\\right] \\le \\mathrm{e}^{c + c \\left\\|f\\right\\|^k_{L^p_q(s,t)}}.&nbsp;&nbsp;&nbsp; \\end{align*}</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $f\\ge 0$. Let $u\\in H^{2,p}_q(s,t)$ be the solution to $(\\partial_r + L_r) u_r = \\lambda u_r + f_r$ and $u_t = 0$, so that $\\mathop{}\\!\\mathrm{d} u_t(X_t) = (\\lambda u_t + f_t)(X_t) \\mathop{}\\!\\mathrm{d} t + \\mathop{}\\!\\mathrm{d} M_t$ for a martingale $M$, so <br>&nbsp;&nbsp;&nbsp; \\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\int_s^t&nbsp; f_r(X_r) \\mathop{}\\!\\mathrm{d} r\\,\\middle\\vert\\, \\mathcal{F}_s \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[u_t(X_t) - u_s(X_s) - \\lambda \\int_s^t u_r(X_r)\\mathop{}\\!\\mathrm{d} r \\,\\middle\\vert\\, \\mathcal{F}_s \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le (2 + \\lambda T) \\left\\|u\\right\\|_\\infty \\le (2 + \\lambda T) c \\left\\|f\\right\\|_{L^p_q}.&nbsp;&nbsp;&nbsp; \\end{align*}<br><br>&nbsp;&nbsp;&nbsp; Say we go from $t_0$ to $t_1$ instead of $s$ to $t$. Since $\\mathcal{K}$ is open, (Krylov) also holds for $(p,q')$ with $q'&lt; q$, so for $\\delta &gt; 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\int_t^{t+\\delta} f_s(X_s) \\mathop{}\\!\\mathrm{d} s \\,\\middle\\vert\\, \\mathcal{F}_t \\right] \\le c \\left\\|f\\right\\|_{L^p_{q'}} \\le c \\delta^{1/k} \\left\\|f\\right\\|_{L^p_{q}},\\\\&nbsp;&nbsp;&nbsp; \\] where $k = (qq')/(q-q')$. Let $n\\in \\mathbb{N}$ so that the above is no larger than $1 / 2$ for $\\delta \\le 1 / n$, which can be chosen such that $n\\le c + c \\left\\|f\\right\\|_{L^p_q}^k$. Then, for $m\\in \\mathbb{N}$,</div><div>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left( \\int_t ^{t+\\delta} f_s(X_s)\\mathop{}\\!\\mathrm{d} s \\right) ^m \\,\\middle\\vert\\,&nbsp; \\mathcal{F}_t \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= m! \\mathbb{E} \\left[\\int_{s_1&lt;\\ldots &lt;s_m} \\prod_{i=1}^m f_{s_j}(X_{s_j}) \\mathop{}\\!\\mathrm{d} s_j \\,\\middle\\vert\\, \\mathcal{F}_t\\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\ldots \\le m! 2^{-m},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and hence $\\mathbb{E} \\left[ \\exp \\left( \\int_t ^{t+\\delta} f_s(X_s)\\mathop{}\\!\\mathrm{d} s \\right) \\,\\middle\\vert\\, \\mathcal{F}_s \\right] \\le \\sum_{m=0}^\\infty 2^{-m} = 2$, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\mathrm{e}^{\\int_{t_0}^{t_1} f_s(X_s) \\mathop{}\\!\\mathrm{d} s} \\right] \\le 2^n \\le \\mathrm{e}^{c' + c' \\left\\|f\\right\\|_{L^p_q}}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "(DDSDE) Well-posedness of (SDE) under assumptions $(A^{1.1})$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $(A^{1.1})$ holds. Then (SDE) is well-posed. In fact, for any $k\\in \\mathbb{N}$ there is $c = c(k) &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\sup_{t\\in [0,T]} \\left| X^x_t- X^y_t \\right|^k&nbsp; \\right] \\le c \\left| x-y \\right| ^k,\\quad x,y\\in \\mathbb{R}^d.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Weak existence follows by Girsanov transform and standard theory. If we show the remaining claim we have pathwise uniqueness and hence well-posedness by Yamada-Watanabe.<br><br>&nbsp;&nbsp;&nbsp; Applying Zvonkin's transform with $\\lambda$ so large that $\\left\\|\\nabla u\\right\\|_\\infty &lt; 1$ gets rid of the singular drift through a diffeomorphism, so it STS the claim for the transformed SDE. Let $Y^1$ and $Y^2$ be two solutions and put $\\xi_t := \\left| Y^1_t - Y^2_t \\right| $. Then, by Ito,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} \\xi_t ^{2k} \\le c_k \\xi_t ^{2k} \\left( 1 + \\frac{\\left| u_t(Y^1_t) - u_t(Y^2_t) \\right| }{\\xi_t} + \\frac{\\left\\|\\widetilde{\\sigma}_t(Y^1_t) - \\widetilde{\\sigma}_t(Y^2_t)\\right\\|^2}{\\xi_t^2} \\right) + \\mathop{}\\!\\mathrm{d} M_t\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for a clm $M$. By maximal functional lemma, the two fractions are at most<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c \\sum_{i = 1,2}\\left[ \\left( M(\\left| \\nabla u_t \\right| + \\left\\|\\nabla \\widetilde{\\sigma}_t\\right\\|^2 \\right) (Y_t^i) + \\left\\|u\\right\\|_\\infty + \\left\\|\\widetilde{\\sigma}\\right\\|_\\infty^2\\right] = \\sum_{i=1,2} f(Y^i_t) =: B_s,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where by $(A^{1.1})$, $f\\in L^{p / 2}_{q / 2}$ for some $(p,q)\\in \\mathcal{K}$, so putting $A_t := c_k \\int_0^t B_s \\mathop{}\\!\\mathrm{d} s$, we have $\\mathbb{E} \\left[ \\mathrm{e}^{\\lambda A_T} \\right] &lt; \\infty$ for all $\\lambda &gt; 0$ by Kasminskii, and \\[&nbsp;&nbsp;&nbsp; \\xi_t ^{2k} \\le \\left| Y_0^1 - Y_0^2 \\right| ^{2k} + \\int_0^t \\xi_s ^{2k} \\mathop{}\\!\\mathrm{d} A_s + M_t.\\] Stochastic Grönwall finishes the proof.</i> </p> </div><div></div>"
  },
  {
    "front": "(DDSDE) $\\log$-Harnack inequality for $P_t$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $(A^{1.1})$ holds, then there is $c &gt; 0$ such that for all $t\\in (0,T]$ and $f &gt; 0$ bounded measurable, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P_t \\log f(y) \\le \\log P_t f(x) + c \\frac{\\left| x-y \\right| ^2}{t}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Put $\\gamma_s := x + \\frac{s}{t} (y-x)$ for $s\\in [0,t]$, let $f\\in C_c^\\infty(\\mathbb{R}^d)$, $f &gt; 0$, and put $Y_t := \\log P_{s,t} f (X_s^{\\gamma_s})$, so $Y_0 = \\log P_t f (x)$ and $\\mathbb{E}\\left[Y_t\\right] = \\mathbb{E}\\left[\\log f(X_t^y)\\right] = P_t \\log f (y)$. Then, since $\\partial_s P_{s,t} f = - L_s P_{s,t} f$ (Kolmogorov backward),<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} Y_s &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left( L_s (\\log P_{s,t}f) - \\frac{L_s P_{s,t}f}{P_{s,t}f} \\right) (Y_s) \\mathop{}\\!\\mathrm{d} s \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\phantom{=}+ \\left&lt;\\nabla_{\\gamma_s'} X_s^{\\gamma_s}, \\nabla\\log P_{s,t}f(Y_s) \\right&gt; (Y_s) \\mathop{}\\!\\mathrm{d} s + \\mathop{}\\!\\mathrm{d} M_s\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now note the general fact that $L_s (\\log g) - \\frac{L_s g}{g} = -\\frac{1}{2}\\left&lt;a_s \\nabla g,\\nabla g \\right&gt; \\le -\\lambda \\left\\|\\nabla g\\right\\|^2$ for some $\\lambda = \\lambda(a) &gt; 0$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} Y_s &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left( \\frac{\\left| x-y \\right| \\left\\|\\nabla X_s^{\\gamma_s}\\right\\|}{t} \\left\\|\\nabla \\log P_{s,t}f(Y_s) \\right\\|- \\lambda \\left\\|\\nabla \\log P_{s,t}f(Y_s)\\right\\|^2&nbsp; \\right) \\mathop{}\\!\\mathrm{d} s + \\mathop{}\\!\\mathrm{d} M_t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\frac{\\left| x-y \\right| ^2}{4\\lambda t^2} \\left\\|\\nabla X_s^{\\gamma_s}\\right\\|^2 \\mathop{}\\!\\mathrm{d} s + \\mathop{}\\!\\mathrm{d} M_s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Finally, use that $\\mathbb{E} \\left[ \\sup_{s\\in [0,t]} \\left| \\nabla_x X_s^x \\right| ^k \\right] &lt; \\infty$ for all $k\\in \\mathbb{N}$ (by regularity in the initial condition), so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P_t \\log f(x) - \\log P_t f(x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ Y_t \\right] - Y_0 \\le c \\frac{\\left| x-y \\right| ^2}{t}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now general $f$ by approximation.</i> </p> </div><div></div>"
  },
  {
    "front": "(DDSDE) Harnack inequality for $P_t$ with power.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $(A^{1.1})$ holds with $\\varepsilon_0:= \\frac{d}{p_0}+ \\frac{2}{q_0}&lt; \\frac{1}{2}$, and that for some $\\alpha \\in (1 / 2,\\varepsilon_0)$, $\\sigma$ is locally $\\alpha$-Hölder continuous in space, uniformly in $t\\in [0,T]$. Then there are $c&gt;0,\\, \\widehat{p} &gt; 1$ such that for all $p\\ge \\widehat{p}$, $x,y\\in \\mathbb{R}^d$, $t\\in (0,T]$, and $f\\in B_b(\\mathbb{R}^d)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| P_t f (y) \\right| ^p \\le \\mathrm{e}^{\\frac{c}{t}\\left( \\left| x-y \\right| ^2 + \\left| x-y \\right| ^{2\\alpha}\\right) } P_t \\left| f \\right| ^p(x).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The idea is to look at a coupling of the form \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} X_s = b_s(X_s) \\mathop{}\\!\\mathrm{d} s + \\sigma_s(X_s) \\mathop{}\\!\\mathrm{d} W_s, &amp; X_0 = x,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} Y_s = (b_s(Y_s) + \\xi_s \\sigma(Y_s))\\mathop{}\\!\\mathrm{d} s + \\sigma_s(Y_s)\\mathop{}\\!\\mathrm{d} W_s, &amp; Y_0 = y,\\\\&nbsp;&nbsp;&nbsp; \\end{cases}\\] where $\\xi = \\xi_s(x,y)\\uparrow \\infty$ as $ s\\to t$ except if $x\\to y$. Then $X_t = Y_t$ almost surely and there is a martingale $(R_s)_{s\\in [0,t]}$ such that under $\\mathop{}\\!\\mathrm{d} \\mathbb{Q}:= R \\mathop{}\\!\\mathrm{d} \\mathbb{P}$, $Y$ satisfies the SDE of $X$, and for some $q &gt; 1$, $c &gt; 0$, \\[\\mathbb{E} \\left[ R_t^q \\right] \\le \\exp \\left( \\frac{c}{t} \\left( \\left| x-y \\right| ^{2} + \\left| x-y \\right| ^{2\\alpha} \\right)&nbsp; \\right) .\\] Hence, if $p\\ge \\widehat{p} := \\frac{q}{q-1}$, (for probability measures you can always take one of either $p$ or $q$ in Hölder larger than it needs to be because $p \\mapsto \\mathbb{E} \\left[ \\left| \\cdot&nbsp; \\right| ^p \\right] ^{1 / p}$ is increasing).<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left| P_t f(y) \\right| ^p &nbsp;&nbsp;&nbsp; &amp;= \\left| \\mathbb{E} \\left[ R_t f(Y_t) \\right]&nbsp; \\right| ^p = \\left| \\mathbb{E} \\left[ R_t f(X_t) \\right]&nbsp; \\right| ^p\\le \\mathbb{E} \\left[ R_t^q \\right] ^{p / q} \\mathbb{E} \\left[ \\left| f(X_t) \\right| ^p \\right] \\\\&nbsp;&nbsp;&nbsp; &amp;\\le P_t \\left| f \\right| ^p(x) \\exp \\left( \\frac{c}{t} \\left( \\left| x-y \\right| ^2 + \\left| x-y \\right| ^{2\\alpha} \\right)&nbsp; \\right) .\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Radon measures, their role as a dual space, and (strongly) Radon spaces.",
    "back": "<div>A Borel measure $\\mu$ on a locally compact Hausdorff topological space $X$ is called <i>Radon measure</i> if it is locally finite and inner regular on open sets.<br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Denote $C_0(X) := \\overline{C_c(X,\\mathbb{R})}^{\\left\\|\\cdot \\right\\|_\\infty}$, which is a Banach space with the supremum norm. Then the positive continuous linear functionals on $C_0(X)$ are exactly the maps $f \\mapsto \\int f \\mathop{}\\!\\mathrm{d} \\mu$ for finite Radon measures $\\mu$ on $X$. The set of all continuous linear functionals (i.e. the elements of the topological dual) are exactly the differences of these.</i> </p> <br><br>$X$ is called <i>(strongly) Radon</i> if every (locally) finite Borel measure is Radon. If $X$ is Suslin (image of a Polish space under continuous map), then $X$ is strongly Radon and every Radon measure is outer regular (i.e. every locally finite Borel measure is regular).</div><div></div>"
  },
  {
    "front": "Finite signed measures on $\\mathbb{R}^d$ as Banach space and role as dual.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A finite signed measure $\\mu$ on $\\mathbb{R}^d$ is a set function on the Borel sets such that, for every disjoint collection $(A_i)_{i\\in \\mathbb{N}}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{i=1}^\\infty \\left| \\mu(A_i) \\right| &lt;\\infty,\\qquad \\mu \\left( \\bigcup_{i=1} ^\\infty A_i&nbsp; \\right) = \\sum_{i=1}^\\infty\\mu(A_i).&nbsp;&nbsp;&nbsp; \\] This becomes a Banach space with \\[&nbsp;&nbsp;&nbsp; \\left\\|\\mu\\right\\| := \\sup \\left\\{ \\sum_{i=1}^\\infty \\left| \\mu(A_i) \\right| \\colon \\sum_{i=1}^\\infty A_i = \\mathbb{R}^d \\right\\} = \\mu_+(\\mathbb{R}^d) + \\mu_-(\\mathbb{R}^d).&nbsp;&nbsp;&nbsp; \\] <br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\mu$ can be uniquely decomposed into the difference of two finite Borel measures $\\mu_+$ and $\\mu_-$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>The space of finite signed measures is the dual of $C_0(\\mathbb{R}^d)$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Hahn-Jordan decomposition.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\mathbb{R}^d$ is locally compact Suslin, so finite Borel measures are exactly finite Radon measures are exactly positive continuous linear funcitonals.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Kantorovich formulation of optimal transport, and criterion for existence of optimal transference plan.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $E$ be Polish and $\\mu,\\nu\\in \\mathcal{M}_1(E)$, and $c\\colon E\\times E\\to [0,\\infty]$ measurable. Denote by $\\Gamma(\\mu,\\nu)\\subset \\mathcal{M}_1(E\\times E)$ the couplings of $\\mu$ and $\\nu$. Then the Kantorovich optimal transport problem (KOT) is to find a minimizer of \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I_K := \\inf \\left\\{ \\Pi(c)\\colon \\Pi \\in \\Gamma(\\mu,\\nu) \\right\\} .&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $c$ is lower semi-continuous, then KOT has a solution.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that it is clear that $\\Gamma(\\mu,\\nu)$ is compact in the weak topology (tight because $\\mu$ and $\\nu$ are, and easy to confirm that closed). Furthermore, there are continuous bounded functions $0\\le c_n \\uparrow c$ because $c$ is lower semicontinuous, so $I_n := \\left[ \\gamma \\mapsto \\gamma(c_n) \\right] $ are continuous and $I := \\left[ \\gamma \\mapsto \\gamma(c) \\right] = \\sup_n I_n$ by MCT, so $I$ is lower semicontinuous. Finally, a lower semicontinuous function on a compact set achieves a minimum.</i> </p> </div><div></div>"
  },
  {
    "front": "Dual problem to Kantorovich optimal transport and idea of proof.",
    "back": "<div>Let $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R}^d)$, $c\\colon \\mathbb{R}^d\\times \\mathbb{R}^d\\to [0,\\infty)$ measurable, and $I_\\star = \\inf \\left\\{ \\Pi(c)\\colon \\Pi\\in \\Gamma(\\mu,\\nu) \\right\\} $. The <i>dual problem</i> is \\[&nbsp;&nbsp;&nbsp; J_\\star := \\sup_{(\\phi,\\psi)\\in \\Phi} \\left( \\mu(\\phi) + \\nu(\\psi) \\right) ,\\\\\\] where $\\Phi = \\left\\{(\\phi,\\psi)\\colon&nbsp; \\phi\\in L^1(\\mu),\\psi \\in L^1(\\nu), \\phi(x) + \\psi(y) \\le c(x,y) \\text{ $\\mu\\times \\nu$-a.e.} \\right\\}$.<br><br><p><b>Theorem.</b> <i>&nbsp; &nbsp; If $c$ is lower semicontinuous, then $I_\\star = J_\\star$.</i> </p> <br><br><p><i><b>Proof.</b>[Sketch of Proof]<br>&nbsp;&nbsp;&nbsp; First note that $\\sup_{\\phi,\\psi\\in C_b} \\left( \\mu(\\phi) + \\nu(\\psi) - \\Pi(\\phi + \\psi) \\right) = \\infty \\boldsymbol{1}_{\\Pi \\in \\Gamma(\\mu,\\nu)}$ for any $\\Pi\\in \\mathcal{M}_F(\\mathbb{R}^d\\times \\mathbb{R}^d)$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I_\\star = \\inf_\\Pi \\sup_{\\phi,\\psi\\in C_b} \\left( \\mu(\\phi) + \\nu(\\psi) + \\Pi(c - \\phi - \\psi) \\right) .&nbsp;&nbsp;&nbsp; \\] Now it is true(!) that we can exchange inf and sup, so this is<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{\\phi,\\psi} \\Big( \\mu(\\phi) + \\nu(\\psi) + \\underbrace{\\inf_\\Pi \\Pi(c - \\phi - \\psi)}_{=-\\infty \\boldsymbol{1}_{\\left\\{\\phi + \\psi \\not\\le c\\right\\}}} \\Big) = \\sup_{\\phi + \\psi \\le c} \\left( \\mu(\\phi) + \\nu(\\psi) \\right) .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; The last step is to see that we can relax $\\phi$ and $\\psi$ to the set $\\Phi$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition and Characterisation of Cameron-Martin space $\\mathbb{H}\\subset C([0,T],\\mathbb{R}^d)$, and Malliavin derivative $D\\colon D^{1,2}\\subset L^2(\\mu_W) \\to L^2(\\mu_W)$. Is this well-defined on $L^2$?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\tau_h$ for $h\\in C([0,T],\\mathbb{R}^m)$ is the shift operator, then $\\mu_W$ and $\\mu_W \\circ \\tau_h^{-1}$ are absolutely continuous iff $h$ is in the Hilbert space \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{H}:= \\left\\{ h\\in C([0,T],\\mathbb{R}^m)\\colon \\int_0^T \\left|h'(s)\\right|^2\\mathop{}\\!\\mathrm{d} s &lt; \\infty \\right\\} ,\\\\&nbsp;&nbsp;&nbsp; \\] (where the derivative is weak.) The converse is easy to see if $h$ is absolutely continuous by Girsanov transform.</p> <br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $F \\colon C([0,T],\\mathbb{R}^m) \\to \\mathbb{R}$ be continuous and in $L^2(\\mu_W)$, such that for $\\mu_W$-a.e. $\\gamma\\in C$, \\[&nbsp;&nbsp;&nbsp; \\mathbb{H} \\to \\mathbb{R};\\, h \\mapsto D_h F(\\gamma) := \\lim_{\\varepsilon \\to 0} \\frac{F(\\gamma + \\varepsilon h) - F(\\gamma) }{\\varepsilon},\\\\\\] exists and defines a bounded linear operator. In that case, for such $\\gamma$ there is $DF(\\gamma) \\in \\mathbb{H}$ such that $D_h F(\\gamma) = \\left&lt;DF(\\gamma),h \\right&gt;_{\\mathbb{H}} $. Suppose furter that $DF\\colon C \\to \\mathbb{H}$ satisfies $\\left\\|DF\\right\\|_{\\mathbb{H}} \\in L^2(\\mu_W)$, then $F$ is called <i>Malliavin differentiable</i>, write $F\\in D^{1,2}$. This can be extended to $\\mathbb{R}^d$-valued $F$ by doing it component-wise (in that case $DF\\colon C\\to \\mathbb{H}^d$).</p> <br><br>Note that by definition of $\\mathbb{H}$, if $F = G$ $\\mu_W$-a.s., then $DF = DG$ $\\mu_W$-a.s., so $D\\colon D^{1,2} \\subset L^2(\\mu_W) \\to L^2(\\mu_W)$ is well-defined.</div><div></div>"
  },
  {
    "front": "Malliavin calculus: Definition of $D_h F$ for $F = f(W)$ for a Brownian motion $W$ on $(\\Omega, \\mathcal{A}, \\mathbb{P})$ and random $h$. If $h$ is adapted, what is $\\mathbb{E} \\left[ D_h F \\right] $?",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; If $f\\in D^{1,2}$ and $F = f(W)$ for a Brownian motion $W$ on $(\\Omega, \\mathcal{A}, \\mathbb{P})$, and $h\\in L^2(\\Omega\\to \\mathbb{H}, \\mathbb{P})$ (resp. $L^\\infty$), then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (D_h F)(\\omega) := (D_{h(\\omega)}f)(W(\\omega)),\\quad \\omega\\in \\Omega,\\\\&nbsp;&nbsp;&nbsp; \\] is an element of $L^1(\\Omega,\\mathcal{A},\\mathbb{P})$ (resp. $L^2$).</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| D_h F \\right| = \\left| D_h f (W) \\right|&nbsp; = \\left| \\left&lt;Df(W), h \\right&gt;_\\mathbb{H}&nbsp; \\right|&nbsp; \\le \\left\\|h\\right\\|_\\mathbb{H} \\left\\|Df(W)\\right\\|_\\mathbb{H},\\\\&nbsp;&nbsp;&nbsp; \\] and both of these factors are in $L^2(\\mathbb{P})$, so $D_h F \\in L^1(\\mathbb{P})$.</i> </p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\in D^{1,2}$, $F = f(W)$ for a BM $(\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\mathbb{P})$, and $h\\in L^2(\\Omega\\to \\mathbb{H},\\mathbb{P})$ is adapted, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ D_h F \\right] = \\mathbb{E} \\left[ F \\int_0^T h'(s) \\mathop{}\\!\\mathrm{d} W_s \\right] .&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By some approximation, assume $\\mathbb{E} \\left[ \\exp \\left( \\frac{1}{2} \\left\\|h\\right\\|_\\mathbb{H} \\right)&nbsp; \\right] &lt; \\infty$. For $\\varepsilon &gt; 0$, put $L^\\varepsilon_t := - \\varepsilon \\int_0^t h'(s) \\mathop{}\\!\\mathrm{d} W_s$, so $\\mathcal{E}(L^\\varepsilon)$ is a martingale by Novikov, and under $\\mathop{}\\!\\mathrm{d} \\mathbb{Q} := \\mathcal{E}(L) \\mathop{}\\!\\mathrm{d} \\mathbb{P}$, $\\widetilde{W} = W + \\varepsilon h$ is a Brownian motion, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ D_h F \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ \\lim_{\\varepsilon \\to 0} \\frac{f(W+\\varepsilon h) - f(W)}{\\varepsilon} \\right] = \\lim_{\\varepsilon \\to 0} \\mathbb{E} \\left[ \\ldots&nbsp; \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{\\varepsilon \\to 0} \\frac{1}{\\varepsilon} \\left( \\mathbb{Q} \\left[ \\mathcal{E}(-\\widetilde{L^\\varepsilon})_T f(\\widetilde{W}) \\right] - \\mathbb{E} \\left[ f(W) \\right]&nbsp;&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{\\varepsilon \\to 0} \\mathbb{E} \\left[ f(W) \\frac{\\mathcal{E}(-L^\\varepsilon)_T - 1}{\\varepsilon} \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ f(W) \\int_0^T h'(s)\\mathop{}\\!\\mathrm{d} W_s \\right] .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Malliaving calculus: Bismut formula for $\\nabla _v P_t f(x)$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $(A^{1.1})$ holds, let $v\\in \\mathbb{R}^d$, and put $\\zeta_s := \\sigma_s^\\top (\\sigma_s \\sigma_s^\\top)^{-1}$. Then, for every $f\\in B_b(\\mathbb{R}^d)$ and $t\\in (0,T]$, \\[\\nabla _v P_tf(x) = \\frac{1}{t} \\mathbb{E} \\left[ f(X^x_t) \\int_0^t \\zeta_s(X^x_s) \\nabla _v X^x_s \\mathop{}\\!\\mathrm{d} W_s \\right] .\\] </i> </p> <br><br><p><i><b>Proof.</b>[Sketch of Proof.]<br>&nbsp;&nbsp;&nbsp; By Zvonkin transform, we may(!) assume $b^{(0)}= 0$. We further assume that that $f\\colon \\mathbb{R}^d\\to \\mathbb{R}$ is bounded Lipschitz. Fix $t\\in (0,T]$, and put $h(s) := \\frac{1}{t} \\int_0^s \\zeta_s(X_s^x) \\nabla _v X^x_s \\mathop{}\\!\\mathrm{d} s$. Then we start by showing that \\[&nbsp;&nbsp;&nbsp; v_t := \\nabla_v X^x_t := \\lim_{\\varepsilon\\to 0} \\frac{X^{x+\\varepsilon v}_t -X^x_t}{\\varepsilon}= D_h X^x_t,\\\\\\] (where $X^x_t = f(W) \\colon C([0,T],\\mathbb{R}^m) \\to \\mathbb{R}^d$). Then $f(W+\\varepsilon h) = X^{x,\\varepsilon}_t$, where $\\mathop{}\\!\\mathrm{d} X^{x,\\varepsilon}_s = b_s(.)\\mathop{}\\!\\mathrm{d} s + \\sigma_s(.) (\\mathop{}\\!\\mathrm{d} W_s + \\varepsilon \\mathop{}\\!\\mathrm{d} h_s)$, $X^{x,\\varepsilon}_0 = X^x_0 = x$. Hence \\[w_s := D_h X^x_s = \\lim_{\\varepsilon \\to 0} \\frac{X^{x,\\varepsilon}_s - X^x_s}{\\varepsilon}\\] solves the SDE <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} w_s &amp;= \\left(\\nabla _{w_s} b_s(X^x_s) + \\sigma_s(X^x_s) h'(s)\\right) \\mathop{}\\!\\mathrm{d} s + \\nabla _{w_s}\\sigma_s(X^x_s) \\mathop{}\\!\\mathrm{d} W_s,\\\\&nbsp;&nbsp;&nbsp; w_0 &amp;= 0.\\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Differentiating the SDE of $X$ in direction $v$ gives <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathop{}\\!\\mathrm{d} v_s &amp;= (\\nabla _{v_s} b_s)(X_s^x) \\mathop{}\\!\\mathrm{d} s + (\\nabla_{v_s} \\sigma_s)(X^x_s)\\mathop{}\\!\\mathrm{d} W_s,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v_0 &amp;= 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] and since $\\sigma_s h'(s) = \\frac{1}{t} v_s$, $(\\frac{s}{t} v_s)_{s\\in [0,t]}$ solves the SDE of $w$, so by uniqueness, \\[D_h X^x_t = w_t = \\frac{t}{t} v_t = v_t = \\nabla _v X^x_t.\\] From here we conclude <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\nabla _v P_t f(x)&nbsp;&nbsp;&nbsp; &amp;= \\nabla _v \\mathbb{E} \\left[ f(X^x_t) \\right] = \\mathbb{E} \\left[ \\nabla _v f(X^ x_t) \\right]&nbsp; = \\mathbb{E} \\left[ (\\nabla f)(X^x_t) \\cdot \\nabla _v X^x_t \\right] \\\\&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ (\\nabla f)(X^x_t) \\cdot D_h X^x_t \\right] = \\mathbb{E} \\left[ D_h (f(X^x_t)) \\right] \\\\&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ f(X^x_t) \\int_0^t h'(s)\\mathop{}\\!\\mathrm{d} W_s \\right] .\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Legendre-Fenchel transform $f^\\star$ and $c$-transform $f^c$ and connection. Characterisation of functions of the form $g^\\star$, and of functions with $f^{cc} = f$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f\\colon \\mathbb{R}^d\\to (-\\infty,\\infty]$ is <i>proper</i> (i.e. $f\\not\\equiv \\infty$), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^\\star(y) := \\sup_{x\\in \\mathbb{R}^d} \\left( x\\cdot y - f(x) \\right) \\in (-\\infty,\\infty] ,\\quad y\\in \\mathbb{R}^d.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]&nbsp;&nbsp;&nbsp;</li>  <li>If $f\\colon \\mathbb{R}^d\\to [-\\infty,\\infty)$ is proper, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^c(y) = \\inf_{x\\in \\mathbb{R}^d} \\left( \\frac{1}{2} \\left| x-y \\right| ^2 - f(x) \\right)\\in [-\\infty,\\infty) ,\\quad y\\in \\mathbb{R}^d,\\\\&nbsp;&nbsp;&nbsp; \\] is called <i>$c$-concave</i>.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\colon \\mathbb{R}^d\\to [-\\infty,\\infty)$ is proper, then $\\frac{1}{2} \\left| \\cdot&nbsp; \\right| ^2 - f^c = \\left( \\frac{1}{2} \\left| \\cdot&nbsp; \\right| ^2 - f \\right) ^\\star$, in particular is convex. Furthermore,<br>&nbsp;&nbsp;&nbsp; <ol>  <li>A proper function $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is of the form $f = g^\\star$ iff it is convex and lower semicontinuous, in which case $f^{\\star\\star}=f$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>A proper function $f\\colon \\mathbb{R}^d \\to [-\\infty,\\infty)$ is $c$-concave iff $f^{cc} = f$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First part is straight-forward, for (i) it is clear that $f=g^\\star$ is lsc and convex. For (ii), suppose that $f$ is $c$-concave and continuous, then $\\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - f$ is convex by the above and lower semicontinuous, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^{c c}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - f \\right) ^\\star \\right) ^c\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2} \\left| \\cdot&nbsp; \\right| ^2 - \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - f \\right) ^\\star \\right)&nbsp; \\right)^\\star \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - f \\right) ^{\\star\\star}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= f.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><div></div></div>"
  },
  {
    "front": "Kolmogorov-Riesz-Fr\\'echet theorem (anaolgue of Arzela-Ascoli in $L^p(\\mathbb{R}^d)$).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $p\\in [1,\\infty)$ and $\\Omega\\subset \\mathbb{R}^d$ bounded, and suppose $\\mathcal{F}\\subset L^p(\\Omega)$ satisfies<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\sup_{f\\in \\mathcal{F}} \\left\\|f\\right\\|_{L^p} &lt; \\infty$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_{f\\in \\mathcal{F}, \\left| y \\right| \\le \\delta} \\left\\|\\widetilde{f}^y - \\widetilde{f}\\right\\|_{L^p} \\stackrel{  }{\\longrightarrow} 0$ as $\\delta\\to 0$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Then $\\mathcal{F}$ is relatively compact. Here $\\widetilde{f}$ is the trivial extension of $f$ to $\\mathbb{R}^d$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $\\Omega$ bounded, so $\\mathcal{F}$ is also $L^1$-bounded. Let $\\rho\\in C(\\mathbb{R}^d)$ with $\\rho \\ge 0$, $\\int \\rho = 1$, and $\\operatorname{supp}(\\rho) \\subset B(0,1)$. Then,<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\sup_{f\\in \\mathcal{F}} \\left\\|\\widetilde{f}_\\varepsilon - \\widetilde{f}\\right\\|_p \\stackrel{  }{\\longrightarrow} 0$ as $\\varepsilon \\to 0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\widetilde{\\mathcal{F}}_\\varepsilon \\subset C(\\Omega)$ is relatively compact for all $\\varepsilon &gt; 0$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Indeed, the first is just because \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\widetilde{f}_\\varepsilon - \\widetilde{f}\\right\\|_p^p \\le \\int \\left\\|\\widetilde{f}^y - \\widetilde{f}\\right\\|_p^p \\rho_\\varepsilon(y) \\mathop{}\\!\\mathrm{d} y \\le \\sup_{\\left| y \\right| \\le \\varepsilon} \\left\\|\\widetilde{f}^y - \\widetilde{f}\\right\\|_p^p.&nbsp;&nbsp;&nbsp; \\] For the second, $\\left\\|\\widetilde{f}_\\varepsilon\\right\\|_\\infty \\le \\left\\|\\rho_\\varepsilon\\right\\|_\\infty \\left\\|\\widetilde{f}\\right\\|_1 \\le C_\\varepsilon$, and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left|\\widetilde{f}_\\varepsilon(x) - \\widetilde{f}_\\varepsilon(y)\\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\left| \\rho_\\varepsilon(x-z) - \\rho_\\varepsilon(y-z) \\right| \\left| \\widetilde{f}(z) \\right| \\mathop{}\\!\\mathrm{d} z \\le C_\\varepsilon \\left| x-y \\right| \\left\\|\\widetilde{f}\\right\\|_1,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; then apply Arzela-Ascoli on $\\overline{\\Omega}$ (which is compact). Now let $\\eta &gt; 0$ and take $\\varepsilon&gt;0$ such that $(\\textrm{i})_\\varepsilon \\le \\eta$, and then $A\\subset \\mathcal{F}$ finite such that $\\widetilde{A}_\\varepsilon&nbsp; $ is an $\\eta$-net of $\\widetilde{\\mathcal{F}}_\\varepsilon \\subset C(\\Omega)$. Then for any $f\\in \\mathcal{F}$ there is $g\\in A$ such that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|f - g\\right\\|_{L^p(\\Omega)}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left\\|\\widetilde{f} - \\widetilde{g}\\right\\|_{L^p(\\Omega)} \\le \\left\\|\\widetilde{f} - \\widetilde{f}_\\varepsilon\\right\\|_{L^p(\\mathbb{R}^d)} + \\left\\|\\widetilde{f}_\\varepsilon -\\widetilde{g}_\\varepsilon\\right\\|_{L^\\infty(\\Omega)} + \\left\\|\\widetilde{g}_\\varepsilon - \\widetilde{g}\\right\\|_{L^p(\\mathbb{R}^d)}\\le 3\\eta.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definition weak derivative and conformance with weak convergence in $L^p$.",
    "back": "<div>Let $\\Omega\\subset \\mathbb{R}^d$ open.<br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $f\\in L^1_\\text{loc}(\\Omega)$, and $\\alpha \\in \\mathbb{N}_0^d$ a multi-index. Then $g\\in L^1_\\text{loc}(\\Omega)$ is called weak $\\alpha$-derivative of $f$ if \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda(f \\partial^\\alpha \\varphi) = (-1)^{\\left| \\alpha \\right| } \\lambda (g \\varphi),\\quad \\varphi\\in C_c^\\infty(\\Omega).&nbsp;&nbsp;&nbsp; \\] In that case we write $g = \\partial^\\alpha f$, which is unique $\\lambda$-a.e. if it exists. Furthermore, the definition doesn't change if the test function space is $C^{\\left| \\alpha \\right| }_c(\\Omega)$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\varphi\\in C^{\\left| \\alpha \\right| }_c(\\Omega)$, then $\\varphi_\\varepsilon:= \\rho_\\varepsilon\\star \\varphi \\in C_c^\\infty(\\Omega)$ and $\\varphi_\\varepsilon\\stackrel{ L^\\infty }{\\longrightarrow} \\phi$ and $\\partial^\\alpha (\\varphi_\\varepsilon) = \\rho_\\varepsilon \\star (\\partial^\\alpha \\varphi) = (\\partial^\\alpha \\varphi)_\\varepsilon \\stackrel{ L^\\infty }{\\longrightarrow} \\partial^\\alpha \\varphi$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda(f \\partial^\\alpha \\varphi) \\longleftarrow\\lambda(f (\\partial^\\alpha \\varphi)_\\varepsilon) = \\lambda(f \\partial^\\alpha (\\varphi_\\varepsilon)) = (-1)^{\\left| \\alpha \\right| } \\lambda(g \\varphi_\\varepsilon) \\stackrel{  }{\\longrightarrow} (-1)^{\\left| \\alpha \\right| } \\lambda(g\\varphi)&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\Omega\\subset \\mathbb{R}^d$ open, $(u_n) \\subset L^p(\\Omega)$ converges to $u$ (weakly is enough) in $L^p(\\Omega)$, and $\\partial^\\alpha u_n \\to v$ in $L^p$, then $u$ has weak $\\alpha$-derivative and $\\partial^\\alpha u = v$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda(u \\partial^\\alpha \\varphi) \\longleftarrow \\lambda(u_n \\partial^\\alpha \\varphi) = \\lambda(v_n \\varphi) \\stackrel{  }{\\longrightarrow} \\lambda(v \\varphi),\\\\&nbsp;&nbsp;&nbsp; \\] so $v$ is a weak $\\alpha$-derivative of $u$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of $W^{k,p}$ and $W_0^{k,p}$ (and proof of completeness).",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $k\\in \\mathbb{N}$ and $p \\in [1,\\infty]$. Then the set of $f\\in L^p(\\Omega)$ with weak derivatives up to order $k$ which are all in $L^p(\\Omega)$ is denoted $W^{k,p}(\\Omega)$, which is a Banach space with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|f\\right\\|_{W^{k,p}} := \\left( \\sum_{\\left| \\alpha \\right| \\le k} \\left\\|\\partial^\\alpha f\\right\\|_{L^p(\\Omega)}^p \\right) ^{1 / p}.&nbsp;&nbsp;&nbsp; \\] If $p = 2$, this is called $H^k(\\Omega)$ which is a Hilbert space with \\[&nbsp;&nbsp;&nbsp; \\left&lt;f,g \\right&gt; _{H^k(\\Omega)} := \\sum_{\\left| \\alpha \\right| \\le k} \\left&lt;\\partial^\\alpha f, \\partial^\\alpha g \\right&gt; _{L^2(\\Omega)}.&nbsp;&nbsp;&nbsp; \\]</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; HTS completeness. If $(u_m)$ is a $W^{k,p}$ Cauchy sequence, then $(\\partial^\\alpha u_m)$ is $L^p$-Cauchy for all $\\alpha$, so converges to $v_\\alpha$ in $L^p$. Put $u:= v_0$. Then $u\\in W^{k,p}$ and $v_\\alpha = \\partial^\\alpha u$ for all $\\alpha$, in particular $u_m \\to u$ in $W^{k,p}$.</i> </p> </div><div></div><div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; If $p\\in[1,\\infty)$, then $W_0^{k,p}(\\Omega) := \\overline{C_c^\\infty(\\Omega)}^{\\left\\|\\cdot \\right\\|_{W^{k,p}}}$. This is still a Banach space because it is a closed subspace of $W^{k,p}(\\Omega)$. Write $H^{k}_0(\\Omega) := W^{k,2}_0(\\Omega)$.</p> </div><div></div>"
  },
  {
    "front": "If $g\\in L^1_\\text{loc}$ and $\\lambda(g\\varphi) = 0$ for all $\\varphi\\in C_c^\\infty(\\Omega)$, then $g= 0$ a.e.",
    "back": "<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\Omega\\subset \\mathbb{R}^d$ open and $g\\in L^1_\\text{loc}(\\Omega)$ and $\\lambda(g\\varphi) = 0$ for all $\\varphi\\in C_c^\\infty(\\Omega)$, then $g = 0$ a.e.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $\\Omega$ bounded and $g\\in L^1(\\Omega)$. Let $\\varepsilon &gt; 0$ and take $h\\in C_c(\\Omega)$ with $\\left\\|g - h\\right\\|_{L^1}&lt; \\varepsilon$, and $\\varphi \\in C_c^\\infty(\\Omega)$ with $\\varphi = 1 (-1)$ on $\\left\\{ h\\ge \\varepsilon (\\le -\\varepsilon) \\right\\} $, so $\\left| \\left\\|h\\right\\|_{L^1} - \\lambda(h\\varphi) \\right| \\le \\lambda (h \\left| \\text{sgn}(h)-\\varphi \\right|)\\le \\varepsilon$, so \\[&nbsp;&nbsp;&nbsp; \\left\\|g\\right\\|_{L^1} \\le \\left\\|h\\right\\|_{L^1} + \\varepsilon \\le 2\\varepsilon.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><div></div>"
  },
  {
    "front": "Integration by parts for $W^{k,p}(\\Omega)$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $\\Omega\\subset \\mathbb{R}^d$ be open, $p\\in (1,\\infty]$, $k\\in \\mathbb{N}_0$, $u\\in W^{k,p}(\\Omega)$, $v\\in W_0^{k,p'}(\\Omega)$, and $\\left| \\alpha \\right| \\le k$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_\\Omega u (\\partial^\\alpha v) \\mathop{}\\!\\mathrm{d} x = (-1)^{\\left| \\alpha \\right| } \\int_\\Omega (\\partial^\\alpha u) v \\mathop{}\\!\\mathrm{d} x.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $v_n \\in C_c^\\infty(\\Omega)$ with $v_n \\stackrel{ W^{k,p'} }{\\longrightarrow} v$, so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda(u(\\partial^\\alpha v))\\longleftarrow\\lambda(u (\\partial^\\alpha v_n)) = (-1)^{\\left| \\alpha \\right| } \\lambda((\\partial^\\alpha u) v_n) \\stackrel{  }{\\longrightarrow} (-1)^{\\left| \\alpha \\right| } \\lambda((\\partial^\\alpha u) v).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Weak derivative and convolution (what is $\\partial^\\alpha (\\rho \\star f)$?)",
    "back": "<div>Let $\\rho \\in C^\\infty_c(\\mathbb{R}^d)$.<br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $p\\in [1,\\infty)$ and $k\\in \\mathbb{N}_0$. If $f\\in W^{k,p}(\\mathbb{R}^d)$, then $\\rho \\star f\\in C^\\infty(\\mathbb{R}^d) \\cap W^{k,p}(\\mathbb{R}^d)$ and for every $\\left| \\alpha \\right| \\le k$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\partial^\\alpha (\\rho \\star f) = \\rho \\star (\\partial^\\alpha f).&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We already know that $\\rho\\star f \\in C^\\infty(\\mathbb{R}^d) \\cap L^p(\\mathbb{R}^d)$. Furthermore, for every $\\left| \\alpha \\right| \\le k$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\partial^\\alpha (\\rho\\star f)(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= (\\partial^\\alpha \\rho) \\star f (x)= \\int \\partial^\\alpha_x \\rho(x - y) f(y)\\mathop{}\\!\\mathrm{d} y = (-1)^{\\left| \\alpha \\right| } \\int \\partial^\\alpha_y \\rho(x-y) f(y) \\mathop{}\\!\\mathrm{d} y \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\rho(x-y) \\partial^\\alpha f(y) \\mathop{}\\!\\mathrm{d} y = \\rho\\star (\\partial^\\alpha f )(x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Since $\\partial^\\alpha f \\in L^p(\\mathbb{R}^d)$, we know that $\\partial^\\alpha (\\rho \\star f) = \\rho \\star (\\partial^\\alpha f) \\in L^p$, so $\\rho\\star f \\in W^{k,p}(\\mathbb{R}^d)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Meyers-Serrin (and proof if $\\Omega=\\mathbb{R}^d$).",
    "back": "<div><p><b>Theorem.</b> <i>[Meyers-Serrin]<br>&nbsp;&nbsp;&nbsp; If $\\Omega\\subset \\mathbb{R}^d$ is open and $p\\in [1,\\infty)$ and $k\\in \\mathbb{N}_0$, then $C^\\infty(\\Omega) \\cap W^{k,p}(\\Omega)$ is dense in $W^{k,p}(\\Omega)$ (w.r.t. $\\left\\|\\cdot \\right\\|_{W^{k,p}}$).</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\Omega = \\mathbb{R}^d$, take $\\rho \\ge 0$, $\\int \\rho \\mathop{}\\!\\mathrm{d} x = 1$, $\\rho\\in C^\\infty_c(\\mathbb{R}^d)$. Then if $f\\in W^{k,p}(\\mathbb{R}^d)$, $f_\\varepsilon \\in C^\\infty(\\mathbb{R}^d) \\cap W^{k,p}(\\mathbb{R}^d)$, and for every $\\left| \\alpha \\right| \\le k$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\partial^\\alpha f_\\varepsilon = (\\partial^\\alpha f)_\\varepsilon \\stackrel{ L^p }{\\longrightarrow} \\partial^\\alpha f,\\\\&nbsp;&nbsp;&nbsp; \\] that is $f_\\varepsilon \\stackrel{ W^{k,p} }{\\longrightarrow} f$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Lipschitz and $C^m$ domain and segment condition (and their relation), and condition under which $C^\\infty_c(\\mathbb{R}^d)\\!\\!\\restriction_{\\Omega}$ is dense in $W^{k,p}(\\Omega)$. In particular, do we have $W_0^{k,p}(\\mathbb{R}^d) = W^{k,p}(\\mathbb{R}^d)$?",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $\\Omega\\subset \\mathbb{R}^d$ be open.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\Omega$ is said to be <i>Lipschitz</i> ($C^m$) if for every $x_0\\in \\partial\\Omega$ there is $r &gt; 0$ and a Lipschitz ($C^m$) function $\\gamma$ such that (potentially after relabelling axes), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Omega \\cap B(x_0,r) = \\left\\{ x\\colon x_d &gt; \\gamma(x_1, \\ldots ,x_{d-1}) \\right\\} \\cap B(x_0,r).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>$\\Omega$ is said to satisfy the <i>segment condition</i> if for every $x_0\\in \\partial\\Omega$ there is $r &gt; 0$ and $y\\in \\mathbb{R}^d\\setminus\\{0\\}$ such that for every $x\\in \\overline{\\Omega}\\cap B(x_0,r)$, the line segment \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (z,z+y) := z + (0,1)\\cdot y \\subset \\Omega.&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Clearly, (i) implies (ii).</p> <br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $p\\in [1,\\infty)$, $k\\in \\mathbb{N}_0$, and $\\Omega\\subset \\mathbb{R}^d$ is open and satisfies the segment condition, then $C_c^\\infty(\\mathbb{R}^d)\\!\\!\\restriction_{\\Omega} $ is dense in $W^{k,p}(\\Omega)$. In particular, $W_0^{k,p}(\\mathbb{R}^d) = W^{k,p}(\\mathbb{R}^d)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Stein's extension theorem (for Sobolev functions).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $\\Omega\\subset \\mathbb{R}^d$ be bounded, open, and with Lipschitz boundary, $p\\in [1,\\infty)$. Then there is a map $E\\colon \\left\\{ f\\colon \\Omega\\to \\mathbb{R} \\right\\} / \\sim \\,\\,\\to \\left\\{ f\\colon \\mathbb{R}^d\\to \\mathbb{R} \\right\\} / \\sim $ (where $\\sim$ denotes equality a.e.) such that $Eu = u$ a.e. on $\\Omega$ and for $u\\in W^{k,p}(\\Omega)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|Eu\\right\\|_{W^{k,p}(\\mathbb{R}^d)}\\le C_{k,p,\\Omega} \\left\\|u\\right\\|_{W^{k,p}(\\Omega)}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Trace operator for Sobolev spaces and characterisation of $W^{1,p}_0(\\Omega)$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $p\\in [1,\\infty)$, $k\\in \\mathbb{N}$, $\\Omega \\subset \\mathbb{R}^d$ open, bounded, with $C^{k-1}_\\text{Lip}$ boundary. Then there exists a unique linear bounded operator $T\\colon W^{k,p}(\\Omega) \\to L^p(\\partial \\Omega)$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; T u = u\\!\\!\\restriction_{\\partial \\Omega} \\text{ for all $u\\in C(\\overline{\\Omega}) \\cap W^{k,p}(\\Omega)$}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Uniqueness is clear because $Tu$ is fixed for all $u\\in C^\\infty_c(\\mathbb{R}^d)\\!\\!\\restriction_{\\Omega} $, which is dense. For existence it suffices (by denseness) to define $Tu = u\\!\\!\\restriction_{\\partial \\Omega}$ on the same set and show that it is bounded there. For an idea we consider the case where $d = 2$ and $(-2,2)\\times \\left\\{ 0 \\right\\} $ is part of the boundary, and $(-2,2) \\times (0,1) \\subset \\Omega$, and we only consider the trace on $(-1,1)\\times \\left\\{ 0 \\right\\} $. Let $u \\in C_c^\\infty(\\mathbb{R}^d)$, and $\\rho \\in C^\\infty_c((-2,2)\\times (-1,1))$ with $\\rho \\equiv 1$ on $(-1,1)\\times \\left\\{ 0 \\right\\} $, so with $B:= (-2,2)\\times (0,1)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{L^p((-1,1)\\times \\left\\{ 0 \\right\\}) }^p &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_{-1}^1 \\left| u(x,0) \\right| ^p \\mathop{}\\!\\mathrm{d} x \\le \\int_{-2}^2 \\rho(x,0) \\left| u(x,0) \\right| ^p \\mathop{}\\!\\mathrm{d} x \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;=- \\int_{-2}^2 \\int_0^1 \\partial_y\\Big( \\rho(x,y) \\left| u(x,y) \\right| ^p \\Big) \\mathop{}\\!\\mathrm{d} y \\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le C \\int_B \\left( \\left| u \\right| ^p + \\left| \\nabla u \\right| \\left| u \\right| ^{p-1} \\right) \\mathop{}\\!\\mathrm{d} x \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le C' \\int_B \\left( \\left| u \\right| ^p + \\left| \\nabla u \\right| ^p \\right) \\mathop{}\\!\\mathrm{d} x\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le C' \\left\\|u\\right\\|_{W^{1,p}(\\Omega)}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used Young's inequality.</i> </p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $p\\in [1,\\infty)$, $\\Omega\\subset \\mathbb{R}^d$ open, bounded Lipschitz, then for any $u\\in W^{1,p}(\\Omega)$ we have $Tu = 0$ iff $u\\in W_0^{1,p}(\\Omega)$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; One direction is easy: If $\\varphi_n \\stackrel{ W^{1,p} }{\\longrightarrow} u$, then $Tu = \\lim_{n\\to \\infty} T \\varphi_n = 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition (strong and weak solution to) DDSDE and definition of well-posedness.",
    "back": "<div>Let $b\\colon [0,T]\\times \\mathbb{R}^d\\times \\mathcal{P} \\to \\mathbb{R}^d$ and $\\sigma\\colon [0,T]\\times \\mathbb{R} ^d \\times \\mathcal{P} \\to \\mathbb{R} ^{d\\times m}$ be measurable, where $\\mathcal{P} = \\mathcal{M}_1(\\mathbb{R}^d)$. Given an $m$-dim. BM $(W_t)$ on a complete filtered probability space, a continuous adapted process $(X_t)_{t\\in [0,T]}$ is called a <i>strong solution</i> to (DDSDE) if \\[&nbsp;&nbsp;&nbsp; X_t = X_0 + \\int_0^t b_s(X_s,\\mathcal{L}_{X_s}) \\mathop{}\\!\\mathrm{d} s + \\int_0^t \\sigma_s(X_s, \\mathcal{L}_{X_s})\\mathop{}\\!\\mathrm{d} W_s.\\] A <i>weak solution</i> is a tuple $(\\widetilde{X},\\widetilde{W})$ that solves this on some filtered probability space. Let $\\widehat{\\mathcal{P}}\\subset \\mathcal{P}$. We say that (DDSDE) is<br><ol>  <li><i>strong $\\widehat{\\mathcal{P}}$-well posed</i> if for every $\\mathcal{F}_0$-measurable $X_0$ with $\\mathcal{L}_{X_0}\\in \\widehat{\\mathcal{P}}$, there is a unique (up to indistinguishability) strong solution $(X_t)_{t\\in [0,T]}$ starting at $X_0$ with $\\mathcal{L}_{X_t} \\in \\widehat{\\mathcal{P}}$ for all $t\\in [0,T]$,&nbsp;&nbsp;&nbsp;</li>  <li><i>weak $\\widehat{\\mathcal{P}}$-well posed</i> if for every $\\nu\\in \\mathcal{\\widehat{P}}$, there is a unique (in distribution) weak solution $(\\widetilde{X},\\widetilde{W})$ with $\\mathcal{L}_{\\widetilde{X}_0} = \\nu$ and $\\mathcal{L}_{\\widetilde{X}_t}\\in \\mathcal{\\widehat{P}}$ for all $t\\in [0,T]$.&nbsp;&nbsp;&nbsp;</li>  <li><i>well-posed</i> if it is both strong and weak well-posed.</li></ol></div><div></div>"
  },
  {
    "front": "Modified Yamada-Watanabe for DDSDE.",
    "back": "<div>Let $\\widehat{\\mathcal{P}}\\subset \\mathcal{P} := \\mathcal{M}_1(\\mathbb{R}^d)$<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Suppose that for every $\\mu \\in C([0,T],\\widehat{\\mathcal{P}})$, the classical SDE \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{(SDE)}_\\mu\\qquad \\mathop{}\\!\\mathrm{d} X_t = b_t(X_t,\\mu_t) \\mathop{}\\!\\mathrm{d} t + \\sigma_t(X_t,\\mu_t) \\mathop{}\\!\\mathrm{d} W_t\\\\&nbsp;&nbsp;&nbsp; \\] satisfies pathwise uniqueness. Then YW holds for DDSDE, that is, pathwise uniqueness and weak existence imply strong existence and uniqueness in law (thus strong and weak well-posedness).</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We apply YW to the classical SDE above: Take a weak solution $(\\widetilde{X},\\widetilde{W})$ of (DDSDE) on some probability space, and put $\\mu_t := \\mathcal{L}_{\\widetilde{X}_t}$. Then $(\\widetilde{X},\\widetilde{W})$ is a weak solution of $(\\text{SDE})_\\mu$, so by classical YW, there is a strong solution $(X_t)$ to $(\\text{SDE})_\\mu$ and $(\\text{SDE})_\\mu$ has uniqueness in law, so $\\mathcal{L}_{X_t}= \\mathcal{L}_{\\widetilde{X}_t}=\\mu_t$, so $X$ is a strong solution to DDSDE. Weak well-posedness similarly.</i> </p> </div><div></div>"
  },
  {
    "front": "Correspondence between weak solutions to (DDSDE) and fixed points of a map. How can one infer weak/strong well-posedness of (DDSDE)?",
    "back": "<div>Let $\\gamma \\in \\widehat{\\mathcal{P}}$ and $C_\\gamma := \\left\\{ \\mu \\in C([0,T],\\widehat{\\mathcal{P}})\\colon \\mu_0 = \\gamma \\right\\} $.<br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Suppose that for any $\\mu\\in C_\\gamma$, $(\\text{SDE})_\\mu$ has a unique weak solution $X^\\mu$ with $\\mathcal{L}_{X^\\mu_0} = \\gamma$, and define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Phi\\colon C_\\gamma \\to C_\\gamma; \\, (\\mu_t) \\mapsto (\\mathcal{L}_{X_t^\\mu}).&nbsp;&nbsp;&nbsp; \\] Then fixed points of $\\Phi$ are exactly laws of weak solutions to (DDSDE). In particular, if $\\Phi$ has a unique fixed point, (DDSDE) is weak well-posed. If additionally $\\text{(SDE)}_\\mu$ is strong well-posed, then so is (DDSDE).</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $(X_t)$ is a weak solution to (DDSDE) and $\\mu := \\mathcal{L}_X$, then $X$ is a weak solution to $(\\text{SDE})_\\mu$ and $\\Phi \\mu = \\mathcal{L}_X = \\mu$, so the law of $X$ is a fixed point. On the other hand, if $\\mu$ is a fixed point and $X$ the corresponding weak solution of $(\\text{SDE})_\\mu$, then $X$ is a weak solution to (DDSDE).<br><br>&nbsp;&nbsp;&nbsp; Suppose that $(\\text{SDE})_\\mu$ is also strong-well posed. Then if $\\mu$ is a fixed point and $X$ a strong solution to $(\\text{SDE})_\\mu$, $X$ is a strong solution to (DDSDE), and if $Y$ is another one, then by weak well-posedness it has the same law as $X$ so is also another strong solution to $(\\text{SDE})_\\mu$, so $X = Y$.</i> </p> </div><div></div>"
  },
  {
    "front": "Well-posedness of (DDSDE) in the case where $\\sigma_t(x,\\mu) = \\sigma_t(x)$ and $b$ is Lipschitz in $\\mu$ w.r.t. total variation distance.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Suppose that $\\sigma_t(x,\\mu) = \\sigma_t(x)$ and $(\\sigma_t, b(\\cdot ,\\delta_0))$ satisfy $(A^{1.1})$ and there is $K &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| b_t(x,\\mu) - b_t(x,\\nu) \\right| \\le K \\left\\|\\mu - \\nu\\right\\|_{TV}.&nbsp;&nbsp;&nbsp; \\] Then (DDSDE) is well-posed for $\\widehat{\\mathcal{P}}= \\mathcal{P}$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By what we already know about SDEs, $(\\text{SDE})_\\mu$ is well-posed for $\\mu\\in C([0,T],\\mathcal{P})$, so it suffices to show that $\\Phi$ is a contraction. Let $\\gamma\\in \\mathcal{P}$, $\\mu,\\nu\\in C_{\\gamma}$, and $X$ a solution of $(\\text{SDE})_{\\mu}$. If we put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\xi_s := \\sigma_s^\\top \\left( \\sigma_s \\sigma_s^\\top \\right)^{-1} \\left( b_s(X_s,\\mu_s) - b_s(X_s,\\nu_s) \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\] then $R := \\mathcal{E}(\\xi \\bullet W)$ is a martingale on $[0,T]$ and if $\\mathop{}\\!\\mathrm{d} \\mathbb{Q}:= R \\mathop{}\\!\\mathrm{d} \\mathbb{P}$, then $\\mathcal{L}_{X | \\mathbb{Q}} = \\Phi \\nu$. Hence,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\Phi_t \\mu - \\Phi_t \\nu\\right\\|_{TV}^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sup_{\\left| f \\right| \\le 1} \\left( \\mathbb{E} \\left[ f(X_t) \\right] - \\mathbb{E} \\left[ R_t f(X_t) \\right]&nbsp; \\right) ^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{E} \\left[ \\left| R_t - 1 \\right|&nbsp; \\right] ^2 \\le 2 \\mathbb{E} \\left[ R_t \\log R_t \\right] \\quad (\\text{Pinsker})\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= 2 \\mathbb{Q}(\\log R_t) = \\mathbb{Q}\\left( \\int_0^t \\left| \\xi_s \\right| ^2 \\mathop{}\\!\\mathrm{d} s \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le K^2 \\int_0^t \\left\\|\\mu_s - \\nu_s\\right\\|_{TV}^2\\mathop{}\\!\\mathrm{d} s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; From here it is straight-forward to check that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\rho_\\lambda(\\Phi \\mu,\\Phi \\nu) := \\sup_{t\\in [0,T]} \\mathrm{e}^{-\\lambda t} \\left\\|\\Phi_t \\mu - \\Phi_t \\nu\\right\\|_{TV} \\le \\frac{K}{\\sqrt{2\\lambda} } \\rho_\\lambda(\\mu,\\nu),\\\\&nbsp;&nbsp;&nbsp; \\] so $\\Phi$ is a contraction for sufficiently large $\\lambda$.</i> </p> </div><div></div>"
  },
  {
    "front": "Well-posedness of (DDSDE) (monotonous case).",
    "back": "<div>\\textbf{Assumption $(A^{2.1})$}. Let $k\\in [1,\\infty)$.<br><ol>  <li>$(\\text{SDE})_\\mu$ is well-posed for any $\\mu\\in C([0,T],\\mathcal{P}_k)$,&nbsp;&nbsp;&nbsp;</li>  <li>There is $K\\in L^1([0,T],(0,\\infty))$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\sigma_t(x,\\mu) - \\sigma_t(y,\\nu)\\right\\|^2 + \\left&lt;b_t(x,\\mu) - b_t(y,\\nu), x-y \\right&gt; _+ \\le K(t) \\left( \\left| x-y \\right| ^2 + W_k(\\mu,\\nu)^2 \\right) ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] for all $t\\in [0,T]$, $x,y\\in \\mathbb{R}^d$, $\\mu,\\nu\\in \\mathcal{P}_k$.</li></ol><br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $(A^{2.1})$ holds for some $k\\ge 1$, then (DDSDE) is $\\mathcal{P}_k$-well-posed and for every $p\\ge k$ there is $c &gt; 0$ such that for any two solutions $X,Y$ to (DDSDE),<br>&nbsp;&nbsp;&nbsp; <ol>  <li>\\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\sup_{t\\in [0,T]} \\left| X_t \\right| ^p \\,\\middle\\vert\\, \\mathcal{F}_0 \\right] \\le c \\left( 1 + \\left| X_0 \\right| ^p + \\mathbb{E} \\left[ \\left| X_0 \\right| ^k \\right]^{p / k}&nbsp; \\right) ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>\\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\sup_{t\\in [0,T]} \\left| X_t - Y_t \\right| ^p \\,\\middle\\vert\\, \\mathcal{F}_0\\right] \\le c \\left( W_k(\\mathcal{L}_{X_0},\\mathcal{L}_{Y_0}) + \\left| X_0 -Y_0 \\right| \\right)^p\\\\&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; There is a $c &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_k(P_t^\\star \\mu, P_t^\\star \\nu) \\le c W_k(\\mu,\\nu),\\quad t\\in [0,T], \\mu,\\nu\\in \\mathcal{P}_k.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Fixed point argument and straight-forward Ito, BDG, and Gronwall.</i> </p> </div><div></div>"
  },
  {
    "front": "Optimal transport with $c(x,y) = \\frac{1}{2}\\left| x-y \\right| ^2$: If $(\\varphi,\\psi)\\in \\Phi_c$, what do you know about $(\\varphi^{c c},\\varphi^c)$, and how can you replace $(\\varphi,\\psi)$ by a nice pair $(\\widetilde{\\varphi},\\widetilde{\\psi}) \\in \\Phi_c$ that is at least as good?",
    "back": "<div>Suppose $c(x,y) = \\frac{1}{2}\\left| x-y \\right| ^2$ and $\\mu,\\nu\\in \\mathcal{P}_2(\\mathbb{R}^d)$.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Suppose that $(\\varphi,\\psi)\\in \\Phi_c$ satisfy $\\varphi + \\psi \\le c$ everywhere.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\varphi^{cc} \\ge \\varphi$ and $\\varphi^c \\ge \\psi$ and $\\varphi^{c c} + \\varphi^{c} \\le c$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>There is $(\\widetilde{\\varphi},\\widetilde{\\psi})\\in \\Phi_c$ with $J[\\widetilde{\\varphi},\\widetilde{\\psi}] \\ge J[\\varphi,\\psi]$ and $\\widetilde{\\varphi}\\vee \\widetilde{\\psi}\\le \\left| \\cdot&nbsp; \\right| ^2$. In fact,&nbsp; $(\\widetilde{\\varphi}, \\widetilde{\\psi}) = (\\eta, \\eta^{c})$ for a $c$-concave function $\\eta$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>We have $\\varphi^c(x) = \\inf_y \\left( c(x,y) - \\phi(y) \\right) \\ge \\psi(x)$ and&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi^{c c}(x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\inf_y \\left( c(x,y) - \\varphi^{c}(y) \\right) = \\inf_y \\sup_z \\left( c(x,y) - c(y,z) + \\varphi(z) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\inf_y \\varphi(x) = \\varphi(x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp; Furthermore, &nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi^{c c}(x) + \\varphi^{c}(y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\inf_z \\left( c(x,z) - \\varphi^{c}(z) + \\varphi^{c}(y) \\right) \\le c(x,y).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp; Since $\\varphi^{c c}$ and $\\varphi^{c}$ are already bounded below by integrable functions, upper bounds suffice to get $(\\varphi^{c c}, \\varphi^{c}) \\in L^1(\\mu) \\times L^1(\\nu)$ and thus $ \\in \\Phi_c$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a&nbsp; := \\inf_y \\left( \\left| y \\right| ^2 - \\varphi ^c(y) \\right) .&nbsp;&nbsp;&nbsp; \\] We show later that $a&nbsp; \\in \\mathbb{R}$, but assuming this $(\\widetilde{\\varphi},\\widetilde{\\psi}) :=&nbsp; (\\varphi ^{c c} - a , \\varphi ^{c} + a )$, so the claimed bounds hold by definition of $a $. Indeed, &nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\varphi ^{c c}(x) - a&nbsp; &nbsp;&nbsp;&nbsp; &amp;= \\sup_y \\inf_z \\left( \\frac{1}{2}\\left| x-z \\right| ^2 - \\varphi ^c(z) - \\left| y \\right|^2 + \\varphi ^c(y) \\right) \\le \\sup_y \\left( \\frac{1}{2}\\left| x-y \\right| ^2 - \\left| y \\right| ^2 \\right) \\\\&nbsp;&nbsp;&nbsp; &amp;\\le \\left| x \\right| ^2,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] and the other one is obvious. Thus $(\\widetilde{\\varphi},\\widetilde{\\psi})\\in \\Phi_c$ and $J[\\widetilde{\\varphi},\\widetilde{\\psi}] = J[\\varphi^{c c},\\varphi^{c}] \\ge J[\\varphi,\\psi]$ by the previous lemma. Now note that $(\\widetilde{\\varphi},\\widetilde{\\psi}) = ((\\varphi - a)^{cc}, (\\varphi - a)^c)$, so put $\\eta := (\\varphi - a)^{c c}$.&nbsp;&nbsp;&nbsp; It remains to show $a \\in \\mathbb{R}$. We have $\\varphi ^c(y) = \\inf_z (c(y,z) - \\varphi (z)) \\ge \\psi (y)$, so $a&nbsp; \\le \\left| y_0 \\right| ^2 - \\psi (y_0) &lt; \\infty$. Furthermore,&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| y \\right| ^2 - \\varphi ^{c}(y) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sup_z \\left( \\left| y \\right| ^2 -&nbsp; \\frac{1}{2}\\left| y-z \\right| ^2 + \\varphi (z) \\right) \\ge \\sup_z \\left( \\varphi (z) - \\left| z \\right| ^2 \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\ge \\varphi (x_0) - \\left| x_0 \\right| ^2,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp; so $a&nbsp; \\ge \\varphi (x_0) - \\left| x_0 \\right| ^2 &gt; -\\infty$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Existence of (nice) maximizer of dual optimal transport problem $J_\\star = \\sup_{(\\varphi,\\psi)\\in \\Phi_c} J[\\varphi,\\psi]$ in the case $c(x,y) = \\frac{1}{2}\\left| x-y \\right| ^2$.",
    "back": "<div></div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $c(x,y) = \\frac{1}{2}\\left| x-y \\right| ^2$ and $\\mu,\\nu\\in \\mathcal{P}_2(\\mathbb{R}^d)$, then there is $(\\varphi_0,\\psi_0) \\in \\Phi_c$ such that $J_\\star = J[\\varphi_0,\\psi_0]$ and $\\varphi_0\\vee \\psi_0\\le \\left| \\cdot&nbsp; \\right| ^2$. In fact it can be chosen so that $(\\varphi_0,\\psi_0) = (\\eta,\\eta^{c})$ for a $c$-concave function $\\eta$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(\\varphi_k,\\psi_k)\\in \\Phi_c$ be a sequence for which the inequality holds everywhere, $J[\\varphi_k,\\psi_k]\\uparrow J_\\star$, and $\\varphi_k\\vee \\psi_k\\le \\left| \\cdot&nbsp; \\right| ^2$. For $l\\in \\mathbb{N}$ define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi_k^{(l)}:= (-l)\\vee \\left( \\varphi_k - \\left| \\cdot&nbsp; \\right| ^2 \\right) + \\left| \\cdot&nbsp; \\right| ^2 \\downarrow \\varphi_k.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Then, $\\varphi_k^{(l)} - \\left| \\cdot&nbsp; \\right| ^2$ is in $[-l,0]$ for all $k\\in \\mathbb{N}$ so $L^2(\\mu)$-bounded so converges weakly (along a subsequence) to some $\\varphi^{(l)} - \\left| \\cdot&nbsp; \\right| ^2 \\in L^2(\\mu) \\subset L^1(\\mu)$, so $\\varphi^{(l)}\\in L^1(\\mu)$ and we may choose a common subsequence for all $l$ and also arrange for $\\left| \\cdot&nbsp; \\right| ^2 \\ge \\varphi^{(l)}\\downarrow$ pointwise everywhere (both holds $\\mu$-a.e. and we can modify on a null set), call the pointwise limit $\\varphi_0$. We do the exact same for $\\psi$. Use $1$ as a test function in the weak convergence to get $\\mu(\\varphi^{(l)}_k) \\to \\mu(\\varphi^{(l)})$. Then $\\mu(\\varphi_0) \\in [-\\infty,\\infty)$ same for $\\psi_0$ and by MCT and what we just said \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; J[\\varphi_0,\\psi_0] = \\lim_{l\\to \\infty} J[\\varphi^{(l)}, \\psi^{(l)}] = \\lim_{l\\to \\infty} \\lim_{k\\to \\infty} J[\\varphi_k^{(l)}, \\varphi_k^{(l)}] \\ge \\lim_{k\\to \\infty} J[\\varphi_k,\\psi_k] = J_\\star.&nbsp;&nbsp;&nbsp; \\] Hence $\\varphi_0\\in L^1(\\mu)$, and it is rather easy to extract from the definition of $\\varphi_k^{(l)}$ that $\\varphi_0+ \\psi_0\\le c$, so $(\\varphi_0,\\psi_0)\\in \\Phi_c$ and achieves the supremum. Using our lemma we can make this into a maximizer of the form $(\\eta,\\eta^c)$ with $\\eta \\vee \\eta^{c} \\le \\left| \\cdot&nbsp; \\right| ^2$.</i> </p> <br><div></div>"
  },
  {
    "front": "For quadratic cost and diffusive $\\mu$, the unique minimizer of the Kantorovich optimal transport problem is of the form of the Monge problem.",
    "back": "<div>Suppose that $c(x,y) = \\frac{1}{2}\\left| x-y \\right| ^2$ and $\\mu,\\nu\\in \\mathcal{P}_2(\\mathbb{R}^d)$.<br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu$ is diffusive, then the unique optimal transferrence plan $\\Pi_0\\in \\Gamma(\\mu,\\nu)$ is given by $\\Pi_0 = \\mu \\circ ( \\boldsymbol{1} \\times T)^{-1}$, where $T = \\nabla \\Psi$ for a convex $\\Psi$ (in fact $T(x) = x - \\nabla \\varphi_0(x)$ for a $c$-concave Kantorovich potential $\\varphi_0$).</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\Pi_0\\in \\Gamma(\\mu,\\nu)$ be an optimal transferrence plan and $(\\varphi_0, \\varphi_0^c) \\in \\Phi_c$ be a maximizer for the dual problem. Then $J_\\star = I_\\star$ implies \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 = \\int\\limits_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\left( c(x,y) - \\varphi_0(x) - \\varphi_0^c(y) \\right) \\Pi_0(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] and the integrand is non-negative everywhere, so $\\varphi_0(x) + \\varphi_0^c(y) = c(x,y)$ holds $\\Pi_0$-a.e., and in fact everywhere on the support of $\\Pi_0$ (non-trivial!). This implies that for $(x_0,y_0) \\in \\operatorname{supp}(\\Pi_0)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi_0^c(y_0) = \\inf_x \\left( c(x,y_0) - \\varphi_0(x) \\right) = c(x_0,y_0) - \\varphi_0(x_0),\\\\&nbsp;&nbsp;&nbsp; \\] hence if $\\varphi_0$ is differentiable at $x_0$, then $0 = \\nabla (c(x,y_0) - \\varphi_0(x))$ at $x = x_0$, i.e. $y_0 = T(x_0)$. Since $\\varphi_0$ is $c$-concave, we have $\\frac{1}{2} \\left| \\cdot&nbsp; \\right| ^2 - \\varphi_0 = \\left( \\frac{1}{2}\\left| \\cdot&nbsp; \\right| ^2 - \\eta \\right) ^\\star$ which is convex, so $\\varphi_0$ is locally Lipschitz and $\\mu$-a.e. differentiable (on the interior of the set where it is finite), so if we disintegrate $\\Pi_0 = \\mu \\otimes K$, then $K(x,\\cdot ) = \\delta_{T(x)}$ for $\\mu$-a.e. $x$, so $\\Pi_0$ is as claimed.<br><br>&nbsp;&nbsp;&nbsp; For the final claim note that $\\Psi = \\frac{1}{2} \\left| \\cdot&nbsp; \\right| ^2 - \\varphi_0$ is convex because $\\varphi_0$ is $c$-concave.</i> </p> </div><div></div>"
  },
  {
    "front": "Connection between Wasserstein distances $d_p$ and $d_q$ for $1\\le q &lt; p &lt; \\infty$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; For $1 \\le q &lt; p &lt; \\infty$ and $\\mu,\\nu\\in \\mathcal{P}_p(E)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_q(\\mu,\\nu) \\le d_p(\\mu,\\nu) \\le \\left( \\textrm{diam} (\\operatorname{supp}(\\mu) \\cup&nbsp; \\operatorname{supp}(\\nu)) \\right) ^{\\frac{p-q}{p}} d_q(\\mu,\\nu)^{q / p}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The first is just Hölder. Now for any $\\gamma\\in \\Gamma(\\mu,\\nu)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int \\rho(x,y)^p \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)&nbsp; \\le c^{p-q} \\int \\rho(x,y)^q \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": " Convergence in Wasserstein distance on bounded subsets of a Polish space is equivalent to weak convergence.",
    "back": "<div>Suppose $E$ is Polish with metric $\\rho$.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $p\\ge 1$, $\\mu_n ,\\mu \\in \\mathcal{P}_p(E)$ are supported on a common bounded set. Then $\\mu_n \\to \\mu$ weakly iff $d_p(\\mu_n,\\mu) \\to 0$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $d_p(\\mu_n,\\mu) \\to 0$, then for any Lipschitz function&nbsp; $f\\colon E\\to \\mathbb{R}$ and any coupling $\\gamma_n \\in \\Gamma(\\mu_n,\\mu)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\mu_n(f) - \\mu(f) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left| \\int f(x) \\Pi_n(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y) - \\int f(y) \\Pi_n(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y) \\right| \\le \\int \\left| f(x) - f(y) \\right| \\Pi_n(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le L \\Pi_n(d),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so upper bounded by $L d_1(\\mu_n,\\mu) \\le L d_p(\\mu_n,\\mu) \\to 0$.<br><br>&nbsp;&nbsp;&nbsp; For the other direction, suppose that $\\mu_n \\to \\mu $ weakly and let $\\varepsilon &gt; 0$. By the dual representation of $d_1$, we find $\\varphi_n\\in \\text{Lip}_1(E)$ such that $d_1(\\mu_n,\\mu) \\le \\left| \\mu_n(\\varphi_n) - \\mu(\\varphi_n) \\right| + \\varepsilon$. By replacing $\\varphi_n \\to \\varphi_n - \\varphi_n(x_0)$ and by our assumption we can assume that $\\left\\{ \\varphi_n \\right\\} $ is uniformly bounded, say by $M &gt; 0$ (in fact $M$ is the radius of the smallest common support ball). Let $K\\subset E$ be compact such that all of $\\mu_n$ and $\\mu$ put all but $\\varepsilon$ of their mass on $K$, then $\\left\\{ \\varphi_n \\!\\!\\restriction_{K} \\colon n\\in \\mathbb{N} \\right\\} \\subset C(K,\\mathbb{R})$ is relatively compact by Arzela-Ascoli and so every subsequence has a subsequence (call it the same) that converges uniformly on $K$ to some $\\varphi\\in \\text{Lip}_1(K)$, which can be extended to $\\varphi \\in \\text{Lip}_1(E)$ bounded. Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{n\\to \\infty}d_1(\\mu_n,\\mu) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\varepsilon + \\varlimsup_{n\\to \\infty} \\left| \\mu_n(\\varphi_n) - \\mu(\\varphi_n) \\right| \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\varepsilon + \\varlimsup_{n\\to \\infty}\\big( \\left| \\mu_n(\\varphi_n - \\varphi) \\right| + \\underbrace{\\left| \\mu_n(\\varphi) - \\mu(\\varphi) \\right|}_{\\to 0} + \\underbrace{\\left| \\mu(\\varphi - \\varphi_n) \\right|}_{\\le M\\varepsilon + \\mu(K) \\left\\|\\varphi - \\varphi_n\\right\\|_K} \\big)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\varepsilon + 2M\\varepsilon.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Convergence in Wasserstein distance on Polish space is equivalent to weak convergence plus convergence of $p$'th moments.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\mu_n,\\mu\\in \\mathcal{P}_p(E)$, then for every $x_0\\in E$, $d_p(\\mu_n,\\mu) \\to 0$ iff $\\mu_n \\to \\mu$ weakly and $\\mu_n(\\rho(\\cdot ,x_0)^p) \\to \\mu(\\rho(\\cdot ,x_0)^p)$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $d_p$-convergence implies $d_1$-convergence implies weak convergence (by the obvious direction of the dual representation), and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\mu_n(\\rho(x_0,\\cdot)^p) - \\mu(\\rho(x_0,\\cdot )^p) \\right| = \\left| d_p(\\mu_n,\\delta_{x_0}) - d_p(\\mu,\\delta_{x_0}) \\right| \\le d_p(\\mu_n,\\mu) \\to 0.&nbsp;&nbsp;&nbsp; \\]<br><br>&nbsp;&nbsp;&nbsp; For the other direction, define by $P_R\\colon E\\to B(x_0,R)$ to be the identity on $B(x_0,R)$ and constant $x_0$ everywhere else. Since $\\mu \\circ \\rho(x_0,\\cdot )^{-1}$ can only have countably many point masses, $P_R$ is $\\mu$-a.e. continuous for all but countably many $R &gt; 0$, so $P_R(\\mu_n) \\to P_R(\\mu)$ as $n\\to \\infty$ weakly for all such $R &gt; 0$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varlimsup_{n\\to \\infty} d_p(\\mu_n,\\mu) \\le \\varliminf_{n\\to \\infty} \\big( d_p(\\mu_n,P_R(\\mu_n) ) + \\underbrace{d_p(P_R(\\mu_n), P_R(\\mu))}_{\\to 0} + d_p(P_R(\\mu), \\mu)\\big)&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for every $R &gt; 0$. Finally, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_p(\\mu_n, P_R(\\mu_n))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\rho(x,P_R(x))^p \\mu_n(\\mathop{}\\!\\mathrm{d} x) = \\int\\limits_{\\left\\{x\\colon \\rho(x_0,x) \\ge R \\right\\}} \\rho(x_0,x)^p \\mu_n(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; goes to zero as $R\\to \\infty$ uniformly in $n$(!).</i> </p> </div><div></div>"
  },
  {
    "front": "Additivity of Wasserstein distance on $\\mathbb{R}^d$ w.r.t. convolution.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $p\\ge 1$ and $f_1,g_1,f_2,g_2\\in \\mathcal{P}_p(\\mathbb{R}^d)$, then<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$d_p(f_1\\star f_2, g_1\\star g_2) \\le d_p(f_1, g_1) + d_p(f_2,g_2)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\mathbb{E} \\left[ f_i \\right] = \\mathbb{E} \\left[ g_i \\right] $, then $d_2(f_1\\star f_2, g_1\\star g_2)^2 \\le d_2(f_1,g_1)^2 + d_2(f_2,g_2)^2$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(X_1,Y_1)$ and $(X_2,Y_2)$ be optimal couplings of $(f_1,g_1)$ and $(f_2,g_2)$, independent on the same probability space. Then for (i), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{LHS} \\le \\left\\|(X_1+X_2) - (Y_1+Y_2)\\right\\|_{L^p} \\le \\left\\|X_1-Y_1\\right\\|_{L^p} + \\left\\|X_2-Y_2\\right\\|_{L^p} = \\text{RHS}.&nbsp;&nbsp;&nbsp; \\] For (ii), <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{LHS} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mathbb{E} \\left[ \\left( X_1+X_2 - (Y_1+Y_2) \\right) ^2 \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E} \\left[ (X_1-Y_1)^2 \\right] + \\mathbb{E} \\left[ (X_2-Y_2)^2 \\right] + 2 \\underbrace{\\mathbb{E} \\left[ (X_1-Y_1)(X_2-Y_2) \\right] }_{=0}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\text{RHS}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "If $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R})$ with $\\mu$ diffusive, what is the unique increasing $T\\colon \\mathbb{R} \\to \\mathbb{R}$ with $T(\\mu) = \\nu$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Suppose $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R})$ and $\\mu$ is diffusive. Then there is a ($\\mu$-a.e. unique) increasing map $T\\colon \\mathbb{R}\\to \\mathbb{R}$ with $T(\\mu) = \\nu$, which is given by $T = G^{-1}\\circ F$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly, if $T$ is like that, it is increasing and $T(\\mu) = G^{-1}(F(\\mu)) = G^{-1}(\\lambda) = \\nu$. Now suppose $T$ is increasing and $T(\\mu) = \\nu$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; F(x) = \\mu ((-\\infty,x]) \\le \\mu\\left( T ^{-1}(T(-\\infty,x]) \\right) = \\nu((-\\infty,T(x)]) = G(T(x)),\\\\&nbsp;&nbsp;&nbsp; \\] so $G^{-1}(F(x)) \\le T(x)$. Now suppose the inequality is strict at some $x\\in \\mathbb{R}$, which means that $G \\ge F(x)$ on some interval $(x',T(x))$. Now for any $y &lt; x$, $G(T(y)) = \\mu(T^{-1}(-\\infty,T(y)]) \\le \\mu((-\\infty,x]) = F(x)$, so in fact $F(x) = G(y)$ for all $y\\in (x',T(x))$, so if $\\Lambda$ denotes the set of values of $G$ on plateaus (which is countable), then $F(x) \\in \\Lambda$, and the set of such $x$ is $\\mu$-negligible (otherwise $F(\\mu) = \\lambda$ had point masses), so the inequality is strict only on a $\\mu$-negligible set.</i> </p> </div><div></div>"
  },
  {
    "front": "Solution to Kantorovich problem in one dimension. In particular, what is $d_p(\\mu,\\nu)$ and $d_1(\\mu,\\nu)$ for $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R})$.",
    "back": "\"<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R})$, then for cost function $c(x,y) = h(x-y)$ with (strictly) convex $h$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I_\\star = \\int_0^1 h(F^{-1}(\\eta) - G^{-1}(\\eta)) \\mathop{}\\!\\mathrm{d} \\eta.&nbsp;&nbsp;&nbsp; \\] If $I_\\star &lt; \\infty$, then the (unique) optimal transport plan $\\Pi\\in \\Gamma(\\mu,\\nu)$ is given by $(F^{-1},G^{-1}) \\# \\lambda$, which is induced by the map $T = G^{-1}\\circ F$ if $\\mu$ is diffusive.</i> </p> <br><br>In particular, for any $\\mu,\\nu\\in \\mathcal{M}_1(\\mathbb{R})$ and $p \\in [1,\\infty)$, $d_p(\\mu,\\nu) = \\left\\|F^{-1} - G^{-1}\\right\\|_{L^p((0,1))}$, and \\[&nbsp;&nbsp;&nbsp; d_1(\\mu,\\nu) = \\int_0^1 \\left| F^{-1}(\\eta) - G^{-1}(\\eta) \\right| \\mathop{}\\!\\mathrm{d} \\eta = \\int_{\\mathbb{R}} \\left| F(x) - G(x) \\right| \\mathop{}\\!\\mathrm{d} x.\\]<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; That $(F^{-1},G^{-1}) \\# \\lambda \\in \\Gamma(\\mu,\\nu)$ is obvious, and <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^1 \\left| F^{-1}(\\eta) - G^{-1}(\\eta) \\right| \\mathop{}\\!\\mathrm{d} \\eta\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^1 \\int_\\mathbb{R} \\boldsymbol{1}_{F(x) \\wedge G(x) \\le \\eta \\le F(x) \\vee G(x)}\\mathop{}\\!\\mathrm{d} x\\mathop{}\\!\\mathrm{d} \\eta\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_\\mathbb{R} \\left| F(x) - G(x) \\right| \\mathop{}\\!\\mathrm{d} x.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>\""
  },
  {
    "front": "Definition of flow map $\\Phi_{s,t}\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ associated with a velocity field $u\\colon [0,T] \\times \\mathbb{R}^d\\to \\mathbb{R}^d$. What regularity does the flow map have?",
    "back": "\"<div>Suppose $u\\colon [0,T] \\times&nbsp; \\mathbb{R}^d\\to \\mathbb{R}^d$ (the <i>velocity field</i>) is globally Lipschitz in space with constant $L$, and continuous in time. Then the associated flow map $\\Phi_{s,t}\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ is defined by $\\Phi_{s,t} (x) = v_t$ where \\[\\begin{cases}&nbsp;&nbsp;&nbsp; \\dot{v} &amp;= u(v), \\text{ on $(s,t)$,}\\\\&nbsp;&nbsp;&nbsp; v_s &amp;= x.\\end{cases}\\] This has a unique solution because $u$ is globally Lipschitz, and Gr\\\"\"onwall implies that $\\Phi_{s,t}$ is globally Lipschitz with constant $\\mathrm{e}^{L(t-s)}$. It also follows from standard theory that $\\Phi_{s,t}$ is a diffeomorphism.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Proof of Lipschitzness. Suppose $v$ and $w$ are solutions starting respectively at $x,y\\in \\mathbb{R}^d$, and say $s = 0$. Then, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| v_s - w_s \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left| x-y \\right| + \\int_0^s \\left| u_r(v_r) - u_r(w_r) \\right| \\mathop{}\\!\\mathrm{d} r\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left| x-y \\right| + L \\int_0^s \\left| v_r - w_r \\right| \\mathop{}\\!\\mathrm{d} r,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so Gr\\\"\"onwall implies \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\Phi_t(x) - \\Phi_t(y) \\right| = \\left| v_t - w_t \\right| \\le \\left| x-y \\right| \\mathrm{e}^{Lt}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>\""
  },
  {
    "front": "Weak form and unique weak solution (as paths in $\\mathcal{M}_1(\\mathbb{R}^d)$ or $\\mathcal{P}_p(\\mathbb{R}^d)$) of \\[&nbsp;&nbsp;&nbsp; \\partial_t \\rho = \\nabla (\\rho\\nabla V),\\\\\\] where $V\\in C^2(\\mathbb{R}^d,\\mathbb{R})$ uniformly convex with bounded second derivatives.",
    "back": "<div>Consider the PDE $\\partial_t \\rho = \\nabla (\\rho \\nabla V)$, where $V\\in C^2(\\mathbb{R}^d,\\mathbb{R})$ has bounded second derivatives and $D^2 V \\ge \\lambda I_d$ for $\\lambda &gt; 0$ (uniformly convex). Then the weak form of this for a path $(\\rho_t)$ in $\\mathcal{M}_1(\\mathbb{R}^d)$ starting at $\\mu\\in \\mathcal{M}_1(\\mathbb{R}^d)$ is that for all $T&gt;0$ and $\\psi \\in C^\\infty_0([0,T]\\times \\mathbb{R}^d)$,<br>\\begin{align}\\label{eq:weakpde}<br>&nbsp;&nbsp;&nbsp; \\int_0^T \\rho_t( \\partial_t \\psi_t) \\mathop{}\\!\\mathrm{d} t = \\rho_T(\\psi_T) - \\mu(\\psi_0) + \\int_0^T \\rho_t(\\nabla \\psi \\nabla V) \\mathop{}\\!\\mathrm{d} t.<br>\\end{align}<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Given $\\mu\\in \\mathcal{M}_1(\\mathbb{R}^d)$, there is a unique solution $(\\rho_t)$ of (\\ref{eq:weakpde}) given by $\\rho_t = \\Phi_t(\\mu)$, where $\\Phi_t\\colon \\mathbb{R}^d\\to \\mathbb{R}^d$ is the flow map associated with $-\\nabla V$. If $\\mu\\in \\mathcal{P}_p(\\mathbb{R}^d)$, then $\\rho\\in C([0,\\infty), \\mathcal{P}_p(\\mathbb{R}^d))$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It is straight-forward to check that the PDE is equivalent to (\\ref{eq:weakpde}) if $\\rho$ has a smooth density, so it is the correct weak form.<br><br>&nbsp;&nbsp;&nbsp; To check that $\\rho_t$ is a solution, it suffices to consider $\\mu = \\delta_x$ (otherwise integrate all of the following against $\\mu$). We will use that $\\partial_t(\\psi_t(\\Phi_t(x))) = \\dot{\\psi_t}(\\Phi_t x) + (\\nabla \\psi_t)(\\Phi_t x) \\partial_t \\Phi_t x$, and $\\partial_t \\Phi_t x = -\\nabla V(\\Phi_t x)$ by definition, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^T \\rho_t(\\partial_t \\psi_t) \\mathop{}\\!\\mathrm{d} t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^T \\dot{\\psi_t}(\\Phi_t x) \\mathop{}\\!\\mathrm{d} t \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\psi_T(\\Phi_T x) - \\psi_0(x) + \\int_0^T \\mathop{}\\!\\mathrm{d} t\\, (\\nabla \\psi_t) (\\Phi_t x) (\\nabla V)(\\Phi_t x)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\ldots + \\int_0^T \\rho_t(\\nabla \\psi_t \\nabla V) \\mathop{}\\!\\mathrm{d} t.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br><br>&nbsp;&nbsp;&nbsp; To prove uniqueness of the solution, let $\\varphi \\in C_c^\\infty(\\mathbb{R}^d)$ and put $\\psi_t(\\cdot ) := \\varphi(\\Phi_{t,T}(\\cdot )) \\in C^\\infty_0([0,T]\\times \\mathbb{R}^d)$. By the method of characteristics, this is the unique solution to $\\dot{\\psi_t} = \\nabla \\psi \\nabla V$ with $\\psi_T = \\varphi$, so any solution to (\\ref{eq:weakpde}) satisfies \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\rho_T(\\varphi) = \\rho_T(\\psi_T) = \\rho_0(\\psi_0) = \\mu(\\Phi_T(\\varphi)),\\\\&nbsp;&nbsp;&nbsp; \\] so the solution is unique.<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; Finally, if $\\mu\\in \\mathcal{P}_p(\\mathbb{R}^d)$, we use that $t \\mapsto \\left| \\Phi_t(x) \\right| $ for $x\\in \\mathbb{R}^d$ is decreasing by convexity of $V$ (assuming that the unique global minimum is in zero, otherwise shift), so&nbsp; $\\rho_t(\\left| x \\right|^p) = \\mu (\\left| \\Phi_t(x) \\right| ^p) \\le \\mu (\\left| x \\right| ^p)&lt; \\infty $ and continuous in $t$ by monotone or dominated convergence.</i> </p> </div><div></div>"
  },
  {
    "front": "Measures in convex potential (Weak form of PDE $\\partial \\rho = \\nabla (\\rho \\nabla V)$): Exponential convergence to stationary solution $\\delta_0$.",
    "back": "\"<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu,\\nu\\in \\mathcal{P}_2(\\mathbb{R}^d)$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_2(\\Phi_t(\\mu), \\Phi_t(\\nu)) \\le \\mathrm{e}^{-\\lambda t} d_2(\\mu,\\nu).&nbsp;&nbsp;&nbsp; \\] In particular, $d_2(\\rho_t,\\delta_0) \\le \\mathrm{e}^{-\\lambda t } d_2(\\mu,\\delta_0)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Given a $d_2$-optimal coupling $\\gamma\\in \\Gamma(\\mu,\\nu)$, we have $(\\Phi_t,\\Phi_t) \\# \\gamma \\in \\Gamma(\\Phi_t(\\mu),\\Phi_t(\\nu))$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\text{LHS}^2 \\le \\int \\left| \\Phi_t(x) - \\Phi_t(y) \\right|^2 \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y).&nbsp;&nbsp;&nbsp; \\] Now use that <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} \\left| \\Phi_t(x) - \\Phi_t(y) \\right| ^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= -2 (\\Phi_t(x) - \\Phi_t(y)) \\left( \\nabla V(\\Phi_t(x)) - \\nabla V(\\Phi_t(y)) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le -2\\lambda \\left| \\Phi_t(x) - \\Phi_t(y) \\right| ^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; We can pull in the derivative because the modulus of the derivative is (because flow and $\\nabla V$ are Lipschitz) bounded by $C\\left| x-y \\right| ^2 \\le C \\left( \\left| x \\right| ^2 + \\left| y \\right| ^2 \\right) $, which is $\\gamma$-integrable because $\\mu,\\nu\\in \\mathcal{P}_2(\\mathbb{R}^d)$. Hence we can pull in the derivative and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} d_2(\\Phi_t(\\mu),\\Phi_t(\\nu))^2\\!\\!\\restriction_{t = 0}&nbsp; \\le -2\\lambda \\int \\left| x-y \\right| ^2 \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y) = -2\\lambda d_2(\\mu,\\nu)^2.&nbsp;&nbsp;&nbsp; \\] We could repeat this at any $t &gt; 0$ (by starting with an optimal coupling of $\\Phi_t(\\mu)$ and $\\Phi_t(\\nu)$), so the claim follows.</i> </p> </div><div></div>\""
  },
  {
    "front": "If $\\Phi \\colon E\\to E$ is $L$-Lipschitz and $p \\ge 1$, then $ \\mathcal{P}_p(E) \\to \\mathcal{P}_p(E); \\, \\mu \\mapsto&nbsp; \\Phi(\\mu)$ is $L$-Lipschitz w.r.t. $d_p$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\Phi \\colon E\\to E$ is $L$-Lipschitz and $p \\ge 1$, then $ \\mathcal{P}_p(E) \\to \\mathcal{P}_p(E); \\, \\mu \\mapsto&nbsp; \\Phi(\\mu)$ is $L$-Lipschitz w.r.t. $d_p$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\gamma\\in \\Gamma(\\mu,\\nu)$. Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_p(\\Phi(\\mu),\\Phi(\\nu))^p \\le \\int \\rho(\\Phi(x),\\Phi(y))^p \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le L^p \\int \\rho(x,y)^p \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "<div>Aggregation equation: If $W\\in C^2(\\mathbb{R}^d)$ with bounded second derivatives:<br><ol>  <li>Definition and niceness of flow map associated with velocity field $u(\\rho) = -\\nabla W\\star \\rho$, &nbsp;&nbsp;&nbsp;</li>  <li>Bound on $\\left| \\Phi_t^1(x) - \\Phi_t^2(x) \\right| $ for two paths $\\rho_{1,2}$ in $\\mathcal{P}_1(\\mathbb{R}^d)$,&nbsp;&nbsp;&nbsp;</li>  <li>Existence and uniqueness of (weak) solution to $\\dot{\\rho} = \\nabla (\\rho(\\nabla W\\star\\rho)) $.</li></ol></div><div></div>",
    "back": "\"<div>Define the velocity field associated with a path $\\rho\\in C([0,T],\\mathcal{P}_1(\\mathbb{R}^d))$ by $u(\\rho) := -\\nabla W \\star \\rho$. Then $u$ is globally Lipschitz in space and continuous in time, so the flow-map is globally Lipschitz and smooth. <br><br>Denote by $\\left\\|\\cdot \\right\\|$ the supremum norm in $C([0,T],\\mathcal{P}_1(\\mathbb{R}^d))$.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\rho_1,\\rho_2 \\in C([0,T],\\mathcal{P}_1(\\mathbb{R}^d))$, and the flow maps associated with $u(\\rho_i)$ are denoted $\\Phi^i$, then for $x\\in \\mathbb{R}^d$, $t\\in [0,T]$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\Phi_t^1(x) - \\Phi_t^2(x) \\right| \\le L \\int_0^t \\mathrm{e}^{L(t-s)} d_1(\\rho_1(s),\\rho_2(s))\\mathop{}\\!\\mathrm{d} s.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] In particular, for $\\mu\\in \\mathcal{P}_1(\\mathbb{R}^d)$, if $\\mathcal{F}\\colon C_\\mu\\to C_\\mu; \\rho \\mapsto (\\Phi_t(\\rho)\\# \\mu)_{t\\in [0,T]}$, \\[&nbsp;&nbsp;&nbsp; \\left\\|\\mathcal{F}\\rho_1 - \\mathcal{F}\\rho_2\\right\\| \\le \\left( \\mathrm{e}^{LT}-1 \\right) \\left\\|\\rho_1-\\rho_2\\right\\|.&nbsp;&nbsp;&nbsp; \\] In particular, there is a unique weak solution.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $u$ is well-defined because $W$ has linear growth and $\\rho\\in \\mathcal{P}_1(\\mathbb{R}^d)$. Fix $\\rho$, then, since $\\nabla W$ is Lipschitz (say with constant $L &gt; 0$),<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| u(t,x) - u(t,y) \\right| \\le \\int \\left| \\nabla W(x-z) - \\nabla W(y-z) \\right| \\rho_t(\\mathop{}\\!\\mathrm{d} z) \\le L \\left| x-y \\right| ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| u(t,x) - u(s,x) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left| \\rho_t(\\nabla W(x-\\cdot )) - \\rho_s(\\nabla W(x - \\cdot )) \\right| \\le L d_1(\\rho_t,\\rho_s).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; We have $\\Phi^i_t(x) = x + \\int_0^t u^i(s,\\Phi^i_s(x)) \\mathop{}\\!\\mathrm{d} s$, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\Phi^1_t(x) - \\Phi^2_t(x) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\underbrace{\\int_0^t \\left| u^1(s,\\Phi^1_s(x)) - u^1(s,\\Phi^2_s(x)) \\right| \\mathop{}\\!\\mathrm{d} s}_{\\le L \\int_0^t \\left| \\Phi^1_s(x) - \\Phi^2_s(x) \\right|\\mathop{}\\!\\mathrm{d} s} + \\int_0^t \\left| u^1(s,\\Phi^2_s(x)) - u^2(s,\\Phi^2_s(x)) \\right| \\mathop{}\\!\\mathrm{d} s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then use that <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| u^1(s,z) - u^2(s,z) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left| \\int \\nabla W(z-x) \\left( \\rho_1(s,\\mathop{}\\!\\mathrm{d} x) \\right) - \\rho_2(s,\\mathop{}\\!\\mathrm{d} x) \\right| \\le L d_1(\\rho_1(s),\\rho_2(s)).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; A variation of Gr\\\"\"onwall's lemma implies the first claim. Now if $\\mu\\in \\mathcal{P}_1(\\mathbb{R}^d)$, and $t\\in [0,T]$, then <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_1(\\Phi^1_t(\\mu),\\Phi^2_t(\\mu))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\left| \\Phi^1_t(x) - \\Phi^2_t(x) \\right| \\mu(\\mathop{}\\!\\mathrm{d} x) \\le \\mu(1) \\left( \\mathrm{e}^{Lt}-1 \\right) \\left\\|\\rho_1-\\rho_2\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Finally, since $C$ is a Banach space, if $T$ is small enough there is a unique fixed point, i.e. a unique weak solution on a small time interval. By gluing together we get a unique global weak solution.</i> </p> </div><div></div>\""
  },
  {
    "front": "Stability in $d_1$ for (weak) solutions to $\\dot{\\rho} = \\nabla (\\rho (\\nabla W\\star \\rho))$.",
    "back": "<div>Assume that $W\\in C^2(\\mathbb{R}^d)$ with second derivatives bounded by say $L &gt; 0$.<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\rho_1,\\rho_2 \\in C([0,\\infty),\\mathcal{P}_1(\\mathbb{R}^d))$ are two (weak) solutions, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_1(\\rho_1(t),\\rho_2(t)) \\le \\mathrm{e}^{2Lt} d_1(\\rho_1(0), \\rho_2(0)).&nbsp;&nbsp;&nbsp; \\]</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\mu = \\rho_1(0), \\nu = \\rho_2(0)$. Denote the flow map associated with $\\rho_i$ by $\\Phi^i$. Then take an optimal $d_1$-coupling $\\gamma\\in \\Gamma(\\mu,\\nu)$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d_1(\\rho_1(t), \\rho_2(t))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= d_1(\\Phi^1_t(\\mu), \\Phi^2_t(\\nu)) \\le \\int \\left| \\Phi^1_t(x) - \\Phi^2_t(y) \\right| \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int \\left| \\Phi^1_t(x) - \\Phi^2_t(x) \\right| \\mu(\\mathop{}\\!\\mathrm{d} x) + \\int \\left| \\Phi^2_t(x) - \\Phi^2_t(y) \\right| \\gamma(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le L \\int \\mathrm{e}^{L(t-s)} d_1(\\rho_1(s),\\rho_2(s))\\mathop{}\\!\\mathrm{d} s + \\mathrm{e}^{Lt} d_1(\\mu,\\nu).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Put $x(s) := \\mathrm{e}^{-Ls} d_1(\\rho_1(s), \\rho_2(s))$, then $x(t) \\le d_1(\\mu,\\nu) + L \\int_0^t x(s) \\mathop{}\\!\\mathrm{d} s$, so $x(t) \\le \\mathrm{e}^{Lt}d_1(\\mu,\\nu)$.</i> </p> </div><div></div>"
  },
  {
    "front": "GNS inequality (Sobolev inequality).",
    "back": "<div><p><b>Theorem.</b> <i>[GNS inequality] If $p\\in [1,d)$, then there is $C_{d,p}&gt; 0$ such that \\[&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{L^{p^\\star}(\\mathbb{R}^d)} \\le C_{d,p} \\left\\|\\nabla u\\right\\|_{L^p(\\mathbb{R}^d)},\\quad u\\in W^{1,p}(\\mathbb{R}^d).\\] If $\\Omega\\subset \\mathbb{R}^d$ is bounded Lipschitz, then for every $q\\in [1,p^\\star]$, there is $C_{d,p,q,\\Omega}$ such that \\[\\left\\|u\\right\\|_{L^q(\\Omega)} \\le C_{d,p,q,\\Omega} \\left\\|u\\right\\|_{W^{1,p}(\\Omega)},\\quad u\\in W^{1,p}(\\Omega).\\] </i> </p> <br><br>It is easy to see by looking at $u_\\lambda(\\cdot ) = u(\\lambda \\cdot )$ that the above can only possibly hold for this specific $p^\\star$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By denseness suffices to consider $u\\in C_c^\\infty(\\mathbb{R}^d)$, by looking at $\\left| u \\right| ^{\\gamma}$ it suffices to consider $p = 1$, and WLOG $d = 2$ (general case just more notation). Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| u(x) \\right| \\le \\int_\\infty^{x_1} \\left| \\partial_{x_1} u(y_1,x_2) \\right| \\mathop{}\\!\\mathrm{d} y_1 \\le \\int_\\mathbb{R} \\left| \\nabla u(y_1,x_2) \\right| \\mathop{}\\!\\mathrm{d} y_1.&nbsp;&nbsp;&nbsp; \\] Multiply this estimate with the one for the second coordinate to get \\[&nbsp;&nbsp;&nbsp; u(x)^2 \\le \\int_\\mathbb{R} \\left| \\nabla u(y_1,x_2) \\right| \\mathop{}\\!\\mathrm{d} y_1 \\int_\\mathbb{R} \\left| \\nabla u(x_1,y_2) \\right| \\mathop{}\\!\\mathrm{d} y_2.&nbsp;&nbsp;&nbsp; \\] Integrate against $\\mathop{}\\!\\mathrm{d} x_1\\mathop{}\\!\\mathrm{d} x_2$ to obtain $\\left\\|u\\right\\|_{L^2}^2 \\le \\left\\|\\nabla u\\right\\|_{L^1}^2$. The claim for domains follows with extension and Hölder.</i> </p> </div><div></div>"
  },
  {
    "front": "Friedrich's inequality (Sobolev inequality for $W^{1,p}_0$ functions).",
    "back": "<div><p><b>Theorem.</b> <i>[Friedrich's inequality]<br>&nbsp;&nbsp;&nbsp; If $\\Omega\\subset \\mathbb{R}^d$ is open and bounded and $p\\in [1,\\infty)$, and \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q\\in [1,p^\\star] &amp;, p &lt; d,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q\\in [1,\\infty) &amp;, p = d,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q\\in [1,\\infty] &amp;, p &gt; d.&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] (In particular $q = p$ is allowed.) Then there is $C_{d,p,q,\\Omega} &gt; 0$ such that&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{L^q(\\Omega)} \\le C_{d,p,q,\\Omega} \\left\\|\\nabla u\\right\\|_{L^p(\\Omega)},\\quad u\\in W^{1,p}_0(\\Omega).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $q = p$, this is simple analysis, if $p &lt; d$ it is GNS, the rest uses Morrey's inequality as well.</i> </p> </div><div></div>"
  },
  {
    "front": "Poincar\\'e's inequality (Sobolev inequality for $\\left\\|u - \\overline{u}\\right\\|$).",
    "back": "<div><p><b>Theorem.</b> <i>[Poincar\\'e's inequality]<br>&nbsp;&nbsp;&nbsp; Suppose $p\\in [1,\\infty]$ and $\\Omega$ a bounded Lipschitz domain. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u - \\overline{u}\\right\\|_{L^p(\\Omega)}\\le C_{d,p,\\Omega} \\left\\|\\nabla u\\right\\|_{L^p(\\Omega)},\\quad u\\in W^{1,p}(\\Omega),\\\\&nbsp;&nbsp;&nbsp; \\] where $\\overline{u} = \\frac{1}{\\left| \\Omega \\right| }\\int_\\Omega u$.</i> </p> <br><br>This is essentially Friedrich's inequality, but since we subtract the average we do not need $u\\in W^{1,p}_0$.</div><div></div>"
  },
  {
    "front": "Morrey's inequality (Embedding of Sobolev space into Hölder space). In particular, what is $W^{1,\\infty}(\\Omega)$?",
    "back": "<div>For $\\alpha\\in (0,1]$, put $[u]_{C^{0,\\alpha}(\\Omega)}:= \\sup_{x\\neq y\\in \\Omega} \\frac{\\left| u(x) - u(y) \\right| }{\\left| x-y \\right| ^{\\alpha}}$, and $\\left\\|u\\right\\|_{C^{0,\\alpha}(\\Omega)} := \\left\\|u\\right\\|_\\infty + [u]_{C^{0,\\alpha}(\\Omega)}$, which makes $C^{0,\\alpha}(\\Omega)$ a Banach space.<br><br><p><b>Theorem.</b> <i>[Morrey's inequality]<br>&nbsp;&nbsp;&nbsp; Suppose $d &lt; p \\le \\infty$ and $\\Omega$ is a bounded Lipschitz domain or $\\Omega = \\mathbb{R}^d$. Then every $u\\in W^{1,p}(\\Omega)$ has a $(1-d / p)$-Hölder continuous representative, and there is $C_{d,p,\\Omega}&gt; 0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{C^{0,1 - d / p}(\\Omega)}\\le C_{d,p,\\Omega} \\left\\|u\\right\\|_{W^{1,p}(\\Omega)}.&nbsp;&nbsp;&nbsp; \\] In particular, if $\\Omega$ bounded: The embedding $W^{1,p}(\\Omega) \\hookrightarrow C^{0,\\beta}(\\Omega)$ is compact for every $0 &lt; \\beta &lt; 1 - d / p$. Furthermore, in fact $W^{1,\\infty}(\\Omega) = C^{0,1}(\\Omega)$ (Lipschitz functions).</i> </p> </div><div></div>"
  },
  {
    "front": "Rellic-Kondrachov's compactness theorem (compact Sobolev embeddings).",
    "back": "<div><p><b>Theorem.</b> <i>[Rellic-Kondrachov's compactness theorem]<br>&nbsp;&nbsp;&nbsp; Let $\\Omega$ be a bounded Lipschitz domain and $p\\in [1,\\infty]$. Let \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q\\in [1,p^\\star) &amp;, p &lt; d,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q \\in [1,\\infty) &amp;, p = d,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; q \\in [1,\\infty] &amp;, p &gt; d.&nbsp;&nbsp;&nbsp; \\end{cases}\\] Then the embedding $W^{1,p}(\\Omega) \\hookrightarrow L^{q}(\\Omega)$ is compact. (The critical embedding $W^{1,p}(\\Omega) \\hookrightarrow L^{p^\\star}(\\Omega)$ is in fact not compact.)</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; When $p = q \\in [1,\\infty)$, it is (up to some extension and truncation), just the lemma (and the AA version for $L^q$).</i> </p> <br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $p\\in [1,\\infty)$, $u\\in W^{1,p}(\\mathbb{R}^d)$, and $y\\in \\mathbb{R}^d$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u^y - u\\right\\|_{L^p(\\mathbb{R}^d)} \\le \\left| y \\right| \\left\\|\\nabla u\\right\\|_{L^p(\\mathbb{R}^d)}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $C^\\infty(\\mathbb{R}^d)\\cap W^{1,p}(\\mathbb{R}^d)$ is dense, and for such functions it is easy.</i> </p> </div><div></div>"
  },
  {
    "front": "Elliptic PDEs: Associated bilinear form $B$, energy estimate, and definition of weak solution to Dirichlet boundary value problem.",
    "back": "<div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.</div><div></div><div>The bilinear form $B\\colon H^1\\times H^1\\to \\mathbb{R}$ associated with $L$ is \\[&nbsp;&nbsp;&nbsp; B(u,v) = \\int_\\Omega \\big( a_{ij} \\partial_j u \\partial_i v + b_i (\\partial_i u) v + cuv \\big) ,\\quad u,v\\in H^1.&nbsp;&nbsp;&nbsp; \\] Then we can cast $L$ as an operator $H^1 \\to H^{-1}(\\Omega) := H^1_0(\\Omega)^\\star$ by \\[(Lu)(\\varphi) := B(u,\\varphi),\\quad u\\in H^1, \\varphi \\in H^1_0.\\] This is well-defined (i.e. $Lu \\in H^{-1}$), and bounded, i.e. $L\\in \\mathcal{L}(H^1,H^{-1})$, by the lemma below. If $f,g_i\\in L^2(\\Omega)$, we interpret $f + \\partial_i g_i \\in H^{-1}$ by $(f+\\partial_i g_i) \\varphi = \\int_\\Omega\\big(f\\varphi - g_i \\partial_i \\varphi\\big)$. This is well-defined because $f,g_i\\in L^2$.</div><div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; We say that $u\\in H^1$ is a <i>weak solution</i> to the Dirichlet boundary value problem if $Lu = f + \\partial_i g_i$ as elements of $H^{-1}$, and $u - u_0 \\in H^1_0(\\Omega)$.</p> <br><br>It is easy to verify that a classical solution is also a weak solution (by confirming it for $\\varphi\\in C_c^\\infty(\\Omega)$ using IBP).</div><div><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; $B$ is bounded, that is, $\\left| B(u,v) \\right| \\le C \\left\\|u\\right\\|_{H^1} \\left\\|v\\right\\|_{H^1}$, and there is $C &gt; 0$ such that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp; &nbsp; &nbsp; &nbsp; \\left\\|u\\right\\|_{H^1}^2 \\le C\\left(B(u,u) + \\left\\|u\\right\\|_{L^2}^2\\right).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The first part is just Hölder's inequality and boundedness of the coefficients. The second part follows from<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda \\left\\|\\nabla u\\right\\|_{L^2}^2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\int_\\Omega a_{ij} \\partial_i u \\partial_j u = B(u,u) - \\int_\\Omega \\big( b_i \\partial_i u u + c u^2\\big)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le B(u,u) + \\left\\|b\\right\\|_\\infty \\left\\|\\nabla u\\right\\|_{L^2} \\left\\|u\\right\\|_{L^2} + \\left\\|c\\right\\|_\\infty \\left\\|u\\right\\|_{L^2}^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le B(u,u) + \\frac{\\lambda}{2} \\left\\|\\nabla u\\right\\|_{L^2}^2 + \\frac{1}{2\\lambda} \\left\\|b\\right\\|_\\infty^2 \\left\\|u\\right\\|_{L^2}^2 + \\left\\|c\\right\\|_\\infty \\left\\|u\\right\\|_{L^2}^2,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used $xy \\le \\frac{\\lambda}{2} x^2 + \\frac{1}{2\\lambda}y^2$ for $x,y\\in \\mathbb{R}$ and $\\lambda &gt; 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Elliptic PDEs: How injectivity/surjectivity of $L\\!\\!\\restriction_{H^1_0} $ correspond to uniqueness/existence of solutions.",
    "back": "<div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.</div><div></div><div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; <ol>  <li>Weak solutions exist for all $u_0 \\in H^1, g\\in L^2,f\\in H^{-1}$ iff $L\\!\\!\\restriction_{H^1_0} \\colon H^1_0 \\to H^{-1}$ is surjective,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Weak solutions are unique for all $u_0\\in H^1, g\\in L^2, f\\in H^{-1}$ iff $L\\!\\!\\restriction_{H^1_0} $ is injective.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For (i), if it is surjective, take $v\\in H^1_0$ with $Lv = -Lu_0 + f + \\partial_ig_i$ and put $u := u_0 + v$. Conversely, if $T\\in H^{-1}$, take a solution $u$ with $u_0 = g = 0$, $f = T$, so that $u \\in H^1_0$ and $Lu = f = T$.<br><br>&nbsp;&nbsp;&nbsp; If it is injective, and $u$ and $\\widetilde{u}$ are solutions, then $u-\\widetilde{u}\\in H^1_0$ and $L(u-\\widetilde{u}) = 0$. If solutions are unique, and $u,\\widetilde{u}\\in H^1_0$ with $Lu = L\\widetilde{u}$, then $u,\\widetilde{u}$ solve (BVP) with $g = u_0 = 0$ and $f = Lu = L\\widetilde{u}$, so $u = \\widetilde{u}$.</i> </p> </div><div></div>"
  },
  {
    "front": "Elliptic PDEs: If $B$ is coercive (in particular if $b = 0$ and $c\\ge 0$), then $L\\!\\!\\restriction_{H_0^1} $ is bijective, so there are unique weak solutions.",
    "back": "<div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.</div><div></div><div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $b = 0$ and $c\\ge 0$ a.e. (or, more generally, if $B$ is coercive), then $L\\!\\!\\restriction_{H^1_0}\\colon H^1_0 \\to H^{-1} $ is bijective. In particular, for all $f,g_i\\in L^2$ and $u_0\\in H^1$, (BVP) has a unique weak solution.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; In this case, for $u\\in H^1_0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B(u,u)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_\\Omega \\big(a_{ij}\\partial_i u \\partial_j u + c u^2\\big) \\ge \\lambda \\left\\|\\nabla u\\right\\|_{L^2} \\ge c\\left\\|u\\right\\|_{H^1}^2,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used Friedrich's inequality, so $B(\\cdot ,\\cdot )$ is a scalar product on $H^1_0$ whose norm is equivalent to $\\left&lt;\\cdot ,\\cdot&nbsp; \\right&gt; _{H^1_0}$, which means we can apply Riesz representation theorem to get that for every $T\\in H^{-1}$, there is a unique $u\\in H^1_0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Tv = B(u,v) = (Lu)(v),\\quad v\\in H^1,\\\\&nbsp;&nbsp;&nbsp; \\] that is, that $T = Lu$.</i> </p> </div><div></div>"
  },
  {
    "front": "Elliptic PDEs: Fredholm alternative. How can (iii) be motivated, and what is the meaning of $N$?",
    "back": "<div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.\\\\</div><div></div><div>Define the adjoint $L^\\star \\in\\mathcal{L}( H^1 ,H^{-1})$ by \\[&nbsp;&nbsp;&nbsp; (Lu)(v) = B(u,v) = (L^\\star v)(u),\\quad u,v\\in H^1,\\\\\\] which in the notation of $L$ is $L^\\star v = -\\partial_i (a_{ij} \\partial_j v) - \\partial_i(b_iv) + cv$. Note then that if there is a $v_0$ such that $L^\\star v_0 = 0$, then $(Lu)(v_0) = (L^\\star v_0)(u) = 0$ for all $u\\in H^1$, so $L$ cannot be surjective. There may still be solutions for particular $u_0,f,g$, and if $u$ is such a solution, then, since $u - u_0 \\in H_0^1$, <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; (f + \\partial_ig_i)v_0 &nbsp;&nbsp;&nbsp; &amp;= (Lu)(v_0) = B(u,v_0) = B(u_0,v_0) + \\underbrace{B(u-u_0,v_0)}_{=(L^\\star v_0)(u-u_0) = 0} = B(u_0,v_0).\\end{align*}[/$$]<br><br><p><b>Theorem.</b> <i>[Fredholm Alternative]<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$L\\!\\!\\restriction_{H^1_0}$ is bijective iff it is injective (i.e. iff the only solution to the homogeneous problem is trivial),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>The kernels $N$ of $L\\!\\!\\restriction_{H^1_0} $ and $N^\\star$ of $L^\\star \\!\\!\\restriction_{H_0^1} $ are of equal, finite dimension,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $N$ is not trivial, then (BVP) has a solution for given $u_0,f,g$ iff $B(u_0,v) = \\left&lt;f,v \\right&gt; - \\left&lt;g_i,\\partial_i v \\right&gt; $ for all $v\\in N^\\star$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br>Note that (i) implies that either there is a unique weak solution for all $u_0,f,g$, or there is a non-trivial weak solution to the homogeneous problem. Indeed, if the former is not true then $L\\!\\!\\restriction_{H_0^1} $ is not injective, so there is $u\\in H^1_0$ with $Lu = 0$. In fact $N\\subset H_0^1$ is the space of homogenuous solutions.</div><div></div>"
  },
  {
    "front": "Elliptic PDEs: What do you know about the natural embedding $L^2(\\Omega)\\hookrightarrow H^{-1}(\\Omega)$?",
    "back": "<div>It is compact.</div><div></div>"
  },
  {
    "front": "Elliptic PDEs: Regularity of solutions for differentiable $a$ (interior $H^2$, global $H^2$, and $C^\\infty$).",
    "back": "<div></div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.<div></div><div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose that $f\\in L^2$, and $u\\in H^1$ satisfies $Lu = f$ in $\\Omega$ in the weak sense.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $a\\in C^1(\\Omega)$, then $u\\in H^2_{\\text{loc}}$, in fact for any open $\\omega$ with $\\overline{\\omega}\\subset \\Omega$, there is $C = C(d,\\Omega,\\omega,a,b,c) &gt; 0$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{H^2(\\omega)} \\le C \\left( \\left\\|f\\right\\|_{L^2(\\Omega)} + \\left\\|u\\right\\|_{H^1(\\Omega)} \\right) .&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>If $a\\in C^1(\\overline{\\Omega})$, $\\Omega$ bounded with $C^2$-boundary, $u\\in H^1_0(\\Omega)$, then $u\\in H^2(\\Omega)$ and there is $C = C(d,\\Omega,a,b,c) &gt; 0$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{H^2(\\Omega)} \\le C \\left\\|f\\right\\|_{L^2(\\Omega)}.&nbsp;&nbsp;&nbsp; \\]</li>  <li>If $a,b,c \\in C^\\infty(\\overline{\\Omega})$, $\\Omega$ is bounded with $C^\\infty$-boundary, $f\\in C^\\infty(\\Omega)$, and $u\\in H_0^1(\\Omega)$, then $u\\in C^\\infty(\\Omega)$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br>We sketch the idea of the proof of (i) in the remaining chapter. Note that to get a global statement, one (1.) needs regularity of the boundary of the domain, and (2.) regularity of $a$ at the boundary. These are precisely the additional assumptions needed in (ii). We assume $b \\equiv c \\equiv 0$ to illustrate the methods.<br><br>\\begin{proofsketch}<br>&nbsp;&nbsp;&nbsp; First use a cutoff to be able to assume that $u$ vanishes near the boundary, then use that to reduce to the case $\\Omega = \\mathbb{R}^d$. If $a$ is constant, then by a change of variables we can assume it is the identity, so $L = -\\Delta$. Looking at the statement, it suffices to bound $\\left\\|D^2 u\\right\\|_{L^2}$ by $\\left\\|f\\right\\|_{L^2}$, and the idea is to use the fact that for $v\\in C^\\infty$, $\\left\\|D^2v\\right\\|_{L^2} = \\left\\|\\Delta v\\right\\|_{L^2}$ by IBP, and we can control $\\Delta u = -f$. Now take mollifiers $(\\rho_\\varepsilon)$ and put $u_\\varepsilon := \\rho_\\varepsilon\\star u$, $f_\\varepsilon:= \\rho_\\varepsilon\\star f$. Some Fubini confirms that $-\\Delta u_\\varepsilon = f_\\varepsilon$ weakly, thus strongly because they are smooth. Hence,&nbsp; \\[&nbsp;&nbsp;&nbsp; \\left\\|D^2 u_\\varepsilon\\right\\|_{L^2} = \\left\\|\\Delta u_\\varepsilon\\right\\|_{L^2} = \\left\\|f_\\varepsilon\\right\\|_{L^2} \\le \\left\\|f\\right\\|_{L^2} \\left\\|\\rho_\\varepsilon\\right\\|_{L^1} = \\left\\|f\\right\\|_{L^2},\\\\\\] where we used Young's convolution inequality towards the end. By reflexivity of $L^2(\\mathbb{R}^d)$, $(D^2u_\\varepsilon)$ converges weakly in $L^2$ along a subsequence, say to $A\\in L^2(\\mathbb{R}^d,\\mathbb{R} ^{d\\times d})$ with $\\left\\|A\\right\\|_{L^2}\\le \\left\\|f\\right\\|_{L^2}$. Now if $\\varphi\\in C^\\infty_c(\\mathbb{R}^d)$, then \\[\\int \\partial_i\\partial_j u_\\varepsilon \\varphi = \\int u_\\varepsilon \\partial_i \\partial_j \\varphi.\\] The LHS converges to $\\int A_{ij} \\varphi$, the RHS to $\\int u \\partial_i\\partial_j \\varphi$, which by definition implies $u\\in H^2$ and $D^2u = A$ weakly. Hence $\\left\\|D^2u\\right\\|_{L^2}\\le \\left\\|f\\right\\|_{L^2}$.<br><br>One possible way to extend to non-constant $a$ is the method of freezing the coefficients. Suppose $\\left| a_{ij} - \\delta_{ij} \\right| &lt; \\delta_0$ in $\\Omega$, then \\[&nbsp;&nbsp;&nbsp; -\\Delta u = (a_{ij} - \\delta_{ij}) \\partial_i\\partial_j u + \\partial_i a_{ij} \\partial_j u + f,\\\\\\] so by the result we already have, \\[\\left\\|u\\right\\|_{H^2(\\omega)} \\le C \\left( \\left\\|f\\right\\|_{L^2} + \\left\\|u\\right\\|_{H^1} + \\sup_{\\omega,i,j}\\left| a_{ij}- \\delta_{ij} \\right| \\left\\|D^2u\\right\\|_{L^2(\\omega)}&nbsp; \\right) .\\] If $\\delta_0$ is small enough, we can absorb the annoying factor on the right to the LHS. The full result is then obtained by covering in small balls (since $a$ is continuous).<br>\\end{proofsketch}</div><div></div>"
  },
  {
    "front": "Elliptic PDEs: Continuity of solutions for general coefficients. Why is this theorem easy if one assumes continuity of $a$?",
    "back": "<div>Assume that $\\Omega\\subset \\mathbb{R}^d$ is a bounded Lipschitz domain (if not otherwise specified, then $H^1 = H^1(\\Omega)$, $L^2 = L^2(\\Omega)$ etc.), $a_{ij},b_i,c\\colon \\Omega\\to \\mathbb{R}$ are measurable, bounded, and $a$ symmetric and uniformly elliptic (with bounds from above and below). Define $L$ by $Lu := -\\partial_i(a_{ij}\\partial_j u) + b_i \\partial_i u + cu$.</div><div><br><div><p><b>Theorem.</b> <i>[De Giorgi-Moser-Nash]<br>&nbsp;&nbsp;&nbsp; If $f\\in L^q(\\Omega)$ for some $q &gt; \\frac{d}{2}$, and $u\\in H^1(\\Omega)$ satisfies $Lu = f$ in $\\Omega$ weakly, then $u$ is locally Hölder continuous, in fact for any open $\\omega$ with $\\overline{\\omega}\\subset \\Omega$, there are $C = C(d,\\Omega,\\omega,a,b,c) &gt; 0$ and $\\alpha = \\alpha(d,\\Omega,\\omega,a) &gt; 0$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|u\\right\\|_{C^{0,\\alpha}(\\omega)}\\le C \\left( \\left\\|f\\right\\|_{L^q(\\Omega)} + \\left\\|u\\right\\|_{H^1(\\Omega)} \\right) .&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><b>Remark.</b>&nbsp;&nbsp;&nbsp; The main point here is that $a$ is not assumed to be continuous. If we did assume that, we could use the method of freezing coefficients and hence assume that $a$ is constant, so we only need to consider $-\\Delta u = f$. This already implies that $u\\in W^{2,q}_\\text{loc}$ (this is obvious in 1d, where $u'' = -f\\in L^q$, but true in any dimension). Then if $d / 2 &lt; q &lt; d$, we have $q^\\star &gt; d$ and so \\[&nbsp;&nbsp;&nbsp; W^{2,q}_\\text{loc} \\hookrightarrow W^{1,q^\\star}_\\text{loc} \\hookrightarrow C^{0,2 - \\frac{d}{q}}_\\text{loc},\\\\&nbsp;&nbsp;&nbsp; \\] where we used GNS and Morrey's inequalities and that $1 - d / q^\\star = 2 - d / q$.</p> </div><div></div></div>"
  },
  {
    "front": "Definition of well-order and principle of induction.",
    "back": "<div>A totally ordered $(X,\\le)$ is called a <i>well-order</i> if every non-empty subset has a minimum, that is, if \\[\\forall S\\subset X\\colon S\\neq \\emptyset \\implies \\left[ \\exists y\\in S\\, \\forall x\\in S\\colon y\\le x \\right] .\\] <br><p><b>Proposition.</b>[Induction]<br>&nbsp;&nbsp;&nbsp; If $S\\subset X$ such that \\[&nbsp;&nbsp;&nbsp; \\forall x\\colon \\left[ \\forall y\\colon y &lt; x \\Rightarrow y\\in S \\right] \\Rightarrow x\\in S,\\\\&nbsp;&nbsp;&nbsp; \\] then $S = X$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $S \\neq X$, and let $x$ be the minimal element of $X\\setminus S$. Then $y\\in S$ for all $y &lt; x$, so $x\\in S$ by assumption, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Isomorphisms between well-ordered sets are unique.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ and $Y$ are isomorphic well-ordered sets, then there is a unique isomorphism.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $f,g\\colon X\\to Y$ are two different isomorphisms, and let $x\\in X$ be the minimum of $\\left\\{ f\\neq g \\right\\} \\subset X$. Let $a\\in Y$ be the minimum of $Y\\setminus S$ where $S := \\left\\{ f(y)\\colon y&lt;x \\right\\} $. Since $f$ is injective, $f(x)\\not\\in S$ is in this set, so $f(x) \\ge a$. If $f(x) &gt; a$, then $x &gt; f^{-1}(a)$, so $a \\in S$, a contradiction. Hence $f(x) = a$, and $g(x) = a$ by the same argument, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition initial segment of a well-order, and characterisation.",
    "back": "<div>A subset $S\\subset X$ is called an <i>initial segment</i> if \\[\\forall x\\in S\\, \\forall y\\colon y &lt; x \\Rightarrow y\\in S.\\] <br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; A set $S\\subset X$ is an initial segment iff it is of the form $I_x = \\left\\{ y\\in X\\colon y &lt; x \\right\\} $ for some $x\\in X$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly, $I_x$ is an initial segment for all $x\\in X$. Suppose $S\\subset X$ is an initial segment, and let $x$ be the minimum of $X\\setminus S$. Then $I_x \\subset S$. Conversely, if $y\\in S$, then certainly $y\\neq x$, and if $y &gt; x$, we had $x\\in S$ which is not true, so $y &lt; x$, i.e. $y\\in I_x$.</i> </p> </div><div></div>"
  },
  {
    "front": "Recursion on well-ordered sets.",
    "back": "<div>In the following, we identify a function $f\\colon X\\to Y$ with the set $\\left\\{ (x,f(x))\\colon x\\in X \\right\\} $.<br><br><p><b>Theorem.</b> <i>[Recursion]<br>&nbsp;&nbsp;&nbsp; Suppose $X$ is a well-ordered set and $G\\colon \\mathcal{P}(X\\times Y)\\to Y$ is a function for a set $Y$. Then there is a unique function $f\\colon X\\to Y$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall x\\colon f(x) = G(f\\!\\!\\restriction_{I_x} ).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For an initial segment $I\\subset X$ we say that $h\\colon I\\to Y$ is an <i>attempt (on $I$)</i> if $h(x) = G(h\\!\\!\\restriction_{I_x} )$ for all $x \\in I$. We first show that two attempts $h$ and $h'$ on $I$ must be the same, using induction: Suppose $h(y) = h'(y)$ for all $y &lt; x$ for some $x \\in I$. Then $h(x) = G(h\\!\\!\\restriction_{I_x} ) = G(h'\\!\\!\\restriction_{I_x} ) = h'(x)$. Next, we show by induction that for every $x\\in X$ there is an attempt $h\\colon I\\to Y$ such that $x\\in I$. For every $y &lt; x$ there is an attempt $h_y$ defined on $I_y$, put $h' := \\bigcup_{y&lt;x} h_y$. This is well-defined(!) and an attempt on $I_x$(!), and we put $h:= h' \\cup \\left\\{ (x,G(h')) \\right\\} $. Then $h$ is an attempt on the initial segment $I_x \\cup \\left\\{ x \\right\\} $, which contains $x$.<br><br>&nbsp;&nbsp;&nbsp; Finally, we define $f(x)$ for $x\\in X$ by $h(x)$ for any attempt defined at $x$. By the above, this is well-defined and satisfies what we want. It is also unique: If $f$ and $f'$ both work, then if they agree on $I_x$, $f(x) = G(f\\!\\!\\restriction_{I_x} ) = G(f'\\!\\!\\restriction_{I_x} ) = f'(x)$, so $f=f'$ by induction.</i> </p> <br><br><p><b>Remark.</b>&nbsp;&nbsp;&nbsp; Obviously $G$ only needs to be defined on functions $f\\colon I\\to Y$ for initial segments $I$ (just define the value of $G$ on other sets as some fixed arbitrary value), and to make a recursion equation of the form \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(x) = G(f\\!\\!\\restriction_{I_x} )&nbsp;&nbsp;&nbsp; \\] work it suffices to show that for any $x\\in X$ the RHS is well-defined under the assumption that $f\\!\\!\\restriction_{I_x} $ is an attempt.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Define $G$ to be some arbitrary value $\\perp$ when the RHS is ill-defined, which yields a completely defined $G$ and thus a function $f$. Now it STS by induction that $G(f\\!\\!\\restriction_{I_x} ) \\neq \\perp$ for all $x\\in X$, which follows from the assumption.<br><br>&nbsp;&nbsp;&nbsp; Also, if $f'$ is another function which satisfies the original equation, then $G(f'\\!\\!\\restriction_{I_x} )$ must be well-defined for all $x\\in X$, so $f'$ also satisfies the recursion equation with the new $G$, so $f = f'$.</i> </p> </div><div></div>"
  },
  {
    "front": "A subset of a well-order $X$ is isomorphic to a unique initial segment. What does this tell us about $X$?",
    "back": "<div><p><b>Proposition.</b>[Subset collapse]<br>&nbsp;&nbsp;&nbsp; If $Y\\subset X$, then $Y$ is isomorphic to a unique initial segment of $X$. In particular, a well-order can never be isomorphic to a proper initial segment of itself.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that we have to prove that there is a uniqe order-preserving injection $f\\colon Y\\to X$ whose image is an initial segment. Next, note that a function $f\\colon Y\\to X$ has this property iff it satisfies \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall x\\in Y\\colon f(x) = \\min \\left( X\\setminus \\left\\{ f(y)\\colon y&lt;x \\right\\}&nbsp; \\right) .&nbsp;&nbsp;&nbsp; \\] We thus have to show that there is a unique such function, so we only need to prove that, for $x\\in X$, if $f\\colon I_x\\to Y $ satisfies the recursion equation on $I_x$, then $\\left\\{ f(y)\\colon y &lt; x \\right\\} \\neq X$. For this it STS by induction that $f(y)\\le y$ for all $y &lt; x$, which is clear (if $f(z) \\le z$ for all $z &lt; y$, then $f(z) \\le z &lt; y$, so $y\\in S:=&nbsp; X\\setminus \\left\\{ f(z)\\colon z &lt; y \\right\\} $, so $f(y) = \\min S \\le y $).<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; The unique initial segment of $X$ to which $X$ is isomorphic is obviously $X$ itself.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition ordinal, order type, and a total order on ordinals.",
    "back": "<div>An ordinal is a well-ordered set, with isomorphic sets considered equal. The order type of a WOS is its \"equivalence class\".<br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; We write $\\alpha \\le \\beta$ if $\\alpha$ is an initial segment of $\\beta$ (that is, if $X$ is isomorphic to an initial segment of $Y$ for two arbitrarily chosen representatives). This is well-defined and a total order on the ordinals.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly well-defined, reflexive, and transitive. Suppose $X\\le Y$ and $Y\\le X$ via injections $f$ and $g$. Then $g\\circ f\\colon X\\to X$ is an isomorphism from $X$ to an initial segment of itself, which must be $X$, so $g$ is surjective, so an isomorphism from $Y$ to $X$. Finally, for $X$ and $Y$ two given WOs, define a function $f\\colon X\\to Y \\sqcup \\left\\{ \\perp \\right\\} $ via recursion, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(x) = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\min (Y\\setminus f(I_x)) &amp;, f(I_x) \\neq Y,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\perp &amp;, \\text{else}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] If $f(x) \\neq \\perp$ for all $x\\in X$, then $f$ is an isomorphism between $X$ and an initial segment of $Y$. Otherwise let $x\\in X$ be minimal with $f(x) = \\perp$. Then $f\\!\\!\\restriction_{I_x} \\colon I_x \\to Y$ is an order-preserving injection to an initial segment of $Y$ and $f(I_x) = Y$, so an isomorphism.</div><div></i> </p> </div><div></div>"
  },
  {
    "front": "Definition of $\\alpha^+$ and $\\sup_i \\alpha_i$. Why is the latter well-defined and indeed a least upper bound?",
    "back": "<div>For ordinals $\\alpha, \\alpha_i, i\\in I, $ define $\\alpha^+$ as the order type of $X^+$ for an arbitrary representative, and $\\sup_{i\\in I} \\alpha_i$ as the order type of the union of $I_{\\alpha_i}, i\\in I$. This is the least upper bound of all $\\alpha_i$.<br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $\\sup_i \\alpha_i$ is well-defined because $(I_{\\alpha_i})_{i\\in I}$ are all genuine initial segments of each other, and obviously an upper bound. If $\\beta \\ge \\alpha_i$ for all $i\\in I$, then on the level of representatives, $I_{\\alpha_i}\\le I_\\beta$ for all $i\\in I$, so $\\bigcup_{i\\in I} I_{\\alpha_i}\\le I_\\beta$, so $\\sup_i \\alpha_i \\le \\beta$.</i> </p> </div><div></div>"
  },
  {
    "front": "Ordinals are well-ordered. In particular, ordinals do not form a set.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Ordinals are well-ordered.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $S$ be a non-empty set of ordinals, take $\\alpha \\in S$. If $\\alpha$ is minimal, we are done, otherwise consider $I_\\alpha = \\left\\{ \\beta\\colon \\beta &lt; \\alpha \\right\\} $, which is a well-ordered set (it is isomorphic to any $X$ with order type $\\alpha$), and $S\\cap I_\\alpha$ is non-empty, so has a least element, which is a least element of $S$.</i> </p> <br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Ordinals are not a set.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose they were, $O$, call its order type $\\alpha\\in O$. Then $O$ is isomorphic to $I_\\alpha$, which is a proper initial segment of $O$ (as it does not contain $\\alpha$).</i> </p> </div><div></div>"
  },
  {
    "front": "Hartog's lemma and $\\omega_1$.",
    "back": "<div><p><b>Lemma.</b> <i>[Hartog]<br>&nbsp;&nbsp;&nbsp; For every set $X$, there is a well-order which does not inject into $X$. The least is called $\\gamma(X)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $L$ be the set of order types of well-ordered subsets $(Y,\\le)$ of $X$. Then $L$ is an initial segment of ordinals, and any $(Y,\\le) \\simeq I_{(Y,\\le)}$, which is a proper initial segment of $L$, so $\\text{otp} (Y,\\le) &lt; \\text{otp} L$. Now if $L \\hookrightarrow X$, then its order type is that of a well-ordered subset of $X$, a contradiction.</i> </p> <br><br>In particular, $\\omega_1 := \\gamma(\\omega)$ is an uncountable ordinal (the least one). It has the properties that every initial segment is countable, and every sequence in it is bounded (because a countable supremum of countable ordinals is countable).</div><div></div>"
  },
  {
    "front": "Definition of successor and limit ordinals.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; An ordinal is a <i>successor</i> if it is of the form $\\beta^+$ iff there is a greatest element $\\beta$ below it. Otherwise, it is called a <i>limit</i>.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; There is a greatest element below it iff $I_\\alpha$ has a maximal element, say $\\beta$, in which case $I_\\alpha = I_\\beta \\cup \\left\\{ \\beta \\right\\} $, so $\\alpha = \\beta^+$.<br><br>&nbsp;&nbsp;&nbsp; Conversely, if $\\alpha = \\beta^+$, then $I_\\alpha = I_{\\beta^+} = I_\\beta \\cup \\left\\{ \\beta \\right\\} $ (since $\\beta^+$ is the least ordinal greater than $\\beta$), so $\\beta$ is a greatest element of $I_\\alpha$.</i> </p> </div><div></div>"
  },
  {
    "front": "Synthetic and inductive definitions of ordinal arithmetic. Proof they are equal for $+$, and associativity of addition.",
    "back": "<div><p><b>Definition.</b>[Ordinal Arithmetic, Synthetic]<br>&nbsp;&nbsp;&nbsp; For ordinals $\\alpha$ and $\\beta$, $\\alpha + \\beta$ is defined as the order type of writing two representatives after each other. $\\alpha \\cdot \\beta$ is defined on $\\alpha \\times \\beta$ by sorting w.r.t. the second argument first.</p> <br><br><p><b>Definition.</b>[Ordinal Arithmetic, Inductive]<br>&nbsp;&nbsp;&nbsp; For fixed $\\alpha$, we define $\\alpha^+$ recursively for all ordinals $\\beta$ (to justify this we do recursion on $I_\\gamma$ for all ordinals $\\gamma$).<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\alpha + 0 =&nbsp; \\alpha$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha + \\beta^+ = (\\alpha + \\beta)^+$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha + \\lambda = \\sup \\left\\{ \\alpha + \\gamma\\colon \\gamma &lt; \\lambda \\right\\} $.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; (This is a case distinction between successor and limit, where the latter is distinguished between $0$ and not $0$).<br>&nbsp;&nbsp;&nbsp; Multiplication is defined via<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\alpha \\cdot 0 = 0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha \\cdot \\beta^+ = \\alpha \\cdot \\beta + \\alpha$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha \\cdot \\lambda = \\sup \\left\\{ \\alpha\\cdot \\gamma\\colon \\gamma &lt; \\lambda \\right\\} $.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Exponentiation is defined via $\\alpha^0 = 1$, $\\alpha ^{\\beta^+} = \\alpha^\\beta \\cdot \\alpha$, and $\\alpha^\\lambda = \\sup_{\\alpha^\\gamma\\colon \\gamma &lt; \\lambda}$.</p> <br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Inductive and synthetic definitions of addition coincide. In particular addition is associative.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Write $+'$ for synthetic definition, we use induction on $\\beta$ for fixed $\\alpha$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\alpha + 0 = \\alpha = \\alpha +' 0$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha + \\beta^+ = (\\alpha +\\beta)^+ = (\\alpha+'\\beta)^+ = \\alpha +' \\beta+$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\alpha + \\lambda = \\sup \\left\\{ \\alpha + \\gamma\\colon \\gamma &lt; \\lambda \\right\\} = \\sup \\left\\{ \\alpha +' \\gamma\\colon \\gamma &lt; \\lambda \\right\\} $. The latter is&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\bigcup_{\\gamma \\in I_\\lambda} (\\alpha +' \\gamma)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\alpha +' \\bigcup_{\\gamma \\in I_\\lambda}&nbsp; \\gamma,\\\\&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = \\alpha +' \\lambda,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where we used in the last step that $\\lambda = \\sup_{\\gamma\\colon \\gamma &lt; \\lambda}$ for a limit ordinal $\\lambda$ (certainly $\\ge$, and if $&gt;$, then the RHS would be a greatest element smaller than $\\lambda$).&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Associativity is obvious with the synthetic definition.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition outer measure, and associated $\\sigma$-algebra and (complete) measure.",
    "back": "<div>An <i>outer measure</i> on $X\\neq \\emptyset$ is a map $\\mu\\colon \\mathcal{P}(X)\\to [0,\\infty]$ which is monotone and $\\sigma$-subadditive and such that $\\mu(\\emptyset) = 0$. In that case, \\[&nbsp;&nbsp;&nbsp; \\mathcal{A}_\\mu := \\left\\{ A\\subset X\\colon \\forall E\\subset X\\colon \\mu(E) = \\mu(E\\cap A) + \\mu(E\\setminus A)&nbsp; \\right\\} \\] is a $\\sigma$-algebra (the <i>$\\mu$-measurable sets</i>) and $\\mu\\colon \\mathcal{A}_\\mu \\to [0,\\infty]$ is a complete measure.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First observe that if $A\\in \\mathcal{A}_\\mu$ and $B\\subset X$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(A\\sqcup B) = \\mu((A \\sqcup B) \\cap A) + \\mu((A \\sqcup B)\\setminus A) = \\mu(A) + \\mu(B).&nbsp;&nbsp;&nbsp; \\] This implies finite additivity and hence $\\sigma$-additivity. Indeed, if $A_i \\in \\mathcal{A}_\\mu$ are disjoint, then for every $n\\in \\mathbb{N}$, \\[&nbsp;&nbsp;&nbsp; \\sum_{k=1}^n \\mu(A_k) = \\mu\\left( \\bigcup_{k=1} ^n A_k\\right) \\le \\mu \\left( \\bigcup_{k=1} ^\\infty A_k \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\] hence the same holds if we replace the LHS by its limit $n\\to \\infty$.<br><br>&nbsp;&nbsp;&nbsp; $\\mathcal{A}_\\mu$ is obviously closed under complements, and contains $\\emptyset$. It is also closed under finite intersections. Indeed if $A,B\\in \\mathcal{A}_\\mu$, and $E\\subset X$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(E\\setminus (A\\cap B))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu(E \\cap A^{c} \\cup E \\cap B^{c}) = \\mu(E\\cap B^{c}\\cap A) + \\mu(E\\cap A^{c})\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu(E\\cap A) - \\mu(E\\cap B\\cap A) + \\mu(E) - \\mu(E\\cap A)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu(E) - \\mu(E\\cap A\\cap B).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; It suffices to show that it is closed under countable disjoint unions (then it is a $\\cap$-stable Dynkin system). Let $A_i\\in \\mathcal{A}_\\mu, i\\in \\mathbb{N}, $ and $E\\subset X$, $A:= \\bigcup_{i\\ge 1} A_i$. Then, similarly how we proved finite additivity, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{i=1}^n \\mu(E\\cap A_i) = \\mu(E\\cap \\bigcup_{i=1}^n A_i) \\le \\mu(E\\cap A) \\le \\sum_{i= 1 }^\\infty\\mu(E\\cap A_i) ,\\\\&nbsp;&nbsp;&nbsp; \\] where the last step is $\\sigma$-subadditivity. This implies that $\\mu(E \\cap \\bigcup_{i=1} ^n A_i) \\to \\mu(E\\cap A)$ as $n\\to \\infty$ and thus<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(E\\setminus A) + \\mu(E\\cap A) \\le \\varliminf_{n\\to \\infty} \\big( \\underbrace{\\mu( E\\setminus \\bigcup_{i=1} ^n A_i ) + \\mu( E\\cap \\bigcup_{i=1} ^n A_i )}_{= \\mu(E)} \\big) = \\mu(E).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Here the $\\le$ comes from the first term (the inequality is just monotonicity of $\\mu$), the second equality comes from the fact that finite unions are in $\\mathcal{A}_\\mu$.<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; If $A\\subset X$ and $\\mu(A) = 0$, then $\\mu(E) \\le \\mu(E\\cap A) + \\mu(E \\setminus A) = \\mu(E\\setminus A) \\le \\mu(E)$, where we first used subadditivity and then monotonicity.</i> </p> </div><div></div>"
  },
  {
    "front": "$\\mathcal{B}(\\mathbb{R}) \\neq \\mathcal{P}(\\mathbb{R})$",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{B}(\\mathbb{R}) \\neq \\mathcal{P}(\\mathbb{R}).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $A\\subset [0,1]$ contain exactly one representative of $\\mathbb{R} / \\sim$, where $x\\sim y$ iff $x-y\\in \\mathbb{Q}$. Suppose $B := \\bigcup_{q\\in [-1,1]\\cap \\mathbb{Q}} (A-q)\\in \\mathcal{B}(\\mathbb{R})$. This is a disjoint union (if $x\\in (A-q) \\cap (A-q'),$ then $x + q, x+q'\\in A$ but they have rational difference), and contains $[0,1]$ (if $x\\in [0,1]$, then some other representative of the same equivalence class is in $A$, and their difference is at most $1$), and is contained in $[-1,2]$. If $B\\in \\mathcal{B}(\\mathbb{R})$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3 \\ge \\lambda(B) = \\sum_{q} \\lambda(A - q) = \\sum_q \\lambda(A),\\\\&nbsp;&nbsp;&nbsp; \\] which is impossible (this is either $0$ or $\\infty$).</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Lebesgue outer measure, value on intervals, and Borel sets are $\\lambda^\\star$-measurable.",
    "back": "<div>It is defined by \\[&nbsp;&nbsp;&nbsp; \\lambda^\\star(A) = \\inf \\left\\{ \\sum_{i=1}^\\infty \\left| I_i \\right| \\colon A\\subset \\bigcup_{i=1} ^\\infty I_i \\text{ is a covering with intervals} \\right\\} .\\] This is an outer measure, and $\\lambda^\\star(I) = \\left| I \\right| $ for intervals, and $\\mathcal{A}_\\mu$ contains $\\mathcal{B}(\\mathbb{R})$.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Write $\\mu := \\lambda^\\star$ here. Clearly $\\mu(\\emptyset) = 0$ and $\\mu$ is monotone. Now let $A_k\\subset \\mathbb{R},k\\in \\mathbb{N},$ be disjoint, and $I_{nk}$ an $\\varepsilon 2^{-n}$-good covering of $A_k$. Then $(I_{nk})_{n,k\\in \\mathbb{N}}$ is an interval covering of $A := \\bigcup_{k=1}^\\infty A_k$, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(A) \\le \\sum_{n,k} \\left| I_{nk} \\right| \\le \\sum_{n=1}^\\infty \\left( \\mu(A_n) + \\varepsilon 2^{-n} \\right) = \\sum_{n=1}^\\infty \\mu(A_n) + \\varepsilon.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br><br>&nbsp;&nbsp;&nbsp; If $I\\subset \\mathbb{R}$ is an interval, then $m(I) \\le \\left| I \\right| $. If $(I_i)$ is an interval covering, then by making them all slightly bigger we may assume $I_i$ are open and cover $\\overline{I}$, so we can reduce to a finite covering, for which $\\sum_i \\left| I_i \\right| \\ge \\left| I \\right| $ obviously holds.<br><br>&nbsp;&nbsp;&nbsp; It STS that open intervals are $\\lambda^\\star$-measurable. Let $I\\subset \\mathbb{R}$ be an interval, $E\\subset \\mathbb{R}$, and $I_i$ an interval covering of $E$. Then $(I\\cap I_i)$ is an interval covering of $E\\cap I$, and $(I_i\\setminus I)$ is an interval covering of $E\\setminus I$ (possibly after replacing some of those by two intervals), so, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(E\\cap I) + \\mu(E\\setminus I) \\le \\sum_{i}|I_i \\cap I| + \\sum_i |I_i \\setminus I| = \\sum_i |I_i|.&nbsp;&nbsp;&nbsp; \\] Now take the infimum on the RHS.</i> </p> </div><div></div>"
  },
  {
    "front": "Knaster-Tarski fixed point theorem (functions on posets).",
    "back": "<div><p><b>Theorem.</b> <i>[Knaster-Tarski]<br>&nbsp;&nbsp;&nbsp; Any order-preserving function on a complete poset has a fixed point.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $s := \\sup S$ were $ S := \\left\\{ x\\in X\\colon x \\le f(x) \\right\\} $. This is well-defined because $\\min X \\in S$. First, any $x\\in S$ satisfies $x\\le f(x) \\le f(s)$, so $f(s)$ is also an upper bound, so $s \\le f(s)$. This implies that $f(s) \\le f(f(s))$, so $f(s) \\in S$, so $f(s) \\le s$.</i> </p> </div><div></div>"
  },
  {
    "front": "Cantor-Schröder-Bernstein theorem on bijections between sets.",
    "back": "<div><p><b>Theorem.</b> <i>[Cantor-Schröder-Bernstein]<br>&nbsp;&nbsp;&nbsp; If $f\\colon A\\to B$ and $g\\colon B\\to A$ are injections, then there is a bijection $h\\colon A\\to B$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If we show that there are $P \\sqcup Q = A$ and $R \\sqcup S = B$ with $f(P) = R$ and $g(S) = Q$, then we can put $h = f$ on $P$ and $h = g^{-1}$ on $Q$. This works iff $g(B\\setminus f(P)) = A\\setminus P$, so iff the map \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{P}(A) \\to \\mathcal{P}(A); \\, P \\mapsto A\\setminus g(B\\setminus f(A))&nbsp;&nbsp;&nbsp; \\] has a fixed point. But the map is order-preserving and $\\mathcal{P}(S)$ is complete.</i> </p> </div><div></div>"
  },
  {
    "front": "Proof of Zorn's lemma from Choice.",
    "back": "<div><p><b>Theorem.</b> <i>[Zorn]<br>&nbsp;&nbsp;&nbsp; If $X$ is a poset in which every chain has an upper bound, then it has a maximal element.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For any chain $T\\subset X$ let $u(T) \\in X$ be an upper bound (here we use Choice). Suppose for contradiction that every $x\\in X$ has a larger element $x'&gt; x$. We show that we can construct \"arbitrarily large\" elements in $X$ in the sense that for every ordinal $\\gamma$ we find an injection $\\gamma \\hookrightarrow X$, which cannot be true by Hartog. We define a map $\\alpha \\mapsto&nbsp; x_\\alpha$ such that $\\alpha &lt; \\beta$ implies $x_\\alpha &lt; x_\\beta$ by recursion (on a fixed initial segment $I_\\gamma$ of well-orders) by<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$x_0 = x$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$x_{\\alpha^+ } = {x_\\alpha}'$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$x_\\lambda = u(\\left\\{ x_\\alpha\\colon \\alpha &lt; \\lambda \\right\\} )'$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; For this to be well-defined, it suffices to show that under the assumption that $x_\\alpha$ has been defined on some initial segment $I_\\lambda$, it is true that for any $\\alpha &lt; \\beta (&lt;\\lambda)$, $x_\\alpha &lt; x_\\beta$. We do this for fixed $\\alpha$. Then<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $\\beta = 0$ then $\\beta \\not&gt; \\alpha$, so there is nothing to show,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\beta = \\zeta^+ &gt; \\alpha$, then $x_\\beta = x_{\\zeta}' &gt; x_\\zeta \\ge x_\\alpha$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\beta = \\sup \\left\\{ \\zeta\\colon \\zeta &lt; \\beta \\right\\} $, then $x_\\beta &gt; u(\\ldots ) \\ge x_\\alpha$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; The recursion is thus well-defined and the above also proves that it is an injection, so we can inject $\\gamma \\simeq I_\\gamma \\hookrightarrow X$ for every ordinal $\\gamma$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Zorn's lemma $\\implies$ AC.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(A_i)_{i\\in I}$ be a non-empty family of non-empty sets. We call a tuple $(J,f)$ with $J\\subset I$ and $f\\colon J\\to A := \\bigcup_{i\\in I} A_i$ a partial choice if $f(j) \\in A_j$ for all $j\\in J$, and consider the poset of all partial choices, where $(J,f) \\le (J',f')$ if $J\\subset J'$ and $f$ and $ f'$ aggree on $J'$. It is not empty (pick $J = \\left\\{ i \\right\\} $ for some $i\\in I$), and if $(J_k,f_k), k\\in \\mathbb{N},$ is a chain, then $(\\cup J_k, \\cup f_k)$ is well-defined and an upper bound. Hence by Zorn's lemma we find a maximal element $(J,f)$. If $J\\neq I$, then we could pick $i\\in I\\setminus J$ and $x\\in A_i$ so that $(J \\cup \\left\\{ i \\right\\} , f \\cup \\left\\{ (i,x) \\right\\} ) &gt; (J,f)$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Well-ordering theorem $\\iff$ AC.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \"$\\implies$\": Take a well-ordering on $A := \\bigcup_{i\\in I} A_i$, and define $f\\colon I\\to A$ by $f(i) := \\min A_i$.<br><br>&nbsp;&nbsp;&nbsp; \"$\\impliedby$\": Let $X$ be a non-empty set and $\\gamma$ an ordinal larger than $X$ (Hartog's lemma). We define $x_\\alpha$ for $\\alpha \\in I_\\gamma$ by recursion on the set $I_\\gamma$: $x_0 = x_0$ for some arbitrary $x_0\\in X$, and if $x$ is defined on $I_\\alpha$, then choose&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x_\\alpha \\in X \\setminus \\left\\{ x_\\beta\\colon \\beta &lt; \\alpha \\right\\} &amp;, \\left\\{ x_\\beta\\colon \\beta &lt; \\alpha \\right\\} \\subsetneq X,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x_\\alpha := \\perp &amp;, \\text{else}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] (Here we need AC.) Then $x\\colon I_\\gamma \\to X \\cup \\left\\{ \\perp \\right\\} $, and if $x_\\alpha \\neq \\perp$ for all $\\alpha \\in I_\\gamma$, then $x\\colon I_\\gamma \\hookrightarrow X$, a contradiction. So if $\\beta$ is the smallest ordinal with $x_\\beta = \\perp$, then $x\\colon I_\\beta \\to X$ is a bijection, through which we can push the well-order of $I_\\beta$ to $X$.</i> </p> </div><div></div>"
  },
  {
    "front": "Bourbaki-Witt theorem (\"AC free\" part of Zorn).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a chain-complete poset and $f\\colon X\\to X$ an inflationary function ($f(x) \\ge x$ for all $x\\in X$), then $f$ has a fixed point.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It suffices to show that $X$ has a maximal element. This follows from Zorn, but we can prove it without AC because we can make the choices in the proof of Zorn explicit: Assuming $f$ has no fixed point, we can make the explicit choice $x':= f(x) &gt; x$ for $x\\in X$, and instead of having to choose an upper bound for each chain we can take its supremum.</i> </p> </div><div></div>"
  },
  {
    "front": "Definitions of $P$, $L(P)$, valuation, semantic entailment, model, tautology.",
    "back": "<div><ol>  <li>$P$ is just a set of atoms, $L(P)$ is the smallest superset that contains $\\perp$ and $p\\Rightarrow q$ whenever $p,q\\in L(P)$.&nbsp;&nbsp;&nbsp;</li>  <li>A <i>valuation</i> is a map $v\\colon L\\to \\left\\{ 0,1 \\right\\} $ such that $v(\\perp) = 0$ and $v(p\\Rightarrow q) = 1 - \\boldsymbol{1}_{\\left\\{ p = 0, q = 1 \\right\\} }$. It is clearly defined by its values on $P$.&nbsp;&nbsp;&nbsp;</li>  <li>If $S\\subset L$, we say that a valuation $v$ is a <i>model</i> for $S$ if $v(s) = 1$ for all $s\\in S$.&nbsp;&nbsp;&nbsp;</li>  <li>If $t\\in L$, then we write $S \\vDash t$ if $t$ is true in any model for $S$, and say that $S$ <i>entails/semantically implies</i> $t$. If $\\vDash t$, we say $t$ is a <i>tautology</i>.</li></ol></div><div></div>"
  },
  {
    "front": "Definition of syntactic entailment and the three axioms.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; If $S\\subset L$ and $t\\in L$, then a <i>proof</i> of $t$ from $S$ is a finite sequence $t_1, \\ldots ,t_n$ such that $t_n = t$ and every $t_i$ is either an axiom, a member of $S$, or was deduced from previous propositions with modus ponens.<br><br>&nbsp;&nbsp;&nbsp; In that case, we say that $S$ <i>proves</i> or <i>syntactically entails</i> $t$ and write $S \\vdash t$. If $\\vdash t$, then $t$ is called a <i>theorem</i>.</p> <br><br>The axioms are<br><ol>  <li>$p \\Rightarrow (q \\Rightarrow p)$ &nbsp;&nbsp;&nbsp;</li>  <li>$(p \\Rightarrow (q\\Rightarrow r)) \\Rightarrow ((p\\Rightarrow q) \\Rightarrow (q\\Rightarrow r))$&nbsp;&nbsp;&nbsp;</li>  <li>$\\neg \\neg p \\Rightarrow p$</li></ol></div><div></div>"
  },
  {
    "front": "Deduction theorem.",
    "back": "<div><p><b>Theorem.</b> <i>[Deduction theorem]<br>&nbsp;&nbsp;&nbsp; If $S\\subset L$ and $p,q\\in L$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S \\vdash (p\\Rightarrow q) \\quad \\iff \\quad S\\cup \\left\\{ p \\right\\} \\vdash q.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \"$\\implies$\": Take a proof and append the lines $p$ and then $q$.<br><br>&nbsp;&nbsp;&nbsp; \"$\\impliedby$\": Take a proof $t_1, \\ldots ,t_n$ from $S\\cup \\left\\{ p \\right\\} $ to $q$. We show inductively that $S \\vdash (p\\Rightarrow t_i)$ for all $i\\in [n]$. If $t_i$ is an atom or in $S$, we can write <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; t_i \\Rightarrow (p\\Rightarrow t_i)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; t_i\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\cline{1-2}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; p\\Rightarrow t_i.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $t_i = p$, we can prove $\\vdash (p\\Rightarrow p)$. If it is the result of two previous lines $t_j$ and $t_j \\Rightarrow t_i$, then we by induction we can prove<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; p \\Rightarrow (t_j \\Rightarrow t_i)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; p \\Rightarrow t_j\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; (p \\Rightarrow (t_j \\Rightarrow t_i)) \\Rightarrow ((p\\Rightarrow t_j) \\Rightarrow (p\\Rightarrow t_i))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\cline{1-2}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; (p\\Rightarrow t_j) \\Rightarrow (p \\Rightarrow t_i)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\cline{1-2}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; p \\Rightarrow t_i.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Definitions of completeness, soundness, and adequacy, and soundness theorem.",
    "back": "<div>Our theory is called <i>sound</i> if $S\\vdash t$ implies $S\\vDash t$, <i>adequate</i> if $S\\vDash t$ implies $S\\vdash t$, and <i>complete</i> if it is sound and adequate.<br><br><p><b>Theorem.</b> <i>[Soundness theorem]<br>&nbsp;&nbsp;&nbsp; If $S\\vdash t$, then $S\\vDash t$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $t_1, \\ldots ,t_n$ be a proof of $t$ from $S$, and $v$ a model for $S$. We inductively prove that $v(t_i) = 1$ for all $i\\in [n]$. This is clear if $t_i$ is an axiom or member of $S$, and if it was derived with MP from $t_j \\Rightarrow t_i$ and $t_j$, both of which are true in $v$ by induction hypothesis, then $v(t_i) = 1$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of (in-)consistence, the model existence theorem, and why it implies adequacy.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; $S\\subset L$ is called <i>inconsistent</i> if $S\\vdash \\perp$, otherwise <i>consistent</i>.</p> <br><br><p><b>Theorem.</b> <i>[Model existence theorem]<br>&nbsp;&nbsp;&nbsp; If $S$ is consistent, then it has a model. (The converse holds by soundness.)</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that for every $p\\in L$, either of $S\\cup\\left\\{ p \\right\\} $ or $S\\cup \\left\\{ \\neg p \\right\\} $ must be consistent. Otherwise we could prove $S \\vdash p$ and $S\\vdash \\neg p$ (deduction theorem) and thus $S\\vdash \\perp$, a contradiction. With AC we may now choose either $t$ or $\\neg t$ according to this rule for all $t\\in L$ and add them to obtain $S$. This is still consistent (proofs are finite) and deductively closed (if $S \\vdash t$ but $t\\not\\in S$, then $\\neg t\\in S$, so $S\\vdash \\perp$). Now check that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v(t) := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 &amp;, t \\in S,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, t\\not\\in S,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] is a valuation.</i> </p> </div><div><br><p><b>Corollary.</b> <i>[Adequacy]<br>&nbsp;&nbsp;&nbsp; If $S\\vDash t$, then $S\\vdash t$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $S\\vDash t$, so $S \\cup \\left\\{ \\neg t \\right\\} \\vDash \\perp$, so $S \\cup \\left\\{ \\neg t \\right\\} \\vdash \\perp$ (model existence), so $S \\vdash \\neg\\neg t$, so by the third axiom and MP $S\\vdash t$.</i> </p> </div><div></div>"
  },
  {
    "front": "Compactness and decidability theorems (as corollaries to completeness).",
    "back": "<div><p><b>Theorem.</b> <i>[Compactness]<br>&nbsp;&nbsp;&nbsp; If $S\\vDash t$, then there is a finite subset $S'\\subset S$ with $S' \\vDash t$. In particular, if every finite subset of $S$ has a model, then so does $S$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Trivial if $\\vDash$ is replaced by $\\vdash$ because proofs are finite. For the second statement, suppose that $S$ does not have a model, so $S\\vdash \\perp$, so $S'\\vdash \\perp$ for a finite subset, a contradiction.</i> </p> <br><br><p><b>Theorem.</b> <i>[Decidability]<br>&nbsp;&nbsp;&nbsp; For every finite $S\\subset L$ and $t\\in L$, there is an algorithm which determines whether $S\\vdash t$ in finite and bounded time.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Trivial for $\\vDash$ by writing out truth tables.</i> </p> </div><div></div>"
  },
  {
    "front": "If $f\\colon [a,b]\\to \\mathbb{R}$ is Riemann-integrable, then it is Lebesgue measurable and the integrals coincide.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By definition of Riemann-integrability, $f$ is bounded, and there are sequences of step functions $\\psi_n \\downarrow$ and $\\phi_n \\uparrow$ with $\\phi_n \\le f \\le \\psi_n$. Then their pointwise limits satisfy $\\phi \\le f \\le \\psi$, and (by MCT) and definition of Riemann integral \\[&nbsp;&nbsp;&nbsp; \\int^L \\psi \\mathop{}\\!\\mathrm{d} x \\longleftarrow \\int^L \\psi_n \\mathop{}\\!\\mathrm{d} x\\stackrel{  }{\\longrightarrow} \\int^R f \\mathop{}\\!\\mathrm{d} x,\\\\\\] same for $\\psi$, so $\\psi = \\phi = f$ a.e. (so $f$ is Lebesgue measurable) and $\\int^L f = \\int^L \\psi = \\int^R f$.</i> </p> <br><div></div></div>"
  },
  {
    "front": "The <i>Devil's staircase:</i> A function which is continuous and a.e. differentiable on $[0,1]$, but $\\int_0^1 f'(x)\\mathop{}\\!\\mathrm{d} x \\neq f(1) - f(0)$.",
    "back": "<div>Define $f$ by&nbsp; \\[&nbsp;&nbsp;&nbsp; f \\left( \\sum_{k=0}^\\infty a_k 3^{-k} \\right)&nbsp; := \\sum_{k=0}^\\infty \\boldsymbol{1}_{\\left\\{ a_k &gt; 0 \\right\\} } 2^{-k}\\] That is, $1 / 2$ on $[1 / 3, 2 / 3]$, $3 / 4$ on $[7 / 9, 8 / 9] $ etc. Then $f$ is continuous, a.e. differentiable, in fact constant on $[0,1] \\setminus C$, so \\[\\int_0^1 f'(x) \\mathop{}\\!\\mathrm{d} x = 0 \\neq f(1) - f(0).\\]</div><div></div>"
  },
  {
    "front": "A sum of exponential RV's is (in-)finite iff their mean is.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\lambda_i &gt; 0,i\\in \\mathbb{N}$, and $E_i \\sim \\text{Exp}(\\lambda_i)$ independent, then $S := \\sum_{i=1}^\\infty E_i$ is a.s. (in-)finite iff $\\mathbb{E} \\left[ S \\right] = \\sum_{i=1}^\\infty \\frac{1}{\\lambda_i} $ is (in-)finite.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Only have to show that infinite mean implies infinite a.s., for which it STS that $\\mathbb{E} \\left[ \\mathrm{e}^{-S} \\right] = 0$, where \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\mathrm{e}^{-S} \\right] = \\lim_{n\\to \\infty} \\prod_{i=1}^n \\mathbb{E} \\left[ \\mathrm{e}^{-E_i} \\right] = \\lim_{n\\to \\infty} \\prod_{i=1}^n \\frac{\\lambda_i}{\\lambda_i + 1},\\\\&nbsp;&nbsp;&nbsp; \\] which is zero iff the following is infinite, \\[&nbsp;&nbsp;&nbsp; -\\sum_{i=1}^\\infty \\log \\left( \\frac{\\lambda_i}{\\lambda_i + 1} \\right) = \\sum_{i=1}^\\infty \\log \\left( 1 + \\frac{1}{\\lambda_i} \\right) &nbsp;&nbsp;&nbsp; \\] If $1 / \\lambda_i \\ge 1$ infinitely often, then this is infinite, otherwise this is at least $c\\sum_{i = i_0}^\\infty \\frac{1}{\\lambda_i} = \\infty$. </i> </p> </div><div></div>"
  },
  {
    "front": "Forward- and backward equations for continuous-time Markov chain.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; The transition matrices $(P(t))_{t\\ge 0}$ of a minimal continuous-time Markov chain with $Q$-matrix $Q$ are the minimal solution to either of the equations \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P'(t) = Q P(t) \\quad \\text{ or }\\quad P'(t) = P(t)Q,\\\\&nbsp;&nbsp;&nbsp; \\] with $P(0) = \\boldsymbol{1}$. If the chain is non-explosive, the solution is unique.</p> </div><div></div>"
  },
  {
    "front": "How to find entries of $P(t)$ for $M(\\cdot ,Q)$ with finite state space?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; $p_{ij}(t) =&nbsp; \\sum_{k\\in S} a_k \\mathrm{e}^{\\lambda_k t}$, where $(\\lambda_k)$ are the eigenvalues of $Q$, and the coefficients can be determined via $P^{(i)}(0) = Q^{i}$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $Q = U \\Lambda U^{-1}$ for some matrix $U$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(t) = \\mathrm{e}^{tQ} = U \\mathrm{e}^{t\\Lambda} U^{-1} = U \\text{diag}(\\mathrm{e}^{t\\lambda_k}) U^{-1},\\\\&nbsp;&nbsp;&nbsp; \\] so $p_{ij}(t)$ is a linear combination of $\\mathrm{e}^{\\lambda_k t}$ terms.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of transience/recurrence of states for $M(\\nu,Q)$. This coincides with the notion for the corresponding jump chain and $i\\in S$ is recurrent iff $\\int_0^\\infty p_{ii}(t) \\mathop{}\\!\\mathrm{d} t = \\infty$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; $i\\in S$ is transient/recurrent if $X_t$, started from $i$, a.s. eventually leaves $i$ for every/is at $i$ for arbitrarily large times.<br><br>&nbsp;&nbsp;&nbsp; Then a state is transient/recurrent iff it is transient/recurrent for the corresponding jump chain, and $i\\in S$ is recurrent iff \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^\\infty p_{ii}(s) \\mathop{}\\!\\mathrm{d} s = \\infty.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $i\\in S$ is recurrent/transient for $X$, then it clearly is for $Y$. If $i\\in S$ is transient for $Y$, say it leaves forever at $n$, then $X_t \\neq i$ for all $t \\ge T_n$, and $T_n &lt; \\infty$ a.s. If $i\\in S$ is recurrent for $Y$, say $Y_{n_j} = i$ for $j\\in \\mathbb{N}$, then $X$ spends time at least $\\sum_{j=1}^\\infty (T_{n_j} - T_{n_j - 1})$ at state $i$, and this is a sum of i.i.d. RV's, so a.s. infinite.<br><br>&nbsp;&nbsp;&nbsp; Finally, if $q_i = 0$ then then $i$ is recurrent and $p_{ii}(t) = 1$ for all $t &gt; 0$, so suppose $q_i &gt; 0$. Then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_0^\\infty p_{ii}(t) \\mathop{}\\!\\mathrm{d} t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^i \\left[\\int_0^\\infty \\boldsymbol{1}_{\\left\\{ X_t = i \\right\\} } \\mathop{}\\!\\mathrm{d} t\\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{E}^i \\left[ \\sum_{n=0}^\\infty Z_n \\boldsymbol{1}_{\\left\\{ Y_n = i \\right\\} } \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=0}^\\infty \\mathbb{P}^i(Y_n = i) \\mathbb{E}^i \\left[Z_n \\,\\middle\\vert\\, Y_n = i\\right]\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{q_i} \\sum_{n=0}^\\infty \\pi^{(n)}_{ii}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "First order logic: Language, closed term, bound/free variables, sentences, substitution, $L$-structure, interpretation of a sentence (in a structure)",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $\\Omega$ (function symbols) and $\\Pi$ (relation symbols) be disjoint sets and $\\alpha\\colon \\Omega\\cup \\Pi \\to \\mathbb{N}_0$ (arity), then the <i>language</i> $L = L(\\Omega,\\Pi,\\alpha)$ is the set of formulae, where<br>&nbsp;&nbsp;&nbsp; <ol>  <li>Variables ($x,x_1,x_2,y,z, \\ldots $) are terms, and for terms $t_i$ and $f\\in \\Omega$, $f(t_1, \\ldots ,t_n)$ is a term,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\perp$ is an AF, $(s=t)$ for terms $s,t$ is an AF, and $\\phi(t_1, \\ldots ,t_n)$ for $\\phi\\in \\Pi$ and terms $t_i$ is an AF,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Atomic Formulae are formulae, and if $p,q$ are formulae, then so are $(p\\Rightarrow q)$ and $(\\forall x)p$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; We make the following definitions.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>A <i>closed</i> term is one without variables,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>An occurrence of a variable $x$ in a formula is <i>bound</i> if it is within the range of $(\\forall x)$, otherwise <i>free</i>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>A <i>sentence</i> is a formula with no free variables,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $p$ is a formula, $t$ a term, $x$ a variable, then $p[t / x]$ is obtained from $p$ by replacing each <i>free</i> occurrence of $x$ in $p$ by $t$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>A <i>structure</i> for $L$ is a set $A\\neq\\varnothing$ together with functions and relations of the appropriate arities,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $A$ is a structure for $L$ and $p$ is a sentence, then $p_A\\in \\left\\{ 0,1 \\right\\} $ is the statement obtained by plugging in the appropriate functions and relations, and replacing $\\forall x$ by $\\forall x\\in A$.&nbsp;&nbsp;&nbsp;</li></ol></p> </div><div></div>"
  },
  {
    "front": "First order logic: Theory, model, semantic entailment, tautology",
    "back": "<div>Suppose a language is given.<br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>A <i>theory</i> $S$ is a set of sentences. An $L$-structure $A$ is called a <i>model</i> for a sentence $p$ if $p_A = 1$, and a model for $S$ if $p_A = 1$ for all $p\\in S$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>We say that $S$ <i>entails</i> a sentence $p$, written $S\\vDash p$, if every model for $S$ is a model for $p$. A <i>tautology</i> is a sentence with $\\vDash p$, for example $(\\forall x)(x=x)$.&nbsp;&nbsp;&nbsp;</li></ol></p> </div><div></div>"
  },
  {
    "front": "First order logic: Syntactic entailment (proofs), deduction theorem",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Axioms of predicate logic are the three axioms from propositional logic (on formulas), and additionally<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$(\\forall x)(x=x)$ for any variable $x$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$(\\forall x)(\\forall y) \\big( (x=y)\\Rightarrow (p\\Rightarrow p[y / x])\\big)$ for any variables $x,y$, and formula $p$, with $y$ not occurring bound in $p$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$[(\\forall x)p] \\Rightarrow p[t / x]$ for formula $p$, variable $x$, and no free variable of $t$ occuring bound in $p$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$[(\\forall x)(p\\Rightarrow q)] \\Rightarrow [p\\Rightarrow (\\forall x)q]$ for formulas $p,q$ and variables $x$ not occurring free in $p$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Deduction rules are moduls ponens, and generalisation: from $r$ can deduce $(\\forall x)r$ if $x$ has not occurred as a free variable in a premise so far in the proof.<br><br>&nbsp;&nbsp;&nbsp; A proof of a formula $p$ from a set of formulae $S$ is a sequence of statements, where each statement is an axiom, an element of $S$, or deduced from previous lines using MP or Gen. In that case write $S\\vdash p$ and call $p$ a <i>theorem</i> of $S$.</p> <br><br><p><b>Theorem.</b> <i>[Deduction theorem]<br>&nbsp;&nbsp;&nbsp; For $S\\subset L$ and $p,q\\in L$, \\[&nbsp;&nbsp;&nbsp; S \\vdash p\\Rightarrow q \\quad \\text{if and only if}\\quad S \\cup \\left\\{ p \\right\\} \\vdash q.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Just like with propositional logic, but for \"$\\impliedby$\" have to check Gen. Hence suppose that a proof of $q$ from $S\\cup \\left\\{ p \\right\\} $ contains a use of Gen to obtain $(\\forall x)r$ from $r$, and that we can prove $p\\Rightarrow r$ from $S$. $x$ is not free in a premise used so far in either proof, so we get $(\\forall x)(p\\Rightarrow r)$. If $x$ is not free in $p$, then we get $p \\Rightarrow (\\forall x)r$ by an axiom, otherwise $p$ has not been used in the proof of $r$ yet, so $S\\vdash r$, so $S\\vdash (\\forall x)r$, so $S\\vdash p\\Rightarrow (\\forall x)r$.</i> </p> </div><div></div>"
  },
  {
    "front": "First order logic: Model existence, completeness",
    "back": "<div>The model existence lemma states that if $S\\subset L$ is consistent, then it has a model. As with propositional logic, soundness is clear and we deduce completeness of first order logic.<br><br><p><b>Theorem.</b> <i>[Completeness of first order logic]<br>&nbsp;&nbsp;&nbsp; If $S$ is a theory, and $p$ a sentence, then $S\\vdash p$ iff $S\\vDash p$.</i> </p> </div><div></div>"
  },
  {
    "front": "First order logic: Compactness, Löwenheim-Skolem",
    "back": "<div><p><b>Theorem.</b> <i>[Compactness]<br>&nbsp;&nbsp;&nbsp; If $S$ is a theory such that every finite subset has a model, then $S$ has a model.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Having a model is equivalent to being consistent, and then it is clear because proofs are finite.</i> </p> <br><br><p><b>Corollary.</b> <i>[Löwenheim-Skolem]<br>&nbsp;&nbsp;&nbsp; Let $S$ be a theory.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $S$ has arbitrarily large (finite) models, then it has an infinite model,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $S$ has an infinite model, then it has an uncountable model (in fact as large as any set),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $L$ is countable and $S$ has a model, then it has a countable model.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $S' = S \\cup \\bigcup_{n\\in \\mathbb{N}} \\left\\{ \\left| A \\right| \\ge n \\right\\} $, where $\\left| A \\right|\\ge 2 $ stands for the sentence $(\\exists x)(\\exists y)(x\\neq y)$, for example. Then every finite subset of $S'$ has a model by assumption, so $S'$ has a model. This must be a model of $S$ and infinite,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Let $I$ be uncountable (or as large as desired), add constants $\\left\\{ c_i\\colon i\\in I \\right\\} $ to $L$, and let \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S' = S \\cup \\bigcup_{i,j\\in I, i\\neq j} \\left\\{ c_i \\neq c_j \\right\\} .&nbsp;&nbsp;&nbsp; \\] Then every finite subset of $S'$ only has finitely many new constants, so has a model, so $S'$ has a model,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>The model constructed in the model existence theorem is countable.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "ZF: Axioms of extension, separation, and empty set.",
    "back": "<div><p><b>Axiom.</b> <i>[Axiom of extension]<br>&nbsp;&nbsp;&nbsp; If two sets have the same elements, they are the same set. (The converse holds automatically by axioms.)</i> </p> <br><br><p><b>Axiom.</b> <i>[Axiom of separation]<br>&nbsp;&nbsp;&nbsp; If $x$ is a set and $p$ a formula (with free variables $t_1, \\ldots ,t_n,z'$), then the set $\\left\\{ z\\in x\\colon p(z) \\right\\} $ exists (where $p(z)$ is short for $p[z / z']$).</i> </p> <br><br><p><b>Axiom.</b> <i>[Axiom of empty set]<br>&nbsp;&nbsp;&nbsp; The empty set exists.</i> </p> </div><div></div>"
  },
  {
    "front": "ZF: Axiom of pair set, ordered pairs, functions.",
    "back": "<div><p><b>Axiom.</b> <i>[Axiom of pair set]<br>&nbsp;&nbsp;&nbsp; Can form $\\left\\{ x,y \\right\\} $. More precisely, for $x,y$ there exists a set whose elements are exactly $x$ and $y$.</i> </p> <br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li><b>Ordered pairs.</b> Write $(x,y)$ for $\\left\\{ \\left\\{ x \\right\\} ,\\left\\{ x,y \\right\\}&nbsp; \\right\\} $.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li><b>Functions.</b> A function is a set that contains only ordered pairs and such that $(x,y)\\in f \\wedge (x,z)\\in f \\Rightarrow y=z$. We can build $\\text{dom} f$ as subset of $\\bigcup \\bigcup f $ using separation, and write $f\\colon x\\to y$ if $x = \\text{dom} f$ and $(u,v)\\in f \\Rightarrow v\\in y$.&nbsp;&nbsp;&nbsp;</li></ol></p> </div><div></div>"
  },
  {
    "front": "ZF: Axioms of union and power set, how to build $\\bigcap x$, $x \\times y$, and the set of functions $x\\to y$.",
    "back": "<div><p><b>Axiom.</b> <i>[Axiom of union]<br>&nbsp;&nbsp;&nbsp; Can build the union $\\bigcup x$ of a set $x$, that is $(\\forall x) (\\exists y)(\\forall z)(z\\subset&nbsp; y \\Leftrightarrow&nbsp; z\\in x)$, where $z\\subset y$ stands for $(\\forall v)(v\\in z \\Rightarrow v\\in y)$. Write $x\\cup y$ for $\\bigcup \\left\\{ x,y \\right\\}&nbsp; $.</i> </p> <br><br>Note that we do not need an axiom for $\\bigcap x$, we can build it by taking $x_1\\in x$ and $\\left\\{ y\\in x_1\\colon&nbsp; (\\forall x')(x'\\in x \\Rightarrow y\\in x')&nbsp; \\right\\} $.<br><br><p><b>Axiom.</b> <i>[Axiom of power set]<br>&nbsp;&nbsp;&nbsp; We can build power sets.</i> </p> <br><br>We can also build $x\\times y$ as subset of $\\mathcal{P}(\\mathcal{P}(x \\cup y ))$, and the set of functions $x \\to y$ as subset of $\\mathcal{P}(x \\times y)$.</div><div></div>"
  },
  {
    "front": "ZF: Axiom of infinity, successor sets, $\\omega$, and induction.",
    "back": "<div>We would like to define $\\mathbb{N}$. For $x$ put $x^+ := x \\cup \\left\\{ x \\right\\} $, then we have $0 := \\emptyset$, $1:= 0^+ = \\left\\{ 0 \\right\\} $, $2:= 1^+ = \\left\\{ 0,1 \\right\\} $ etc. We need an axiom to make sure there is a set which contains all these.<br><br><p><b>Axiom.</b> <i>[Axiom of infinity]<br>&nbsp;&nbsp;&nbsp; $(\\exists x) (\\emptyset \\in x \\wedge (\\forall y)(y\\in x \\Rightarrow y^+ \\in x))$.</i> </p> <br><br>A set is called a <i>successor set</i> if it has this property. We can define the intersection of all successor sets by taking any successor set $x$ and defining the set $S$ of all successor subsets of $x$ as subset of $\\mathcal{P}(x)$, and put $\\omega := \\bigcap S$. Then by definition \\[&nbsp;&nbsp;&nbsp; (\\forall x) [(x\\subset \\omega \\wedge x \\text{ is a successor set}) \\Rightarrow x = \\omega].\\] This is genuine induction!</div><div></div>"
  },
  {
    "front": "ZF: Axiom of foundation/regularity.",
    "back": "<div>We say $y\\in x$ is <i>$\\in$-minimal in $x$</i> if $z\\in x \\Rightarrow z\\not\\in y$.<br><p><b>Axiom.</b> <i>[Axiom of foundation]<br>&nbsp;&nbsp;&nbsp; Every set has an $\\in$-minimal member.</i> </p> </div><div></div>"
  },
  {
    "front": "ZF: Classes, function classes, and axiom of replacement.",
    "back": "<div>A <i>class</i> is a formula with a single free variable, we think of it as $\\left\\{ x\\colon p(x) \\right\\} $ (where $p(x) = p[x/*]$ where $*$ is the free variable). This is a <i>function class</i> if $p$ is of the form \\[p(x) = (\\exists y)(\\exists z)(x = (y,z) \\wedge q(y,z)],\\\\\\] where $q$ is a formula with two free variables satisfying $q(y,z)\\wedge q(y,z') \\Rightarrow z = z'$.<br><br><p><b>Axiom.</b> <i>[Axiom of replacement]<br>&nbsp;&nbsp;&nbsp; If $p$ is a function class (equivalently if $q$ has the above property) and $x$ a set, then the image of $x$ under $p$ is a set, i.e. \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (\\exists y)(\\forall z)[z\\in y \\Leftrightarrow (\\exists t)(t\\in x \\wedge q(t,z))].&nbsp;&nbsp;&nbsp; \\]</i> </p> </div><div></div>"
  },
  {
    "front": "Three easy criteria for a continuous-time Markov chain to be non-explosive.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X \\sim M(\\nu,Q)$ be a continuous-time Markov chain with state space $S$. If any of the following hold, $X$ is non-explosive.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$S$ is finite,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_i q_i &lt; \\infty$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$X_0 = i$ and $i$ is recurrent (for $X$ or the jump chain).&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) implies (ii). One way to obtain $X$ is to take $E_i \\sim \\text{Exp}(1)$ i.i.d. and let $X$ follow the jumps of $Y$ with waiting times $T_i = E_i / q_{Y_i}$. Then \\[&nbsp;&nbsp;&nbsp; T_\\infty \\ge \\frac{1}{\\sup q_i} \\sum_{i=1}^\\infty E_i = \\infty.\\] If (iii) holds, then $T_\\infty$ is lower bounded by an infinite sum of i.i.d. exponentials (the holding times in state $i$), which is infinite.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of invariant measure for a continuous-time MC and characterisation with $Q$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A measure $\\xi = (\\xi_i)_{i\\in S}$ on $S$ is called <i>invariant</i> for $Q$ if $\\xi P(t) = \\xi$ for all $t\\ge 0$, equivalently if $\\xi Q = 0$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} (\\xi P(t)) = \\xi Q P(t) = \\xi P(t) Q.&nbsp;&nbsp;&nbsp; \\] If $\\xi$ is invariant, then $\\text{LHS} = 0$ and $\\text{RHS} = \\xi Q$, and if $\\xi Q = 0$ then $\\text{RHS} = 0$, so $\\xi P(t) = \\xi P(0) = \\xi$.</i> </p> </div><div></div>"
  },
  {
    "front": "Positive recurrence is a class property for continuous-time MC's, and connection with invariant measures. (No proof.)",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Positive recurrence is a class property. If $Q$ is irreducible, then it is positive recurrent iff it has an invariant distribution, in which case \\[&nbsp;&nbsp;&nbsp; \\xi_i = \\frac{1}{m_i q_i},\\quad i\\in S,\\\\&nbsp;&nbsp;&nbsp; \\] and every invariant measure is a multiple of this. In partiuclar, invariant distributions are unique if they exist.</p> </div><div></div>"
  },
  {
    "front": "Convergence to equilibrium and ergodic theorem for continuous-time Markov chains.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $Q$ be irreducible and positive recurrent, and $\\xi$ the unique invariant distribution. Then, for $X \\sim M(\\nu,Q)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\nu P(t) \\stackrel{  }{\\longrightarrow} \\xi,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] (entry-wise), and \\[&nbsp;&nbsp;&nbsp; \\frac{1}{t} \\int_0^t \\boldsymbol{1}_{\\left\\{ X_s = i \\right\\} }\\mathop{}\\!\\mathrm{d} s \\stackrel{  }{\\longrightarrow} \\xi_i\\\\&nbsp;&nbsp;&nbsp; \\] a.s. as $t\\to \\infty$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $X' \\sim M(\\xi,Q)$, and $Y := (X,X')$. Then $Y$ is a continuous-time Markov chain on $S\\times S$ with stationary distribution $\\xi \\otimes \\xi$, in particular recurrent, so $T:= \\inf \\left\\{ t \\ge 0\\colon X_t = X_t' \\right\\} \\le T_{(s,s)} &lt; \\infty$ a.s. (for any fixed $s\\in S$). Put \\[&nbsp;&nbsp;&nbsp; Z_t := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_t &amp;, t \\le T,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_t'&amp;, t &gt; T.&nbsp;&nbsp;&nbsp; \\end{cases}\\] By the strong Markov property, $Z\\sim M(\\nu,Q) \\sim X$, so \\[\\mathbb{P}(X_t = j) = \\mathbb{P}(Z_t = j) = \\underbrace{\\mathbb{P}(T\\le t)}_{\\to 1}&nbsp; \\underbrace{\\mathbb{P}(X_t' = j)}_{= \\xi_j} + \\underbrace{\\mathbb{P}(T&gt;t)}_{\\to 0} \\mathbb{P}(X_t = j) \\stackrel{  }{\\longrightarrow} \\xi_j.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Definition and transition probabilities of time-reversal of an irreducible pos. recurrent Markov chain. Reversability and detailed balance equations.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $(Y_m)_{0\\le m \\le n}$ is an irreducible Markov chain with (and started at) stationary distribution $\\mu$, then $\\widehat{Y}:= (Y_{n-m})_{0\\le m\\le n}$ is also an irreducible Markov chain with stationary distribution $\\mu$ and transition probabilities \\[&nbsp;&nbsp;&nbsp; \\widehat{\\pi}_{ij} = \\xi_j \\pi_{ji} / \\xi_i,\\quad i,j\\in S.\\]</p> <br><br>$Y$ is said to be <i>reversible</i> if $\\widehat{Y} \\stackrel{d}{=} Y$, that is if it satisfies the <i>detailed balance equations</i> \\[\\mu_j \\pi_{ji} = \\mu_i \\pi_{ij},\\quad i,j\\in S.\\] If $\\mu$ satisfies these, then it is automatically a stationary measure. If it has infinite mass, then $Y$ is null-recurrent.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P} \\left( \\widehat{Y}_{0} = i_0, \\ldots ,\\widehat{Y}_n = i_n \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P} \\left( Y_0 = i_n, \\ldots ,Y_n = i_0 \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu_{i_n} \\pi_{i_n i_{n-1}}\\cdot \\ldots\\cdot&nbsp; \\pi_{i_1i_0}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu_{i_0} \\widehat{\\pi}_{i_0i_1}\\cdot \\ldots \\cdot \\widehat{\\pi}_{i_{n-1}i_n}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $\\mu$ satisfies the balance equations, then summing over $i$ gives $\\mu_j = \\sum_i \\mu_i \\pi_{ij}$. If $\\mu$ has infinite mass, it cannot be positive recurrent (because then every stationary measure is a constant multiple of the invariant distribution).</i> </p> <br><div></div></div>"
  },
  {
    "front": "Definition, $Q$-matrix and transition probabilities of time-reversal of an irreducible pos. recurrent continuous-time Markov chain. Reversability and detailed balance equations.",
    "back": "<div>If $Q$ is irreducible with stationary distribution $\\xi$, and $X \\sim M(\\xi,Q)$, fix $t &gt; 0$ and define $\\widehat{X} := (X_{(t-s)-})_{0\\le s \\le t}$ (so that $\\widehat{X}$ is right-continuous).<br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; $\\widehat{X}$ is also an irreducible positive recurrent continuous-time Markov chain with stationary distribution $\\xi$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{q}_{ij} = \\xi_j q_{ji} / \\xi_i,\\qquad \\widehat{p}_{ij}(t) = \\xi_j p_{ji}(t) / \\xi_i,\\quad i,j\\in S.&nbsp;&nbsp;&nbsp; \\] </p> <br><br>$X$ is said to be <i>reversible</i> if $X \\stackrel{d}{=} \\widehat{X}$, i.e. if the <i>detailed balance equations</i> \\[\\xi_j q_{ji} = \\xi_i q_{ij},\\quad i,j\\in S,\\\\\\] are satisfied. Conversely, if they are, then $\\xi$ is an invariant measure. If it has infinite mass, $X$ is null-recurrent.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First confirm that $\\widehat{Q}$ is a $Q$-matrix with transition probabilities $\\widehat{P}$. Off-diagonal elements are non-negative, and $\\sum_{j\\in S} \\widehat{q}_{ij} = \\frac{1}{\\xi_i} \\sum_{j\\in S} \\xi_j q_{ji} = 0$. Furthermore, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{p}_{ij}\\,'(t) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\xi_j p_{ji}'(t) / \\xi_i = \\xi_j \\sum_{k\\in S} q_{jk} p_{ki}(t) / \\xi_i = \\sum_{k\\in S} \\widehat{q}_{kj} \\widehat{p}_{ik}(t),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; i.e. $\\widehat{P}\\, '(t) = \\widehat{P}(t) \\widehat{Q}$. Now we confirm that $\\widehat{X} \\sim M(\\xi,\\widehat{Q})$ by finding its fidis. <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}\\left( \\widehat{X}_{t_1} = i_1, \\ldots ,\\widehat{X}_{t_n} =i_n \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( X_{(t-t_n)-}=i_n, \\ldots ,X_{(t-t_1)-} = i_1 \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\xi_{i_n} p_{i_n i_{n-1}}(t-t_n) \\cdot \\ldots \\cdot p_{i_2 i_1}(t_2-t_1)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\xi_{i_1} \\widehat{p}_{i_1i_2}(t_2-t_1)\\cdot \\ldots \\cdot \\widehat{p}_{i_{n-1}i_n}(t-t_n).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br><br>&nbsp;&nbsp;&nbsp; Finally, if $\\xi$ satisfies the balance equations, then summing $i$ gives $0 = \\sum_{i\\in S} \\xi_i q_{ij}$, so $\\xi$ is invariant. If $\\xi$ has infinite mass then $X$ must be null-recurrent because otherwise every invariant measure is multiple of the unique stationary distribution (and so has finite mass).</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of omega ordinals (and proof that they are exactly the initial ordinals).",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Define $\\omega_\\alpha$ for ordinals $\\alpha$ by<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\omega_0 = \\omega$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\omega_{\\alpha + 1} = \\gamma(\\omega_\\alpha)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\omega_\\lambda = \\sup \\left\\{ \\omega_\\alpha\\colon \\alpha &lt; \\lambda \\right\\} $.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; Call an ordinal <i>initial</i> if it does not biject with any smaller ordinal. Then the omega ordinals are <i>exactly</i> the infinite initial ordinals.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; All are initial by induction: $\\omega_0$ does not biject with finite sets, $\\gamma(\\omega_\\alpha)$ is initial by definition of $\\gamma(\\cdot )$, and if $\\lambda$ is a limit ordinal and $\\omega_\\lambda \\sim \\beta$ for some $\\beta &lt; \\omega_\\lambda$, then there is $\\alpha &lt; \\lambda$ with $\\beta &lt; \\omega_\\alpha &lt; \\omega_\\lambda$, so $\\beta \\sim \\omega_\\alpha$, contradiction.<br><br>&nbsp;&nbsp;&nbsp; Now let $\\delta$ be an infinite initial ordinal, and $\\alpha$ the smallest ordinal for which $\\omega_\\alpha \\ge \\delta$ (exists bc the omega ordinals are unbounded). Done if $\\alpha = 0$. If $\\alpha = \\beta^+$, then $\\delta &lt; \\omega_\\alpha = \\gamma(\\omega_\\beta)$ would imply $\\delta \\le \\omega_\\beta$, a contradiction. If $\\alpha$ is a limit and $\\delta &lt; \\omega_\\alpha$, then there is $\\beta &lt; \\alpha$ with $\\delta &lt; \\omega_\\beta$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of cardinals and cardinal arithmetic.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; For a set $x$, $\\text{card}(x)$ is the smallest ordinal which bijects with $x$. This is always an initial ordinal, so every set has cardinality $\\aleph_\\alpha = \\text{card}(\\omega_\\alpha)$ for some ordinal $\\alpha$. Define sum, product, and exponentiation of ordinals in the obvious way, then $m + n = n + m$, $mn = nm$, and $(m^n)^p = m^{np}$.</p> </div><div></div>"
  },
  {
    "front": "What are $\\aleph_\\alpha \\aleph_\\alpha$, $\\aleph_\\alpha + \\aleph_\\beta$, and $\\aleph_\\alpha \\aleph_\\beta$?",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; For every ordinal $\\alpha$, \\[&nbsp;&nbsp;&nbsp; \\aleph_\\alpha \\aleph_\\alpha= \\aleph_\\alpha.&nbsp;&nbsp;&nbsp; \\] In particular, if $\\alpha \\le \\beta$, then $\\aleph_\\alpha + \\aleph_\\beta = \\aleph_\\alpha \\aleph_\\beta = \\aleph_\\beta$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume that $\\alpha \\neq 0$. Obviously $\\omega_\\alpha$ injects into $\\omega_\\alpha \\times \\omega_\\alpha$. To show the opposite, it suffices to define a well-order on the latter set of order type at most $\\omega_\\alpha$. Define an order of the form $(x,y) &lt; (x',y')$ if $\\max(x,y) &lt; \\max(x',y')$, and if they are equal we order by $y$ first (say). This is a well-order because we can first choose a smallest $\\max$, so we are left with the edge of a square, which looks like (an initial segment of) $\\alpha$. Now it STS that every proper initial segment has order type $&lt; \\omega_\\alpha$. For any $x,y\\in \\omega_\\alpha$ we have $I_{(x,y)}\\subset \\beta\\times \\beta$ for $\\beta := I_{\\max(x,y)^+} &lt; \\omega_\\alpha$ (because $\\omega_\\alpha$ is a limit), and by induction hypothesis (we may assume $\\beta$ is infinite because $\\alpha \\neq 0$), $\\beta\\times \\beta \\sim \\beta$,so $I_{(x,y)}$ has order type smaller than $\\omega_\\alpha$ (otherwise its cardinality was at least $\\aleph_\\alpha$).<br><br>&nbsp;&nbsp;&nbsp; Finally, if $\\alpha \\le \\beta$, then \\[&nbsp;&nbsp;&nbsp; \\aleph_\\beta \\le \\aleph_\\beta + \\aleph_\\alpha \\le \\aleph_\\beta + \\aleph_\\beta \\le \\aleph_\\beta\\aleph_\\beta = \\aleph_\\beta.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Cardinality of $\\mathbb{R}^{\\mathbb{N}}$ and continuum hypothesis.",
    "back": "<div>We have \\[&nbsp;&nbsp;&nbsp; \\text{card}(\\mathbb{R} ^{\\mathbb{N}}) = (2^{\\aleph_0})^{\\aleph_0} = 2^{\\aleph_0 \\aleph_0}= 2^{\\aleph_0} = \\text{card}(\\mathbb{R}).\\] The continuum hypothesis is $2^{\\aleph_0} = \\aleph_1$. It is independent of ZFC.</div><div></div>"
  },
  {
    "front": "Definition renewal process, $m(t)$ (with representation and local boundedness), and renewal equation.",
    "back": "<div>A <i>renewal process</i> with i.i.d. inter-arrival times $Z_k$ is \\[X_t = \\left| \\left\\{ k\\in \\mathbb{N}\\colon T_k \\le t \\right\\}&nbsp; \\right| = \\sum_{k=1}^\\infty \\boldsymbol{1}_{\\left\\{ T_k \\le t \\right\\} },\\\\\\] where $T_k = \\sum_{i=1}^k Z_k$.<br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; $m(t) = \\sum_{k=1}^\\infty F_k(t)\\lesssim \\mathrm{e}^t$, where $F_k = F_{k-1} \\star f$, and $m$ is the unique locally bounded solution to the <i>renewal equation</i> \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; m = F + m\\star f.&nbsp;&nbsp;&nbsp; \\] (Here $f$ must not be a pdf but just a probability measure on $(0,\\infty)$.)</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We have $X_t = \\sum_{k=1}^\\infty \\boldsymbol{1}_{\\left\\{ T_k \\le t \\right\\} }$, so the first equality is clear. Also, $T_k\\sim f^{\\star(k)}$, integrating gives cdf $F_k$. Now, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; F_k(t)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathbb{P}\\left( T_k \\le t \\right) \\le \\mathrm{e}^t \\mathbb{E} \\left[ \\mathrm{e}^{-T_k} \\right] = \\mathrm{e}^t \\mathbb{E} \\left[ \\mathrm{e}^{-Z_1} \\right]^k,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so putting $\\theta := \\mathbb{E} \\left[ \\mathrm{e}^{-Z_1} \\right] \\in (0,1)$, $m(t) \\le \\mathrm{e}^t \\frac{\\theta}{1 - \\theta}$.<br><br>&nbsp;&nbsp;&nbsp; To prove the renewal equation, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; m(t) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^t \\mathbb{E} \\left[X_t \\,\\middle\\vert\\, T_1 = s\\right] \\mathop{}\\!\\mathrm{d} f(s) = \\int_0^t \\left( 1 + \\mathbb{E} \\left[ X_{t-s} \\right] \\right) \\mathop{}\\!\\mathrm{d} f(s)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= F(t) + \\int_0^t m(t-s) \\mathop{}\\!\\mathrm{d} f(s).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $\\widetilde{m}$ is another locally bounded solution, then $\\alpha := m - \\widetilde{m}$ satisfies $\\alpha = \\alpha \\star f$, so<br>&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{k=1}^\\infty \\left| \\alpha(t) \\right| =\\sum_{k=1}^\\infty\\left|\\int_0^t \\alpha(t-s) \\mathop{}\\!\\mathrm{d} f^{\\star(k)}(s)\\right| \\le m(t) \\sup_{s\\le t} \\left| \\alpha(s) \\right| &lt; \\infty\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $\\alpha(t) = 0$.</i> </p> </div><div></div>"
  },
  {
    "front": "Strong law and CLT of renewal theory, and elementary renewal theorem.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $X$ be a renewal process with $\\mu := \\mathbb{E} \\left[ Z_1 \\right] \\in (0,\\infty)$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\displaystyle\\frac{X_t}{t} \\stackrel{ a.s. }{\\longrightarrow} \\frac{1}{\\mu},\\quad t \\to \\infty.$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\displaystyle&nbsp;\\frac{m(t)}{t} \\stackrel{  }{\\longrightarrow} \\frac{1}{\\mu},\\quad t\\to \\infty.$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $Z_1$ is square-integrable and $\\sigma^2 := \\mathbb{V}(Z_1) \\in (0,\\infty)$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widetilde{X_t}\\stackrel{ d }{\\longrightarrow} \\mathcal{N}(0,1),\\quad t\\to \\infty.&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (SLLN) essentially follows from $\\frac{T_n}{n} \\stackrel{ a.s. }{\\longrightarrow} \\mu$.</i> </p> </div><div></div>"
  },
  {
    "front": "Renewal theory: Age and excess lifetime processes, Renewal property (vanilla and for delayed processes).",
    "back": "<div>A renewal process where $Z_1$ is allowed to have a different distribution then $X_2,X_3, \\ldots$ is called a <i>delayed</i> renewal process. For $t\\ge 0$, define <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; A_t &amp;:= t -T_{X_t} \\text{ (time since last arrival)},\\\\&nbsp;&nbsp;&nbsp; E_t &amp;:= T_{X_t + 1} - t \\text{ (time until next arrival)}.\\end{align*}[/$$]<br>$(A_t)$ is called the <i>age process</i> and $(E_t)$ the <i>excess lifetime process</i>. Note that $(E_t)$ is a Markov process which decreases linearly and jumps up by $Z_1$ whenever it hits zero.<br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a renewal process, and $T$ a jump-time-valued stopping time (f.ex. just $T_k$), then $(X_{T+s}-X_T)$ has the same distribution as $X$ and is independent of $(X_s)_{s\\le T}$.</p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a (delayed) renewal process and $t \\ge 0$, then $(X_{t+s}-X_t)_{s\\ge 0}$ is a delayed renewal process with $\\widetilde{Z}_1 = E_t$ (but not necessarily independent of $\\mathcal{F}_t$). In particular, if the distribution of $E_t$ is independent of $t$, then $X$ has stationary increments.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Obvious.</i> </p> </div><div></div>"
  },
  {
    "front": "Renewal theory: Size-biased distributions, and limit of $(A_t,E_t)$ as $t\\to \\infty$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; The <i>size-biased</i> distribution associated with a probability $f$ on $(0,\\infty)$ with finite mean $\\mu$ is $\\mathop{}\\!\\mathrm{d} \\widehat{f}(t) := \\frac{1}{\\mu} t \\mathop{}\\!\\mathrm{d} f(t)$. If $L\\sim \\widehat{f}, $$U\\sim \\mathcal{U}(0,1)$, $U\\perp \\!\\!\\! \\perp L$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(LU &gt; x) = \\frac{1}{\\mu} \\int_x^\\infty (y-x)\\mathop{}\\!\\mathrm{d} f(y).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Condition on $L$.</i> </p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; For every $x \\ge 0$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{P}(E_t \\ge x) \\stackrel{  }{\\longrightarrow} \\frac{1}{\\mu}\\int_x^\\infty (y-x) \\mathop{}\\!\\mathrm{d} f(y).&nbsp;&nbsp;&nbsp; \\] That is, $E_t \\stackrel{ d }{\\longrightarrow} LU$, where $L$ is the size-biased distribution corresponding to $Z_1$, and $U\\sim \\mathcal{U}(0,1),\\,U \\perp \\!\\!\\! \\perp L$. In fact, \\[&nbsp;&nbsp;&nbsp; (A_t,E_t) \\stackrel{ d }{\\longrightarrow} (L(1-U),LU).&nbsp;&nbsp;&nbsp; \\] </p> </div><div></div>"
  },
  {
    "front": "Associated stationary process of a renewal process and convergence to it (renewal theorem).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a renewal process with finite inter-arrival mean, and $\\widetilde{X}$ is a delayed version with $Z_1 \\stackrel{d}{=} LU$, then $E_t \\stackrel{d}{=} LU$ for all $t\\ge 0$, in particular $\\widetilde{X}$ has stationary increments. Furthermore, if $X$ also has a continuous inter-arrival distribution, then for any fixed $s \\ge 0$, \\[&nbsp;&nbsp;&nbsp; X_{t+s} - X_t \\stackrel{ d }{\\longrightarrow} \\widetilde{X}_s,\\quad t\\to \\infty,\\\\&nbsp;&nbsp;&nbsp; \\] and \\[&nbsp;&nbsp;&nbsp; m(t+s) - m(t) \\stackrel{  }{\\longrightarrow} \\frac{s}{\\mu},\\quad t \\to \\infty.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Key renewal theorem.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $X$ be a renewal process with continuous inter-arrival distribution of finite mean $\\mu$. If $h\\colon [0,\\infty)\\to [0,\\infty)$ is integrable and decreasing, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (h\\star m')(t) \\stackrel{  }{\\longrightarrow} \\frac{1}{\\mu} \\left\\|h\\right\\|_{L^1},\\quad t\\to \\infty.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Definition convex hull of a set and characterisation.",
    "back": "<div>The <i>convex hull</i> $\\operatorname{conv} A$ of $A\\subset \\mathbb{R}^d$ is the intersection of all convex supersets.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; $\\operatorname{conv} A = \\left\\{ \\sum_{i=1}^k \\lambda_i x_i\\colon k\\in \\mathbb{N}, \\lambda_i \\ge 0, \\sum_i \\lambda_i = 1, x_i \\in A \\right\\} $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The RHS is a convex superset of $A$, so $\\subset $. On the other hand, if $C\\supset A$ is convex, then it must contain all convex combinations of elements of $A$, so it must contain the RHS.</i> </p> </div><div></div>"
  },
  {
    "front": "Minkowski combinations of sets. How do they interact with convexity, and how do $(\\alpha+\\beta)A$, $\\alpha A+\\beta A$ and convexity of $A$ relate?",
    "back": "<div>If $A,B\\subset \\mathbb{R}^d$, and $\\alpha,\\beta\\in \\mathbb{R}$, then \\[\\alpha A + \\beta B := \\left\\{ \\alpha x + \\beta y\\colon x\\in A,y\\in B \\right\\} .\\] <br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $A$ and $B$ are convex, then so is $\\alpha A + \\beta B$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$(\\alpha + \\beta) A \\subset \\alpha A + \\beta A$. The converse is true for all $\\alpha,\\beta \\in \\mathbb{R}$ iff $A$ is convex.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $\\mu+\\lambda = 1$, then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu (\\alpha x_1+\\beta y_1) + \\lambda (\\alpha x_2 + \\beta y_2) = \\alpha \\underbrace{(\\mu x_1 + \\lambda x_2)}_{\\in A} + \\beta \\underbrace{(\\mu y_1 + \\lambda y_2)}_{\\in B} \\in \\alpha A + \\beta b.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\subset $ is clear. Now suppose $A$ is convex, $x,y\\in A$. Then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\alpha x + \\beta y = (\\alpha + \\beta) \\underbrace{\\left(\\frac{\\alpha}{\\alpha + \\beta}x + \\frac{\\beta}{\\alpha + \\beta}y\\right)}_{\\in A}&nbsp; \\in (\\alpha+\\beta) A.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; On the other hand if $\\supset$ holds and $x,y\\in A$, $\\mu + \\lambda = 1$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu x + \\lambda y \\in \\mu A + \\lambda A = (\\mu+\\lambda)A = A.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "(Pre-)images of convex sets under affine maps are convex.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ is affine and $A\\subset \\mathbb{R}^n$ and $B\\subset \\mathbb{R}^m$ are convex, then so are $f(A)$ and $f^{-1}(B)$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\mu + \\lambda = 1$ and $x,y\\in A$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu f(x) + \\lambda f(y) = f(\\mu x + \\lambda y) \\in f(A).&nbsp;&nbsp;&nbsp; \\] If $x,y\\in f^{-1}(B)$, then \\[&nbsp;&nbsp;&nbsp; f(\\mu x + \\lambda y) = \\mu f(x) + \\lambda f(y) \\in B,\\\\\\] so $\\mu x + \\lambda y \\in f^{-1}(B)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of polyhedral sets, polytopes, simplices. Why are polytopes closed (hence compact)?",
    "back": "<div>A <i>polyhedral set</i> is the intersection of finitely many closed half spaces.<br><br>A <i>polytope</i> is the convex hull of finitely many points. It is called an <i>$r$-simplex</i> if it is the convex hull of $r + 1$ affinely independent points.<br><br><p><b>Lemma.</b> <i>&nbsp; &nbsp; Polytopes are compact.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; STS that $P$ is closed. Let $P = \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} $ and $p_n = \\sum_{i=1}^k \\lambda^{(n)}_i x_i \\in P$ for $n\\in \\mathbb{N}$ and suppose $p_n \\to p\\in \\mathbb{R}^d$. We find a subsequence along which $\\lambda_i^{(n)} \\to \\lambda_i \\in [0,1]$ for each $i\\in [k]$, so along that subsequence $p_n \\to \\sum_{i=1}^k \\lambda_i x_i \\in P$, so $p\\in P$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition and characterisation of vertices of a polytope, and $P = \\operatorname{conv} (\\operatorname{cvert} P)$.",
    "back": "<div>If $P$ is a polytope then $x\\in P$ is called a <i>vertex</i> of $P$ is $P\\setminus \\left\\{ x \\right\\} $ is convex.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $P = \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} $ for distinct $x_i\\in \\mathbb{R}^d$, then $x_1\\in \\operatorname{cvert} P$ iff $x_1\\not\\in \\operatorname{conv}\\left\\{ x_2, \\ldots ,x_k \\right\\} $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\implies$\":} If $x_1\\in \\operatorname{cvert} P$ then $P\\setminus \\left\\{ x_1 \\right\\} $ is convex, so it contains $\\operatorname{conv} \\left\\{ x_2, \\ldots ,x_k \\right\\} $.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\impliedby$\":} Suppose $x_1\\not\\in \\operatorname{cvert} P$, so there are $y,z \\in P\\setminus \\left\\{ x_1 \\right\\} $ and $\\mu + \\lambda = 1$ such that $\\mu y + \\lambda z = x_1$. Now $y$ and $z$ are convex combinations of $x_1, \\ldots ,x_k$ with less than full weight on $x_1$ (otherwise $y = z = x_1$, a contradiction), so the LHS above is $(1-a)x_1 + X$ where $X$ is a combination of $x_2, \\ldots ,x_k$ with total weight $a \\in (0,1)$, rearranging gives $x_1 = X / a$, where the RHS is now a convex combination of $x_2, \\ldots ,x_k$.</i> </p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $P$ is a polytope, then $P = \\operatorname{conv} (\\operatorname{cvert} P)$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $P$ is convex and contains $\\operatorname{cvert} P$, so $\\supset $ is clear. For the other direction let $x_1, \\ldots ,x_k$ be distinct points such that $P = \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} $.&nbsp; If $x_1\\not\\in \\operatorname{cvert} P$, then $x_1 \\in \\operatorname{conv} \\left\\{ x_2, \\ldots ,x_k \\right\\} $, so $P = \\operatorname{conv} \\left\\{ x_2, \\ldots ,x_k \\right\\} $. This way we can successively remove points that are not vertices until we are left with a subset which still produces all of $P$ but consists only of vertices.<br><br>&nbsp;&nbsp;&nbsp; (In fact those are all vertices. Indeed, if we had not $\\operatorname{cvert} P \\subset&nbsp; \\left\\{ x_1, \\ldots ,x_k \\right\\} $, then $P = \\operatorname{conv} \\left\\{ x,x_1, \\ldots ,x_k \\right\\} $, so $x\\not\\in \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} = P$.)</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Characterisation of a simplex.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; A convex set $A\\subset \\mathbb{R}^d$ is a ($k$-)simplex iff there are $x_0, \\ldots ,x_k\\in A$ such that every $x\\in A$ is a unique convex combination of $x_0, \\ldots ,x_k$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $A = \\operatorname{conv} \\left\\{ x_0, \\ldots ,x_k \\right\\} $ is a simplex, then $x_i$ are affinely independent so convex combinations are unique. Conversely if the statement holds, then clearly $A = \\operatorname{conv} \\left\\{ x_0, \\ldots ,x_k \\right\\} $, and since all convex combinations are unique, $x_i$ must be affinely independent.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Radon's and Helly's theorems",
    "back": "<div><p><b>Theorem.</b> <i>[Radon]<br>&nbsp;&nbsp;&nbsp; If $x_1, \\ldots ,x_n$ are affinely independent, then there is a partition $I \\dot J = [n]$ such that \\[&nbsp;&nbsp;&nbsp; \\operatorname{conv} x_I \\cap \\operatorname{conv} x_J \\neq \\emptyset\\\\&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; There are $\\alpha_i \\in \\mathbb{R}$ such that \\[&nbsp;&nbsp;&nbsp; \\sum_i \\alpha_i = 0,\\qquad \\sum_i \\alpha_i x_i = 0.\\] Now let $I = \\left\\{ i\\colon \\alpha_i \\ge 0 \\right\\} $ and $J = [n] \\setminus I$.</i> </p> <br><br><p><b>Theorem.</b> <i>[Helly]<br>&nbsp;&nbsp;&nbsp; If $(A_i)$ is a family of convex sets in $\\mathbb{R}^d$ (if they are compact it may be infinite) such that every $d+1$ of them have non-empty intersection, then \\[&nbsp;&nbsp;&nbsp; \\bigcap_i A_i \\neq \\emptyset\\\\&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For compact sets, finite intersections being non-empty implies full intersection being non-empty, so suppose there are $\\infty&gt;m &gt; d + 1$ sets (if $m \\le d+1$ there is nothing to show), and assume the claim is already true for smaller $m$. Then choose points \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x_i \\in A_1 \\cap \\ldots \\cap \\check{A_i} \\cap\\ldots \\cap A_m,\\\\&nbsp;&nbsp;&nbsp; \\] which exist by hypothesis. Since $m \\ge d+2$ these are affinely dependent, so take $I \\dot J = [m]$ according to Radon's theorem, and let $x_0 \\in \\operatorname{conv} x_I \\cap \\operatorname{conv} x_J$. If $i\\in I$ then $x_J \\subset A_i$ and vice versa, so $x_0\\in A_k$ for all $k\\in [m]$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Carathéodory's theorem (characterisation of convex hull with simplices)",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $x\\in \\mathbb{R}^d$ is a convex combination of $x_1, \\ldots ,x_n$, then there is an affinely independent subset of which $x$ is also a convex combination.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose they are not yet affinely independent, we show that we can remove one of them. Suppose $x = \\sum_i \\alpha_i x_i$ with $\\alpha_i \\in (0,1]$ and $\\sum_i \\alpha_i = 1$, and that $\\beta_i \\in \\mathbb{R}$ with $\\sum_i \\beta_i x_i = 0$ and $\\sum_i \\beta_i = 0$. Then choose $c &gt; 0$ such that $\\alpha_i + c \\beta_i \\ge 0$ for all $i$ and $\\alpha_{i_0} + c \\beta_{i_0} = 0$ for some index (among those with $\\beta_i &lt; 0$, the one for which $\\left| \\alpha_i / \\beta_i \\right| $ is smallest), so that $x = \\sum_i (\\alpha_i + c\\beta_i) x_i$ is a convex combination using one less vertex.</i> </p> <br><br><p><b>Theorem.</b> <i>[Carathéodory]<br>&nbsp;&nbsp;&nbsp; If $A\\subset \\mathbb{R}^d$ and $x\\in \\mathbb{R}^d$, then $x\\in \\operatorname{conv} A$ iff there is a simplex with vertices in $A$ containing $x$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \"$\\impliedby$\" is obvious. If $x\\in \\operatorname{conv} A$ then it is a convex combination of vertices in $A$, which we can choose affinely independent by the lemma. Then the polytope spanned by those is the desired simplex.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Definitions of $\\dim A$, $\\operatorname{aff} A$, $\\operatorname{cl} A$, $\\operatorname{relint} A$ for $A\\subset \\mathbb{R}^d$. Characterisation of $\\operatorname{relint} A$ for a simplex $A$. In particular, $\\operatorname{relint} A \\neq \\emptyset$ for a convex set $A\\neq \\emptyset$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\operatorname{aff}(A)$ is the intersection of all affine spaces containing $A$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\dim A = \\dim (\\operatorname{aff} A) $ is the largest $k\\in \\mathbb{N}_0$ for which there are affinely independent $x_0, \\ldots ,x_k\\in A$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\operatorname{cl} A$ is the closure,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\operatorname{relint} A$ is the interior w.r.t. $\\operatorname{aff} A$ as ambient space.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $A = \\operatorname{conv} \\left\\{ x_0, \\ldots ,x_k \\right\\} $ is a simplex, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{relint} A = \\left\\{ \\sum_{i=0}^k \\alpha_i x_i\\colon \\alpha_i\\in (0,1), \\sum_i \\alpha_i = 1 \\right\\} .&nbsp;&nbsp;&nbsp; \\] In particular, if $B\\neq \\emptyset$ is convex, then $\\operatorname{relint} B \\neq \\emptyset$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The map \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Phi\\colon \\mathbb{R} ^{k} \\to \\text{lin} (x_1-x_0, \\ldots ,x_k-x_0);\\, \\beta \\mapsto \\sum_{i=1}^k \\beta_i (x_i-x_0)&nbsp;&nbsp;&nbsp; \\] is a homeomorphism, and $\\Phi^{-1}(A) = \\left\\{ \\beta \\in [0,1]^{k}\\colon \\sum_i \\beta_i = 1 \\right\\}$, which has interior $\\left\\{ \\beta \\in (0,1)^k\\colon \\sum_i \\beta_i = 1 \\right\\} $.<br><br>&nbsp;&nbsp;&nbsp; If $B\\neq\\emptyset$ is convex then it contains a simplex so we are done.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: If $A\\subset \\mathbb{R}^d$ is convex then when is $[y,x)\\subset \\operatorname{relint} A$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $A\\subset \\mathbb{R}^d$ is convex, $x\\in \\operatorname{cl} A$ and $y\\in \\operatorname{relint} A$ then $[y,x) \\subset \\operatorname{relint} A$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $\\dim A = d$. Let $y\\in V \\subset A$ be open, and $A\\ni x_k \\to x$, and $z = \\alpha y + (1-\\alpha) x$. Then if $y_k$ is such that $z = \\alpha y_k + (1-\\alpha) x_k$ then&nbsp; $y_k \\in V$ for large $k$, so $z \\in \\alpha V + (1-\\alpha)x_k \\subset A$ which is an open set.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Characterisation of $\\operatorname{cl} A$ and $\\operatorname{relint} A$ using rays $[x,y)$ for a convex set $A$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $A\\subset \\mathbb{R}^d$ is convex, then <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{cl} A &amp;= \\left\\{ x\\in \\mathbb{R}^d\\colon \\exists y\\in A\\colon [y,x) \\subset A \\right\\} ,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{relint} A &amp;= \\left\\{ x\\in \\mathbb{R}^d\\colon \\forall y\\in \\operatorname{aff}(A)\\setminus \\left\\{ x \\right\\} \\exists z\\in (x,y)\\colon [x,z] \\subset A \\right\\} \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left\\{ x\\in \\mathbb{R}^d\\colon \\forall y\\in \\operatorname{aff}(A) \\setminus \\left\\{ x \\right\\} \\exists z\\in A\\colon x\\in (z,y) \\right\\} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The RHS is dynamically called $B$, and we assume $\\dim(A) = d$ and $A\\neq \\emptyset$. First one is clear because $\\operatorname{relint} A \\neq \\emptyset$.<br><br>&nbsp;&nbsp;&nbsp; Suppose $x\\in \\operatorname{relint} A$ and $y\\neq x$, then take $z = \\varepsilon x + (1-\\varepsilon) y$ for small $\\varepsilon &gt; 0$. If $x \\in B$, take $y\\in \\operatorname{relint} A$, $y' = 2x-y$ (mirror over $x$), so $[x,z]\\subset A$ for a $z\\in (x,y')$, so $x \\in (z,y) \\subset \\operatorname{relint} A$.<br><br>&nbsp;&nbsp;&nbsp; Suppose $x\\in \\operatorname{relint} A$ and $y\\neq x$. Take $z = (1+\\varepsilon) x -\\varepsilon y$ for small $\\varepsilon &gt; 0$. If $x\\in B$, take $y\\in \\operatorname{relint} A$ so then there is $z\\in A$ with $x\\in (z,y) \\subset \\operatorname{relint} A$.</i> </p> </div><div></div>"
  },
  {
    "front": "If $A\\subset \\mathbb{R}^d$ is convex, then so are $\\operatorname{relint} A$ and $\\operatorname{cl} A$.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $x,y\\in \\operatorname{relint} A$ then $[x,y) \\subset \\operatorname{relint} A$. Suppose $x_1,x_2\\in \\operatorname{cl} A$. Then there are $y_1,y_2\\in \\operatorname{relint} A$ such that $[y_1,x_1) \\subset A$ and $[y_2,x_2)\\subset A$. If $x_\\alpha = \\alpha x_1 + (1-\\alpha)x_2$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [y_\\alpha,x_\\alpha) = \\alpha [y_1,x_1) + (1-\\alpha) [y_2,x_2) \\subset A,\\\\&nbsp;&nbsp;&nbsp; \\] so $x_\\alpha \\in \\operatorname{cl} A$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: \"$\\operatorname{relint} \\operatorname{cl} = \\operatorname{relint} $\" and \"$\\operatorname{cl} \\operatorname{relint}&nbsp; = \\operatorname{cl}$\" on convex sets.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $A\\subset \\mathbb{R}^d$ is convex then \\[&nbsp;&nbsp;&nbsp; \\operatorname{relint} A = \\operatorname{relint} \\operatorname{cl} A \\quad \\text{and}\\quad \\operatorname{cl} A = \\operatorname{cl} \\operatorname{relint} A.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $x\\in \\operatorname{relint} \\operatorname{cl} A$, and we take $y\\in \\operatorname{relint} A$, then there is $z\\in \\operatorname{cl} A$ such that $x\\in (z,y) \\subset \\operatorname{relint} A$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $x\\in \\operatorname{cl} A$ and $y\\in \\operatorname{relint} A$ then $[y,x) \\subset \\operatorname{relint} A$ so $x\\in \\operatorname{cl} \\operatorname{relint} A$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: The convex hull operation preserves (relative) openness and compactness.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $x = \\sum_i \\alpha_i x_i$ take $U$ neighbourhood of the origin such that $x_i + U \\subset A$ for all $i$, then $x + U \\subset&nbsp; \\operatorname{conv} A$. Now suppose $A$ is compact and let $\\varepsilon &gt; 0$. Then there is a finite set $F$ such that $A \\subset F + B_\\varepsilon(0)$, so $\\operatorname{conv} A \\subset \\operatorname{conv} F + B_\\varepsilon(0)$. $F$ is finite so $\\operatorname{conv} F$ is compact so there is another finite set $F'$ such that $\\operatorname{conv} F \\subset F' + B_\\varepsilon(0)$, so $\\operatorname{conv} A \\subset F' + B_{2\\varepsilon}(0)$, so $\\operatorname{conv} A$ is totally bounded.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: <i>Metric projection</i> $p(A,\\cdot )\\colon \\mathbb{R}^d \\to A$ for non-empty convex closed set $A$.<br><ol>  <li>How it can be used to separate $x\\in \\mathbb{R}^d\\setminus A$ from $A$ with a hyperplane,&nbsp;&nbsp;&nbsp;</li>  <li>$p(A,\\cdot )$ is a contraction.</li></ol>",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $A\\neq \\emptyset$ is closed and convex, then for every $x\\in \\mathbb{R}^d$ there is a unique $p(A,x)\\in A$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|p(A,x) - x\\right\\| = \\inf_{y\\in A} \\left\\|y-x\\right\\|.&nbsp;&nbsp;&nbsp; \\] Then $p(A,\\cdot )$ is contractive, and the hyperplane through $x$ orthogonal to $x-p(A,x)$ supports $A$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $y_k \\in A$ approaches the infimum then $(y_k) \\subset A \\cap B(0,N)$ for some large $N$ so WLOG converges to some $y\\in A$. Uniqueness follows because if $y_1,y_2\\in A$ achieve the minimum then by Pythagoras, $y := \\frac{1}{2} (y_1+y_2)$ would be even better.<br><br>&nbsp;&nbsp;&nbsp; If $x\\in \\mathbb{R}^d\\setminus A$ and $a\\in A$, then&nbsp; $F(t):=&nbsp; \\left\\|(1-t)p(x) + ta - x\\right\\|^2$ for $t\\in [0,1]$ has a global minimum at $t = 0$, so $F'(0) \\ge 0$ which implies<br>&nbsp;&nbsp;&nbsp; \\begin{equation}\\label{eq1}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\forall x\\in \\mathbb{R}^d, a\\in A\\colon \\left&lt;a-p(x), x-p(x) \\right&gt; \\le 0,<br>&nbsp;&nbsp;&nbsp; \\end{equation} (trivially true if $x\\in A$) and thus implies that \\[&nbsp;&nbsp;&nbsp; A \\subset \\left\\{ z\\in \\mathbb{R}^d\\colon \\left&lt;z-p(x),x-p(x) \\right&gt; \\le 0 \\right\\}.&nbsp;&nbsp;&nbsp; \\] Now let $x,y\\in \\mathbb{R}^d$. Then by Eq.\\~(\\ref{eq1}), $\\left&lt;p(y) - p(x),x-p(x) \\right&gt; \\le 0$ and vice versa, adding those and rearranging gives \\[\\left\\|p(y)-p(x)\\right\\|^2 \\le \\left&lt;p(y)-p(x),y-x \\right&gt; \\le \\left\\|p(y)-p(x)\\right\\|\\left\\|y-x\\right\\|.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Definition of supporting hyperplanes/halfspace/point, and (proper) separation of sets. Characterisation of a closed convex set through its supporting halfspaces.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A hyperplane $E = \\left\\{ f = \\alpha \\right\\} $ is a <i>supporting hyperplane</i> of a closed and convex set if $E\\cap A \\neq \\emptyset $ and $A\\subset \\left\\{ f\\le \\alpha \\right\\} $ or $A\\subset \\left\\{ f \\ge \\alpha \\right\\} $. The halfspace containing $A$ is a <i>supporting halfspace</i>, points in $A\\cap E$ are <i>supporting points</i>.<br><br>&nbsp;&nbsp;&nbsp; We say that $E$ <i>separates</i> two sets $A,B\\subset \\mathbb{R}^d$ if $A\\subset \\left\\{ f\\le \\alpha \\right\\} $ and $B\\subset \\left\\{ f\\ge \\alpha \\right\\} $ (or vice versa). It separates them <i>properly</i> if not both $A$ and $B$ are subsets of $E$.</p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; A closed convex set $A\\neq \\emptyset $ is the intersection of its supporting halfspaces.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It is obviously a subset, and if $x\\not\\in A$ then it can be separated from $A$ (using the metric projection).</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Support theorem (separating boundary points from a closed convex set).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $A$ is closed and convex and $x\\in \\operatorname{bd} A$ then there is a supporting hyperplane of $A$ that contains $x$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It suffices to find $y\\not\\in A$ with $p(y) = x$. Let $A\\not\\ni x_k \\to x$, and $y_k \\not\\in A$ with $\\left\\|y_k-A\\right\\| = 1$ and $p(y_k) = p(x_k)$ (by walking along the line from $p(x_k)$ to $x_k$). Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|x - p(y_k)\\right\\|= \\left\\|p(x) - p(x_k)\\right\\|\\le \\left\\|x-x_k\\right\\|\\to 0,\\\\&nbsp;&nbsp;&nbsp; \\] and $\\left\\|y_k - x\\right\\|\\le 1 + o(1)$, so WLoG $y_k \\to y \\not\\in A$, so $p(y) = \\lim p(y_k) = x$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Separation theorem (characterisation of proper separability of two convex sets).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $A,B\\subset \\mathbb{R}^d$ are non-empty and convex, then they can be properly separated iff $\\operatorname{relint} A\\cap \\operatorname{relint} B = \\emptyset $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \"$\\implies $\" is clear. For the converse, note that $0 \\not\\in&nbsp; \\operatorname{relint} A - \\operatorname{relint} B = \\operatorname{relint} (A-B)$. Hence (whether $0\\in \\operatorname{cl}(A-B)$ or not), there is a hyperplane $\\left\\{ f = 0 \\right\\} $ separating $0$ from $\\operatorname{cl}(A-B) \\subset \\left\\{ f\\le 0 \\right\\} $. Then, $f(a) \\le f(b)$ for $a\\in A$ and $b\\in B$, so $\\alpha := \\sup_{a\\in A} f(a) \\le \\inf_{b\\in B}f(b)$, so \\[&nbsp;&nbsp;&nbsp; A\\subset \\left\\{ f\\le \\alpha \\right\\} ,\\qquad B\\subset \\left\\{ f \\ge \\alpha \\right\\} .\\] If we first do this in the ambient space of $A \\cup B $, then this must be properly separating (otherwise $A\\cup B$ had not full dimension in its ambient space), and we can extend it by adding orthogonal dimensions to the hyperplane.</i> </p> </div><div></div>"
  },
  {
    "front": "A subset of a metric space is sequentially compact iff it is compact.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $M$ is a metric space, then $K\\subset M$ is compact iff it is sequentially compact.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\implies$\":} Suppose it was not, so that there was a sequence $(x_n)$ without accumulation points. Then for every $x\\in K$ there is a ball $B_x \\ni x$ such that $x_n \\in B_x$ for at most finitely many $n$. Then $K\\subset \\bigcup_{x\\in K} B_x$ can be reduced to a finite subcover, but then $x_n \\in K$ for only finitely many $n$, a contradiction.<br><br>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\impliedby$\":} Let $\\mathcal{C}$ be a family of open sets covering $K$ and assume there is no finite subcover. Since $K$ is sequentially compact it is totally bounded, so for every $n\\in \\mathbb{N}$ there is a ball $B_n = B(x_n, 1 / n)$ such that $B_n \\cap K$ cannot be covered by a finite subset of $\\mathcal{C}$. Then $x_n$ has an accumulation point $\\widehat{x}\\in K$, and there must be $\\widehat{x}\\in O \\in \\mathcal{C}$, but then $B_n \\subset O$ for sufficiently large $n$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "A finite-dimensional subspace of a normed space is closed.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $Y$ is a finite-dimensional subspace of a normed space $X$, then $Y$ is closed.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $b_1, \\ldots ,b_n \\in Y$ be a basis of $Y$, and let $\\widetilde{y} \\in \\mathbb{R}^n$ for $y\\in Y$ be the unique coefficient vector of $y$ (i.e. $y = \\sum_i \\widetilde{y}_i b_i$), and put $\\left\\|\\widetilde{y}\\right\\|:= \\left\\|y\\right\\|$, so that $(\\mathbb{R}^n,\\left\\|\\cdot \\right\\|)$ is a normed space, which must be complete. Then if $Y \\in y_n \\to x\\in X$, then $(y_n)$ is Cauchy, hence so is $(\\widetilde{y}_n)$, so $\\widetilde{y_n} \\to \\widetilde{y}\\in \\mathbb{R}^n$, and if $y:= \\sum_i \\widetilde{y}_i b_i\\in Y$ then \\[&nbsp;&nbsp;&nbsp; \\left\\|y - y_n\\right\\| = \\left\\|\\widetilde{y} - \\widetilde{y}_n\\right\\| \\to 0,\\\\&nbsp;&nbsp;&nbsp; \\] so $x = y\\in Y$.</i> </p> </div><div></div>"
  },
  {
    "front": "If $Y$ is a proper closed linear subspace of a normed space $X$, then $\\overline{B}(0,1) \\not\\subset Y^\\delta$ for any $\\delta &lt; 1$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a normed space and $Y\\subsetneq&nbsp; X$ a closed linear subspace, then for every $\\delta \\in (0,1)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\overline{B}(0,1) \\not\\subset Y^\\delta,\\\\&nbsp;&nbsp;&nbsp; \\] where $Y^\\delta = \\left\\{ x\\in X\\colon \\left\\|x - Y\\right\\|&lt; \\delta \\right\\} $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x\\in X\\setminus Y$, then $d:= \\left\\|x-Y\\right\\|&gt; 0$ (because $X\\setminus Y$ is open). Let $y\\in Y$ and put $\\overline{x}:= \\frac{x-y}{\\left\\|x-y\\right\\|}$. Then $\\left\\|\\overline{x}\\right\\|=1$ and for any $\\overline{y}\\in Y$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\overline{x}-\\overline{y}\\right\\|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{\\left\\|x-y\\right\\|} \\left\\|x-(y+\\left\\|x-y\\right\\|\\overline{y})\\right\\| \\ge \\frac{d}{\\left\\|x-y\\right\\|},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; which we can make $\\ge \\delta$ for any $\\delta \\in (0,1)$ by choosing $y$ appropriately.</i> </p> </div><div></div>"
  },
  {
    "front": "If $X$ is a metric space, then $\\overline{B}(0,1)$ is compact iff $\\dim X &lt; \\infty$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a metric space, then $\\overline{B}(0,1)$ is compact iff $\\dim X &lt; \\infty$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \"$\\implies$\" is clear. Choose $x_1 \\in \\partial B(0,1)$. Now suppose $x_1,\\ldots,x_n$ have been chosen, then $Y_n:= \\left&lt;x_1,\\ldots,x_n\\right&gt;$ is a finite dimensional (hence proper and closed) linear subspace of $X$, so we can choose $x_{n+1} \\in \\partial B(0,1)$ with $\\left\\|x_{n+1}-Y_n\\right\\| \\ge 1 / 2$. Then we inductively obtain a sequence $(x_n) \\subset \\partial B(0,1)$ with $\\left\\|x_n - x_m\\right\\|\\ge 1 / 2$ for all $n\\neq&nbsp; m$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Characterisation of $0$-support sets and representation of all support sets of a polytope.",
    "back": "<div><p><b>Proposition.</b></div><div>&nbsp; &nbsp; Let $P\\subset \\mathbb{R}^d$ be a polytope.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>The $0$-support sets of $P$ are exactly $\\left\\{ x \\right\\} $ for $x\\in \\operatorname{cvert} P$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $F$ is a support set of $P$, then $F = \\operatorname{conv} (F \\cap \\operatorname{cvert} P)$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; In particular, every support set is a polytope and there are only finitely many.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $\\left\\{ x \\right\\} $ is a support set with half space $\\left\\{ f\\le \\beta \\right\\} $, then $x\\in P$ and $P\\setminus \\left\\{ x \\right\\} = P \\cap \\left\\{ f &lt; \\beta \\right\\} $ is convex. Conversely, if $x$ is a vertex (and $x_1, \\ldots ,x_k$ are the others), then let $Q := \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} $, and $y \\in (x,p(Q,x)) $, and $\\left\\{ f\\le \\beta \\right\\} $ the half space separating $\\left\\{ y \\right\\} $ and $Q$ going through $y$ orthogonal to $y-p(y)$. Then $\\alpha := f(x) &gt; \\beta$, and we claim that $ \\left\\{ f \\le \\alpha \\right\\} $ is a support plane going through $x$. It clearly contains $x$, now suppose $z \\in P $, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\sum_{i=1}^k \\mu_i \\underbrace{f(x_i)}_{&lt;\\alpha} + \\mu \\underbrace{f(x)}_{=\\alpha} \\le \\alpha,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] and equality holds iff $z = x$.&nbsp;&nbsp;&nbsp;</li>  <li>Suppose $\\operatorname{cvert} P = \\left\\{ x_1, \\ldots ,x_k \\right\\} $ and exactly the first $1\\le m \\le k$ lie in a support set $F = P \\cap \\left\\{ f\\le \\alpha \\right\\} $ of $P$. Then for any $z = \\sum_{i=1}^k \\mu_i x_i \\in P$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\sum_{i\\le m} \\mu_i \\underbrace{f(x_i)}_{=\\alpha} + \\sum_{i &gt; m} \\mu_i \\underbrace{f(x_i)}_{&lt; \\alpha},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $z\\in F$ iff $f(z) = \\alpha$ iff $z \\in \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_m \\right\\} $.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><div></div></div>"
  },
  {
    "front": "Convex geometry: Polytopes are polyhedral sets.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Every polytope is a polyhedral set.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume $\\dim P = d$. Let $F_1, \\ldots ,F_m$ be the support sets of $P$ and $H_1, \\ldots ,H_m$ the corresponding half spaces. Then, \\[&nbsp;&nbsp;&nbsp; P \\subset \\bigcup_{i=1} ^m H_i =: P',\\\\\\] and it suffices to show the opposite inclusion. Suppose there was $x\\in P'\\setminus P$, take $y\\in \\operatorname{relint} P$, so that there is a unique $z\\in (y,x) \\cap \\operatorname{bd} P$. By the support theorem there is an $i\\in [m]$ such that $z\\in \\operatorname{bd} H_i$, but also $y,x \\in \\text{int} H_i$, so $z\\in \\text{int} H_i$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Extremal and exposed points of a convex set $A$, their relation, and the case where $A$ is a polytope.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Let $A\\subset \\mathbb{R}^d$ be closed and convex. Then $x\\in A$ is called<br>&nbsp;&nbsp;&nbsp; <ol>  <li>an <i>extremal point</i> of $A$, written $x\\in \\operatorname{ext} A$, if $A\\setminus \\left\\{ x \\right\\} $ is convex, i.e. if $x$ cannot be written as non-trivial convex combination of points in $A$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>an <i>exposed point</i> of $A$, written $x\\in \\exp A$, if $\\left\\{ x \\right\\} $ is a support set of $A$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>$\\exp A \\subset \\operatorname{ext} A$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $A$ is a polytope, then $\\exp A = \\operatorname{ext} A = \\operatorname{cvert} A$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $\\left\\{ x \\right\\}&nbsp; = A \\cap \\left\\{ f = \\alpha \\right\\} $ and $A \\subset \\left\\{ f\\le \\alpha \\right\\} $, then $A\\setminus \\left\\{ x \\right\\} = A \\cap \\left\\{ f &lt; \\alpha \\right\\} $ is convex.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\operatorname{ext} A = \\operatorname{cvert} A$ by definition, and $\\operatorname{cvert} A \\subset \\exp A$ by the support theorem.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Minkowski's theorem (on $\\operatorname{ext} K$ for compact convex $K$).",
    "back": "<div><p><b>Theorem.</b> <i>[Minkowski]<br>&nbsp;&nbsp;&nbsp; If $K\\subset \\mathbb{R}^d$ is compact and convex, and $A\\subset K$, then $K = \\operatorname{conv} A$ iff $\\operatorname{ext} K \\subset A$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly it is necessary that $\\operatorname{ext} K \\subset A$ (otherwise $\\operatorname{conv} A \\subset K \\setminus \\left\\{ x \\right\\} $), so it STS that $K = \\operatorname{conv} \\operatorname{ext} K$, which we prove by induction on $d$. If $d = 1$ then $K = [a,b]$ and $\\operatorname{ext} K = \\left\\{ a,b \\right\\} $. Now suppose $d \\ge 2$. Let $x\\in K$, take an arbitrary line $g$ through $K$, then $g\\cap K = [y,z]$ and $y,z\\in \\operatorname{bd} K$. Then there are supporting hyperplanes $E_y,E_z$ of $K$, and $K_y := K \\cap E_y = \\operatorname{conv} \\operatorname{ext} K_y$ by induction hypothesis, same for $z$. It is easy to see that $\\operatorname{ext} K_y \\subset \\operatorname{ext} K$, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x \\in [y,z] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\subset \\operatorname{conv} \\left(K_y \\cup K_z&nbsp; \\right) = \\operatorname{conv} \\left( \\operatorname{conv} \\operatorname{ext} K_y \\cup \\operatorname{conv} \\operatorname{ext} K_z \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\subset \\operatorname{conv} \\operatorname{ext} K.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Characterisation of polytopes via $\\operatorname{ext} P$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; A compact convex set $P\\subset \\mathbb{R}^d$ is a polytope iff $\\operatorname{ext} P $ is finite.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $P$ is a polytope, then $\\operatorname{ext} P = \\operatorname{cvert} P$ is finite. If $\\operatorname{ext} P = \\left\\{ x_1, \\ldots ,x_k \\right\\} $ is finite, then $P = \\operatorname{conv} \\left\\{ x_1, \\ldots ,x_k \\right\\} $ is a polytope.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of compact sets in $\\ell^p$ for $p \\in [1,\\infty)$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; $A\\subset \\ell^p$ is relatively compact iff it is bounded and \\[&nbsp;&nbsp;&nbsp; \\sup_{x\\in A} \\sum_{k=n}^\\infty \\left| x_k \\right| ^p \\stackrel{  }{\\longrightarrow} 0,\\quad n\\to \\infty.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly this is necessary (otherwise there was a sequence without a converging subsequence). Now suppose this is satisfied, then it STS that $A$ is totally bounded. Let $\\varepsilon &gt; 0$ and choose $N\\in \\mathbb{N}$ such that $\\sup_{x\\in A} \\left(\\sum_{k &gt; N} \\left| x_k \\right| ^p\\right)^{1 / p} &lt; \\varepsilon$. Then $A \\subset (\\left\\{ (x_1, \\ldots ,x_N,0, \\ldots )\\colon x\\in A \\right\\})^\\varepsilon $, so it STS that $\\pi_N(A) \\subset \\mathbb{R}^N$ is totally bounded, which is clear because it is bounded.</i> </p> </div><div></div>"
  },
  {
    "front": "Arzela-Ascoli. In particular, compact embeddings of Hölder spaces.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $K$ is a compact metric space, then $A\\subset C(K)$ is relatively compact iff it is bounded and <i>equicontinuous</i>, i.e. \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{x\\in A} \\omega(x,\\delta) \\stackrel{  }{\\longrightarrow} 0,\\quad \\delta \\to 0.&nbsp;&nbsp;&nbsp; \\] In particular, $C^{0,\\alpha(K)} \\hookrightarrow C(K)$ is compact for all $\\alpha \\in (0,1]$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\implies$\":} If $A$ is relatively compact then it is totally bounded, hence bounded. Now since $\\sup_A \\le \\sup_{\\overline{A}}$, we may assume $A$ is compact. Then since $\\omega(\\cdot ,\\delta)\\colon C(K) \\to \\mathbb{R}$ is continuous we can for every $\\varepsilon &gt; 0$ cover $A$ in finitely many balls $B_i$ such that $\\sup_{x\\in B_i} \\omega(x,\\delta_i) &lt; \\varepsilon$, so if $\\delta := \\min_i \\delta_i$ then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{x\\in A} \\omega(x,\\delta) &lt; \\varepsilon.&nbsp;&nbsp;&nbsp; \\] <br><br>&nbsp;&nbsp;&nbsp; \\textbf{\"$\\impliedby$\":} For $N\\in \\mathbb{N}$ let $\\delta_N$ be according to equicontinuity of $A$ and cover $K$ in finitely many balls with radius $\\delta_N$ and let $S_N$ be their centres. Now if $(x_n) \\subset A$ is a sequence, then $(x_n(s))_{n\\in \\mathbb{N}}$ is bounded for every $s \\in \\bigcup_{N\\in \\mathbb{N}} S_N$ so has a convergent subsequence, so we find a subsequence (denoted the same) such that $x_n(s)$ converges as $n\\to \\infty$ for all such $s$. Now if $\\varepsilon &gt; 0$ take $N &gt; 1 / \\delta$, and then take $n_0\\in \\mathbb{N}$ so large that $\\left| x_n(s) - x_m(s) \\right| &lt; \\varepsilon$ for all $n,m\\ge n_0$ and $s\\in S_N$. Then if $t\\in K$ it is in $B(s,\\delta_N)$ for some $s\\in S_N$ so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| x_n(t) - x_m(t) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left| x_n(t) - x_n(s) \\right| + \\left| x_n(s) - x_m(s) \\right| + \\left| x_m(s) - x_m(t) \\right| &lt; 3\\varepsilon\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for all $n,m\\ge n_0$ independent of $t$, so this is a Cauchy sequence so $A$ is totally bounded hence relatively compact because $C(K)$ is complete.</i> </p> </div><div></div>"
  },
  {
    "front": "If $k\\in C([0,1]^2)$, then $T\\colon C([0,1]) \\to C([0,1]);\\, f \\mapsto&nbsp; \\int_0^1 k(\\cdot ,s) f(s)\\mathop{}\\!\\mathrm{d} s$ is compact.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; HTS that $T(\\overline{B}(0,1))$ is compact. It is bounded because $\\left\\|T f\\right\\|_\\infty \\le \\left\\|k\\right\\|_\\infty$ for $\\left\\|f\\right\\|_\\infty \\le 1$. Now if $t,s\\in [0,1]$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| (Tf)(t) - (Tf)(s) \\right| \\le \\int_0^1 \\left| k(t,r) - k(s,r) \\right| \\left| f(r) \\right| \\mathop{}\\!\\mathrm{d} r \\le \\omega(k,\\left| t-s \\right| ),\\\\&nbsp;&nbsp;&nbsp; \\] so it is equicontinuous.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Bounded polyhedral sets are polytopes.",
    "back": "<div><p><i><b>Proof.</b>&nbsp; &nbsp; K is convex and compact, so it STS that $\\operatorname{ext} K$ is finite. Let $H_1, \\ldots ,H_m$ be the half spaces with boundary planes $E_1, \\ldots ,E_m$. Let $x\\in \\operatorname{ext} K$ and put \\[&nbsp;&nbsp;&nbsp; D := \\bigcap_{i=1} ^m \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; E_i&amp;,x\\in E_i,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; H_i^\\circ &amp;, \\text{else}.&nbsp;&nbsp;&nbsp; \\end{cases}\\] Then $D$ is a convex and (relatively) open subset of an affine space (the intersection of the $E_i$) and $x\\in D \\subset K$. Since $x$ is extremal, $D = \\left\\{ x \\right\\} $. Since there are only finitely many such sets $D$, $\\operatorname{ext} K$ must be finite.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: How to obtain (compact convex) $K\\subset \\mathbb{R}^d$ from $\\exp K$?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $K\\subset \\mathbb{R}^d$ is compact and convex, then \\[&nbsp;&nbsp;&nbsp; K = \\operatorname{cl} \\operatorname{conv} \\exp K.&nbsp;&nbsp;&nbsp; \\] In particular, \\[&nbsp;&nbsp;&nbsp; \\exp K \\subset \\operatorname{ext} K \\subset \\operatorname{cl} \\exp K.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>[Proof of Proposition]<br>&nbsp;&nbsp;&nbsp; For $z\\in \\mathbb{R}^d$ let $y_z \\in K$ be the furthest point from $z$ in $K$, then the hyperplane $E$ through $y_z$ orthogonal to $z-y_z$ supports $K$ and $E\\cap K = \\left\\{ y_z \\right\\} $, so $y_z \\in \\exp K$. It thus STS that if \\[&nbsp;&nbsp;&nbsp; \\widehat{K}:= \\operatorname{cl} \\operatorname{conv} \\left\\{ y_z\\colon z\\in \\mathbb{R}^d \\right\\} \\] then $K\\subset \\widehat{K}$. Suppose for contradiction that there is $x \\in K\\setminus \\widehat{K}$. Then along the line $s = x + [0,\\infty)\\cdot (\\overline{x}-x)$ where $\\overline{x} = p(x,\\widehat{K})$ (exists because $\\widehat{K}$ is closed) we find a point $z\\in \\mathbb{R}^d$ for which $\\sup_{y\\in \\widehat{K}}\\left| z-y \\right| &lt; \\left| z-x \\right| $, but this is a contradiction because $\\left| z-x \\right| &lt; \\left| z-y_z \\right| $ and $y_z \\in \\widehat{K}$.<br><br>&nbsp;&nbsp;&nbsp; In particular, \\[&nbsp;&nbsp;&nbsp; K = \\operatorname{cl} \\operatorname{conv} \\exp K \\subset \\operatorname{cl} \\operatorname{conv} \\operatorname{cl} \\exp K = \\operatorname{conv} \\operatorname{cl} \\exp K,\\\\&nbsp;&nbsp;&nbsp; \\] so $\\operatorname{ext} K \\subset \\operatorname{cl} \\exp K$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Definition of convex and quasi-convex functions and characterisation. Characterisation of convexity for a positively homogenous function.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A function $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is called<br>&nbsp;&nbsp;&nbsp; <ol>  <li><i>convex</i> if \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{epi} f := \\bigcup_{x\\in \\mathbb{R}^d} \\left\\{ x \\right\\} \\times [f(x),\\infty) \\subset \\mathbb{R} ^{d+1}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] is convex, iff $f(\\alpha x + (1-\\alpha) y) \\le \\alpha f(x) + (1-\\alpha) f(y)$ for all $\\alpha \\in [0,1]$ and $x,y\\in \\mathbb{R}^d$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li><i>quasi-convex</i> if $\\left\\{ f\\le \\alpha \\right\\} $ is convex for all $\\alpha \\in \\mathbb{R}$ iff $f(\\alpha x + (1-\\alpha) y) \\le f(x) \\vee f(y)$ for all $\\alpha \\in [0,1]$ and $x,y\\in \\mathbb{R}^d$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li><i>positively homogenous</i> if $f(\\alpha x) = \\alpha f(x)$ for all $x\\in \\mathbb{R}^d$ and $\\alpha \\ge 0$.&nbsp;&nbsp;&nbsp;</li></ol></p> </div><div></div><div>Then convex functions are quasi-convex, and a positively homogenous function is convex iff it is sub-additive.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Everything is sort of clear except maybe the claim with the homogenous functions. Suppose it is sub-additive, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(\\alpha x + (1-\\alpha) y) \\le f(\\alpha x) + f((1-\\alpha)y) = \\alpha f(x) + (1-\\alpha) f(y).&nbsp;&nbsp;&nbsp; \\] Suppose it is convex, then \\[&nbsp;&nbsp;&nbsp; f(x+y) = 2 f( \\frac{1}{2} (x+y)) \\le 2 (\\frac{1}{2} f(x) + \\frac{1}{2} f(y)) = f(x) + f(y).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "How to obtain a convex function $f_A \\colon \\mathbb{R}^d \\to \\mathbb{R}$ from a convex set $A \\subset \\mathbb{R} \\times \\mathbb{R} ^{d+1}$. How does $A$ relate to $\\operatorname{epi} f_A$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; If $A$ is convex and $A_x := \\left\\{ \\alpha \\colon (x,\\alpha) \\in A \\right\\} $ is bounded from below for all $x\\in \\mathbb{R}^d$, then \\[&nbsp;&nbsp;&nbsp; f_A\\colon \\mathbb{R}^d \\to \\mathbb{R}; \\, x \\mapsto \\inf A_x\\\\\\] is convex. Then $A \\subset \\operatorname{epi} f_A$ and equality holds iff $A_x = [\\inf A_x,\\infty)$ for all $x\\in \\mathbb{R}^d$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x,y\\in \\mathbb{R}^d$ and $\\alpha \\in (0,1)$, may assume that $f_A(x) , f_A(y) &lt; \\infty$, and $\\inf A_x \\in A_x$, same for $y$ (otherwise use $\\varepsilon$'s). Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A \\ni \\alpha (x,f_A(x)) + (1-\\alpha) (y,f_A(y)) = (\\alpha x + (1-\\alpha) y, \\alpha f_A(x) + (1-\\alpha) f_A(y)),\\\\&nbsp;&nbsp;&nbsp; \\] so $f_A(\\alpha x + (1-\\alpha) y) \\le \\alpha f_A(x) + (1-\\alpha) f_A(y)$.<br><br>&nbsp;&nbsp;&nbsp; By definition, $\\operatorname{epi} f_A = \\bigcup_{x\\in \\mathbb{R}^d} \\left\\{ x \\right\\} \\times [f_A(x),\\infty) \\supset A$, and the final claim follows.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Closure of a convex function.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is convex, then there is a unique convex function $\\operatorname{cl} f$ with $\\operatorname{epi} \\operatorname{cl} f = \\operatorname{cl} \\operatorname{epi} f$. Then $\\operatorname{cl} f \\le f$ and for any closed convex $g \\le f$ we have $g \\le \\operatorname{cl} f$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $A := \\operatorname{cl} \\operatorname{epi} f$ and consider the convex function $f_A \\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ (we have $f_A \\ge h &gt; -\\infty$ for any affine function $h \\le f$ so this is well-defined). If we can show that $A_x = [f_A(x),\\infty)$ for all $x \\in \\mathbb{R}^d$ then $\\operatorname{epi} f_A = A$ and so the first claim holds. Indeed, if $x\\in \\mathbb{R}^d$ and $b \\in [f_A(x),\\infty)$ (assume $f_A(x) &lt; \\infty$, otherwise $A_x = \\emptyset =[f_A(x),\\infty)$), then there are $\\operatorname{epi} f \\ni (x_n,b_n) \\to (x,f_A(x))$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{epi} f \\ni (x_n,b_n \\vee b) \\to (x, b),\\\\&nbsp;&nbsp;&nbsp; \\] so $(x,b) \\in A$, so $b \\in A_x$.<br><br>&nbsp;&nbsp;&nbsp; Finally, $\\operatorname{cl} \\operatorname{epi} f \\supset \\operatorname{epi} f$ implies $\\operatorname{cl} f \\le f$, and if $g\\le f$ is closed and convex, then $\\operatorname{epi} g \\supset \\operatorname{epi} f$ is closed, so $\\operatorname{epi} g \\supset \\operatorname{cl} \\operatorname{epi} f = \\operatorname{epi} \\operatorname{cl} f$, so $g \\le \\operatorname{cl} f$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: $\\operatorname{cl} f = \\sup \\left\\{ h\\colon h\\le f \\text{ affine} \\right\\} $ for any convex $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$.",
    "back": "<div></div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is convex, then $\\operatorname{cl} f = \\sup \\left\\{ h\\colon h \\le f \\text{ affine} \\right\\} $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Note that $h\\le f$ iff $h\\le \\operatorname{cl} f$ for any affine $h$, so we may assume that $f$ is closed. It suffices to show that any $(x,b) \\not\\in \\operatorname{epi} f$ can be separated from $\\operatorname{epi} f$ with a <i>non-vertical</i> hyperplane. We know that any such $(x_0,b_0)$ can be strongly separated using some hyperplane $\\left\\{ I_0 \\le 0 \\right\\} $, say, so suppose it is a vertical one then we must have $x_0\\not\\in \\operatorname{dom} f$ and we can wish for $I_0(x_0) &gt; 0$ and $I_0(x) \\le 0$ for all $x\\in \\operatorname{dom} f$. Now take any affine function $I_1 \\le f$ (f.ex. by separating $(x,f(x)-1)$ from $\\operatorname{epi} f$ for any $x\\in \\operatorname{dom} f$), so that \\[&nbsp;&nbsp;&nbsp; h_a := a I_0 + I_1 \\le f \\quad \\forall a &gt; 0.\\] Furthermore, $h_a(x_0) = aI_0(x_0) + I_1(x_0) &gt; b_0$ for sufficiently large $a$.</i> </p> <br><br><div></div>"
  },
  {
    "front": "Convex geometry: Definition and properties of complex conjugate $f^\\star$, and what is $f^{\\star\\star}$.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ be proper and convex. Then the <i>convex conjugate</i> of $f$ is \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^\\star(y) = \\sup_{x\\in \\mathbb{R}^d} \\left( \\left&lt;x,y \\right&gt; -f(x) \\right) ,\\quad y\\in \\mathbb{R}^d.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Then $f^\\star\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is closed, proper, and convex, and&nbsp; \\[&nbsp;&nbsp;&nbsp; f^{\\star \\star} = \\operatorname{cl} f.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $f^\\star &gt;-\\infty$ because $f$ is proper, and $f^\\star$ is closed and convex as supremum of affine (hence closed and convex) functions. To see that $f^\\star$ is proper, note that for any affine function $h = \\left&lt;y,\\cdot&nbsp; \\right&gt; -\\alpha \\le f$, we have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^\\star(y) \\le \\sup_{x\\in \\mathbb{R}^d} \\left( \\left&lt;y,x \\right&gt;&nbsp; - (\\left&lt;y,x \\right&gt; -\\alpha) \\right) = \\alpha.&nbsp;&nbsp;&nbsp; \\] Now let $x\\in \\mathbb{R}^d$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^{\\star \\star}(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sup_{y\\in \\mathbb{R}^d} \\left( \\left&lt;y,x \\right&gt; - \\sup_{z\\in \\mathbb{R}^d} \\left( \\left&lt;z,y \\right&gt; - f(z) \\right)&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\sup_{y\\in \\mathbb{R}^d} \\left( \\left&lt;y,x \\right&gt; - (\\left&lt;x,y \\right&gt; -f(x)) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= f(x).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now let $h = \\left&lt;y,\\cdot&nbsp; \\right&gt; -\\alpha \\le f$, then $f^\\star(y) \\le \\alpha$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^{\\star\\star}(x) = \\sup_{y\\in \\mathbb{R}^d} \\left( \\left&lt;z,x \\right&gt; -f^\\star(z) \\right) \\ge \\left&lt;y,x \\right&gt; -\\alpha = h,\\\\&nbsp;&nbsp;&nbsp; \\] so $f^{\\star\\star} \\ge \\sup \\left\\{ h\\colon h\\le f \\text{ affine} \\right\\} = \\operatorname{cl} f$.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Lipschitz continuity of convex functions.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is convex, then it is uniformly Lipschitz on compact subsets of $\\operatorname{interior} \\operatorname{dom} f$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that $f$ is locally bounded in $D := \\operatorname{interior} \\operatorname{dom} f$: If $x\\in D$ then there is a simplex $P = \\left&lt;x_0, \\ldots ,x_n \\right&gt; \\subset D$ such that $x\\in \\operatorname{interior} P$, and $\\sup_{x\\in P}\\left| f(x) \\right|&nbsp; \\le \\max_{i = 0, \\ldots ,n} \\left| f(x_i) \\right| $ by convexity.<br><br>&nbsp;&nbsp;&nbsp; Let $A \\subset \\operatorname{interior}\\operatorname{dom} f$ be compact. Then there is an $\\varepsilon &gt; 0$ such that $A^\\varepsilon \\subset \\operatorname{interior} \\operatorname{dom} f$ (take $\\varepsilon = d(A,(\\operatorname{interior} \\operatorname{dom} f)^{c}) / 2$), and $C := \\sup_{x\\in A} \\left| f(x) \\right| &lt; \\infty$. Let $x,y \\in A$ with $\\left| x-y \\right| \\le \\varepsilon$. Then there is $z \\in \\partial B(x,\\varepsilon)$ such that $y = (1-\\alpha) x + \\alpha z$ where $ \\left| x-y \\right| = \\alpha \\varepsilon$ (go along $x \\to y$ until distance $\\varepsilon$), so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(y) \\le (1-\\alpha) f(x) + \\alpha f(z) \\iff f(y) - f(x) \\le \\alpha (f(z) - f(x)) \\le \\frac{2C}{\\varepsilon} \\left| x-y \\right| .&nbsp;&nbsp;&nbsp; \\] By symmetry, we obtain \\[&nbsp;&nbsp;&nbsp; \\left| f(x) - f(y) \\right| \\le \\frac{2C}{\\varepsilon} \\left| x-y \\right| ,\\\\&nbsp;&nbsp;&nbsp; \\] for all $x,y\\in A$ with $\\left| x-y \\right| &lt; \\varepsilon$. If $\\left| x-y \\right| \\ge \\varepsilon$, this holds automatically.</i> </p> </div><div></div>"
  },
  {
    "front": "Elementary characterisation of boundedness for linear operator between normed spaces.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ and $Y$ are normed spaces, and $T\\colon X \\to Y$ is linear, then TFAE:<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$T$ is Lipschitz continuous,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$T$ is continuous,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$T$ is continuous at zero,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$T$ is bounded.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) $\\implies$ (ii) $\\implies$ (iii) and (iv) $\\implies$ (i) are clear. Assume $T$ is continuous at zero, so since $T0 = 0$ there is $\\delta &gt; 0$ with $T \\overline{B}(0,\\delta) \\subset B(0,1)$. Then if $x\\in X $, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|T x\\right\\| = \\frac{\\left\\|x\\right\\|}{\\delta} \\left\\|T \\left( \\delta \\frac{x}{\\left\\|x\\right\\|} \\right) \\right\\| \\le \\frac{\\left\\|x\\right\\|}{\\delta}.&nbsp;&nbsp;&nbsp; \\]</i> </p> </div><div></div>"
  },
  {
    "front": "When is $\\mathcal{L}(X,Y)$ a Banach space.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a normed space and $Y$ is a Banach space, then $\\mathcal{L}(X,Y)$ is a Banach space. In particular, $X^\\star$ is a Banach space.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $(T_n)\\subset \\mathcal{L}(X,Y)$ is a Cauchy sequence. Then for any $x\\in X$, \\[&nbsp;&nbsp;&nbsp; \\left\\|T_n x - T_m x\\right\\|\\le \\left\\|T_n - T_m\\right\\| \\left\\|x\\right\\|,\\\\\\] so $(T_n x) \\subset Y$ is Cauchy, so $Tx := \\lim_{n\\to \\infty} T_n x$ exists. Clearly $T$ is linear, and $\\left\\|T x\\right\\| \\le \\varliminf_{n\\to \\infty} \\left\\|T_n x\\right\\| \\le (\\sup_{n\\in \\mathbb{N}} \\left\\|T_n\\right\\|) \\left\\|x\\right\\|$, so $T\\in \\mathcal{L}(X,Y)$. Finally, for any $x\\in X$ with $\\left\\|x\\right\\| = 1$, \\[\\left\\| T_n x - T x\\right\\| \\le \\varliminf_{m\\to \\infty} \\left\\|T_n x - T_m x\\right\\|\\le \\varliminf_{m\\to \\infty} \\left\\|T_n - T_m\\right\\| \\stackrel{  }{\\longrightarrow} 0,\\quad n \\to \\infty,\\\\\\] and the upper bound does not depend on $x$.</i> </p> </div><div></div>"
  },
  {
    "front": "Sufficient condition for the range of $T \\in \\mathcal{L}(X,Y)$ to be closed.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a Banach space and $Y$ normed, and $T \\in \\mathcal{L}(X,Y)$ satisfies $\\left\\|Tx\\right\\|\\ge c \\left\\|x\\right\\|$ for some $c &gt; 0$, then $R(T)$ is closed.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $x_n \\in X$ such that $T x_n \\to y$ in $Y$. Then, \\[&nbsp;&nbsp;&nbsp; \\left\\|x_n - x_m\\right\\| \\le \\frac{1}{c} \\left\\|T_n - T_m\\right\\|,\\quad n,m\\in \\mathbb{N},\\\\\\] so $(x_n)$ is Cauchy, so $x_n \\to x$, so \\[y = \\lim_{n\\to \\infty} T x_n = T x \\in R(T).\\] </i> </p> <br><div></div></div>"
  },
  {
    "front": "If $\\mu \\in \\mathcal{M}_1(\\mathbb{R})$ and $f \\in C^1(\\mathbb{R})$ with $f(-\\infty) = 0$, how can you express $\\mu(f)$ in terms of $\\mu(\\cdot&nbsp; \\ge t)$?",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu \\in \\mathcal{M}_1(\\mathbb{R})$ and $f\\in C^1(\\mathbb{R})$ is increasing, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(f) = \\int_{-\\infty}^{\\infty} f'(t) \\mu( (t,\\infty)) \\mathop{}\\!\\mathrm{d} t.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp; &nbsp; We have<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(f) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int f(x) \\mu(\\mathop{}\\!\\mathrm{d} x) = \\int \\int_{-\\infty}^\\infty f'(s) \\boldsymbol{1}_{\\left\\{ s \\le x \\right\\} } \\mathop{}\\!\\mathrm{d} s\\, \\mu(\\mathop{}\\!\\mathrm{d} x) = \\int_{-\\infty}^{\\infty} f'(s) \\mu(\\cdot \\ge s) \\mathop{}\\!\\mathrm{d} s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "In a separable normed space, the Borel sigma algebra is generated by cylindrical sets.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a normed vector space, then its Borel $\\sigma$-algebra is generated by the <i>cylindrical</i> sets \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{M} := \\left\\{\\bigcap_{i=1} ^k \\ell_i^{-1}(B_i) \\colon&nbsp; k\\in \\mathbb{N}, \\ell_1, \\ldots ,\\ell_k \\in X^\\star, B_1, \\ldots ,B_k \\in \\mathcal{B}(\\mathbb{R})\\right\\}.&nbsp;&nbsp;&nbsp; \\] Furthermore, $\\mathcal{M}$ is a $\\pi$-system that contains $B$, and is invariant under dilations and translations.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $\\mathcal{M}$ is clearly a $\\pi$-system, and $\\ell^{-1}(\\mathbb{R}) = B$ for any $\\ell\\in B^\\star$. Now if $\\ell_i \\in B^\\star$, $B_i \\in \\mathcal{B}(\\mathbb{R})$, and $\\alpha &gt; 0$, $x_0\\in B$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x \\in \\alpha \\left( \\bigcap_{i=1} ^k \\ell_i^{-1}(B_i) \\right) + x_0\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\iff \\forall i \\in [k]\\colon \\frac{x}{\\alpha} - x_0 \\in \\ell_i^{-1}(B_i)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\iff \\forall i\\in [k]\\colon&nbsp; \\frac{1}{\\alpha} \\ell_i(x) - \\ell(x_0) \\in B_i\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\iff x \\in \\bigcap_{i=1} ^k \\ell_i^{-1} \\big( \\alpha(B_i + \\ell(x_0))\\big) \\in \\mathcal{M}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then if $\\varphi$ is an affine map, $\\varphi^{-1} (\\sigma(\\mathcal{M})) = \\sigma (\\varphi^{-1}(\\mathcal{M})) = \\sigma(\\mathcal{M})$, so both $\\mathcal{M}$ and $\\mathcal{E}:= \\sigma(\\mathcal{M})$ are invariant under tranlsations and dilations.<br><br>&nbsp;&nbsp;&nbsp; Since $X$ is separable, every open set is a countable union of closed balls, so it suffices to show that $\\mathcal{E}$ contains closed balls. Since $\\mathcal{M}$ and hence $\\mathcal{E}$ are invariant under translations and dilations, it suffices to show that $B(0,1) \\in \\mathcal{E}$. For that purpose let $D $ be a dense subset of $\\partial B(0,1)$, and let $\\ell_x \\in B^\\star$ with $\\left\\|\\ell_x\\right\\| = 1$ and $\\ell_x(x) = 1$ (Hahn-Banach) for $x\\in D$. Then put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; E := \\left\\{ x\\in B\\colon \\left|\\ell_y(x)\\right| \\le 1 \\forall y\\in D \\right\\} = \\bigcap_{y\\in D} \\ell_y^{-1} ([-1,1]) \\in \\mathcal{E}.&nbsp;&nbsp;&nbsp; \\] Clearly, $\\overline{B}(0,1) \\subset E$, so need to show that if $\\left\\|x\\right\\| &gt; 1$ then $x\\not\\in E$. In that case, let $\\widehat{x} := x / \\left\\|x\\right\\|$ and $D\\ni x_{n}\\to x$. Then, putting $\\ell_n := \\ell_{x_n}$, \\[&nbsp;&nbsp;&nbsp; \\left| \\ell_n(\\widehat{x}) - 1 \\right| =\\left| \\ell_n(\\widehat{x}) - \\ell_n(x_n) \\right| \\le \\left\\|\\widehat{x} - x_n\\right\\| \\to 0,\\\\\\] so $\\ell_n(x) = \\left\\|x\\right\\| \\ell_n(\\widehat{x}) &gt; 1$ for large $n$.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of probability measure on separable normed space by one-dimensional distributions and its Fourier transform.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\mu, \\nu$ are probability measures on a separable normed space such that $\\ell^\\star \\mu = \\ell^\\star \\nu$ for all $\\ell\\in B^\\star$, then $\\mu = \\nu$. In particular, $\\mu$ is uniquely determined by its <i>Fourier transform</i> \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{\\mu}\\colon X^\\star\\to \\mathbb{R};\\, \\ell\\mapsto \\int_X \\mathrm{e}^{\\mathrm{i} \\ell(x)}\\mu(\\mathop{}\\!\\mathrm{d} x).&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that if $k\\in \\mathbb{N}$ and $\\ell_1, \\ldots ,\\ell_k \\in B^\\star$, then for any $\\alpha \\in \\mathbb{R}^k$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{i=1}^k \\alpha_i \\ell_i^\\star \\mu = \\left( \\sum_{i=1}^k \\alpha_i \\ell_i \\right) ^\\star \\mu = \\sum_{i=1}^k \\alpha_i \\ell_i^\\star \\nu,\\\\&nbsp;&nbsp;&nbsp; \\] so that $(\\ell_1, \\ldots ,\\ell_k)^\\star \\mu = (\\ell_1, \\ldots ,\\ell_k)^\\star \\nu$ as measures in $\\mathbb{R}^k$. Thus $\\mu$ and $\\nu$ coincide on cylindrical sets $\\bigcap_{i=1} ^k \\ell_i^{-1}(B_i)$ for $B_i \\in \\mathcal{B}(\\mathbb{R})$, which are a $\\pi$-generator of $\\mathcal{B}(B)$ that contains $B$, so $\\mu = \\nu$ by uniqueness of measures. Finally, the Fourier transform characterises $\\ell^\\star \\mu$ for all $\\ell\\in X^\\star$ and hence $\\mu$.</i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Gaussian probability measure on separable Banach space and its covariance operator. What is its Fourier transform, and its finite-dimensional distributions?",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A Borel probability measure $\\mu$ on a separable Banach space $B$ is called a <i>(centred) Gaussian measure</i> if $\\ell^\\star \\mu \\in \\mathcal{M}_1(\\mathbb{R})$ is (centred) Gaussian for all $\\ell\\in B^\\star$. Its <i>covariance operator</i> is \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C_\\mu\\colon B^{\\star}\\times B^{\\star}\\to \\mathbb{R};\\, (\\ell,\\ell') \\mapsto \\int \\ell(x) \\ell'(x) \\mu(\\mathop{}\\!\\mathrm{d} x),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] which can also be written as $C_\\mu(\\ell,\\ell') = \\ell'(\\widehat{C}_\\mu \\ell)$ where \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{C}_\\mu \\colon B^\\star \\to B;\\, \\ell \\mapsto \\int x \\ell(x) \\mu(\\mathop{}\\!\\mathrm{d} x). \\qquad \\textup{\\tiny (Bochner integral)}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] ($C_\\mu$ and $\\widehat{C}_\\mu$ are well-defined and bounded because $\\int \\left\\|x\\right\\|^2 \\mu(\\mathop{}\\!\\mathrm{d} x) &lt; \\infty$.) This is in fact a compact operator! One can show that there exists $m = m(\\mu) \\in B$, called its <i>mean</i>, such that \\[&nbsp;&nbsp;&nbsp; \\forall \\ell\\in B^\\star\\colon \\ell(m) = \\int \\ell(x) \\mu(\\mathop{}\\!\\mathrm{d} x).&nbsp;&nbsp;&nbsp; \\] Then $\\mu$ is centred iff $m = 0$.</p> <br><br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $\\mu$ is centred Gaussian and $\\ell_1, \\ldots ,\\ell_k \\in B^\\star$ and $T := (\\ell_1, \\ldots ,\\ell_k) \\colon B \\to \\mathbb{R}^k$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; T^\\star \\mu = \\mathcal{N}(0,K),\\\\&nbsp;&nbsp;&nbsp; \\] where $K_{ij} = C_\\mu(\\ell_i,\\ell_j)$. Furthermore, \\[&nbsp;&nbsp;&nbsp; \\widehat{\\mu}(\\ell) = \\mathrm{e}^{-\\frac{1}{2} C_\\mu(\\ell,\\ell)}.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By definition, $T^\\star \\mu$ is Gaussian, so it suffices to find mean and covariance. Indeed, $\\ell_i^\\star \\mu$ is centred and $\\int \\ell_i(x) \\ell_j(x) \\mu(\\mathop{}\\!\\mathrm{d} x) = C_\\mu(\\ell_i,\\ell_j)$ by definition of $C_\\mu$. This implies the second claim by the well-known Fourier transform of the one-dimensional Guassian.</i> </p> </div><div></div>"
  },
  {
    "front": "Rotation invariance of centred Gaussian measure $\\mu\\otimes \\mu$ on $B\\times B$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu$ is a centred Gaussian measure on a separable Banach space $B$, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R \\colon B\\times B \\to B\\times B;\\, (x,y) \\mapsto (x \\sin \\varphi + y\\cos \\varphi, x \\cos \\varphi - y \\sin \\varphi),\\\\&nbsp;&nbsp;&nbsp; \\] then $R^\\star (\\mu \\otimes \\mu) = \\mu \\otimes \\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It suffices to show equality of Fourier transforms. If $\\ell \\in (B\\times B)^\\star$ then $\\ell(x,y) = \\ell_1(x) + \\ell_2(y)$ where $\\ell_1 = \\ell(\\cdot ,0) \\in B^\\star$ and $\\ell_2:= \\ell(0,\\cdot )$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{R^\\star (\\mu \\otimes \\mu)}(\\ell)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\mathrm{e}^{\\mathrm{i} \\ell(R(x,y))}(\\mu\\otimes \\mu)(\\mathop{}\\!\\mathrm{d} x,\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\mathrm{e}^{\\mathrm{i} (\\sin\\varphi \\ell_1 + \\cos\\varphi \\ell_2)(x)} \\mu(\\mathop{}\\!\\mathrm{d} x) \\int\\mathrm{e}^{\\mathrm{i} (\\cos \\varphi \\ell_1 - \\sin \\varphi \\ell_2)(y)} \\mu(\\mathop{}\\!\\mathrm{d} y)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\exp \\left( -\\frac{1}{2} \\Big(C_\\mu(\\sin\\varphi \\ell_1 + \\cos \\varphi \\ell_2,.) + C_\\mu(\\cos\\varphi \\ell_1 - \\sin \\varphi \\ell_2,.)\\Big) \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\exp \\left( -\\frac{1}{2} \\left( C_\\mu(\\ell_1,\\ell_1) + C_\\mu (\\ell_2,\\ell_2) \\right)&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\widehat{\\mu \\otimes \\mu} (\\ell).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Fernique's theorem (on tails of rotation invariant measures), and corollary on moments.",
    "back": "<div><p><b>Theorem.</b> <i>[Fernique]<br>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a probability measure on a normed space $X$, such that $\\mu \\otimes \\mu$ is invariant under rotation by $\\frac{\\pi}{4}$ (for example a centred Gaussian measure). Then there is $\\alpha &gt; 0$ such that $\\int \\mathrm{e}^{\\alpha \\left\\|x\\right\\|^2}\\mu(\\mathop{}\\!\\mathrm{d} x) &lt; \\infty$. In fact, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_\\mu \\int \\mathrm{e}^{\\alpha (\\left\\|x\\right\\| / M_\\mu)^2} \\mu(\\mathop{}\\!\\mathrm{d} x) &lt; \\infty,\\\\&nbsp;&nbsp;&nbsp; \\] where $M_\\mu = \\int \\left\\|x\\right\\|\\mu(\\mathop{}\\!\\mathrm{d} x)$ and the supremum is over all $\\mu$ satisfying the assumptions. In particular, \\[&nbsp;&nbsp;&nbsp; \\int \\left\\|x\\right\\|^{2k} \\mu(\\mathop{}\\!\\mathrm{d} x) \\le c(k)M_\\mu^{2k},\\quad k\\in \\mathbb{N},\\\\\\] for universal constants $c(k) = K k! \\alpha^{-k}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The magic ingredient is this: If $0 &lt; t &lt; \\tau$, then <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(\\left\\|x\\right\\| &gt; t) \\mu(\\left\\|y\\right\\| &lt; \\tau) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= (\\mu\\otimes \\mu) ( \\left\\{ \\left\\|x\\right\\|&gt; t, \\left\\|y\\right\\|&lt;\\tau \\right\\} )\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu \\left( \\left\\{ \\left\\|\\frac{x+y}{\\sqrt{2} }\\right\\| &gt; t, \\left\\|\\frac{x-y}{\\sqrt{2} }\\right\\| &gt; t\\right\\} \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\mu \\left( \\left\\{ \\left\\|x\\right\\|\\wedge \\left\\|y\\right\\|\\ge \\frac{t-\\tau}{\\sqrt{2} } \\right\\}&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mu \\left( \\left\\|x\\right\\| \\ge \\frac{t-\\tau}{\\sqrt{2} } \\right) ^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Fix $\\tau &gt; 0$ sufficiently large such that $\\mu(\\left\\|x\\right\\|&lt;\\tau) \\ge 1 / 2$, and put $t_0:= \\tau$, $t_{n+1} := \\sqrt{2} t_n + \\tau$, so that $t_n \\le c\\tau 2^{n / 2}$\\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu( \\left\\|x\\right\\| &gt; t_{n+1}) \\le 2 \\mu (\\left\\|x\\right\\|\\ge t_n)^2,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(\\left\\|x\\right\\|&gt;t_0) \\le \\frac{1}{2},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\implies &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu (\\left\\|x\\right\\|&gt; t_n) \\lesssim 2^{-2^n} \\le \\mathrm{e}^{-\\frac{c t_n^2}{\\tau^2}}&nbsp;&nbsp;&nbsp; \\] This implies that $\\mu(\\left\\|x\\right\\|&gt;t) \\le \\mathrm{e}^{-\\beta t^2 / \\tau^2}$ for all $t \\ge \\tau$ and a constant $\\beta &gt; 0$ that does not depend $\\mu$. Then, if $\\alpha \\in (0,\\beta)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int \\mathrm{e}^{\\alpha (\\left\\|x\\right\\| / \\tau)^2}\\mu(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{2\\alpha}{\\tau^2} \\int_{-\\infty}^\\infty t \\mathrm{e}^{\\alpha t^2 / \\tau^2} \\mu(\\left\\|x\\right\\| &gt; t) \\mathop{}\\!\\mathrm{d} t = 2\\alpha \\int_{-\\infty}^\\infty t \\mathrm{e}^{\\alpha t^2} \\mu(\\left\\|x\\right\\| &gt; t\\tau) \\mathop{}\\!\\mathrm{d} t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le c(\\alpha) + 2\\alpha \\int_1^{\\infty} \\mathrm{e}^{(\\alpha - \\beta)t^2}\\mathop{}\\!\\mathrm{d} t &lt; \\infty.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; This upper bound does not depend on $\\tau$ or $\\mu$, and the corollary follows by taking $\\tau = 2M$ (Chebyshev). Finally, note that $\\mathrm{e}^{\\alpha (\\left\\|x\\right\\| / M)^2} \\ge \\frac{\\alpha^k}{k!} \\frac{\\left\\|x\\right\\|^{2k}}{M^{2k}}$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int \\left\\|x\\right\\|^{2k} \\mu(\\mathop{}\\!\\mathrm{d} x) \\le K \\frac{k!}{\\alpha^k} M^{2k}&nbsp;&nbsp;&nbsp; \\] for a universal constant.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of covariance operators of centred Gaussian measures in Hilbert spaces.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; In a Hilbert space $\\mathcal{H}$, the covariance operators $\\widehat{C}_\\mu \\colon \\mathcal{H} \\to \\mathcal{H}$ of centred Gaussian measures are exactly the positive symmetric trace class operators (in particular compact), and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{trace} (\\widehat{C}_\\mu) = \\int \\left\\|x\\right\\|^2\\mu(\\mathop{}\\!\\mathrm{d} x).&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $\\mu$ is a Gaussian measure on $\\mathcal{H}$ and $\\widehat{C}_\\mu \\colon \\mathcal{H} \\to \\mathcal{H}$ its covariance operator, then it is symmetric and $\\left&lt;h,\\widehat{C}_\\mu h \\right&gt; = \\int \\left&lt;h,x \\right&gt; ^2 \\mu(\\mathop{}\\!\\mathrm{d} x) \\ge 0$, and for any ONB $(e_n)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int \\left\\|x\\right\\|^2 \\mu(\\mathop{}\\!\\mathrm{d} x) = \\sum_{n=1}^\\infty \\int \\left&lt;x,e_n \\right&gt; ^2 \\mu(\\mathop{}\\!\\mathrm{d} x) = \\sum_{n=1}^\\infty \\left&lt;e_n,\\widehat{C}_\\mu e_n \\right&gt;&nbsp; = \\operatorname{trace}(\\widehat{C}_\\mu). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Conversely, if $K$ is trace class, then it is compact so there is an ONB $(e_n)$ and $\\lambda_n \\ge 0$ with $\\sum_n \\lambda_n &lt; \\infty$ and $K e_n =\\lambda_n e_n$. Let $(\\xi_i)_{i=1}^\\infty$ be i.i.d. $\\mathcal{N}(0,1)$, and define $\\mu$ to be the law of $X:= \\sum_{n=1}^\\infty \\sqrt{\\lambda_n}&nbsp; \\xi_n e_n$ (this converges absolutely a.s. by assumption on $\\lambda_n$). If $h \\in \\mathcal{H}$, then $\\left&lt;h,X \\right&gt; = \\sum_{n=1}^\\infty \\sqrt{\\lambda_n} \\xi_n \\left&lt;h,e_n \\right&gt; $. Denote by $S_N$ the $N$'th partial sum, so $S_N \\to \\left&lt;h,X \\right&gt; $ a.s. Then $S_N$ is centred Gaussian with \\[&nbsp;&nbsp;&nbsp; \\mathbb{V}(S_N) = \\mathbb{E} \\left[ S_N^2 \\right] = \\sum_{n,m=1}^N \\sqrt{\\lambda_n \\lambda_m}&nbsp; h_n h_m \\mathbb{E} \\left[ \\xi_n \\xi_m \\right] = \\sum_{n=1}^N \\lambda_n \\left&lt;h,e_n \\right&gt; ^2 \\to \\left&lt;h,Kh \\right&gt; ,\\quad N \\to \\infty.\\] This implies that $\\left&lt;h,X \\right&gt; \\sim \\mathcal{N}(0,\\left&lt;h,Kh \\right&gt; )$ as required.</i> </p> </div><div></div>"
  },
  {
    "front": "Relation between Gaussian measures and Gaussian processes in $\\mathbb{R}^T$ and $C(K,\\mathbb{R})$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $T$ is a topological space, then Gaussian measures on $\\mathbb{R}^T$ (with the product topology) are exactly Gaussian processes,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $K$ is a compact Hausdorff space, then Gaussian measures in $C(K,\\mathbb{R})$ are exactly Gaussian processes.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>One can show that $(\\mathbb{R}^T)^\\star$ consists exactly of finite linear combinations of point evaluations (see my CS Bachelors thesis), so the law of an $\\mathbb{R}^T$ values random variable $(X_t)_{t\\in T}$ is a Gaussian measure iff its fidis are Gaussian, i.e. iff it is a Gaussian process.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>The non-negative elements of the dual of $C(K,\\mathbb{R})$ are exactly Radon measures (finite and inner regular on open sets). Since Dirac delta's are Radon, a Gaussian measure is always the law of a Gaussian process. Conversely, let $f = (f(t))_{t\\in K}$ be a $C(K,\\mathbb{R})$-valued Gaussian process. The set of linear combinations of Dirac delta functions is dense w.r.t. weak convergence of measures in the set of Radon measures, so for any $\\mu$ there is a sequence $\\mu_n = \\sum_{i=1}^{m_n} a^{(n)}_i \\delta_{t^{(n)}_i} \\to \\mu$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(f) \\longleftarrow \\mu_n(f) = \\sum_{i=1}^{m_n} a^{(n)}_i f(t ^{(n)}_i) \\sim \\mathcal{N}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "For $\\lambda_k \\in \\mathbb{R}$ and i.i.d. $\\eta_i \\sim \\mathcal{N}(0,1)$, when is $\\sum_{k=1}^\\infty \\lambda_k \\eta_k$ absolutely convergent?",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\lambda_k \\ge 0$ and $\\eta_k \\sim \\mathcal{N}(0,1)$ are i.i.d.\\, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{k=1}^\\infty \\lambda_k^2 &lt; \\infty &amp;\\implies \\sum_{k=1}^\\infty \\lambda_k \\left| \\eta_k \\right| &lt; \\infty \\text{ a.s.},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sum_{k=1}^\\infty \\lambda_k^2 = \\infty &amp;\\implies \\sum_{k=1}^\\infty \\lambda_k \\left| \\eta_k \\right| = \\infty \\text{ a.s.}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Recall that $\\mathbb{E} \\left[ \\mathrm{e}^{-\\lambda \\eta_1} \\right] \\approx \\mathrm{e}^{-\\lambda^2 / 2}$ for small $\\lambda &gt; 0$ (bounded above and below for uniformly in $\\lambda \\in [0,K]$ for any $K &gt; 0$). We may assume that $\\sup_{k\\ge 0}\\lambda_k &lt; \\infty$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\mathrm{e}^{-\\sum_{k=1}^\\infty \\lambda_k \\left| \\eta_k \\right| } \\right] = \\prod_{k=1}^\\infty \\mathbb{E} \\left[ \\mathrm{e}^{-\\lambda_k \\left| \\eta_k \\right| } \\right] \\approx \\prod_{k=1}^\\infty \\mathrm{e}^{-\\lambda_k^2 / 2} = \\mathrm{e}^{-\\frac{1}{2}\\sum_{k=1}^\\infty \\lambda_k^2}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Hence if $\\sum_{k=1}^\\infty \\lambda_k^2 = \\infty$ then this is zero so the random sum is zero almost-surely, and if $\\sum_{k=1}^\\infty \\lambda_k^2 &lt; \\infty$ then this is positive so the random sum is finite with positive probability and hence with probability one (because it is a tail event).</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Directional derivatives $f'(x;u)$ of a convex function on $\\mathbb{R}^d$. What properties does $f'(x;\\cdot )$ have?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $f\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ be convex. Then all directional derivatives \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f'(x;u) := \\lim_{h \\to 0} \\frac{f(x+hu) - f(x)}{h},\\quad x \\in \\operatorname{interior} \\operatorname{dom} f,\\,u\\in&nbsp; \\mathbb{R}^d,\\\\&nbsp;&nbsp;&nbsp; \\] in particular left- and right partial derivatives exist. $f'(x;\\cdot ) \\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ is positively homogenous and convex. $f$ is differentiable $\\lambda$-a.e.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Existence of directional derivatives follows from the one-dimensional result. Positive homogenity is obvious, and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f'(x;\\lambda u + \\mu u') &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{h\\to 0} \\frac{f(x + \\lambda (hu) + \\mu (hu')) - f(x)}{h}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\lim_{h\\to 0} \\left( \\lambda \\frac{f(x+hu) - f(x)}{h} + \\mu \\frac{f(x+hu') - f(x)}{h}&nbsp;&nbsp;&nbsp;&nbsp; \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lambda f'(x;u) + \\mu f'(x;u').&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Differentiability we do not prove here.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of convexity for once and twice differentiable functions $f\\colon \\mathbb{R}^d \\supset A \\to \\mathbb{R}$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $A\\subset \\mathbb{R}$ open and $f\\colon A \\to \\mathbb{R}$. Then<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f$ is differentiable, then $f$ is convex iff $f'$ is increasing,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $f$ is twice differentiable, then $f$ is convex iff $f'' \\ge 0$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f$ is convex and differentiable, then $f' = f'_+$ is increasing. If $f$ is differentiable and $f' \\ge 0$, let $x,y\\in A$ and $\\mu + \\lambda = 1$, $\\mu,\\lambda \\ge 0$, $z := \\lambda x + \\mu y$, then there are $z_1 \\in [x,z]$ and $z_2 \\in [z,y]$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{f(z) - f(x)}{z-x} = f'(z_1) \\le f'(z_2) = \\frac{f(y) - f(z)}{y-z}.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Then, $z - x = \\mu(y-x)$ and $y-z = \\lambda(y-x)$, so rearranging yields $f(z) \\le \\lambda f(x) + \\mu f(y)$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f'$ is increasing iff $f'' \\ge 0$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><b>Corollary.</b> <i>&nbsp;&nbsp;&nbsp; Let $A\\subset \\mathbb{R}^d$ be open and $f\\colon A \\to \\mathbb{R}$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f$ is differentiable, then $f$ is convex iff \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;\\nabla f(x) - \\nabla f(y),x-y \\right&gt; \\ge 0,\\quad x,y\\in A.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $f$ is twice differentiable, then $f$ is convex iff the Hessian $H(x)$ is positive semi-definite for all $x\\in A$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: Definition of support function $h_A\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ of a convex set $\\emptyset \\neq A\\subset \\mathbb{R}^d$, and some properties:<br><ol>  <li>Does $h_A$ characterise $A$?&nbsp;&nbsp;&nbsp;</li>  <li>When is $h_A$ real-valued everywhere?&nbsp;&nbsp;&nbsp;</li>  <li>Monotonicity&nbsp;&nbsp;&nbsp;</li>  <li>Minkowski combinations&nbsp;&nbsp;&nbsp;</li>  <li>If $A = \\operatorname{conv} \\bigcup_{i\\in I} A_i$, then what is $h_A$?&nbsp;&nbsp;&nbsp;</li>  <li>If $A = \\bigcap_{i\\in I} A_i$, then what is $h_A$?</li></ol>",
    "back": "<div>If $A \\subset \\mathbb{R}^d$ is non-empty and convex, define $h_A\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ by \\[&nbsp;&nbsp;&nbsp; h_A(u) := \\delta_A^\\star = \\sup_{x\\in A} \\left&lt;x,u \\right&gt; .\\]<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; $h_A$ is closed, convex, and positively homogenous, and<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$h_A = h_{\\operatorname{cl} A}$ and $\\operatorname{cl} A = \\left\\{ x\\colon \\left&lt;x,\\cdot&nbsp; \\right&gt; \\le h_A \\right\\} $,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$h_A$ is real-valued iff $A$ is bounded,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$A\\subset B \\implies h_A \\le h_B \\implies \\operatorname{cl} A \\subset \\operatorname{cl} B$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\alpha,\\beta \\ge 0$ then $h_{\\alpha A + \\beta B}= \\alpha h_A + \\beta h_B$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $A = \\operatorname{conv} \\bigcup_{i\\in I} A_i$ then $h_A = \\sup_i h_{A_i}$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $A = \\bigcap_{i\\in I} A_i$ then $h_A = \\operatorname{cl} \\operatorname{conv} (h_{A_i})_{i\\in I}$ (where $\\operatorname{conv}$ of functions is largest convex lower bound, i.e. convex hull of epigraph).&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $h_A$ is closed and convex because it is a complex conjugate, and obviously positively homogenous. $h_A =h_{\\operatorname{cl} A}$ is also obvious. For $(i)$, $\\supset$ is clear, and if $\\left&lt;x,u \\right&gt; &gt; h_A(u)$ for some $u$ then $x \\not\\in \\operatorname{cl} A$. If $A$ is bounded then $h_A$ is always finite, if $A$ is unbounded then it contains a ray so $h_A$ is unbounded in that direction. (iii) is also clear. For (iv) note that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_{\\alpha A + \\beta B}(u)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sup_{x\\in A, y\\in B} \\left( \\alpha \\left&lt;x,u \\right&gt; + \\beta \\left&lt;y,u \\right&gt;&nbsp; \\right) = \\alpha h_A(u) + \\beta h_B(u).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For (v), $\\ge$ follows from (iii), and if $x\\in A$, then $x = \\sum_{i=1}^k \\lambda_i x_i$, so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;x,u \\right&gt; = \\sum_{i=1}^k \\lambda_i\\left&lt;x_i,u \\right&gt; \\le \\sum_{i=1}^k \\lambda_i h_{A_i}(u) \\le \\sup_i h_{A_i}(u).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For (vi), $h_A \\le h_{A_i}$ for all $i$ so $h_A \\le \\operatorname{conv} h_{A_i}$ because $h_A$ is convex so $h_A \\le \\operatorname{cl} \\operatorname{conv} h_{A_i}$ because $h_A$ is closed. Suppose $f \\le h_{A_i}$ for all $i$ is closed and convex. Other direction no idea.</i> </p> </div><div></div>"
  },
  {
    "front": "Convex geometry: One-to-one correspondence between convex bodies and and support functions.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; There is a one-to-one correspondence between<br>&nbsp;&nbsp;&nbsp; <ol>  <li>Closed convex non-empty sets and $h\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ closed, convex, and positively homogenous,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Convex bodies and $h\\colon \\mathbb{R}^d \\to \\mathbb{R}$ closed, convex, and positively homogenous,&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; via $A \\mapsto h_A$ and $h \\mapsto \\operatorname{dom} h^\\star$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; One direction follows by taking $h=h_A$. For the other direction, let $h$ be given (proper, otherwise take $A = \\mathbb{R}^d$) and note that positive homogenity of $h$ implies that $h^\\star\\colon \\mathbb{R}^d \\to (-\\infty,\\infty]$ (which is closed convex and proper) takes only $0$ and $\\infty$ as values, so $A := \\operatorname{dom} h^\\star$ is closed and convex and non-empty, and $h^\\star = \\delta_A$, so \\[&nbsp;&nbsp;&nbsp; h = h^{\\star\\star} = \\delta_A^\\star = h_A.&nbsp;&nbsp;&nbsp; \\] (ii) follows from (i) because $h_A$ is real-valued iff $A$ is bounded.</i> </p> </div><div></div>"
  },
  {
    "front": "Embeddings $C(K) \\hookrightarrow C_0(\\mathbb{R}^d)$ and $C_b(B) \\hookrightarrow L^p(B)$ for $B,K\\subset \\mathbb{R}^d$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; <ol>  <li>If $K\\subset \\mathbb{R}^d$ is compact, then there is an isometric embedding $C(K) \\hookrightarrow C_0(\\mathbb{R}^d)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $B\\in \\mathcal{B}(\\mathbb{R}^d)$ such that $\\lambda(B) &lt; \\infty$ and $\\lambda(B \\cap B(x,r)) &gt; 0$ for all $x\\in B$ and $r &gt; 0$, then there is an embedding $C_b(B) \\hookrightarrow L^p(B)$ for every $p\\in [1,\\infty]$. If $\\lambda(\\partial B) = 0$, then it has dense image.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) in the case $K = [0,1]$ follows by connecting the endpoints linearly with zero, the general statement follows from Tietze's extension theorem. For (ii) note that the natural identification yields a continuous linear map $C_b(B) \\to L^p(B)$ because $\\lambda(B) &lt; \\infty$, it remains to show injectivity. If $f = 0$ a.e. then for every $x\\in B$ there is $x_n \\to x$ with $f(x_n) = 0$ for all $n\\in \\mathbb{N}$ (by assumption on $B$), so $f(x) = 0$ by continuity, so $f \\equiv 0$. If $\\lambda(\\partial B) = 0$, then $C_c^\\infty(B)$ is dense in $L^p(B)$, in particular $C_b(B)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Extension theorem for densely defined linear operators on normed spaces.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a normed space and $Y$ a Banach space, and $T\\colon D\\subset X \\to Y$ is a densely defined bounded linear operator, then there is a unique extension of $T$ to $\\overline{T}\\colon X \\to Y$ with $\\left\\|\\overline{T}\\right\\|= \\left\\|T\\right\\|$, and if $T$ is an isometry then so is $\\overline{T}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Clearly the extension is unique if it exists. Let $x\\in X$ and $x_n \\to x$ with $x_n \\in D$. Then, $(T x_n)$ is Cauchy in $Y$ so $\\overline{T}x := \\lim_{n\\to \\infty}T x_n $ exists. This is well-defined because if $x_n'\\to x$ as well, then $\\left\\|T x_n - T x_n'\\right\\| \\le \\left\\|T\\right\\| \\left\\|x_n - x_n'\\right\\|\\to 0$, and obviously linear, and \\[&nbsp;&nbsp;&nbsp; \\left\\|\\overline{T} x\\right\\|= \\lim_{n\\to \\infty} \\left\\|T x_n\\right\\| \\le \\left\\|T\\right\\| \\lim_{n\\to \\infty} \\left\\|x_n\\right\\| = \\left\\|T\\right\\|\\left\\|x\\right\\|.&nbsp;&nbsp;&nbsp; \\] Hence $\\overline{T}$ is a bounded linear operator that extends $T$ with $\\left\\|\\overline{T}\\right\\|\\le \\left\\|T\\right\\|$, and obviously $\\left\\|\\overline{T}\\right\\|\\ge \\left\\|T\\right\\|$.</i> </p> </div><div></div>"
  },
  {
    "front": "Direct sum and projection for normed spaces and connection.",
    "back": "<div>A normed space $X$ is the <i>direct sum</i> of two closed linear subspaces $X_1$ and $X_2$, written $X = X_1 \\oplus X_2$ if $X_1 \\cap X_2 = \\left\\{ 0 \\right\\} $ and $X_1 + X_2 = X$. A linear operator $P\\colon X \\to X$ (not necessarily bounded) is a <i>projection</i> if $P^2 = P$.<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a normed space.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $P$ is a <i>bounded</i> projection and $X_1 = R(P)$, $X_2 = N(P)$, then $X = X_1 \\oplus X_2$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X = X_1 \\oplus X_2$, then $P$ defined by $x_1 + x_2 \\mapsto x_1$ is a projection.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; In either case, $\\left\\|P\\right\\| \\ge 1$ (except if $P = 0$), and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_1 = R(P) = N(I-P),\\qquad X_2 = N(P) = R(I-P).&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $P$ is a projection, then so is $I-P$, in which case $R(P) =N(I-P) =: X_1$ and $R(I-P) = N(P) =: X_2$ are closed linear spaces (because $P$ is bounded), $X_1 \\cap X_2 = R(P) \\cap N(P) = \\left\\{ 0 \\right\\} $, and if $x\\in X$ then $x = Px + (I-P)x \\in X_1 + X_2$.<br><br>&nbsp;&nbsp;&nbsp; If $X = X_1\\oplus X_2$ for closed linear spaces $X_1$ and $X_2$, then for every $x\\in X$ there is a unique decomposition $x = x_1 + x_2$, put $Px := x_1$. This is well-defined, linear, and $P^2 = P$.</i> </p> </div><div></div>"
  },
  {
    "front": "Metastability (potential theory): Definition of harmonic function $h_{A,B}$ and equilibrium potential $e_{A,B}$",
    "back": "<div>Let $A,B\\subset S$ be disjoint.<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $S$ is finite, then the unique solution to the Dirichlet problem \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -Lh = 0 &amp; \\text{on $S\\setminus (A\\cup B)$}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h = 1 &amp; \\text{on $A$}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h = 0 &amp; \\text{on $B$}&nbsp;&nbsp;&nbsp; \\end{cases}\\qquad \\text{is given by}\\qquad\\\\&nbsp;&nbsp;&nbsp; h_{A,B}(x) = \\mathbb{P}^x(\\tau_A &lt; \\tau_B) \\quad (x\\not\\in A\\cup B).\\] Then, \\[e_{A,B}(x) := -Lh_{A,B}(x) = \\begin{cases}&nbsp;&nbsp;&nbsp; \\mathbb{P}_x(\\tau_B &lt; \\tau_A) &amp;, x\\in A,\\\\&nbsp;&nbsp;&nbsp; -\\mathbb{P}_x(\\tau_A &lt; \\tau_B) &amp;, x\\in B,\\\\&nbsp;&nbsp;&nbsp; 0 &amp;, \\text{else},\\\\\\end{cases}\\] is called the <i>equilibrium potential</i>.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $x\\in A\\cup B$, then by conditioning on the first jump, $\\mathbb{P}_x(\\tau_A &lt; \\tau_B) = (Ph)(x) = Lh(x) + h(x)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Metastability (potential theory): Green's function (definition, connection with inhomogenous Dirichlet problem, and connection with $h$ and $e$).",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; The <i>Green's function</i> of $X$ killed at $B$ is \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; G_B(x,y) := \\mathbb{E}^x \\left[ \\sum_{n=0}^{\\tau_B} \\boldsymbol{1}_{\\left\\{X_n =y\\right\\}} \\right] ,\\quad x,y\\not\\in B.&nbsp;&nbsp;&nbsp; \\] Then the solution to \\[&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -Lh = g &amp; \\text{on $S \\setminus B$},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h = 0 &amp; \\text{on $B$},\\\\&nbsp;&nbsp;&nbsp; \\end{cases}\\] is given by \\[h(x) = \\mathbb{E}_x \\left[ \\sum_{n=0}^{\\tau_B} g(X_n) \\right] = \\sum_{y\\not\\in B}G_B(x ,y) g(y).\\]&nbsp; In particular, \\[h_{A,B} = \\sum_{y\\in A} G_B(\\cdot ,y) e_{A,B}(y),\\qquad h_{a,B}(x) = G_B(x,a) e_{a,B}(a).\\] Also, $x\\mapsto \\mathbb{E}^x[\\tau_B] = \\sum_y G(x,y)$ is the solution of $-Lf = 1$.</p> </div><div></div>"
  },
  {
    "front": "Metastability (potential theory): Dirichlet form $\\mathcal{E}(f,g)$ on $L^2(S,\\mu)$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Define the Dirichlet form associated with $\\mu$ and $S$ by \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{E}(f,g) := \\left&lt;f,-Lg \\right&gt; _{L^2(S,\\mu)} = \\frac{1}{2}\\sum_{x,y\\in S} \\mu(x) p(x,y) (f(x)-f(y))(g(x)-g(y)).&nbsp;&nbsp;&nbsp; \\] In particular, it is positive semi-definite.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; To get the second equality, note that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{E}(f,g) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\sum_{x\\in S} f(x) (-Lg)(x) &amp;= \\sum_{x,y\\in S} \\mu(x) p(x,y) f(x) (g(x)-g(y))\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{x,y\\in S} \\mu(y) p(y,x) f(x)(g(x)-g(y)),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where reversability was used in the final step. Symmetrising the last two representations yields the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Metastability (potential theory): Capacity $\\operatorname{capc}(A,B)$, last-exit biased distribution $\\nu_{A,B}$, formula for $\\nu_{A,B}(f)$ and corollary on $\\mathbb{E}_x[\\tau_B]$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; Define by \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{capc}(A,B) = \\sum_{x\\in A} \\mu(x) e_{A,B}(x),\\quad, \\nu_{A,B}(x) := \\frac{\\mu(x) e_{A,B}(x)}{\\operatorname{capc} (A,B)},\\\\&nbsp;&nbsp;&nbsp; \\] respectively the <i>capacity</i> of $A$ and $B$ and the <i>last-exit biased</i> distribution on $A$. Then, if $-L f = g$ on $B^{c}$ and $f = 0$ on $B$, \\[&nbsp;&nbsp;&nbsp; \\nu_{A,B}(f) = \\frac{1}{\\operatorname{capc} (A,B)} \\mu(h_{A,B}g).&nbsp;&nbsp;&nbsp; \\] In particular (use $g\\equiv 1$), \\[&nbsp;&nbsp;&nbsp; \\mathbb{E}_x\\left[ \\tau_B \\right] = \\frac{1}{\\operatorname{capc}(x,B)} \\mu(h_{x,B}).&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Multiply $\\mu(x) G(x,y) = \\mu(y) G(y,x)$ by $g(x)e(y)$ and sum over $x,y\\in S$ to obtain<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(hg) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{x,y} \\mu(x) G(x,y) g(x) e(y) = \\sum_{x,y} e(y) \\mu(y) G(y,x) g(x) =\\sum_y \\mu(y) f(y)e(y) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\operatorname{capc}(A,B)\\nu(f)&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Metastability (Potential theory): Dirichlet and Thomson principles.",
    "back": "<div><p><b>Theorem.</b> <i>[Dirichlet and Thomson principles]<br>&nbsp;&nbsp;&nbsp; We have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{capc}(A,B) = \\inf_{\\phi \\in \\Phi_{A,B}} \\mathcal{E}(\\phi,\\phi) = \\sup_{u\\in U_{A,B}} \\mathcal{D}(u,u)^{-1},\\\\&nbsp;&nbsp;&nbsp; \\] where $\\Phi_{A,B} = \\left\\{ \\phi\\colon S \\to [0,1]\\colon \\phi\\!\\!\\restriction_{A} \\equiv 1, \\phi\\!\\!\\restriction_{B} \\equiv 0 \\right\\}$, and $U_{A,B}$ denotes the set of unit flows $u\\colon S\\times S\\to \\mathbb{R}$ from $A$ to $B$, and \\[&nbsp; &nbsp; \\mathcal{D} (u,u)= \\sum_{x,y\\in S} \\frac{1}{\\mu(x)p(x,y)} u(x,y)^2.\\] The infimum is uniquely attained at $\\phi = h_{A,B}$, and the supremum is uniquely attained at \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u(x,y) = \\frac{\\mu(x) p(x,y) \\big( h_{A,B}(y) - h_{A,B}(x)\\big)_+}{\\operatorname{capc}(A,B)}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Metastability (potential theory): $h$, $e$, and capacity in the case of nearest neighbout random walk on $\\mathbb{Z}$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Consider the nearest-neighbour random walk on $[a,b]\\subset \\mathbb{Z}$. Define the <i>resistance</i> between $u$ and $v$ by \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R(u,v) = \\sum_{y=u+1}^v \\frac{1}{\\mu(y) p(y,y-1)},\\quad u &lt; v.&nbsp;&nbsp;&nbsp; \\] (Interpret $\\mu(x) p(x,y)$ as conductivity from $x$ to $y$). Then, \\[&nbsp;&nbsp;&nbsp; h_{b,a}(x) = \\mathbb{P}_x(\\tau_b &lt; \\tau_a) = \\frac{R(a,x)}{R(a,b)},\\quad a &lt; x &lt; b.\\] The equilibrium measure (in this case the escape probability from $a$ to $b$) \\[e_{a,b}(a) = \\frac{1}{\\mu(a) R(a,b)},\\\\\\] and the capacity is $\\operatorname{capc}(a,b) = R(a,b)^{-1}$.</p> </div><div></div>"
  },
  {
    "front": "Quotient space $X/Y$ of a normed vector space. What is the norm of the projection map? When is the result Banach?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a normed space and $Y\\subset X$ a closed linear subspace. Then, \\[&nbsp;&nbsp;&nbsp; X / Y := \\left\\{\\widehat{x}:=&nbsp; x + Y\\colon x\\in X \\right\\} ,\\quad \\left\\|\\widehat{x}\\right\\|:= \\left\\|x-Y\\right\\|,\\\\&nbsp;&nbsp;&nbsp; \\] is a normed space, and the map $Q\\colon X \\to X/Y\\colon x \\mapsto x+Y$ has $\\left\\|Q\\right\\|=1$. If $X$ is complete, then so is $X / Y$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We may assume that $Y \\neq X$. It is clearly a vector space, and $\\left\\|\\widehat{x}\\right\\|$ is well-defined. It is obviously homogenous and definite, and if $x_1,x_2\\in X$, then for any $y_1,y_2\\in Y$,<br>&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\widehat{x}_1 + \\widehat{x}_2\\right\\|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left\\|x_1 + x_2 - y_1-y_2\\right\\| \\le \\left\\|x_1-y_1\\right\\|+\\left\\|x_2-y_2\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Taking the infimum over $y_1$ and $y_2$ yields the triangle inequality. Now, $\\left\\|\\widehat{x}\\right\\| \\le \\left\\|x - 0\\right\\|= \\left\\|x\\right\\|$, so $\\left\\|Q\\right\\|\\le 1$. Furthermore, for any $\\delta \\in (0,1)$ there is an $x\\in X$ such that $\\left\\|x\\right\\|=1$ and $\\left\\|\\widehat{x}\\right\\|=\\left\\|x-Y\\right\\|\\ge 1-\\delta$, so $\\left\\|Q\\right\\|\\ge 1$.<br><br>&nbsp;&nbsp;&nbsp; Finally, suppose that $X$ is complete, and $(\\widehat{x}_k)$ a Cauchy sequence in $X / Y$. It suffices to show that it has a converging subsequence, so we may assume that $\\left\\|\\widehat{x}_n - \\widehat{x}_{n-1}\\right\\|\\le 2^{-n}$. Take any representative $x_1$ of $\\widehat{x}_1$. Then if we have chosen representatives $x_1, \\ldots ,x_{n}$, take $x_{n+1}\\in X$ so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|x_n - x_{n+1}\\right\\| \\le 2\\left\\|\\widehat{x}_n - \\widehat{x}_{n+1}\\right\\| \\le 2^{-n}.&nbsp;&nbsp;&nbsp; \\] Then, $(x_n)$ is a Cauchy sequence in $X$, hence $x_n \\to x$, so $\\widehat{x}_n \\to \\widehat{x}$.</i> </p> </div><div></div>"
  },
  {
    "front": "Connection between direct sums and quotient for Banach spaces.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X = Y \\oplus Z$ is a Banach space, then $Z$ and $X / Y$ are isomorphic (not necessarily isometrically).</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Define $J\\colon Z \\to X / Y;\\, z \\mapsto \\widehat{z} = z+Y$. Then $J$ is linear and bounded. If $Jz = 0$, then $z\\in Y$, so $z = 0$. If $x\\in X$, then $x = y + z$, so $Jz = \\widehat{x}$, so $J$ is bijective, hence $J^{-1}$ is continuous by the open mapping theorem.</i> </p> </div><div></div>"
  },
  {
    "front": "Sum of Banach spaces (not direct sum).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $X,Y$ are Banach spaces that are subspaces of a topological vector space $Z$ and that the inclusions are continuous. Define <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X+Y &amp;= \\left\\{ x+y\\colon x\\in X,y\\in Y \\right\\},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|z\\right\\|_{X+Y}&amp;:= \\inf \\left\\{ \\left\\|x\\right\\|_X + \\left\\|y\\right\\|_Y\\colon z=x+y,x\\in X,y\\in Y \\right\\} .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then this is a Banach space with coarser metric, and isometrically isomorphic to $X\\times Y / D$, where $D = \\left\\{ (u,-u)\\colon u\\in X\\cap Y \\right\\} $.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $X+Y$ is a vector space. Suppose $\\left\\|z\\right\\| = 0$, so there are $z = x_n + y_n$ with $x_n \\to 0$ in $X$ and $y_n \\to 0$ in $Y$, so $x_n + y_n \\to 0$ in $Z$, so $z = 0$ (this uses that addition and the embeddings are continuous in $Z$). Triangle inequality and homogenity are clear, so it is a norm. If $x\\in X$, then $\\left\\|x\\right\\| \\le \\left\\|x\\right\\|_X + \\left\\|0\\right\\|_Y = \\left\\|x\\right\\|_X$. Similar for $y\\in Y$.<br><br>$D$ is closed, so $X\\times Y / D$ is a Banach space. Define $J\\colon X\\times Y/D \\to X+Y;\\, (x,y)+D \\mapsto x+y$. This is obviously well-defined and linear. Furthermore, if $x\\in X$ and $y\\in Y$, then <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\left\\|(x,y)+D\\right\\| &nbsp;&nbsp;&nbsp; &amp;= \\inf \\left\\{ \\left\\|x+u\\right\\|_X + \\left\\|y-u\\right\\|_Y\\colon u\\in X\\cap Y \\right\\} \\\\&nbsp;&nbsp;&nbsp; &amp;= \\inf \\left\\{ \\left\\|x'\\right\\|_X + \\left\\|y'\\right\\|_Y\\colon x'\\in X,y'\\in Y,x'+y' = x+y \\right\\} \\\\&nbsp;&nbsp;&nbsp; &amp;= \\left\\|x+y\\right\\|_{X+Y} \\\\&nbsp;&nbsp;&nbsp; &amp;= \\left\\|J [(x,y)+D]\\right\\|_{X+Y}.\\end{align*}[/$$]<br>Hence, $J$ is an isometry, in particular injective. If $x\\in X$, $y\\in Y$, then $J[(x,y)+D] = x+y$, so it is also surjective. Since $X\\times Y/D$ is complete, so is $X+Y$.</i> </p> </div><div></div>"
  },
  {
    "front": "Inner product spaces: Cauchy-Schwartz, lipschitz continuity of $\\left&lt;\\cdot ,\\cdot&nbsp; \\right&gt; $, parallelogram identity.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; In an inner product space, $\\left&lt;\\cdot ,\\cdot&nbsp; \\right&gt; $ is Lipschitz-continuous on balls in $X\\times X$, and the <i>parallelogram identity holds:</i> \\[&nbsp;&nbsp;&nbsp; \\left\\|x+y\\right\\|^2 + \\left\\|x-y\\right\\|^2 = 2 \\left\\|x\\right\\|^2 + 2 \\left\\|y\\right\\|^2.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x,y\\in X$, then for any $\\alpha \\in \\mathbb{R}$, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 \\le \\left\\|x-\\alpha y\\right\\|^2 = \\left\\|x\\right\\|^2 - 2 \\alpha \\Re \\left&lt;x,y \\right&gt; + \\alpha^2 \\left\\|y\\right\\|^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$] Minimising for $\\alpha$ gives $\\alpha = \\Re \\left&lt;x , y \\right&gt; / \\left\\|y\\right\\|^2$, in which case this becomes \\[&nbsp;&nbsp;&nbsp; 2 \\left( \\Re \\left&lt;x,y \\right&gt;&nbsp; \\right)^2 / \\left\\|y\\right\\|^2 \\le \\left\\|x\\right\\|^2 + (\\Re \\left&lt;x,y \\right&gt; )^2 / \\left\\|y\\right\\|^2 \\implies \\left| \\Re \\left&lt;x,y \\right&gt; \\right| \\le \\left\\|x\\right\\|\\left\\|y\\right\\|.&nbsp;&nbsp;&nbsp; \\] Use $\\mathrm{i} \\alpha$ to get this for $\\Im \\left&lt;x,y \\right&gt; $. For Lipschitz continuity, note that<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\left&lt;x_1,y_1 \\right&gt; -\\left&lt;x_2,y_2 \\right&gt;&nbsp; \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left\\| x_1-x_2 \\right\\| \\left\\|y_1\\right\\|+ \\left\\|x_2\\right\\|\\left\\| y_1-y_2 \\right\\| .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "$A^\\perp$ for a subset of an inner product space. Relation to $A$ and $\\overline{\\operatorname{lin} A}$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $A$ is a subset of an inner product space, then $A^\\perp$ is a closed, linear subspace, $A^\\perp = (\\overline{\\operatorname{lin} A})^\\perp$ and $A\\subset A^{\\perp \\perp}$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; All obvious by continuity of scalar product.</i> </p> </div><div></div>"
  },
  {
    "front": "If $Y$ is a closed linear subspace of a Hilbert space, then $H = Y \\oplus \\,?$. What do you know about the associated projection, and about $Y$ and $Y^{\\perp\\perp}$?",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $Y$ is a closed linear subspace of a Hilbert space $H$, then $Y = Y^{\\perp\\perp}$ and $H = Y\\oplus Y^\\perp$, so there is a unique orthogonal projection $P\\colon H\\to H$ with $R(P) = Y$ and $N(P) = Y^\\perp$. It satisfies $\\left\\|P\\right\\| = 1$ (except if $Y = 0$), and \\[&nbsp;&nbsp;&nbsp; \\left\\|x-Px\\right\\|=\\min_{y\\in Y} \\left\\|x-y\\right\\|.&nbsp;&nbsp;&nbsp; \\]</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x\\in X$, and $y_n \\in Y$ with $\\left\\|x-y_n\\right\\| \\to \\left\\|x-Y\\right\\|=: d$. Then, by the parallelogram identity,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|y_n - y_m\\right\\|^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left\\|y_n - x - (y_m - x)\\right\\|^2= \\underbrace{2\\left\\|y_n - x\\right\\|^2 + 2\\left\\|y_m - x\\right\\|^2}_{\\to 4d^2} - 4\\underbrace{\\left\\|\\frac{1}{2}(y_n+y_m) - x\\right\\|^2}_{\\ge d^2}\\to 0,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; as $n,m\\to \\infty$. Since $H$ is complete, this implies the existence of a unique(!) closest point $Px$ in $Y$ to $x$. We don't know yet if $P$ is linear! Now show that $Px-x \\perp Y$. Let $y\\in Y$, then for any $\\alpha &gt;0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d^2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left\\|x - Px - \\alpha y\\right\\|^2 = d^2 - 2\\alpha \\Re \\left&lt;x-Px,y \\right&gt; + \\alpha^2 \\left\\|y\\right\\|^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; This implies $2\\Re \\left&lt;x-Px,y \\right&gt; \\le \\alpha \\left\\|y\\right\\|$, then let $\\alpha \\downarrow 0$. Repeat with $- \\alpha$ and $\\pm \\mathrm{i} \\alpha$ to get $\\left&lt;x-Px,y \\right&gt; = 0$. In particular, $x = (x-Px) + Px \\in Y^\\perp + Y$, and $Y\\cap Y^\\perp = 0$ and both are closed linear subspaces, so $H = Y \\oplus Y^\\perp$. Then there exists a unique linear projection with range $Y$ and kernel $Y^\\perp$. But since the decomposition is unique, this must be $P$, in particular it is linear. It remains to show that $\\left\\|P\\right\\|\\ge 1$ and $Y \\supset Y^{\\perp\\perp}$. If $x\\in X$, then \\[&nbsp;&nbsp;&nbsp; \\left\\|x\\right\\|^2 = \\left\\|x-Px\\right\\|^2 + \\left\\|Px\\right\\|^2,\\\\&nbsp;&nbsp;&nbsp; \\] so $\\left\\|Px\\right\\|\\le \\left\\|x\\right\\|$. Now if $y\\in Y^{\\perp\\perp}$, then \\[&nbsp;&nbsp;&nbsp; \\left\\|y - Py\\right\\|^2 = \\left&lt;y,y-Py \\right&gt; - \\left&lt;Py,y-Py \\right&gt; = 0,\\\\&nbsp;&nbsp;&nbsp; \\] so $y = Py\\in Y$.</i> </p> <br><div></div></div>"
  },
  {
    "front": "Riesz representation theorem (Isometric isomorphism $H \\simeq H^\\star$).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $H$ is a Hilbert space, then $\\Phi\\colon H \\to H^\\star; \\, x \\mapsto \\left&lt;\\cdot ,x\\right&gt; $ is an (anti-)linear isometric bijection.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It is clearly well-defined and antilinear, and $\\left\\|\\Phi\\right\\|\\le 1$, and for any $x\\in H$ with $\\left\\|x\\right\\| = 1$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\Phi\\right\\|_{\\mathcal{L}(H,H^\\star)} \\ge \\left\\|\\Phi(x)\\right\\|_{H^\\star} = \\left\\|\\left&lt;\\cdot ,x \\right&gt; \\right\\|_{H^\\star} \\ge \\left| \\left&lt;x,x \\right&gt;&nbsp; \\right| _H = 1.&nbsp;&nbsp;&nbsp; \\] Now let $0\\neq \\varphi\\in H^\\star$, and $x_0\\in N(\\varphi)^\\perp$ (which is not trivial because $N \\neq H$ and $H = N \\oplus N^\\perp$) with $\\varphi(x_0) = 1$. Then any $y\\in H$ satisfies \\[&nbsp;&nbsp;&nbsp; \\varphi(y - \\varphi(y)x_0) = \\varphi(y) - \\varphi(y) = 0,\\\\\\] so $y-\\varphi(y)x_0\\in N$, so \\[0 = \\left&lt;y-\\varphi(y)x_0,x_0 \\right&gt; = \\left&lt;y,x_0 \\right&gt; - \\varphi(y) \\left\\|x_0\\right\\|^2,\\\\\\] so \\[\\varphi(y) = \\left&lt;y,\\frac{x_0}{\\left\\|x_0\\right\\|^2} \\right&gt; ,\\quad y\\in H.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Orthonormal system, orthonormal basis, and Gram-schmidt procedure in an inner product space. Existence of a countable orthonormal basis in a separable Hilbert space.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A subset $\\left\\{ v_i\\colon i\\in I \\right\\} \\subset X$ of an inner product space is called <i>orthonormal system</i> if $\\left&lt;v_i,v_j \\right&gt; =\\delta_{ij}$ for $i,j\\in I$. It is called an <i>orthonormal basis</i> if it is maximal.</p> <br><br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; For any countable linearly independent set $\\left\\{ w_i\\colon i\\in I \\right\\} \\subset X$, there is an orthonormal system $\\left\\{ v_i\\colon i\\in I \\right\\} $ with the same linear span and \\[&nbsp;&nbsp;&nbsp; v_1 = \\frac{w_1}{\\left\\|w_1\\right\\|},\\qquad \\widetilde{v}_{i+1} = w_{i+1} - \\sum_{j=1}^i \\left&lt;w_{i+1},v_j \\right&gt; v_j,\\quad v_{i+1} = \\frac{\\widetilde{v}_{i+1}}{\\left\\|\\widetilde{v}_{i+1}\\right\\|}.&nbsp;&nbsp;&nbsp; \\] In particular, if $X$ is a separable Hilbert space, then it has a countable orthonormal basis.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $I = \\mathbb{N}$. We inductively show that $\\left\\{ v_i\\colon i \\in [n] \\right\\} $ is orthonormal with the same span as $\\left\\{ w_i\\colon i\\in [n] \\right\\} $. Clear if $i = 1$, so it remains to show that in the inductive step, $0\\neq \\widetilde{v}_{i+1} \\perp v_j$ for $j\\in [i]$. It is non-zero because $w_{i+1} \\not\\in \\operatorname{lin} \\left\\{ w_j\\colon j\\in [i] \\right\\} = \\operatorname{lin} \\left\\{ v_j\\colon j\\in [i] \\right\\} $, and for any $j\\in [i]$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;\\widetilde{v}_{i+1},v_j \\right&gt; = \\left&lt;w_{i+1},v_j \\right&gt; - \\sum_{k=1}^i \\left&lt;w_{i+1},v_k \\right&gt; \\delta_{kj} = 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $X$ is a separable Hilbert space, take a countable dense set $\\Sigma = \\left\\{ w_i\\colon i\\in \\mathbb{N} \\right\\} $, then take $k(1) = 1$, and $k(n+1)$ the first $j$ such that $w_j \\not\\in \\operatorname{lin} \\left\\{ w_{k(i)}\\colon i \\in [n] \\right\\} $, and $v_n := w_{k(n)}$. Then $\\operatorname{lin} \\left\\{ v_n\\colon n\\in \\mathbb{N} \\right\\} = \\operatorname{lin} \\Sigma$ and is linearly independent, so we can turn it into an orthonormal system with the same dense span, so it is an orthonormal basis.</i> </p> </div><div></div>"
  },
  {
    "front": "Projection $P$ onto $\\overline{\\operatorname{lin} S}$ for an orthonormal system $S$ in a Hilbert space (in particular Bessel's inequality).",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $H$ is a Hilbert space and $S = \\left\\{ v_n\\colon n\\in \\mathbb{N} \\right\\} $ an orthonormal system, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P\\colon H \\to H;\\, x \\mapsto \\sum_{n=1}^\\infty \\left&lt;x,v_n \\right&gt; v_n\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] is (well-defined and) an orthogonal projection with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|Px\\right\\|^2 = \\sum_{n=1}^\\infty \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 \\le \\left\\|x\\right\\|^2,\\quad \\textit{(Bessel's inequality)}\\] (in particular $\\left\\|P\\right\\|\\le 1$), and $H = \\overline{\\operatorname{lin} S} \\oplus S^\\perp$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Firstly, for any $N\\in \\mathbb{N}$ let $x_N := x - \\sum_{n=1}^N \\left&lt;x,v_n \\right&gt; v_n$, so that \\[&nbsp;&nbsp;&nbsp; \\left\\|x\\right\\|^2 = \\left\\|x_N\\right\\|^2 + \\sum_{n=1}^N \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 \\ge \\sum_{n=1}^N \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2\\quad \\forall N\\in \\mathbb{N},\\\\&nbsp;&nbsp;&nbsp; \\] so $\\sum_{n=1}^\\infty \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 \\le \\left\\|x\\right\\|^2$ (Bessel's inequality), so \\[&nbsp;&nbsp;&nbsp; \\left\\|\\sum_{n=N}^M \\left&lt;x,v_n \\right&gt; v_n\\right\\|^2 = \\sum_{n=N}^M \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 \\to 0,\\qquad M,N\\to \\infty,\\\\\\] so the limit $Px := \\sum_{n=1}^\\infty \\left&lt;x,v_n \\right&gt; v_n$ exists, $P$ is linear, $P^2 = P$ and $\\left\\|Px\\right\\|^2 = \\sum_{n=1}^\\infty \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 \\le \\left\\|x\\right\\|^2$ (all by continuity of the scalar product). In particular, $N(P) = S^\\perp$ and $R(P)$ is a closed linear subspace with $S\\subset R(P) \\subset \\overline{\\operatorname{lin} S}$. So $P$ is an orthogonal projection and $H = \\overline{\\operatorname{lin} S} \\oplus S$. (The convergence of $Px$ is unconditional, but not necessarily absolute.)</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of maximality of a countable orthonormal system in a Hilbert space.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $S = \\left\\{ v_n\\colon v\\in \\mathbb{N} \\right\\} $ is an orthonormal system in a Hilbert space $H$, then TFAE:<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$S$ is maximal (i.e. an orthonormal basis)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$S^\\perp = \\left\\{ 0 \\right\\} $,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$X = \\overline{\\operatorname{lin} S}$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For all $x\\in X$, $x = \\sum_{n=1}^\\infty \\left&lt;x,v_n \\right&gt; v_n $,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For all $x,y\\in X$, $\\left&lt;x,y \\right&gt; = \\sum_{n=1}^\\infty \\left&lt;x,v_n \\right&gt; \\left&lt;v_n,y \\right&gt; $,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For all $x\\in H$, $\\left\\|x\\right\\|^2 = \\sum_{n=1}^\\infty \\left| \\left&lt;x,v_n \\right&gt;&nbsp; \\right| ^2 $ <i>(Parseval's equality)</i>.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Denote by $P$ the orthogonal projection of $H$ onto $\\overline{\\operatorname{lin} S}$. (i) $\\implies$ (ii) is clear, and (ii) $\\implies$ (iii) follows from $H = \\overline{\\operatorname{lin} S} \\oplus S^\\perp$. If (iii) holds then $R(P) = X$ and $R(1-P) = N(P) = \\left\\{ 0 \\right\\} $, so $x = Px + (1-P)x = Px = \\sum_{n=1}^\\infty \\left&lt;x,v_n \\right&gt; v_n$. By continuity of the scalar product (iv) $\\implies$ (v), which specialises to (vi). If (vi) holds and $w \\perp S$, then $\\left\\|w\\right\\| = 0$ so $S$ is maximal.</i> </p> </div><div></div>"
  },
  {
    "front": "Orthonormal basis of $L^2(0,2\\pi)$ over $\\mathbb{R}$ and $\\mathbb{C}$.",
    "back": "<div><p><b>Proposition.</b>Over $\\mathbb{C}$, $(\\frac{1}{\\sqrt{2\\pi} }\\mathrm{e}^{\\mathrm{i} n t})_{n\\in \\mathbb{N}_0}$ is an ONB. Over $\\mathbb{R}$, $(\\frac{1}{2\\pi}) \\cup (\\frac{1}{\\sqrt{\\pi}}\\cos(n\\cdot ))_n \\cup (\\frac{1}{\\sqrt{\\pi} }\\sin(n\\cdot ) )_n$ is an ONB.</p> </div><div></div>"
  },
  {
    "front": "All separable Hilbert spaces of equal dimension ($\\mathbb{R}^d$ or infinite) are isometrically isomorphic.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; In infinite dimensions, the map $x \\mapsto (\\left&lt;x,v_n \\right&gt; )_{n\\in \\mathbb{N}}$ for an ONB $(v_n)_{n\\in \\mathbb{N}}$ is an isometric isomorphism to $\\ell^2$ with inverse $(x_n) \\mapsto \\sum_{n=1}^\\infty x_n v_n$. Trivial in finite dimensions.</i> </p> </div><div></div>"
  },
  {
    "front": "Baire's theorem (for open and closed sets).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $(X,d)$ is a complete metric space, and $O_n$ are open and dense sets, then $\\bigcap_{n=1} ^\\infty O_n$ is dense. If $A_n$ are closed and $\\bigcup_{n=1} ^\\infty A_n = X$, then one of the $A_n$ has non-emtpy interior.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $x_0\\in X$ and $\\delta =: \\delta_0 &gt; 0$. Then there is $\\overline{B(x_1,\\delta_1)}\\subset&nbsp; \\in B(x_0,\\delta_0) \\cap O_1$ with $\\delta_1 &lt; \\frac{1}{2}\\delta_0$. Inductively, take $\\overline{B(x_n,\\delta_n)} \\subset B(x_{n-1},\\delta_{n-1}) \\cap O_n$ with $\\delta_n &lt; \\frac{1}{2}\\delta_{n-1}$. Then $(x_n)$ is Cauchy so converges to some $x\\in X$, which by construction is in every $\\overline{B(x_n,\\delta_n)} \\subset O_n$ ($n\\in \\mathbb{N}$), and in $B(x_0,\\delta_0)$.<br><br>&nbsp;&nbsp;&nbsp; If $A_n$ are as given, then if all of them had empty interior, $O_n := X \\setminus A_n$ would all be dense open sets, so $\\bigcap_{n=1} ^\\infty O_n = X \\setminus (\\bigcup_{n=1} ^\\infty A_n) = \\emptyset $ would be dense, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Uniform boundedness principle.",
    "back": "<div><p><b>Theorem.</b> <i>[Uniform boundedness]<br>&nbsp;&nbsp;&nbsp; If $X$ is a Banach space and $Y$ a normed space, then every collection $\\mathcal{T}\\subset \\mathcal{L}(X,Y)$ that is pointwise bounded, i.e. $\\sup_{T\\in \\mathcal{T}} \\left\\|Tx\\right\\|&lt; \\infty$ for all $x\\in X$, is uniformly bounded, i.e. $\\sup_{T\\in \\mathcal{T}}\\left\\|T\\right\\|&lt;\\infty$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $A_n := \\left\\{ x\\colon \\left\\|Tx\\right\\| \\le n \\forall T\\in \\mathcal{T} \\right\\} $. Then, $A_n$ is closed and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\bigcup_{n=1} ^\\infty A_n = \\left\\{ x\\colon \\sup_{T\\in \\mathcal{T}} \\left\\|Tx\\right\\|&lt;\\infty \\right\\} =X.&nbsp;&nbsp;&nbsp; \\] By Baire, there is $N\\in \\mathbb{N}$ such that $A_N$ contains a ball, say $B(x_0,\\delta)$. Then, for any $x\\in X$ with $\\left\\|x\\right\\| &lt; 1$, we have $z := x_0 + \\delta x \\in B(x_0,\\delta)$, so \\[&nbsp;&nbsp;&nbsp; \\left\\|Tx\\right\\| =\\frac{1}{\\delta} \\left\\|T(z-x_0)\\right\\| \\le \\frac{1}{\\delta } (N + \\sup_{T\\in \\mathcal{T}} \\left\\|Tx_0\\right\\|),\\\\&nbsp;&nbsp;&nbsp; \\] which is an upper bound independent of $x$ and $T$.</i> </p> </div><div></div>"
  },
  {
    "front": "When does pointwise convergence of operators $T_n \\in \\mathcal{L}(X,Y)$ on a subset $S\\subset X$ imply strong convergence to some $T$?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is normed and $Y$ Banach, and $T_n \\in \\mathcal{L}(X,Y)$ are uniformly bounded and converge pointwise on $S\\subset X$ with $\\overline{\\operatorname{lin} S} = X$, then there is a $T\\in \\mathcal{L}(X,Y)$ with $T_n \\to T$ strongly and $\\left\\|T\\right\\|\\le \\varliminf_{n\\to \\infty} \\left\\|T_n\\right\\|$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $S = \\operatorname{lin} S$, because $T_n$ converge on $\\operatorname{lin} S$ as well. Define $Tx := \\lim_{n\\to \\infty} T_n x$ for $x\\in S$, then&nbsp; $\\left\\|T x\\right\\|\\le (\\varliminf_{n\\to \\infty} \\left\\|T_n\\right\\|) \\left\\|x\\right\\|$ for $x\\in S$, so $T$ is a densely defined bounded linear operator, which extends uniquely to $T \\in \\mathcal{L}(X,Y)$ with the claimed bound on the norm. Let $x\\in X$, and choose $y\\in S$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|T_n x - Tx\\right\\|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le (\\left\\|T_n\\right\\| + \\left\\|T\\right\\|)\\left\\|x-y\\right\\| + \\left\\|T_n y - T y\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then $\\varlimsup_{n\\to \\infty} \\left\\|T_n x - T x\\right\\| \\le 2 (\\sup_n \\left\\|T_n\\right\\|) \\left\\|x-y\\right\\|$ for every $y\\in S$, and letting $y\\to x$ gives the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Product rule for weak derivatives.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $U\\subset \\mathbb{R}^d$ is open, $p,p'\\in [1,\\infty]$ conjugate, $f\\in W^{1,p}(U)$, and $g\\in W^{1,p'}(U)$, then $fg \\in W^{1,1}(U)$ and&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\partial_j (fg) = (\\partial_j f)g + (f \\partial_j g)&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $h_\\varepsilon := G_\\varepsilon h := \\rho_\\varepsilon \\star \\overline{h}$ where $\\rho_\\varepsilon$ is a mollifier and $\\overline{h}$ is the extension to $\\mathbb{R}^d$ by zero of a function $h\\in L^1_\\text{loc}(U)$. Then if $p \\in (1,\\infty)$, $G_\\varepsilon f \\stackrel{ L^p }{\\longrightarrow} f$ on compacts, same for $g$ and derivatives, so by Hölder,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int (\\partial \\varphi) fg\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{\\varepsilon\\to 0} \\int (\\partial \\varphi) f_\\varepsilon g_\\varepsilon = -\\lim_{\\varepsilon\\to 0} \\int \\varphi \\left[ (\\partial f_\\varepsilon) g_\\varepsilon + f_\\varepsilon(\\partial g_\\varepsilon) \\right] = - \\int \\varphi \\left[ (\\partial f) g + f (\\partial g) \\right] .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $p = 1$, then $f_\\varepsilon \\stackrel{ L^1 }{\\longrightarrow} f$ on compacts, same for $g$ and derivatives, so passing to subsequences gives pointwise a.e. convergence and we can repeat the argument with DCT.</i> </p> </div><div></div>"
  },
  {
    "front": "Vitali covering lemma.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mathcal{F}$ is a family of balls with bounded diameter in a separable metric space, then there is a countable disjoint subfamily $\\mathcal{F}'$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\bigcup_{B\\in \\mathcal{F}} B \\subset \\bigcup_{B \\in \\mathcal{F}'} (5B).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Hardy-Littlewood maximal function and estimates.",
    "back": "<div>If $f\\in L^1_\\text{loc}$, then $\\mathcal{M}f (x) := \\sup_{r &gt; 0} \\frac{1}{\\left| B(x,r) \\right| }\\int_{B(x,r)} \\left| f(y) \\right| \\mathop{}\\!\\mathrm{d} y$, where $\\left| \\cdot&nbsp; \\right| $ denotes Lebesgue measure.<br><br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\in L^1(\\mathbb{R}^d)$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\left\\{&nbsp;&nbsp; \\mathcal{M}f \\ge \\alpha&nbsp; \\right\\}\\right|&nbsp; \\le \\frac{c_d}{\\alpha} \\left\\|f\\right\\|_{L^1} \\quad \\bigg(\\text{in fact } \\le \\frac{c_d}{\\alpha} \\int\\limits_{\\left\\{ \\left| f \\right| \\ge \\alpha / 2 \\right\\} } \\left| f \\right|\\bigg).&nbsp;&nbsp;&nbsp; \\] If $f\\in L^p(\\mathbb{R}^d)$ for $p\\in (1,\\infty]$, then \\[&nbsp;&nbsp;&nbsp; \\left\\|\\mathcal{M}f\\right\\|_{L^p} \\le c_p \\left\\|f\\right\\|_{L^p}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For fixed $\\alpha &gt; 0$, if $\\mathcal{M}f(x) \\ge \\alpha$, then there is a ball $B_x$ centred at $x$ such that $\\int_{B_x} \\left| f \\right| \\ge \\frac{\\alpha}{2} \\left| B_x \\right| $. They will have bounded radius because $\\frac{1}{\\left| B \\right| } \\int_B \\left| f \\right| \\le \\frac{1}{\\left| B \\right| } \\left\\|f\\right\\|$, so by Vitali's covering lemma there is a countable subset $B_j$ of disjoint balls such that $(5B_j)_j$ covers $\\left\\{ \\mathcal{M}f \\ge \\alpha \\right\\} $, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\left\\{ \\mathcal{M}f \\ge \\alpha \\right\\}&nbsp; \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le 5^d \\sum_j \\left| B_j \\right| \\le \\frac{2\\cdot&nbsp; 5^d}{\\alpha} \\sum_j \\int_{B_j} \\left| f \\right| \\le \\frac{c_d}{\\alpha} \\left\\|f\\right\\|_{L^1}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For the refinement, let $g := f \\boldsymbol{1}_{\\left\\{ \\left| f \\right| \\ge \\alpha / 2 \\right\\} }$, so that $\\mathcal{M}f \\le \\mathcal{M}g + \\alpha / 2$, and thus \\[&nbsp;&nbsp;&nbsp; \\left| \\left\\{ \\mathcal{M}f \\ge \\alpha \\right\\}&nbsp; \\right| \\le \\left| \\left\\{ \\mathcal{M}g \\ge \\alpha / 2 \\right\\}&nbsp; \\right| \\le \\frac{2c_d}{\\alpha} \\left\\|g\\right\\|_{L^1} = \\frac{2c_d}{\\alpha} \\int\\limits_{\\left| f \\right| \\ge \\alpha / 2} \\left| f \\right| .&nbsp;&nbsp;&nbsp; \\] We prove the second estimate with dimension-dependent constant.<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\mathcal{M}f\\right\\|_{L^p}^p &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_{\\mathbb{R}^d} \\int_0^{\\left| \\mathcal{M}f(x) \\right| } pt^{p-1} \\mathop{}\\!\\mathrm{d} t \\mathop{}\\!\\mathrm{d} x = p \\int_0^\\infty t ^{p-1} \\lambda \\left( \\mathcal{M}f \\ge t \\right) \\mathop{}\\!\\mathrm{d} t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le p c_d \\int_0^\\infty t ^{p-2} \\int \\left| f(x) \\right| \\boldsymbol{1}_{\\left\\{ t \\le 2 \\left| f(x) \\right|&nbsp; \\right\\} }\\mathop{}\\!\\mathrm{d} x \\mathop{}\\!\\mathrm{d} t\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= 2^{p-1}\\frac{p}{p-1} c_d \\int \\left| f(x) \\right| ^p \\mathop{}\\!\\mathrm{d} x.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Lebesgue differentiation and density theorems.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\in L^1_\\text{loc}(\\mathbb{R}^d)$, then a.e. $x\\in \\mathbb{R}^d$ is a <i>Lebesgue point</i> of $f$, that is, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(x) = \\lim_{r\\to 0} \\frac{1}{\\left| B(x,r) \\right| } \\int_{B(x,r)} f(y) \\mathop{}\\!\\mathrm{d} y.&nbsp;&nbsp;&nbsp; \\] In particular, if $A\\subset \\mathbb{R}^d$ is measurable, then \\[&nbsp;&nbsp;&nbsp; \\lim_{r\\to 0} \\frac{\\lambda(A\\cap B(x,r))}{\\lambda(B)} = \\boldsymbol{1}_{A}(x) \\quad \\text{a.e.}&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We may assume that $f$ is compactly supported and thus integrable. Let $g\\in C_c(\\mathbb{R}^d)$ with $\\left\\|f-g\\right\\|_{L^1} &lt; \\varepsilon$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{\\left| B \\right| } \\int_B \\left|f - f(x) \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\frac{1}{\\left| B \\right| } \\int_B \\left| f-g \\right| +&nbsp; \\frac{1}{\\left| B \\right| } \\int_B \\left|g - g(x) \\right| + \\left| f(x) - g(x) \\right| .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; The first term is bounded by $\\mathcal{M}(f-g)(x)$, and the second term vanishes as $B \\to x$. Hence, for any $\\alpha &gt; 0$, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda \\left( x\\colon \\varlimsup_{B\\to x} \\frac{1}{\\left| B \\right| } \\int_B \\left| f-f(x) \\right| \\ge 2\\alpha \\right) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\lambda \\left( \\mathcal{M}(f-g) \\ge \\alpha \\right) + \\lambda \\left( x\\colon \\left| f(x) - g(x) \\right| \\ge \\alpha \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le c(\\alpha,d) \\left\\|f-g\\right\\|_{L^1} \\le c(\\alpha,d) \\varepsilon.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Thus, the LHS above is zero for all $\\alpha &gt; 0$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lambda \\left( x\\colon \\varlimsup_{B\\to x} \\frac{1}{\\left| B \\right| } \\int_B \\left| f-f(x) \\right| &gt; 0 \\right) = 0.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Neumann formula for inversion of sum of operators. In particular, is the set of invertible operators in that setting open, closed, or neither?",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a Banach space and $Y$ a normed space, and $T\\in \\mathcal{B}(X,Y)$ is invertible, and $S\\in \\mathcal{B}(X,Y)$ with $\\left\\|S\\right\\| &lt; \\left\\|T ^{-1}\\right\\|^{-1}$, then $S + T$ is invertible and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (S+T)^{-1} = \\sum_{n=0}^\\infty (- T ^{-1}S)^n T^{-1} = \\sum_{n=0}^\\infty T ^{-1} (- ST^{-1})^n,\\\\&nbsp;&nbsp;&nbsp; \\] with \\[&nbsp;&nbsp;&nbsp; \\left\\|(S+T)^{-1}\\right\\| \\le \\frac{\\left\\|T^{-1}\\right\\|}{1 - \\left\\|S T ^{-1}\\right\\|}.\\] In particular, the set of invertible operators is an open subset of $\\mathcal{B}(X,Y)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The idea is that $T + S = (I + S T ^{-1}) T$. Since $\\left\\|T^{-1}S\\right\\|&lt;1$ by assumption and $\\mathcal{B}(X)$ is a Banach space, the series in the definition of \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R := \\sum_{n=0}^\\infty (-T^{-1}S)^n T^{-1} &nbsp;&nbsp;&nbsp; \\] converges absolutely and hence converges, and $\\left\\|R\\right\\| \\le \\left\\|T^{-1}\\right\\| / (1 - \\left\\|ST^{-1}\\right\\|)$. Furthermore, <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R(S+T) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=0}^\\infty \\left[-(-T^{-1}S)^{n+1} + (-T^{-1}S)^n\\right] = I,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and similarly $(S+T)R = I$.</i> </p> </div><div></div>"
  },
  {
    "front": "Open mapping theorem",
    "back": "<div><p><b>Theorem.</b> <i>[Open mapping theorem]<br>&nbsp;&nbsp;&nbsp; If $X,Y$ are Banach spaces and $T\\in \\mathcal{B}(X,Y)$ is surjective, then $T$ is open. In particular, a bijection $T\\in \\mathcal{B}(X,Y)$ has a continuous inverse.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Denote $U_r = B_X(0,r)$ and $V_r = B_Y(0,r)$. Then it suffices to show that $V_\\varepsilon \\subset T U_R$ for some $\\varepsilon &gt; 0,\\, R&gt; 0$. Since $Y = \\bigcup_{n=1} ^\\infty \\overline{T U_n}$ and $Y$ is Banach, there is $N\\in \\mathbb{N}$ such that $B_Y(y_0,r) \\subset \\overline{T U_N} = (2N) \\overline{T U_{1 / 2}}$, so $B_Y(z_0,\\varepsilon) \\subset \\overline{TU_{1 / 2}}$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V_\\varepsilon = B_Y(z_0,\\varepsilon) - z_0 \\subset \\overline{TU_{1 / 2}} - \\overline{TU_{1 / 2}} \\subset \\overline{TU_{1 / 2} - TU_{1 / 2}} = \\overline{TU_1}.&nbsp;&nbsp;&nbsp; \\] In particular, $V_{\\alpha \\varepsilon} = \\alpha V_\\varepsilon \\subset \\alpha \\overline{TU_1} = \\overline{TU_{\\alpha}}$ Now it STS that $\\overline{T U_1} \\subset T U_2$. Let $y \\in \\overline{T U_1}$. Then there is $x_1 \\in U_1$ such that $\\left\\|y - Tx_1\\right\\| &lt; \\varepsilon / 2$, so $y - Tx_1 \\in V_{\\varepsilon / 2} \\subset \\overline{T U_{1 / 2}}$. Inductively we now construct $x_i \\in U_{2^{1-i}}$ such that $y - \\sum_{i=1}^nTx_i \\in V_{\\varepsilon 2^{-i}}$. Then $\\sum \\left\\|x_i\\right\\| \\le 2$, so (because $X$ is Banach) $x = \\sum_{i=1}^\\infty x_i \\in U_2$ exists and $y = Tx \\in T U_2$.</i> </p> </div><div></div>"
  },
  {
    "front": "One-sided bound between complete norms implies equivalence.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a vector space with complete norms $\\left\\|\\cdot \\right\\|_1$ and $\\left\\|\\cdot \\right\\|_2$ such that $\\left\\|\\cdot \\right\\|_1 \\le c \\left\\|\\cdot \\right\\|_2$ for some $c &gt; 0$, then the norms are equivalent.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The identity from $(X,\\left\\|\\cdot \\right\\|_2)$ to $(X,\\left\\|\\cdot \\right\\|_1)$ is a bijective linear continuous map between Banach spaces, hence the inverse is continuous.</i> </p> </div><div></div>"
  },
  {
    "front": "In Banach spaces, cartesian products and direct sums are the same, and the associated projections are continuous.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $X = Y \\oplus Z$ be a Banach space with closed subspaces $Y$ and $Z$. Then,<br>&nbsp;&nbsp;&nbsp; <ol>  <li>The projection onto $Y$ (or $Z$) is continuous,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$X \\simeq Y \\times Z$ as Banach spaces.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Recall that $Y\\times Z$ with $\\left\\|(y,z)\\right\\|= \\left\\|y\\right\\|+\\left\\|z\\right\\|$ is a Banach space. Then the map $T\\colon Y\\times Z \\to X;\\, (y,z) \\mapsto y+z$ is linear, bounded, and bijective, so it has continuous inverse by the OMT, which proves (ii). Finally, $P = \\pi_Y \\circ T^{-1}$, where $\\pi_Y\\colon Y\\times Z \\to Y; (y,z) \\mapsto&nbsp; y$, is the composition of two continuous linear operators.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of continuity of inverse of an injective $T \\in \\mathcal{B}(X,Y)$ between Banach spaces.",
    "back": "<br><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X,Y$ are Banach spaces and $T\\in \\mathcal{B}(X,Y)$ is injective, then TFAE.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$T^{-1}\\colon R(T) \\to T$ is continuous,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>There is $c &gt; 0$ such that $\\left\\|Tx\\right\\|\\ge c\\left\\|x\\right\\|$ for all $x\\in X$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$R(T)$ is closed.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) obviously implies (ii), which implies (iii): If $Tx_n \\to y$, then by assumption $(x_n)$ is Cauchy in $X$, so $x_n \\to x$, so $Tx_n \\to Tx = y \\in R(T)$. If (iii) holds, then $T\\colon X \\to R(T)$ is a bijective continuous linear map between Banach spaces and hence has continuous inverse by the OMT.</i> </p> <br><div></div>"
  },
  {
    "front": "Radon-Nikodym theorem",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $(\\Omega,\\mathcal{A})$ is a measurable space with two $\\sigma$-finite measure $\\mu$ and $\\nu$ such that $\\mu \\ll \\nu$, then there is a unique non-negative $f\\in L^1(\\nu)$ such that $\\mathop{}\\!\\mathrm{d} \\mu = f \\mathop{}\\!\\mathrm{d} \\nu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume $\\mu$ and $\\nu$ are finite, and define $\\tau := \\mu + \\nu$. Then by finiteness, $\\left\\|\\cdot \\right\\|_{L^1(\\mu)} \\lesssim \\left\\|\\cdot \\right\\|_{L^2(\\mu)} \\le \\left\\|\\cdot \\right\\|_{L^2(\\tau)}$, so that $\\varphi\\colon L^2(\\tau) \\to \\mathbb{R};\\, g \\mapsto \\mu(g)$ is a bounded linear map. Hence by Riesz there exists $h \\in L^2(\\tau) \\subset L^1(\\tau)$ such that $\\mu(g) = \\tau(hg)$ for all $g\\in L^2(\\tau)$, so $\\mathop{}\\!\\mathrm{d} \\mu = h \\mathop{}\\!\\mathrm{d} \\tau$. In particular $h\\ge 0$. By the same argument, we find $\\mathop{}\\!\\mathrm{d} \\nu = g \\mathop{}\\!\\mathrm{d} \\tau$. Now note that if $N := \\left\\{ g = 0 \\right\\} $ then $\\nu(N) = 0$ so $\\mu(N) = 0$. Hence if \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(\\omega) := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{h(\\omega)}{g(\\omega)} &amp;, \\omega \\not\\in N,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, \\omega\\in N,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] then for any $A\\in \\mathcal{A}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(A) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\boldsymbol{1}_{A} h \\mathop{}\\!\\mathrm{d} \\tau = \\int \\boldsymbol{1}_{A \\cap N^{c}} fg\\mathop{}\\!\\mathrm{d} \\tau = \\int \\boldsymbol{1}_{A \\cap N^{c}} g \\mathop{}\\!\\mathrm{d} \\nu = \\int \\boldsymbol{1}_{A} g \\mathop{}\\!\\mathrm{d} \\nu.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; If $f'$ is another such function, then $\\nu((f-f') \\boldsymbol{1}_{A}) = 0$ for all $A\\in \\mathcal{A}$ so $f = f'$ $\\nu$ a.e.</i> </p> </div><div></div>"
  },
  {
    "front": "Dual space of $L^p(\\Omega,\\mathcal{A},\\mu)$ (Proof for finite $\\mu$ and $p \\in (1,\\infty)$).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $(\\Omega,\\mathcal{A},\\mu)$ is a measure space and $p\\in [1,\\infty)$, and $\\mu$ $\\sigma$-finite if $p = 1$, then $(L^p)^\\star = L^{p'}$ with the identification \\[&nbsp;&nbsp;&nbsp; f \\in L^{p'}\\quad \\leftrightarrow \\quad g \\mapsto \\int f g \\mathop{}\\!\\mathrm{d} \\mu.\\] (More precisely, this identification is an isometric isomorphism.)</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume $\\mu$ is finite and $p \\in (1,\\infty)$. One only needs to show surjectivity of the above identification. Given $\\varphi \\in (L^p)^{\\star}$, let $\\nu(A) := \\varphi( \\boldsymbol{1}_{A})$ for $A \\in \\mathcal{A}$, then if $A = \\bigcup_{n=1} ^\\infty A_n$ is a disjoint union, then $\\sum_{n=1}^N \\boldsymbol{1}_{A_n} \\to \\boldsymbol{1}_{A}$ in $L^p$ by DCT, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\nu(\\bigcup_{n=1} ^\\infty A_n) = \\varphi( \\sum_{n=1}^\\infty \\boldsymbol{1}_{A_n}) = \\sum_{n=1}^\\infty \\varphi( \\boldsymbol{1}_{A_n}) = \\sum_{n=1}^\\infty \\nu(A_n),\\\\&nbsp;&nbsp;&nbsp; \\] so $\\nu$ is a signed measure on $(\\Omega,\\mathcal{A})$. If $\\mu(A) = 0$, then $ \\boldsymbol{1}_{A} = 0$ in $L^p$, so $\\nu(A) = \\varphi( \\boldsymbol{1}_{A}) = \\varphi(0) = 0$, hence there exists a unique $g \\in L^1(\\mu)$ with $\\mathop{}\\!\\mathrm{d} \\nu = g \\mathop{}\\!\\mathrm{d} \\mu$.<br><br>&nbsp;&nbsp;&nbsp; Now let $h := \\operatorname{sgn}(g) \\left| g \\right| ^{p'- 1}$, so that $hg = \\left| h \\right| ^p = \\left| g \\right| ^{p'}$, and $A_n := \\left\\{ \\left| g \\right|&nbsp; \\le n \\right\\} $. Then $ \\boldsymbol{1}_{A_n}h, \\boldsymbol{1}_{A_n}g \\in L^\\infty \\subset L^{p'}$ (because $\\mu$ is finite), so <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int \\boldsymbol{1}_{A_n} \\left| g \\right| ^{p'}\\mathop{}\\!\\mathrm{d} \\mu\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\boldsymbol{1}_{A_n} hg \\mathop{}\\!\\mathrm{d} \\mu = \\varphi( \\boldsymbol{1}_{A_n} h) \\le \\left\\|\\varphi\\right\\| \\left\\| \\boldsymbol{1}_{A_n} h\\right\\|_{L^p} = \\left\\|\\varphi\\right\\| \\left[ \\int \\boldsymbol{1}_{A_n} \\left| g \\right| ^{p'} \\mathop{}\\!\\mathrm{d} \\mu \\right] ^{1 / p},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $\\left[ \\int \\boldsymbol{1}_{A_n} \\left| g \\right| ^{p'} \\mathop{}\\!\\mathrm{d} \\mu\\right]^{1 / p'} \\le \\left\\|\\varphi\\right\\| $ for all $n\\in \\mathbb{N}$, so $\\left\\|g\\right\\|_{L^{p'}} \\le \\left\\|\\varphi\\right\\|$. Now for a simple function $f$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi(f) = \\int f \\mathop{}\\!\\mathrm{d} \\nu = \\int f g \\mathop{}\\!\\mathrm{d} \\mu.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Since simple functions are dense in $L^p$, this implies the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Hahn-Banach, version with sublinear form",
    "back": "<div><p><b>Theorem.</b> <i>[Hahn-Banach, sublinear form]<br>&nbsp;&nbsp;&nbsp; If $X$ is a vector space and $\\varphi\\colon Y\\subset X \\to \\mathbb{R}$ a linear map on a linear subspace $Y$ of $X$, and $\\varphi \\le p\\!\\!\\restriction_{Y} $ for a sublinear form $p\\colon X \\to \\mathbb{R}$, then there exists a linear map $\\psi\\colon X\\to \\mathbb{R}$ with $\\psi \\le p$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Take the set of all linear continuations of $\\varphi$ that are dominated by $p$ with the obvious order, which has a maximal element $(V,\\psi)$ by Zorn's lemma. It suffices to show that $V = X$, so assume there is $x_0 \\in X \\setminus V$, in particular $x_0 \\neq&nbsp; 0$. We let $V' := V + \\operatorname{lin} \\left\\{ x_0 \\right\\} $ and $\\varphi'(v + tx_0) = \\varphi(v) + \\alpha t$, for some $\\alpha \\in \\mathbb{R}$ we have yet to choose. This is a linear continuation, we have to verify that it is dominated by $p$. If $t &gt; 0$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi'(v + t x_0) &amp;= \\varphi(v) + t \\alpha \\le p(v+tx_0) \\quad \\text{requires}\\quad \\alpha \\le p(v / t + x_0) - \\varphi(v / t),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi'(v - t x_0) &amp;= \\varphi(v) - t \\alpha \\le p(v-tx_0) \\quad \\text{requires}\\quad \\alpha \\ge \\varphi(v / t) - p(v / t - x_0).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Such an $\\alpha$ exists because, for $x_0,u,w\\in X$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p(u+x_0) - \\varphi(u) \\ge \\varphi(w) - p(w - x_0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\iff \\varphi(u+w) \\le p(u+x_0) + p(w-x_0)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\impliedby \\varphi(u+w) \\le p(u+w)&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Example of $\\varphi \\in (\\ell^\\infty)^\\star$ with $\\varphi(e_n) = 0$ for all $n\\in \\mathbb{N}$. In particular, $\\ell^\\infty \\supsetneq \\ell^1$.",
    "back": "<div>Take $\\varphi(y) = \\lim_{n\\to \\infty} y_n$ on $c \\subset \\ell^\\infty$, and the extension with Hahn-Banach ($\\ell^\\infty$ is a Banach space and $\\varphi$ a bounded linear operator on a linear subspace).<br><br><p><b>Theorem.</b> <i>[Hahn-Banach, bounded linear functionals]<br>&nbsp;&nbsp;&nbsp; If $X$ is a normed vector space, and $y^\\star$ a bounded linear functional on a subspace $Y$, then there exists an extension $x^\\star \\in X^\\star$ with the same norm.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Apply Hahn-Banach to $y^\\star$ and the sublinear form $p(x) := \\left\\|x\\right\\| \\left\\|y^\\star\\right\\|$.</i> </p> </div><div></div>"
  },
  {
    "front": "Corollaries to Hahn-Banach in normed vector spaces:<br><ol>  <li>Distinguish a closed linear subspace from a vector,&nbsp;&nbsp;&nbsp;</li>  <li>Distinguish a non-zero vector, or two different vectors,&nbsp;&nbsp;&nbsp;</li>  <li>Alternate representation of $\\left\\|x\\right\\|$ using the dual space.</li></ol>",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a normed vector space.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $Y$ is a closed linear subspace and $x_0 \\in X\\setminus Y$, then there exists a unit functional $x^\\star$ such that $x^\\star \\!\\!\\restriction_{Y}&nbsp; = 0$ and $x^\\star(x_0) = d(x_0,Y)$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $x\\neq&nbsp; 0$, then there exists a unit functional with $x^\\star(x) = \\left\\|x\\right\\|$. If $x_1\\neq x_2$, then there exists a unit functional with $x^\\star(x_1) \\neq&nbsp; x^\\star(x_2)$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $x\\in X$, then $\\left\\|x\\right\\| = \\sup_{\\left\\|x^\\star\\right\\|\\le 1} \\left| x^\\star(x) \\right| $.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Define $z^\\star(y + tx_0) := t d(x_0,Y)$ on the linear subspace $Y + \\operatorname{lin} \\left\\{ x_0 \\right\\} $. This is linear and bounded, in fact&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| z^\\star(y+tx_0) \\right| = t \\inf_{y'\\in Y} \\left\\|x_0 - y'\\right\\| \\le \\left\\|y + tx_0\\right\\|,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and it is clear that this norm bound is attained. Hahn-Banach yields a continuation on $X$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Follows from (i) with $Y = \\left\\{ 0 \\right\\} $, and $x := x_1 - x_2$ for the second part.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Clearly $\\left| x^\\star(x) \\right| \\le \\left\\|x\\right\\|$, and this is attained by (ii).&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of boundedness of a subset of a normed vector space using the dual space.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a normed vector space, then $M\\subset X$ is bounded iff $x^\\star(M) \\subset \\mathbb{R}$ is bounded for all $x^\\star \\in X^\\star$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $x\\in X$ let $\\delta_x\\in X^{\\star\\star}$ be the evaluation map. Then, for every $x^\\star \\in X^\\star$ \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\sup_{x\\in M} \\left|\\delta_x(x^\\star) \\right| = \\sup_{x\\in M} \\left| x^\\star(x) \\right| &lt; \\infty,\\\\&nbsp;&nbsp;&nbsp; \\] so by the uniform boundedness principle, $\\sup_{x\\in M} \\left\\|\\delta_x\\right\\| &lt; \\infty$. Finally, \\[&nbsp;&nbsp;&nbsp; \\left\\|\\delta_x\\right\\| = \\sup_{\\left\\|x^\\star\\right\\|\\le 1} x^\\star(x) = \\left\\|x\\right\\|&nbsp;&nbsp;&nbsp; \\] by Hahn-Banach.</i> </p> </div><div></div>"
  },
  {
    "front": "How do separability of $X$ and $X^\\star$ relate for a normed space?",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a normed space and $X^\\star$ is separable, then so is $X$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\left\\{ x_n^\\star\\colon n\\in \\mathbb{N} \\right\\} $ be dense and take $y_n \\in X$ with $\\left\\|y_n\\right\\| = 1$ and $x_n^\\star(y_n) \\ge \\left\\|x_n^\\star\\right\\| / 2$. Then if $Y := \\left\\{ y_n\\colon n\\in \\mathbb{N} \\right\\} $ was not dense, there would be a non-zero functional $x^\\star$ vanishing on $Y$ (by Hahn-Banach), but if $n\\in \\mathbb{N}$ is such that $\\left\\|x^\\star -x_n^\\star\\right\\| &lt; 1 / 4$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left|x^\\star(y_n) - 1 / 2\\right| \\le \\left\\|x^\\star - x_n^\\star\\right\\| &lt; 1 / 4,\\\\&nbsp;&nbsp;&nbsp; \\] so $x^\\star(y_n) \\neq&nbsp; 0$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "If a subspace of a normed space has finite dimension or codimension, then it has a complement.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If a subspace of a normed space has finite dimension or codimension, then it has a complement.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If $U \\subset X$ is finite dimensional with basis $x_1, \\ldots ,x_n$, then it is closed and we let $x_k^\\star \\in X^\\star$ with $x_k^\\star(x_j) = \\delta_{kj}$, which exist by Hahn-Banach. Then, the map \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; T := \\sum_{k=1}^n x_k^\\star(\\cdot ) x_k\\\\&nbsp;&nbsp;&nbsp; \\] is bounded and linear with $T^2 = T$, so it is a projection, and $R(T) = U$, so $X = U \\oplus N(T)$.<br><br>&nbsp;&nbsp;&nbsp; If $Y \\subset X$ is closed and $X / Y$ is finite dimensional, let $x_1, \\ldots ,x_n \\in X$ be representatives of a basis. Then they must be linearly independent in $X$, in particular $V \\cap Y = \\left\\{ 0 \\right\\} $. Put $V := \\operatorname{lin} \\left\\{ x_1, \\ldots ,x_n \\right\\} $, then $V +&nbsp; Y = X$ and thus $V \\oplus&nbsp; Y = X$.</i> </p> </div><div></div>"
  },
  {
    "front": "Geomtric Hahn-Banach",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $X$ be a normed space and $A,B\\subset X$ non-empty and convex.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $A,B$ are open, then they can be separated by $X^\\star$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $A$ is compact and $B$ is closed, then they can by strictly separated by $X^\\star$.&nbsp;&nbsp;&nbsp;</li></ol></div></i> </p> <br><div></div>"
  },
  {
    "front": "For $A\\subset X$ in a normed space, what is&nbsp;${}^\\perp(A^\\perp)$?",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a normed vector space and $A \\subset X$, then ${}^\\perp(A^\\perp) = \\overline{\\operatorname{lin}{A}}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We may assume $A$ is a closed linear subspace. $\\supset$ is obvious. For the converse assume that $x_0 \\in {}^\\perp (A^\\perp) \\setminus A$. Then there is $x^\\star$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; s := \\sup_{x\\in A}x^\\star(x) &lt; x^\\star(x_0).&nbsp;&nbsp;&nbsp; \\] In particular, the LHS is finite, so $x^\\star$ vanishes on $A$ (because it is a linear space), so $x^\\star \\in A^\\perp$, so $x^\\star(x) = 0$, so $0 \\le s &lt; 0$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Let $X$ be a reflexive normed vector space. Then,<br><ol>  <li>Which subspaces are reflexive?&nbsp;&nbsp;&nbsp;</li>  <li>How do separability of $X$ and $X^\\star$ relate?</li></ol>",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a reflexive normed vector space.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $X$ is reflexive, then so is every closed linear subspace.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is reflexive, then $X$ is separable iff $X^\\star$ is separable.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $y^{\\star\\star} \\in Y^{\\star\\star}$. Define $x^{\\star\\star} \\in X^{\\star\\star}$ by \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x^{\\star\\star} (x^\\star) := y^{\\star\\star}(x^\\star \\!\\!\\restriction_{Y} ),\\quad x^\\star \\in X^\\star.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Then by reflexivity of $X$ there is $x\\in X$ such that $x^{\\star\\star}(x^\\star) = x^\\star(x)$ and thus $y^{\\star\\star}(x^{\\star}\\!\\!\\restriction_{Y} ) = x^\\star(x)$, so we have to show that $x\\in Y$ (then it follows because every $y^\\star\\in Y^\\star$ is the restriction of some $x^\\star$ by Hahn-Banach). If $x\\not\\in Y$, then there is $x^\\star \\in X^\\star$ with $x^\\star \\!\\!\\restriction_{Y}&nbsp; = 0$ and $x^\\star(x) = 1$, so that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 = y^{\\star\\star}(x^\\star \\!\\!\\restriction_{Y} ) = x^\\star(x) = 1,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] a contradiction.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X^\\star$ is separable then so is $X$, and if $X = X^{\\star\\star}$ is separable then so is $X^\\star$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "For a Banach space $X$, how do reflexivity of $X$ and $X^\\star$ relate?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X$ is a Banach space, then $X$ is reflexive iff $X^\\star$ is reflexive.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume $X^\\star$ is reflexive and $X$ is not. Then by Hahn-Banach there is an element $x^{\\star\\star\\star} \\neq&nbsp; 0$ which vanishes on $X \\subset X^{\\star\\star}$, but it corresponds to an $x^\\star \\in X^\\star$, which then vanishes on $X$ but is not zero, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Dual spaces are weak${}^\\star$ sequentially complete, and reflexive spaces are weakly sequentially complete.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a normed vector space.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $(\\left&lt;x^\\star_n,x \\right&gt;) $ is Cauchy for all $x\\in X$, then $x^\\star_n \\rightharpoonup x^\\star$ for some $x^\\star \\in X^\\star$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is reflexive and $(\\left&lt;x^\\star,x_n \\right&gt; )$ is Cauchy for all $x^\\star\\in X^\\star$, then $x_n \\rightharpoonup x$ for some $x\\in X$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) is clear, just define $x^\\star(x) := \\lim_{n\\to \\infty} x_n^\\star(x)$. For (ii), apply (i) to $(J_X(x_n))_{n\\in \\mathbb{N}} \\in (X^{\\star\\star})^\\mathbb{N}$, which will converge weakly to some $x^{\\star\\star} = J_X(x)$.</i> </p> </div><div></div>"
  },
  {
    "front": "\"Fatou's lemma\" for strong convergence of operators.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X,Y$ are normed vector spaces, and $T_n \\in \\mathcal{B}(X,Y),n\\in \\mathbb{N}$ converge strongly to $T\\in \\mathcal{B}(X,Y)$, then $\\left\\|T\\right\\|\\le \\varliminf_{n\\to \\infty} \\left\\|T_n\\right\\|$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If a subsequence $(\\left\\|T_{k(n)}\\right\\|)_{n\\in \\mathbb{N}}$ converges, then for every $x\\in X$, $T_{k(n)}x\\to Tx$ in $Y$ and so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|Tx\\right\\| = \\lim_{n\\to \\infty} \\left\\|T_{k(n)}x\\right\\| \\le \\left\\|x\\right\\| \\lim_{n\\to \\infty} \\left\\|T_{k(n)}\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of strong convergence in $\\mathcal{B}(X,Y)$ if $X,Y$ are Banach spaces (involving boundedness and dense subsets.)",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $X,Y$ are Banach spaces and $T_n \\in \\mathcal{B}(X,Y)$, and $D\\subset X$ with $\\overline{\\operatorname{lin}{D}} = X$, then TFAE.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$(T_n)$ converges strongly in $\\mathcal{B}(X,Y)$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\sup_{n\\in \\mathbb{N}} \\left\\|T_n\\right\\| &lt; \\infty$ and $(T_n x)$ converges in $Y$ for all $x\\in D$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; (i) $\\implies$ (ii) holds if only $X$ is complete, and (ii) $\\implies$ (i) holds if only $Y$ is complete.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) $\\implies$ (ii) follows from the uniform boundedness principle. If (ii) holds and $Y$ is complete, then $Tx := \\lim_{n\\to \\infty} T_n x$ for $x\\in \\operatorname{lin}{D}$ extends uniquely to a linear operator $T\\in \\mathcal{B}(X,Y)$. Then for any $x\\in X$ and $z\\in \\operatorname{lin}{D}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|T_n x - T x\\right\\|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left\\|T_n\\right\\| \\left\\|x-z\\right\\| + \\left\\|T_n z - T z \\right\\| + \\left\\|T\\right\\| \\left\\|z - x\\right\\|.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then let $n\\to \\infty$ and then $z \\to x$.</i> </p> <br><div></div></div>"
  },
  {
    "front": "Characterisation of weak(${}^\\star$) convergence in normed spaces (involving boundedness and dense subsets).",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $X$ be a normed space and $D^\\star\\subset X^\\star$ with $\\operatorname{lin}{D^\\star}$ dense. Then $(x_n)$ converges weakly if and only if $\\sup_{n\\in \\mathbb{N}}\\left\\|x_n\\right\\|&lt; \\infty$ and $x^\\star(x_n)$ converges for all $x^\\star\\in D^\\star$.<br><br>&nbsp;&nbsp;&nbsp; For weak${}^\\star$ convergence, the converse always holds and $\\implies$ requires completeness of $X$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Weak convergence of $(x_n)$ is strong convergence of $(J_X(x_n))$ in $X^{\\star\\star} = \\mathcal{B}(X^\\star,\\mathbb{R})$, and both $X^\\star$ and $\\mathbb{R}$ are complete. Then the equivalence is just the characterisation of strong convergence for operators between Banach spaces.</i> </p> </div><div></div>"
  },
  {
    "front": "Weak(${}^\\star$) convergence in $c_0$ and $\\ell^p$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Weak (same as weak${}^\\star$ because reflexive) convergence in $\\ell^p$ for $p\\in (1,\\infty)$, and weak convergence in $c_0$ are equivalent to boundedness plus component-wise convergence.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>The same holds <i>only</i> for weak${}^\\star$ convergence in $\\ell^1$ and $\\ell^\\infty$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>$D := \\left\\{ e_n\\colon n\\in \\mathbb{N} \\right\\} $ has dense span in $\\ell^{p'} = (\\ell^p)^\\star$ and $\\ell_1 = c_0^\\star$ (but not $\\ell^\\infty$!)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>They are dual to the complete spaces $c_0$ and $\\ell^1$ respectively, in both of which $D$ from (i) has dense span.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Mazur's theorem (on weak convergence in normed spaces).",
    "back": "<div><p><b>Theorem.</b> <i>[Mazur]<br>&nbsp;&nbsp;&nbsp; Let $x_n \\rightharpoonup x$ be a weakly convergent sequence in a normed vector space $X$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $(x_n) \\subset C \\subset X$ is closed and convex, then $x\\in C$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>There exists a sequence $(y_n)$ such that $y_n$ is a convex combination of $\\left\\{ x_k\\colon k\\ge n \\right\\} $ with $y_n \\to x$ in norm.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $x\\not\\in C$, then there is an $x^\\star\\in X^\\star$ that strongly separates $x$ and $C$, a contradiction.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>For every $n\\in \\mathbb{N}$, the set $C_n$ of convex combinations of $\\left\\{ x_k\\colon k\\ge n \\right\\} $ is convex and contains almost all $x_k$, so $x\\in \\overline{C_n}$. Hence we can choose $y_n \\in C_n $ with $\\left\\|y_n-x\\right\\|\\le 1 / n$, say.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Banach-Alaoglu theorem (and extension to Bolzano-Weierstraß theorem).",
    "back": "<div><p><b>Theorem.</b> <i>[Bolzano-Weierstraß]<br>&nbsp;&nbsp;&nbsp; The unit ball in a complete Banach space which is adjoint to a separable normed space is weak${}^\\star$ sequentially relatively compact.</i> </p> <br><br><p><b>Theorem.</b> <i>[Banach-Alaoglu]<br>&nbsp;&nbsp;&nbsp; Let $X$ be a normed vector space<br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $X$ is separable, then the unit ball in $X^\\star$ is weakly${}^\\star$ sequentially relatively compact.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is reflexive, then the unit ball in $X$ is weakly sequentially compact.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Let $(x_n^\\star)$ be bounded, and $\\left\\{ x_k\\colon k\\in \\mathbb{N} \\right\\} $ dense in $X$. Then we can extract a subsequence $(x_{k(n)}^\\star)$ which converges on $x_k$ for every $k\\in \\mathbb{N}$ (by a diagonal argument and because of the boundedness), so $(x_{k(n)}^\\star)$ is bounded and converges pointwise on a dense subset of $X$, so it has a strong limit $x^\\star$ with $\\left\\|x^\\star\\right\\|\\le \\varliminf_{n\\to \\infty}\\left\\|x_{k(n)}^\\star\\right\\| \\le \\varliminf_{n\\to \\infty} \\left\\|x_n^\\star\\right\\|$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Let $(x_n)$ be a bounded sequence, and replace $X$ by the separable reflexive space $\\overline{\\operatorname{lin} \\left\\{ x_n\\colon n\\in\\mathbb{N} \\right\\} }$. Then, $X^\\star$ is separable and $(J_X(x_n)) \\subset X^{\\star\\star}$ is bounded and by (i) has a weakly${}^\\star$ convergent subsequence, which by reflexivity implies that the corresponding subsequence of $(x_n)$ converges weakly in $X$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Convergence in a normed space is just \"uniform weak convergence\".",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Hahn-Banach, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|x_n - x\\right\\| = \\sup_{\\left\\|x^\\star\\right\\|\\le 1} \\left| x^\\star(x_n) - x^\\star(x) \\right| .&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "If $X$ is a Banach space with a measure $\\mu$, what is the connection between $X^\\star$ and $L^p(\\mu)$?",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $X$ be a Banach space with a measure $\\mu$ that has finite $p$'th moments for some $p\\in [1,\\infty)$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$x^\\star_n \\rightharpoonup x^\\star$ implies $x^\\star_n \\stackrel{ L^p }{\\longrightarrow} x^\\star$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $X$ is separable then $X^\\star \\hookrightarrow L^{p}(\\mu)$ is compact.&nbsp;&nbsp;&nbsp;</li></ol>&nbsp; </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By definition we have $x_n^\\star(x) \\to x^\\star(x)$ for all $x\\in X$, and because $X$ is complete, $(x_n^\\star)$ is bounded in $X^\\star$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int |x^\\star_n(x) - x^\\star(x)|^p \\mu(\\mathop{}\\!\\mathrm{d} x) \\to 0\\\\&nbsp;&nbsp;&nbsp; \\] by dominated convergence. Now if $X$ is separable and $(x_n^\\star)$ bounded, then by Banach-Alouglu there is a weakly${}^\\star$ convergent subsequence which is $L^p$-convergent.</i> </p> </div><div></div>"
  },
  {
    "front": "The covariance operator $\\widehat{C}_\\mu \\colon X^\\star \\to X$ of a centred Gaussian measure on a separable Banach space is compact.",
    "back": "<div><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By Banach-Alouglu, it suffices to show that if $x_n^\\star \\rightharpoonup ^\\star x^\\star$, then $\\widehat{C}_\\mu(x^\\star_n) \\to \\widehat{C}_\\mu(x^\\star)$. Indeed, $x^\\star_n \\stackrel{ L^2(\\mu) }{\\longrightarrow} x^\\star$ (because $\\mu$ has moments), so by Hahn-Banach<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\widehat{C}_\\mu(x^\\star_n) - \\widehat{C}_\\mu(x^\\star)\\right\\| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sup_{\\left\\|y^\\star\\right\\|\\le 1} \\left| C_\\mu(x^\\star_n,y^\\star) - C_\\mu(x^\\star,y^\\star) \\right| \\le \\sup_{\\left\\|y^\\star\\right\\|\\le 1} \\left\\|(x_n^\\star-x^\\star) y^\\star\\right\\|_{L^1(\\mu)}\\\\&amp;\\le \\left\\|x_n^\\star - x^\\star\\right\\|_{L^2(\\mu)} \\int \\|x\\|^2 \\mu(\\mathop{}\\!\\mathrm{d} x)\\to 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "If $\\varphi,\\varphi_1, \\ldots ,\\varphi_n$ are linear functionals on a vector space $X$, when can you conclude that there are $\\lambda_i$ such that $\\varphi = \\sum_{i=1}^n \\lambda_i \\varphi_i$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\varphi,\\varphi_1, \\ldots ,\\varphi_n$ are linear functionals on a vector space $X$ such that $\\bigcap_{i=1} ^n \\ker \\varphi_i \\subset \\ker \\varphi$, then there are $\\lambda_i$ such that $\\varphi = \\sum_{i=1}^n \\lambda_i \\varphi_i$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\ell \\colon X \\to \\mathbb{R}^n;\\, x \\mapsto (\\varphi_1(x), \\ldots ,\\varphi_n(x))$. Then there is a linear map $\\varphi' \\colon \\mathbb{R}^n \\to \\mathbb{R}$ such that $\\varphi = \\varphi' \\circ \\ell$. Indeed, define $\\varphi'$ on $\\operatorname{img} \\ell$ by $\\varphi(\\ell(x)) := \\varphi(x)$ which is well-defined by assumption and linear, then extend it to all of $\\mathbb{R}^d$. Then $\\varphi'(y) = \\sum_{i=1}^n \\lambda_i y_i$, that is $\\varphi = \\sum_{i=1}^n \\lambda_i \\varphi_i$.</i> </p> </div><div></div>"
  },
  {
    "front": "If $F$ is a vector space of linear functionals over a vector space $X$, and we equip $X$ with the weak $F$-topology, then what is the topological dual of $X$?<div></div>",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $F$ is a vector space of linear functionals over a vector space $X$, and we equip $X$ with the weak $F$-topology, then the topological dual of $X$ is $F$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\varphi\\colon X \\to \\mathbb{R}$ be $w_F$-continuous. Then $\\varphi^{-1}((-1,1))$ is open, so there are $f_1, \\ldots ,f_n \\in F$ and $\\varepsilon &gt; 0$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\cap_{i=1}^n f_i^{-1}((-\\varepsilon,\\varepsilon)) \\subset \\varphi^{-1}((-1,1)).&nbsp;&nbsp;&nbsp; \\] In particular, if $x \\in \\cap_{i=1}^n \\ker f_i \\subset \\text{LHS} \\subset \\varphi^{-1}((-1,1))$ then this is also true for $\\lambda x$ for all $\\lambda &gt; 0$ and thus $\\lambda |\\varphi(x)| &lt; 1$ for all $\\lambda &gt; 0$, so $x\\in \\ker \\varphi$. But then $\\varphi$ is a linear combination of the $f_i$.</div><div></i> </p> </div><div></div>"
  },
  {
    "front": "If $X$ is a topological vector space of functions such that point evaluations are continuous, then point evaluations are weak${}^\\star$ dense in $X'$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $X$ is a topological vector space of functions such that point evaluations are continuous, then point evaluations are weak${}^\\star$ dense in $X'$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; The topological dual of $X'$ with the weak topology (i.e. $\\varphi_n \\to \\varphi$ iff $\\varphi_n(f) \\to \\varphi(f)$ for all $f\\in X$) is exactly $X$. Now if point evaluations weren't dense then there was by Hahn-Banach (for topological vector spaces) an $f\\in X$ such that $f\\neq&nbsp; 0$ but $\\delta_x(f) = f(x) = 0$ for all $x$ in the domain of $f$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Cameron Martin space of a Gaussian measure on a space of functions as RKHS",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $\\mu$ is a Gaussian measure on a Banach space $B$, and $D^\\star \\subset B^\\star$ is weak${}^\\star$-dense, then $\\widehat{C}_\\mu(D^\\star)$ is dense in $\\mathcal{H}_\\mu$. In particular, if $B$ is a space of functions on which $\\delta_t,t\\in T,$ are continuous, then $\\mathcal{H}_\\mu$ is the completion of the inner product space defined by \\[\\mathcal{H}_\\mu^K := \\operatorname{lin} \\left\\{ k(t,\\cdot )\\colon t\\in T \\right\\} ,\\quad \\left&lt;k(t,\\cdot ),k(s,\\cdot ) \\right&gt; = k(t,s),\\\\\\] where $k(t,s) =C_\\mu(\\delta_t,\\delta_s) = \\mathbb{E} \\left[ f(t)f(s) \\right] $. Furthermore, for $f\\in \\mathcal{H}_\\mu$ and $t\\in T$, \\[f(t) = \\left&lt;f, k(t,\\cdot ) \\right&gt;_{\\mathcal{H}_\\mu} .\\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For the first claim simply recall that $\\ell_n \\to \\ell$ weak${}^\\star$ implies $\\ell_n \\to \\ell$ in $L^2(\\mu)$, hence $\\widehat{C}_\\mu (\\ell_n) \\to \\widehat{C}_\\mu(\\ell)$ in $\\mathcal{H}_\\mu$ (by the isomorphism between $\\mathcal{R}_\\mu$ and $\\mathcal{H}_\\mu$). Now if $B$ is a space of functions (over $T$), then point evaluations are in $B^\\star$, whose dual w.r.t. weak${}^\\star$ convergence is just $B$, so by HB (the span of) point evaluations is dense, so $\\mathcal{H}_\\mu$ is the completion of \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{C}_\\mu(\\delta_t) = \\int f(t) f \\mu(\\mathop{}\\!\\mathrm{d} f) = \\int f(t) f(\\cdot ) \\mu(\\mathop{}\\!\\mathrm{d} f) = k(t,\\cdot ),\\quad t\\in T,\\\\&nbsp;&nbsp;&nbsp; \\] where we used that $g:= \\int f(t) f \\mu(\\mathop{}\\!\\mathrm{d} f)$ satisfies $g(s) = \\delta_s(g) = \\int f(t) f(s) \\mu(\\mathop{}\\!\\mathrm{d} f) = k(t,s)$ for all $s\\in T$, w.r.t. \\[&nbsp;&nbsp;&nbsp; \\left&lt;k(t,\\cdot ),k(s,\\cdot ) \\right&gt; = C_\\mu(\\delta_t,\\delta_s) = \\int f(t)f(s) \\mu(\\mathop{}\\!\\mathrm{d} f) = k(t,s).\\] The final statement follows through denseness.</i> </p> </div><div></div>"
  },
  {
    "front": "What is the Cameron Martin space of the Wiener measure on $C[0,1]$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; The Cameron Martin space of the Wiener measure on $C[0,1]$ is $H^{1,2}_0$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Either use the representation and realise that piecewise linear functions starting in $0$ are dense in $H^{1,2}_0$. Or note that the dual of $C[0,1]$ are finite signed Borel measures, and for such $\\nu$, $h_\\nu := \\widehat{C}_\\mu(\\nu)$ satisfies<br>&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_\\nu(t) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\int f(t) \\nu(f) \\mu(\\mathop{}\\!\\mathrm{d} f) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\left( \\int f(t) f(s) \\nu(\\mathop{}\\!\\mathrm{d} f) \\right) \\nu(\\mathop{}\\!\\mathrm{d} s)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int (t\\wedge s) \\nu(\\mathop{}\\!\\mathrm{d} s)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^1 \\int_0^1 \\boldsymbol{1}_{\\{x &lt; t\\}} \\boldsymbol{1}_{\\{x &lt; s\\}} \\mathop{}\\!\\mathrm{d} x \\nu(\\mathop{}\\!\\mathrm{d} s)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^t \\nu((x,1)) \\mathop{}\\!\\mathrm{d} x.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; The function $x \\mapsto \\nu((x,1))$ is bounded hence in $L^2([0,1])$, so $h_\\nu \\in H^{1,2}_0$. Furthermore,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;h_\\nu, h_\\gamma \\right&gt; _{\\mathcal{H}_\\mu}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = C_\\mu(\\nu,\\gamma) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^1 \\int_0^1 (t\\wedge s) \\nu(\\mathop{}\\!\\mathrm{d} s)\\gamma(\\mathop{}\\!\\mathrm{d} t)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int_0^1 \\nu((s,1)) \\gamma((s,1)) \\mathop{}\\!\\mathrm{d} s.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; It suffices to show that $\\left\\{ h_\\nu\\colon \\nu \\right\\} $ is dense in $H^{1,2}_0$. If $f\\in H^{1,2}_0$ then there is $g\\in L^2((0,1))$ with $f(t) = \\int_0^t g(s)\\mathop{}\\!\\mathrm{d} s$, and we find $g_n\\in L^2$ simple with $g_n \\to g$ in $L^2$, and we find $\\nu_n$ with $g_n(t) = \\nu((t,1))$ for $t\\in (0,1)$, so \\[&nbsp;&nbsp;&nbsp; \\left\\|h_{\\nu_n} - f\\right\\|_{\\mathcal{H}_\\mu}^2 = \\left\\|g-g_n\\right\\|_{L^2}^2 \\to 0.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Definition of Cameron-Martin space of a Gaussian measure $\\mu$, and $\\mathcal{H}_\\mu \\subset B$.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; The CM space of $\\mu$ is the closure of the inner product space \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{C}_\\mu(B^\\star) = \\left\\{ h:= \\int x h^\\star(x) \\mu(\\mathop{}\\!\\mathrm{d} x)\\colon h^\\star\\in B^\\star \\right\\} ,\\quad \\left&lt;h,k \\right&gt; _{\\mathcal{H}_\\mu} := C_\\mu(h^\\star,k^\\star).&nbsp;&nbsp;&nbsp; \\] This is well-defined, $\\mathcal{H}_\\mu \\subset B$, and \\[&nbsp;&nbsp;&nbsp; \\left\\|h\\right\\|_\\mu^2 \\ge \\frac{1}{\\left\\|C_\\mu\\right\\|} \\left\\|h\\right\\|^2.&nbsp;&nbsp;&nbsp; \\] </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; To show well-definedness, by polarisation it STS that $\\left\\|h\\right\\|_\\mu^2 = C_\\mu(h^\\star,h^\\star)$ does not depend on the choice of $h^\\star$. Indeed, if $h = \\widehat{C}_\\mu(h_1^\\star) = \\widehat{C}_\\mu(h_2^\\star)$, then (putting $k:= h_1^\\star + h_2^\\star$),<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C_\\mu(h_1^\\star,h_1^\\star) - C_\\mu(h_2^\\star,h_2^\\star) = C_\\mu(k,h_1^\\star) - C_\\mu(k,h_2^\\star) = k(h) - k(h) = 0.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; For the next statement, if $h^\\star \\in B$ then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|h\\right\\|^2 = \\sup_{\\left\\|\\ell\\right\\|\\le 1} \\ell(h)^2 = \\sup_\\ell C_\\mu(h^\\star,\\ell)^2 \\le \\sup C_\\mu(h^\\star,h^\\star) C_\\mu(\\ell,\\ell) \\le \\left\\|C_\\mu\\right\\| \\left\\|h\\right\\|_\\mu^2.&nbsp;&nbsp;&nbsp; \\] In particular, Cauchy-sequences in $\\mathcal{H}_\\mu^\\circ$ are also Cauchy-sequences in $B$ so the closure is $\\subset B$.</i> </p> </div><div></div>"
  },
  {
    "front": "Gaussian measures: Definition of $\\mathcal{R}_\\mu$ and isomorphism with $\\mathcal{H}_\\mu$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; The canonical identification $B^\\star \\to \\mathcal{H}_\\mu^\\circ;\\, h^\\star \\mapsto&nbsp; h = \\widehat{C}_\\mu(h^\\star)$ extends to an isomorphism \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\iota\\colon \\mathcal{R}_\\mu \\to \\mathcal{H}_\\mu;\\, h^\\star \\mapsto h:=&nbsp; \\int_B x h^\\star(x) \\mu(\\mathop{}\\!\\mathrm{d} x),\\\\&nbsp;&nbsp;&nbsp; \\] where $\\mathcal{R}_\\mu$ is the closure of $B^\\star$ in $L^2(B,\\mu)$. In particular, $\\mathcal{H}_\\mu$ is separable, and for $h, k \\in \\mathcal{H}_\\mu$, $\\left&lt;h,k \\right&gt; _{\\mathcal{H}_\\mu} = \\int h^\\star k^\\star \\mathop{}\\!\\mathrm{d} \\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Firstly $B^\\star \\subset L^2(B,\\mu)$ by Fernique. The map $B^\\star \\to \\mathcal{H}_\\mu^\\circ;\\, h^\\star \\mapsto h := \\widehat{C}_\\mu(h^\\star)$ is linear and surjective, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|h\\right\\|_\\mu^2 = C_\\mu(h^\\star,h^\\star) = \\int h^\\star(x)^2 \\mu(\\mathop{}\\!\\mathrm{d} x) = \\left\\|h^\\star\\right\\|_{L^2}^2,\\\\&nbsp;&nbsp;&nbsp; \\] so it is an isomorphism. Then taking the closure on both sides gives an isomorphism $\\iota \\colon \\mathcal{R}_\\mu \\to \\mathcal{H}_\\mu$. Now let $f\\in \\mathcal{R}_\\mu$ and $f_n \\in B^\\star$, $f_n \\stackrel{ L^2 }{\\longrightarrow} f$. Then $\\iota(f_n) = \\int x f_n(x) \\mu(\\mathop{}\\!\\mathrm{d} x) \\to \\iota(f)$ in $\\mathcal{H}_\\mu$ hence also in $B$, but<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\iota(f_n) - \\int x f(x) \\mu(\\mathop{}\\!\\mathrm{d} x)\\right\\|_B \\le \\left\\|[x\\mapsto x]\\right\\|_{L^2(\\mu)} \\left\\|f_n - f\\right\\|_{L^2(\\mu)} \\to 0,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so in fact $\\iota(f) = \\int x f(x) \\mu(\\mathop{}\\!\\mathrm{d} x)$.<br><br>&nbsp;&nbsp;&nbsp; Then separability of $\\mathcal{H}_\\mu$ follows from separability of $L^2(B,\\mu)$ ($\\mu$ is a finite measure and the Borel $\\sigma$-algebra on $B$ is countably generated), and the final claim from the scalar product on $L^2(B,\\mu)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Gaussian measures: In what sense are elements of $\\mathcal{R}_\\mu$ almost linear?",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Every $\\ell\\in \\mathcal{R}_\\mu$ is $\\mu$-a.e. equal to a linear map. In fact, $\\mathcal{R}_\\mu$ consists exactly of (equivalence classes of) measurable maps $B\\to \\mathbb{R}$ which are linear on a measurable linear subspace of $B$ with full $\\mu$-measure.<br>&nbsp;&nbsp;&nbsp; </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\ell\\in \\mathcal{R}_\\mu$, then there exists $\\ell_n \\stackrel{ L^2(\\mu) }{\\longrightarrow} \\ell$ with $\\ell_n \\in B^\\star$. By passing to a subsequence, $\\ell_n(x) \\to \\ell(x)$ for $\\mu$-a.e. $x$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V_\\ell := \\left\\{ x\\in B\\colon \\lim_{n\\to \\infty} \\ell_n(x) \\text{ exists} \\right\\} &nbsp;&nbsp;&nbsp; \\] is a (measurable!) linear space with $\\mu(V_\\ell) = 1$, and $\\ell = \\ell'$ $\\mu$-a.e.</i> </p> </div><div></div>"
  },
  {
    "front": "Guassian measures: What is the distribution of elements of $\\mathcal{R}_\\mu$ (as random variables $(B,\\mu) \\to \\mathbb{R}$).",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Every $h^\\star\\in \\mathcal{R}_\\mu$ is centred Gaussian (as a random variable $(B,\\mu) \\to \\mathbb{R}$) with variance $\\left\\|h^\\star\\right\\|_{L^2(\\mu)} = \\left\\|h\\right\\|_\\mu^2$. The covariance of $h^\\star$ and $k^\\star$ is $\\left&lt;h^\\star,k^\\star \\right&gt; _{L^2(\\mu)} = \\left&lt;h,k \\right&gt; _\\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Take $h_n^\\star \\in B^\\star$ with $h_n^\\star \\to h^\\star$ in $L^2(\\mu)$, which we can choose to have $\\left\\|h_n^\\star\\right\\|_{L^2} = \\left\\|h^\\star\\right\\|_{L^2} ( = \\left\\|h\\right\\|_\\mu)$. Then $h_n$ is $\\mathcal{N}(0,\\left\\|h\\right\\|_\\mu^2)$ for all $n\\in \\mathbb{N}$, and $L^2$-convergence implies convergence in law. About the covariance, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ h^\\star k^\\star \\right] = \\int h^\\star(x) k^\\star(x) \\mu(\\mathop{}\\!\\mathrm{d} x) = \\left&lt;h^\\star,k^\\star \\right&gt; _{L^2} = \\left&lt;h,k \\right&gt; _\\mu.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of weak${}^\\star$ denseness of $D^\\star \\subset X^\\star$ for a topological vector space $X$.",
    "back": "<div><p><b>Lemma.</b> <i></div><div>$D^\\star \\subset X^\\star$ is weak${}^\\star$ dense iff $\\varphi(x) = 0$ for all $\\varphi\\in D^\\star$ implies $x=0$.</div><div></i> </p> </div><div><p><i><b>Proof.</b></div><div>The dual of $X^\\star$ w.r.t. the weak${}^\\star$ topology is just $X$, so by Hahn-Banach a subset is dense iff the only dual element (i.e. element of $x$) vanishing on all of the elements is zero.</div><div></i> </p> </div><div></div>"
  },
  {
    "front": "If $\\mu$ is a Gaussian measure and $\\dim \\mathcal{H}_\\mu = \\infty$, then dilations of $\\mu$ are mutually singular.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a centred Gaussian measure on a separable Banach space $B$ with $\\dim \\mathcal{H}_\\mu = \\infty$. If $c\\in \\mathbb{R}$ and $D_c\\colon B\\to B;\\, x \\mapsto cx$, then $D_c(\\mu)$ and $\\mu$ are mutually singular except if $c = \\pm 1$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(e_n)$ be an orthonormal sequence in $\\mathcal{R}_\\mu \\subset L^2(\\mu)$. Then $e_n\\colon B \\to \\mathbb{R}$ are i.i.d. standard Gaussians, so if $S_N := \\frac 1N \\sum_{n=1}^\\infty e_n^2$ then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{N\\to \\infty} S_N(x) = 1 \\quad \\mu-\\text{a.a. } x\\in B.&nbsp;&nbsp;&nbsp; \\] Now $D_c(\\mu)$ is also a centred Gaussian measure w.r.t. which $e_n \\in \\mathcal{N}(0,c^2)$ are also i.i.d., so \\[&nbsp;&nbsp;&nbsp; \\lim_{N\\to \\infty} S_N(x) = c^2 \\quad D_c(\\mu)-\\text{a.a. } x\\in B.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Cameron-Martin theorem: If $\\mu$ is a Gaussian measure, for which elements $h\\in B$ is $T_h(\\mu)$ and $\\mu$ mutually absolutely continuous, and what is the density?",
    "back": "<div><p><b>Theorem.</b> <i>[Cameron-Martin]<br>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a centred Gaussian measure on a separable Banach space $B$, and $T_h\\colon B \\to B; x \\mapsto x+h$. Then $T_h(\\mu)$ and $\\mu$ are mutually absolutely continuous iff $h\\in \\mathcal{H}_\\mu$, in which case $T_h(\\mu)$ has density \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; G_h(x) :=&nbsp; \\mathrm{e}^{h^\\star(x) - \\left\\|h\\right\\|_\\mu^2 / 2},\\quad x\\in B,\\\\&nbsp;&nbsp;&nbsp; \\] w.r.t. $\\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose that $h\\in \\mathcal{H}_\\mu$. Then, for $\\ell\\in B^\\star$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{T_h(\\mu)}(\\ell)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\int \\mathrm{e}^{\\mathrm{i} \\ell(x)} T_h(\\mu)(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\mathrm{e}^{\\mathrm{i} \\ell(h)} \\mathrm{e}^{C_\\mu(\\mathrm{i}\\ell,\\mathrm{i}\\ell) / 2} = \\mathrm{e}^{\\mathrm{i} \\ell(h)} \\mathrm{e}^{-C_\\mu(\\ell,\\ell)&nbsp; /2}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; On the other hand,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{(G_h \\mu)}(\\ell)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\int \\mathrm{e}^{\\mathrm{i} \\ell(x)} G_h(x) \\mu(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-\\left\\|h\\right\\|_\\mu^2 / 2} \\int \\mathrm{e}^{(\\mathrm{i} \\ell + h^\\star)(x)} \\mu(\\mathop{}\\!\\mathrm{d} x)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{-\\left\\|h\\right\\|_\\mu^2 / 2} \\mathrm{e}^{\\frac{1}{2} C_\\mu(\\mathrm{i}\\ell+h^\\star,\\mathrm{i} \\ell+h^\\star)}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{\\frac{1}{2}C_\\mu(\\mathrm{i}\\ell,\\mathrm{i}\\ell)}&nbsp; \\mathrm{e}^{\\mathrm{i} C_\\mu(\\ell,h^\\star)}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\mathrm{e}^{\\mathrm{i} \\ell(h)} \\mathrm{e}^{-\\frac{1}{2}C_\\mu(\\ell,\\ell)}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Now suppose that $h\\not\\in \\mathcal{H}_\\mu$, so that there are $\\ell_n\\in B^\\star$ with $C_\\mu(\\ell_n,\\ell_n) = 1$ and $\\ell_n(h) \\to \\infty$. Then $\\ell_n \\sim \\mathcal{N}(0,1)$ under $\\mu$, and $\\ell_n \\sim \\mathcal{N}(\\ell_n(h),1)$ under $T_h(\\mu)$ (because $\\ell(T_h(\\cdot )) = \\ell(h) + \\ell(\\cdot )$). Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|\\mu - T_h(\\mu)\\right\\|_{\\text{TV}} \\ge \\left\\|\\ell_n(\\mu) - \\ell_n(T_h(\\mu))\\right\\|_\\text{TV} = \\left\\|\\mathcal{N}(0,1) - \\mathcal{N}(\\ell_n(h),1)\\right\\| _\\text{TV} \\to 2,\\\\&nbsp;&nbsp;&nbsp; \\] so $\\mu$ and $T_h(\\mu)$ are mutually singular.</i> </p> </div><div></div>"
  },
  {
    "front": "Connection of Cameron-Martin space with linear subspaces of full measure. What is $\\mu(\\mathcal{H}_\\mu)$?",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a centred Gaussian measure on a separable Banach space $B$. Then $\\mathcal{H}_\\mu$ is the intersection of all measurable linear subspaces of full measure. If $\\dim \\mathcal{H}_\\mu = \\infty$, then $\\mu(\\mathcal{H}_\\mu) = 0$</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $V$ be a measurable linear subspace of full measure and $h\\in \\mathcal{H}_\\mu$. Then $V-h$ has full measure by the Cameron-Martin theorem, in particular $(V-h)\\cap V \\neq&nbsp; \\emptyset $ so $h\\in V$.<br><br>&nbsp;&nbsp;&nbsp; Now suppose that $h\\not\\in \\mathcal{H}_\\mu$, so that there are $\\ell_n\\in B^\\star$ with $C_\\mu(\\ell_n,\\ell_n) = 1$ and $\\ell_n(h) \\ge 2^{n / 2}$. Then put $\\varphi(x) := \\sum_{n=1}^\\infty 2^{-n} \\ell_n(x)^2$ for $x\\in B$, which is measurable so a non-negative random variable, and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_B \\varphi(x) \\mu(\\mathop{}\\!\\mathrm{d} x) = \\sum_{n=1}^\\infty 2^{-n} C_\\mu(\\ell_n,\\ell_n) &lt; \\infty,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $\\varphi(x) &lt; \\infty$ for $\\mu$-a.a. $x\\in \\mathcal{B}$. Then $V := \\left\\{ x\\in B\\colon \\varphi(x) &lt; \\infty \\right\\} $ is a measurable linear subspace with full measure and $x\\not\\in V$.<br><br>&nbsp;&nbsp;&nbsp; Finally if $\\dim \\mathcal{H}_\\mu = \\infty$, take an orthonormal sequence $(e_n)$ in $\\mathcal{R}_\\mu$, so that $\\sup_n |e_n(x)| = \\infty$ for $\\mu$-a.a. $x\\in B$, say for $x\\in \\Omega_0$ (they are i.i.d. Gaussians). Then if there exists $x\\in \\mathcal{H}_\\mu \\cap \\Omega_0$, we would have \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\infty &gt; \\left\\|x\\right\\|_\\mu ^2 = \\left\\|x^\\star\\right\\|_{L^2(\\mu)}^2 = \\sum_{n=1}^\\infty \\left&lt;e_n,x^\\star \\right&gt;^2 _{L^2(\\mu)} = \\sum_{n=1}^\\infty C_\\mu(e_n,x^\\star)^2 = \\sum_{n=1}^\\infty e_n(x)^2 = \\infty.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Gaussian measures: Representation of $\\left\\|\\cdot \\right\\|_\\mu$ that allows for extension from $\\mathcal{H}_\\mu$ to $B$.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a centred Gaussian measure on a separable Banach space, and define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|x\\right\\|_\\mu := \\sup_{C_\\mu(\\ell,\\ell) \\le 1} \\ell(x),\\quad x\\in B.&nbsp;&nbsp;&nbsp; \\] Then $\\mathcal{H}_\\mu = \\left\\{ x\\in B\\colon \\left\\|x\\right\\|_\\mu &lt; \\infty \\right\\} $, and $\\left\\|\\cdot \\right\\|_\\mu$ coincides with the norm on $\\mathcal{H}_\\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $h\\in \\mathcal{H}_\\mu$. Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|h\\right\\|_{\\mathcal{H}_\\mu}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\sup_{k\\in \\mathring{\\mathcal{H}}_\\mu\\text{ normed}} \\left&lt;h,k \\right&gt; _{\\mathcal{H}_\\mu} = \\sup_{k^\\star \\in B^\\star} k^\\star(h) = \\left\\|h\\right\\|_\\mu, &nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and this is finite. Recalling the isomorphism $\\iota\\colon \\mathcal{R}_\\mu \\subset L^2(\\mu) \\to \\mathcal{H}_\\mu$, it now suffices to show that if $x\\in B$ with $\\left\\|x\\right\\|_\\mu &lt; \\infty$ then there is $x^\\star\\in \\mathcal{R}_\\mu$ with $\\iota(x^\\star)=x$. Indeed, let us define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi\\colon \\mathcal{R}_\\mu \\to \\mathbb{R};\\, f \\mapsto f(x).&nbsp;&nbsp;&nbsp; \\] This is linear and bounded, because for $\\ell \\in B^\\star$ (which is dense),<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| \\varphi(\\ell) \\right| ^2 = \\ell(x)^2 \\le C_\\mu(\\ell,\\ell) \\left\\|x\\right\\|_\\mu^2 = \\left\\|\\ell\\right\\|_{L^2(\\mu)}^2 \\left\\|x\\right\\|_\\mu^2,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so by Riesz there is $x^\\star \\in \\mathcal{R}_\\mu$ with<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\ell(x) = \\varphi(\\ell) = \\left&lt;\\ell,x^\\star \\right&gt; _{L^2(\\mu)} = \\ell(\\iota(x^\\star))&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; for all $\\ell\\in B^\\star$, so $x = \\iota(x^\\star)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Extension of Hilbert-Schmidt operator $A\\colon \\mathcal{H}_\\mu \\to \\mathcal{H}$ to $\\widehat{A}\\colon B\\to \\mathcal{H}$.",
    "back": "\"<div>Suppose $\\mu$ is a Gaussian measure on a separable Banach space $B$.<br><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $A\\colon \\mathcal{H}_\\mu\\to \\mathcal{H}$ is a Hilbert-Schmidt operator into a separable Hilbert space $\\mathcal{H}$, then there exists a measurable extension $\\widehat{A}\\colon B \\to \\mathcal{H}$ which is linear on a measurable linear subspace of full $\\mu$-measure, and such that $\\nu := \\widehat{A}^\\# \\mu$ is Gaussian with $C_\\nu(h,k) = \\left&lt;A^\\star h, A^\\star k \\right&gt; _\\mu$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(e_n^\\star)$ be an ONS in $\\mathcal{R}_\\mu$, and $V\\subset B$ a linear measurable subspace of full measure on which all $e_n^\\star$ are linear, and put, for $N\\in \\mathbb{N}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{A}_N(\\cdot ) := \\sum_{n=1}^N e_n^\\star(\\cdot ) (A e_n)\\colon B \\to \\mathcal{H}.&nbsp;&nbsp;&nbsp; \\] Then $\\widehat{A}_N$ is an $\\mathcal{H}$-valued martingale ($(e_n^\\star)$ are i.i.d. standard normals), and <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E}^\\mu \\left[ \\left\\|\\widehat{A}_N\\right\\|_\\mathcal{H}^2 \\right] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\int_B \\left&lt;\\widehat{A}_N(x),\\widehat{A}_N(x) \\right&gt; _\\mathcal{H} \\mu(\\mathop{}\\!\\mathrm{d} x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n,m=1}^N \\left&lt;A e_n,A e_m \\right&gt; _\\mathcal{H} \\int e_n^\\star(x) e_m^\\star(x) \\mu(\\mathop{}\\!\\mathrm{d} x)\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=1}^N \\left\\|A e_n\\right\\|_\\mathcal{H}^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left\\|A\\right\\|_{\\text{HS}}^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Hence by martingale convergence, the linear measurable space $V := \\left\\{ x\\in B\\colon \\widehat{A}_N(x) \\text{ converges} \\right\\} $ has full $\\mu$-measure, and we can put $\\widehat{A}$ to be the limit on $V$ and zero everywhere else. Also, if $x\\in \\mathcal{H}_\\mu$ then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A x = \\sum_{n=1}^\\infty \\left&lt;x,e_n \\right&gt; _\\mu (A e_n) = \\sum_{n=1}^\\infty e_n^\\star(x) (A e_n) = \\widehat{A} x.&nbsp;&nbsp;&nbsp; \\] Finally, if $h\\in \\mathcal{H}^\\star = \\mathcal{H}$, then $\\left&lt;\\widehat{A}(\\cdot ),h \\right&gt; = \\lim_{N\\to \\infty} \\sum_{n=1}^N e_n^\\star(\\cdot ) \\left&lt;h,Ae_n \\right&gt;_\\mathcal{H} $ is centred Gaussian with covariance<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C_\\nu(h,h) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\sum_{n=1}^\\infty \\left&lt;h,A e_n \\right&gt; _\\mathcal{H}^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\sum_{n=1}^\\infty \\left&lt;A^\\star h,e_n \\right&gt; _\\mu^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\left\\|A^\\star h\\right\\|_\\mu^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = C_\\mu(A^\\star h,A^\\star h).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>\""
  },
  {
    "front": "Unique measurable extension of $A\\colon \\mathcal{H}_\\mu \\to B'$ to $A\\colon B \\to B'$ (provided a suitable Gaussian measure on $B'$ exists).",
    "back": "\"<div>Suppose $\\mu$ is a Gaussian measure on a separable Banach space $B$.</div><div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Suppose $A\\colon \\mathcal{H}_\\mu \\to B'$ is a bounded linear operator into a separable Banach space $B'$, and there exists a centred Gaussian measure $\\nu$ on $B'$ with $C_\\nu(\\ell_1,\\ell_2) = \\left&lt;A^\\star \\ell_1,A^\\star \\ell_2 \\right&gt;_\\mu $. Then there exists a unique measurable extension $\\widehat{A}\\colon B \\to B'$ that is linear on a linear measurable subspace of full measure. It satsifies $\\nu = \\widehat{A}^\\# \\mu$.</i> </p> <br><br><p><i><b>Proof.</b>[Proof (Existence).]<br>&nbsp;&nbsp;&nbsp; We first show that there is such a map $\\widehat{A}$ which satisfies $\\nu = \\widehat{A}^\\# \\mu$. Let $\\mathcal{H}$ be a Hilbert space in which $B'$ embeds densely and continuously. Then let $\\nu' := \\iota^\\# \\nu$ where $\\iota\\colon B' \\hookrightarrow \\mathcal{H}$ is open by OMT, so that $\\nu'(B') = 1$, and if $\\ell,\\ell'\\in \\mathcal{H}^\\star \\subset B'^\\star$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C_{\\nu'}(\\ell,\\ell') = \\int_\\mathcal{H} \\ell(x) \\ell'(x) \\nu'(\\mathop{}\\!\\mathrm{d} x) = \\int_{B'} \\ell(x)\\ell'(x) \\nu(\\mathop{}\\!\\mathrm{d} x) = \\left&lt;A^\\star \\ell,A^\\star\\ell' \\right&gt; _\\mu.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] Now there exists a measurable map $\\widehat{A}'\\colon B \\to \\mathcal{H}$ which is linear on a measurable linear subspace of full $\\mu$-measure $V$ and which coincides with $A$ on $\\mathcal{H}_\\mu$, and such that $\\widehat{A}'^\\# \\mu = \\nu'$ (because it is Gaussian with the same covariance as $\\nu'$ above). Then, $V' := \\left\\{x\\colon \\widehat{A}'x \\in B_2\\right\\}$ satisfies $\\mu(V') = \\nu'(B_2) = 1$, so $V \\cap V'$ is linear measurable of full $\\mu$-measure, and \\[&nbsp;&nbsp;&nbsp; \\widehat{A}x := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{A}'x &amp;, x \\in V\\cap V',\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&amp;, \\text{ else},\\\\&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] satisfies<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (\\widehat{A}^\\# \\mu)(C) = \\mu(x\\colon \\widehat{A}x \\in C) = \\mu(x\\colon \\widehat{A}'x \\in C) = \\nu'(C) = \\nu(C).&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and all other requirements.</i> </p> <br><br><p><i><b>Proof.</b>[Proof (Uniqueness).]<br>&nbsp;&nbsp;&nbsp; It suffices to show that if $A\\colon B \\to B'$ is measurable, linear on a measurable linear subspace $V\\subset B$ with full measure, and $A = 0$ on $\\mathcal{H}_\\mu$, then $A = 0$ $\\mu$-a.e. <br><br>&nbsp;&nbsp;&nbsp; Let for $c\\in \\mathbb{R}$ and $\\ell\\in B'^\\star$ the set $V_{c,\\ell} := \\left\\{ x\\in V\\colon \\ell(Ax) \\le c \\right\\} $. This is measurable, and if $h\\in \\mathcal{H}_\\mu \\subset V$ and $x\\in V$, then $x + h \\in V$ and $\\ell(A(x+h)) = \\ell(Ax + Ah) = \\ell(Ax) \\le c$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(V_{c,\\ell}) = \\mu(V_{c,\\ell} + \\mathcal{H}_\\mu) = \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 &amp;, \\mu(V_{c,\\ell}) &gt; 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;, \\mu(V_{c,\\ell}) = 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] so there is $c_\\ell \\in\\mathbb{R}$ with $\\mu(V_{c,\\ell}) = 0$ for $c &lt; c_\\ell$ and $1$ for $c &gt; c_\\ell$ (it can't always be one or always be zero by $\\sigma$-additivity of $\\mu$ and because $\\mu(V) = 1$). Since $\\mu$ is reflection-invariant, $c_\\ell = 0$, that is $\\ell(Ax) = 0$ for $\\mu$-a.e. $x\\in B$ and every $\\ell\\in B'^\\star$, so&nbsp; $Ax = 0$ for $\\mu$-a.e. $x\\in B$.</i> </p> </div><div></div>\""
  },
  {
    "front": "Gaussian measure theory: Zero-one law for linear measurable subsets.",
    "back": "<div>Suppose $\\mu$ is a Gaussian measure on a separable Banach space $B$.<br><p><b>Proposition.</b>[$0$-$1$-law]<br>&nbsp;&nbsp;&nbsp; If $V\\subset B$ is linear and measurable, then $\\mu(V) \\in \\left\\{ 0,1 \\right\\} $.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It suffices to show that $\\mu(V) &gt; 0$ implies $\\mu(V) = 1$. If there is $h\\in \\mathcal{H}_\\mu$ with $h\\not\\in V$, then $\\alpha h + V$ for $\\alpha \\in \\mathbb{R}$ are uncountably many disjoint linear subspaces of positive measure, a contradiction, so $\\mathcal{H}_\\mu \\subset V$, but then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mu(V) = \\mu(V+\\mathcal{H}_\\mu) = 1\\\\&nbsp;&nbsp;&nbsp; \\] by the isoperimetric inequality.</i> </p> </div><div></div>"
  },
  {
    "front": "Isoperimetric inequality for Gaussian measure theory.",
    "back": "<div>Suppose $\\mu$ is a Gaussian measure on a separable Banach space $B$.<br><p><b>Lemma.</b> <i>[Isoperimetric inequality]<br>&nbsp;&nbsp;&nbsp; If $A\\subset B$ with $\\mu(A) &gt; 0$, then there is $\\alpha \\in \\mathbb{R}$ with $\\mu(A) = \\Phi(\\alpha)$, and for all $\\varepsilon &gt; 0$, $\\mu(A + B_\\varepsilon^\\mathcal{H}) \\ge \\Phi(\\alpha + \\varepsilon)$. In particular, $\\mu(A + \\mathcal{H}_\\mu) = 1$. </i> </p> <br><br><p><i><b>Proof.</b>[Proof Idea.]<br>&nbsp;&nbsp;&nbsp; In $d = 1$, we obtain the standard normal by projecting the uniform measure on $\\partial B_d(0,\\sqrt{d} )$ for large $d$. Then the statement is just saying that the volume of $A^\\varepsilon$ for $A\\subset \\partial B(0,1)$ is at least that of the fattening of an \"initial segment\" on the circles surface with the same volume as $A$.</i> </p> </div><div></div>"
  },
  {
    "front": "Every separable Banach space can be continuously embedded into a Hilbert space.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Every separable Banach space $B$ can be continuously embedded into a Hilbert space.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $(x_n)_{n\\in \\mathbb{N}}$ be dense in $B$, and $\\ell_n \\in B^\\star$ with $\\ell_n(x_n) \\ge \\left\\|x_n\\right\\| / 2$ and $\\left\\|\\ell_n\\right\\| = 1$. Then put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;x,y \\right&gt; _\\mathcal{H} := \\sum_{n=1}^\\infty 2^{-n} \\ell_n(x) \\ell_n(y).&nbsp;&nbsp;&nbsp; \\] This is bilinear, $\\left\\|x\\right\\|_\\mathcal{H} \\le \\left\\|x\\right\\|_B$, and definite. The completion is a Hilbert space in which $B$ is continuously embedded.</i> </p> <br><div></div></div>"
  },
  {
    "front": "Restriction of a centred Gaussian measure to a $\\mu$-full continuously embedded Banach space is a Gaussian measure.",
    "back": "<div>Suppose $\\mu$ is a Gaussian measure on a separable Banach space $B$.<br><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $B_0\\subset B$ is a continuously embedded Banach space such that $\\mu(B_0) = 1$, then $B_0^\\star \\hookrightarrow \\mathcal{R}_\\mu$ (by setting to zero on $B\\setminus B_0$), and $\\nu :=&nbsp; \\mu \\big\\vert_{B_0}$ is a Gaussian measure.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that $\\iota\\colon B_0 \\to B$ is continuous and injective, so open and hence bi-measurable by the open mapping theorem. That is, subsets of $B_0$ are measurable in $B_0$ iff they are measurable in $B$. Hence we can extend $\\ell_0\\in B_0^\\star$ to a map \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\ell(x) := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\ell_0(x)&amp;,x\\in B_0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 &amp;,\\text{ else},\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] which is measurable, and linear on the measurable linear subspace $B_0$ which has full $\\mu$-measure, so $\\ell\\in \\mathcal{R}_\\mu$. But then for $A\\subset \\mathbb{R}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\nu(\\ell_0\\in A) = \\mu(B_0 \\cap \\left\\{ \\ell\\in A \\right\\} ) = \\mu(\\ell\\in A) = \\mathbb{P}(\\mathcal{N}(0,\\left\\|\\ell\\right\\|_\\mu^2) \\in A),\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $\\ell_0(\\nu) = \\mathcal{N}(0,\\left\\|\\ell\\right\\|_\\mu^2)$, and $\\left\\|\\ell\\right\\|_\\mu^2 = \\int \\ell(x)^2 \\mu(\\mathop{}\\!\\mathrm{d} x) = \\int_{B_0} \\ell_0(x)^2 \\nu(\\mathop{}\\!\\mathrm{d} x)$.</i> </p> </div><div></div>"
  },
  {
    "front": "Cameron-Martin space of a Gaussian measure on a Hilbert space in terms of its covariance operator.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $\\mu$ be a Gaussian measure on a Hilbert space with covariance operator $K$. Then $\\mathcal{H}_\\mu^\\circ = \\operatorname{rng} K $ with $\\left&lt;h,k \\right&gt; _\\mu = \\left&lt;K^{-1}h, k \\right&gt;$. If $K e_n = \\lambda_n e_n$ denotes the eigendecomposition of $K$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathcal{H}_\\mu &amp;= \\left\\{ h\\colon \\sum_{n=1}^\\infty \\frac{1}{\\lambda_n} \\left&lt;h,e_n \\right&gt; ^2 &lt; \\infty \\right\\}, \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;h,k \\right&gt; _\\mu &amp;= \\sum_{n=1}^\\infty \\frac{1}{\\lambda_n} \\left&lt;h,e_n \\right&gt; \\left&lt;e_n,k \\right&gt; .&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $h\\in \\mathcal{H}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\widehat{C}_\\mu(h) = \\int \\left&lt;h,x \\right&gt; x \\mu(\\mathop{}\\!\\mathrm{d} x) = k \\mapsto \\int \\left&lt;h,x \\right&gt; \\left&lt;k,x \\right&gt; \\mu(\\mathop{}\\!\\mathrm{d} x) = k \\mapsto \\left&lt;Kh,k \\right&gt; = Kh,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and $\\left&lt;Kh,Kk \\right&gt; _\\mu = C_\\mu(h,k) = \\left&lt;Kh,k \\right&gt;$. Now if $h\\in \\mathcal{H}$, then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left\\|h\\right\\|_\\mu &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\sup \\left\\{ \\left&lt;k,h \\right&gt; \\colon C_\\mu(k,k) = 1 \\right\\},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and for any such $k$, $C_\\mu(k,k) = \\left&lt;Kk,k \\right&gt; = \\sum_{n=1}^\\infty \\lambda_n \\left&lt;k,e_n \\right&gt; ^2$, so<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;k,h \\right&gt; ^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\left(\\sum_{n=1}^\\infty (\\lambda_n^{1 / 2} \\left&lt;k,e_n \\right&gt; ) (\\lambda_n^{-1 / 2} \\left&lt;h,e_n \\right&gt; ) \\right)^2\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\le \\left( \\sum_{n=1}^\\infty \\lambda_n \\left&lt;k,e_n \\right&gt; ^2 \\right) \\left( \\sum_{n=1}^\\infty \\lambda_n ^{-1} \\left&lt;h,e_n \\right&gt; ^2 \\right) \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\sum_{n=1}^\\infty \\lambda_n^{-1} \\left&lt;h,e_n \\right&gt; ^2.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Choosing $\\left&lt;k,e_n \\right&gt; = \\alpha \\lambda_n^{-1} \\left&lt;h,e_n \\right&gt; $ for $n \\le N$ and $\\left&lt;k,e_n \\right&gt; = 0$ for $n &gt; N$ with $\\alpha$ such that $C_\\mu(k,k) = 1$ gives equality in Hölder, so $\\left&lt;k,h \\right&gt; ^2 = \\sum_{n=1}^N \\lambda_n^{-1} \\left&lt;h,e_n \\right&gt; ^2$, and letting $N\\to \\infty$ gives the claim.</i> </p> </div><div></div>"
  },
  {
    "front": "Kolmogorov's continuity criterion for Hilbert space-valued Gaussian processes (without proof).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $C\\colon [0,1]\\times [0,1] \\to \\mathcal{L}(\\mathcal{H},\\mathcal{H})$ is such that $C(t,s)$ is positive, symmetric, and trace class for all $t,s\\in [0,1]$, and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{tr} C(t,t) + \\operatorname{tr} C(s,s) - 2 \\operatorname{tr} C(t,s) \\le K \\left| t-s \\right| ^\\alpha\\\\&nbsp;&nbsp;&nbsp; \\] for some $K &gt; 0$ and $\\alpha \\in (0,2]$, then there exists a Gaussian measure on the Banach space $C([0,1],\\mathcal{H})$ with \\[&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left&lt;h,X_t \\right&gt; \\left&lt;k,X_t \\right&gt;&nbsp; \\right] = \\int\\limits_{C([0,1],\\mathcal{H})} \\left&lt;h,x(t) \\right&gt; \\left&lt;k,x(s) \\right&gt; \\mu(\\mathop{}\\!\\mathrm{d} x) = \\left&lt;C(x,y) h, k \\right&gt; .\\] That is, the fidis of $X$ are determined by $\\operatorname{cov}(X_t,X_s) = C(t,s)$.</i> </p> </div><div></div>"
  },
  {
    "front": "General Brownian motion in a Hilbert space, and its Cameron-Martin space.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Suppose $\\mathcal{H}$ is a separable Hilbert space with a Gaussian measure $\\mu$ (covariance operator $K$). Then there exists a Gaussian measure on $C([0,1],\\mathcal{H})$ satisfying $\\operatorname{cov}(X_t,X_s) = (t\\wedge s) K$, that is \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left&lt;h,X_t \\right&gt; \\left&lt;k,X_s \\right&gt;&nbsp; \\right] = (t\\wedge s) \\left&lt;K h, k \\right&gt; .\\] Its Cameron-Martin space is $H^{1,2}_0([0,1],\\mathcal{H}_\\mu)$. If $Ke_n = \\lambda_n e_n$, this amounts to taking i.i.d. increments $X_t - X_s \\sim \\sqrt{t-s} \\sum_{n=1}^\\infty \\sqrt{\\lambda_n} N_n e_n$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; If we define $C(t,s) := (t\\wedge s) K$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\operatorname{tr} C(t,t) + \\operatorname{tr} C(s,s) - 2\\operatorname{tr} C(t,s) = |t-s| \\operatorname{tr} K,\\\\&nbsp;&nbsp;&nbsp; \\] so by Kolmogorov there exists a Gaussian measure $\\nu$ on $B:= C([0,1],\\mathcal{H})$ with the given properties. To find the Cameron-Martin space, note that a determining (hence weak$^\\star$ dense) set in the dual space of $B$ is given by elements of the form \\[&nbsp;&nbsp;&nbsp; \\ell_{h,t} := \\left&lt;h,\\cdot&nbsp; \\right&gt; \\circ \\delta_t,\\qquad h\\in \\mathcal{H},t\\in [0,1].&nbsp;&nbsp;&nbsp; \\] Then,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f_{h,t} := \\widehat{C}_\\mu (\\ell_{h,t}) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\int \\left&lt;h,x(t) \\right&gt; x \\nu(\\mathop{}\\!\\mathrm{d} x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\left[ s \\mapsto \\left( k\\mapsto \\int \\left&lt;h,x(t) \\right&gt; \\left&lt;k,x(s) \\right&gt; \\nu(\\mathop{}\\!\\mathrm{d} x) \\right) \\right] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\bigg[ s \\mapsto \\Big( k \\mapsto (t\\wedge s) \\left&lt;Kh,k \\right&gt;&nbsp; \\Big) \\bigg] \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= s \\mapsto (t\\wedge s) Kh,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;f_{h,t}, f_{k,s} \\right&gt; _\\nu = C_\\mu (\\ell_{h,t},\\ell_{k,s}) = \\int \\left&lt;h,x(t) \\right&gt; \\left&lt;k,x(s) \\right&gt; \\nu(\\mathop{}\\!\\mathrm{d} x) = (t\\wedge s)\\left&lt;K h, k \\right&gt; ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and also<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left&lt;f_{h,t},f_{k,s} \\right&gt; _{H^{1,2}_0} = \\int_0^1 \\left&lt; f_{h,t}', f_{k,s}' \\right&gt;_{\\mathcal{H}_\\mu} = (t\\wedge s) \\left&lt;Kh, Kk \\right&gt; _{\\mathcal{H}_\\mu} = (t\\wedge s) \\left&lt;K h, k \\right&gt; _\\mathcal{H}.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; It remains to see that the span of the $f_{h,t}$ is dense in $H^{1,2}_0([0,1],\\mathcal{H}_\\mu)$ (with its own norm) just as with ordinary Brownian motion.</i> </p> </div><div></div>"
  },
  {
    "front": "Motivation, definition, and Cameron-Martin space of \"standard normal\" and cylindrical Brownian motion in a Hilbert space (using larger Hilbert space).",
    "back": "<div><b>Motivation.</b> We would like to make sense of something like $\\sum_{n=1}^\\infty N_n e_n$ for $N_n \\sim \\mathcal{N}(0,1)$ i.i.d. For that purpose, let $(\\varepsilon_n &gt; 0)$ be a summable sequence and let $\\mathcal{H}'$ be the separable Hilbert space of formal linear combinations $\\sum_{n=1}^n \\alpha_n e_n$ with $\\sum_{n=1}^\\infty \\varepsilon_n \\alpha_n^2 &lt; \\infty$. This can be made precise by letting $\\mathcal{H}'$ be the closure of $\\mathcal{H}$ under the norm \\[\\left\\|h\\right\\|_{\\mathcal{H}'} := \\sum_{n=1}^\\infty \\varepsilon_n \\left&lt;h,e_n \\right&gt; ^2.\\] (This is a complete inner product space, and $\\mathcal{H}$ is dense in it by construction. The $(e_n)$ are still an orthogonal basis, so the elements of $\\mathcal{H}'$ are exactly $ h = \\sum_{n=1}^\\infty \\alpha_n e_n$ with $\\left\\|h\\right\\|_\\mathcal{H}'^2 = \\sum_{n=1}^\\infty \\varepsilon_n \\alpha_n^2 &lt; \\infty$.)<br><br>Then let $X := \\sum_{n=1}^\\infty N_n e_n$, which is a.s. absolutely convergent in $\\mathcal{H}'$, and<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left&lt;h,X \\right&gt; _{\\mathcal{H}'} \\left&lt;k,X \\right&gt; _{\\mathcal{H}'} \\right] = \\sum_{n,m=1}^\\infty \\delta_{nm} \\left&lt;h,e_n \\right&gt;_{\\mathcal{H}'} \\left&lt;k,e_n \\right&gt; _{\\mathcal{H}'} = \\sum_{n=1}^\\infty \\left&lt;\\iota^\\star h,e_n \\right&gt; _{\\mathcal{H}} \\left&lt;\\iota^\\star k,e_n \\right&gt; _{\\mathcal{H}} = \\left&lt;\\iota^\\star h, \\iota^\\star k \\right&gt; _{\\mathcal{H}} = \\left&lt;\\iota \\iota^\\star h,k \\right&gt; _{\\mathcal{H}'},\\\\\\end{align*}[/$$]<br>so the covariance operator in $\\mathcal{H}'$ is $\\iota\\iota^\\star$, which is trace-class.<br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A <i>standard normal variable in $\\mathcal{H}$</i> is a Gaussian measure on $\\mathcal{H}'$ with covariance operator $\\iota \\iota^\\star$. Its Cameron-Martin space is $\\mathcal{H}$.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $\\mathcal{H}_\\mu^\\circ$ is the range of $\\iota \\iota^\\star$ with \\[&nbsp;&nbsp;&nbsp; \\left&lt;\\iota \\iota^\\star h, \\iota \\iota^\\star k \\right&gt; _\\mu = \\left&lt;\\iota \\iota^\\star h, k \\right&gt; _{\\mathcal{H}'} = \\left&lt;\\iota^\\star h, \\iota^\\star k \\right&gt; _\\mathcal{H}.\\] Thus with the identification $\\iota$, $\\mathcal{H}_\\mu^\\circ $ is isomorphic to the range of $\\iota^\\star$ with the norm of $\\mathcal{H}$, whose completion is $\\mathcal{H}$ (it contains all $e_n$).</i> </p> <br><br><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; A <i>cylindrical Wiener process on $\\mathcal{H}$</i> is a $\\iota\\iota^\\star$-Brownian motion in $\\mathcal{H}'$, that is a Gaussian measure on $C([0,1],\\mathcal{H}')$ with \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left&lt;h,W(t) \\right&gt; _{\\mathcal{H}'} \\left&lt;k,W(s) \\right&gt;_{\\mathcal{H}'}&nbsp; \\right] = (t\\wedge s) \\left&lt;\\iota\\iota^\\star h, k \\right&gt; _{\\mathcal{H}'}.&nbsp;&nbsp;&nbsp; \\] Its Cameron Martin space is $H^{1,2}_0([0,1], \\mathcal{H})$.</p> <br><br>If it was true that $W$ was $\\mathcal{H}$-valued, and $h = \\iota^\\star h'$, $k = \\iota^\\star k'$, then <br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\mathbb{E} \\left[ \\left&lt;h,W(t) \\right&gt; _\\mathcal{H} \\left&lt;k,W(s) \\right&gt; _\\mathcal{H} \\right] &nbsp;&nbsp;&nbsp; = \\mathbb{E} \\left[ \\left&lt;h', \\iota W(t) \\right&gt; _{\\mathcal{H}'} \\left&lt;k', \\iota W(s) \\right&gt; _{\\mathcal{H}'} \\right] &nbsp;&nbsp;&nbsp; = (t\\wedge s) \\left&lt;\\iota \\iota^\\star h', k' \\right&gt; _{\\mathcal{H}'} = (t\\wedge s) \\left&lt;h,k \\right&gt; _{\\mathcal{H}}.\\end{align*}[/$$]</div><div></div>"
  },
  {
    "front": "Hauptsatz der Integralrechnung für Pfadintegrale in $\\mathbb{C}$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; If $f\\colon D \\subset \\mathbb{C} \\to \\mathbb{C}$ is continuous with antiderivative $F\\colon D \\to \\mathbb{C}$ and $\\gamma\\colon I \\to D$ is a pdb path, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_\\gamma f(z) \\mathop{}\\!\\mathrm{d} z = F(\\gamma(b)) - F(\\gamma(a)).&nbsp;&nbsp;&nbsp; \\] In particular, $\\int_\\gamma f(z) \\mathop{}\\!\\mathrm{d} z = 0$ if $\\gamma$ is closed.</p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_\\gamma f(z) \\mathop{}\\!\\mathrm{d} z = \\int_a^b F'(\\gamma(t)) \\gamma'(t) \\mathop{}\\!\\mathrm{d} t = \\int_a^b \\frac{\\mathop{}\\!\\mathrm{d} }{\\mathop{}\\!\\mathrm{d} t} F(\\gamma(t)) \\mathop{}\\!\\mathrm{d} t = F(\\gamma(b)) - F(\\gamma(a)).&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Umlaufzahl $n(\\gamma,z)$ for path $\\gamma$ and&nbsp; $z\\in \\mathbb{C}\\setminus |\\gamma|$.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $\\gamma$ be a piecewise db closed path and $z \\not\\in |\\gamma|$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n(\\gamma,z) := \\frac{1}{2\\pi i} \\int_\\gamma \\frac{1}{w-z} \\mathop{}\\!\\mathrm{d} w\\\\&nbsp;&nbsp;&nbsp; \\] is $\\mathbb{Z}$-valued, constant on connected components of $\\mathbb{C} \\setminus |\\gamma|$ and $0$ on the unbounded component.</p> </div><div></div><div>Intuitively, this is just the Hauptsatz ($\\int_\\gamma 1/w \\mathop{}\\!\\mathrm{d} w = \\log \\gamma(b) - \\log \\gamma(a)$) except that we can end up on different branches of the logarithm at the start and end of the path.<br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLoG $z = 0$. Then, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi(t) := \\exp \\left( \\int_a^t \\frac{\\gamma'(s)}{\\gamma(s)} \\mathop{}\\!\\mathrm{d} s \\right) &nbsp;&nbsp;&nbsp; \\] satisfies $\\varphi' = \\gamma' \\varphi / \\gamma$, so $(\\varphi / \\gamma)' = 0$, so \\[&nbsp;&nbsp;&nbsp; \\frac{\\varphi(b)}{\\gamma(b)} = \\frac{\\varphi(a)}{\\gamma(a)} = \\frac{1}{\\gamma(a)}\\implies 1 = \\varphi(b) = \\exp \\left( \\int_\\gamma \\frac{1}{w}\\mathop{}\\!\\mathrm{d} s \\right) ,\\\\&nbsp;&nbsp;&nbsp; \\] so $\\int_\\gamma \\frac{1}{w}\\mathop{}\\!\\mathrm{d} w = 2\\pi k$ for some $n(\\gamma,z) := k\\in \\mathbb{Z}$. Since $n(\\gamma,\\cdot )$ is continuous on $\\mathbb{C}\\setminus |\\gamma|$ (DCT) and $\\mathbb{Z}$-valued, it is constant on connected components. For the unbounded component, note that \\[|n(\\gamma,z)| \\le \\frac{1}{2\\pi} \\int_a^b \\left| \\frac{\\gamma'(t)}{\\gamma(t) - z} \\right| \\mathop{}\\!\\mathrm{d} t \\to 0\\\\\\] as $|z| \\to \\infty$.</i> </p> </div><div></div>"
  },
  {
    "front": "Local Cauchy Theorem (and Lemma von Goursat)",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $D \\subset \\mathbb{C}$ open and convex, $p\\in D$. If $f\\colon D \\to \\mathbb{C}$ is continuous and holomorphic on $D\\setminus \\left\\{ p \\right\\} $, then <br>&nbsp;&nbsp;&nbsp; <ol>  <li>(<b>Lemma von Goursat</b>) $\\int_{\\partial \\Delta} f(z) \\mathop{}\\!\\mathrm{d} z = 0$ for every triangle $\\Delta \\subset D$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>(<b>Local Cauchy Theorem</b>) $f$ has an antiderivative on $D$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$\\int_\\gamma f = 0$ for every closed path $\\gamma$ in $D$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $\\Delta$ is degenerate there is nothing to show. Suppose that $p\\not\\in \\Delta$. Half all sides to obtain a decomposition in four trianges, and we have $\\int_{\\partial \\Delta} = \\sum_{j=1}^4 \\int_{\\partial \\Delta^j}$. Hence at least one of the triangles must satisfy $|J| := \\left| \\int_{\\partial\\Delta} f\\right| \\le 4 |J_1|$. Inductively we obtain a decreasing sequence of triangles $\\Delta_n$ with $|J| \\le 4^n |J_n|$. Their diameter tends to zero, so their intersection is a singleton $z_0 \\in \\Delta$. Now, the function $z \\mapsto f(z_0) + f'(z_0) (z-z_0)$ has an antiderivative so &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |J_n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\left| \\int_{\\partial \\Delta_m} f(z) \\mathop{}\\!\\mathrm{d} z \\right|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = \\left| \\int_{\\partial \\Delta_m} \\left( f(z) - f(z_0) - f'(z_0)(z-z_0) \\right) \\mathop{}\\!\\mathrm{d} z \\right| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Furthermore $\\left|\\frac{f(z) - f(z_0)}{z-z_0} - f'(z_0)\\right| &lt; \\varepsilon|z-z_0|$ for $z$ sufficiently close to $z_0$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |J_n| \\le \\varepsilon L(\\partial \\Delta_m)^2 = \\varepsilon 4^{-m} L(\\partial \\Delta)^2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $|J| &lt; \\varepsilon L(\\partial \\Delta)^2$ for any $\\varepsilon &gt; 0$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If $p$ is one of the three corners we can use a sub-triangle and use continuity in the triangle's corner points, if it is inside or on one of the edges we can subdivide into triangles that have $p$ as their corner.&nbsp;&nbsp;&nbsp;</li>  <li>Pick $a \\in D$ and put $F(z) := \\int_{[a,z]}f(w)\\mathop{}\\!\\mathrm{d} w$, so for $z_0,z \\in D$ with $\\Delta = (a,z,z_0)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 = \\int_{\\partial \\Delta} f = F(z) + \\int_{[z,z_0]}f - F(z_0),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; so &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\varphi(z) := \\frac{F(z) - F(z_0)}{z-z_0} - f(z_0) = \\frac{1}{z-z_0} \\int_{[z,z_0]} (f(w) - f(z_0))\\mathop{}\\!\\mathrm{d} w\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; satisfies $|\\varphi(z)| \\le \\sup_{w\\in [z,z_0]} \\left| f(w) - f(z_0) \\right| \\to 0 $ as $z \\to z_0$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Follows from (ii).&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Cauchy Integral Formula",
    "back": "<div><p><b>Theorem.</b> <i>[Local Cauchy Integral Formula]<br>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and convex, $\\gamma$ a closed ssd path in $D$, and $f\\in H(D)$. Then,\\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\frac{1}{2\\pi \\mathrm{i} n(\\gamma,z)} \\int_\\gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w,\\quad z\\in D\\setminus |\\gamma|.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\]</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Fix $z\\in D\\setminus |\\gamma|$, and define \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g(w) := \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{f(w) - f(z)}{w-z},&amp; w \\neq z,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f'(z) ,&amp; w = z.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}&nbsp;&nbsp;&nbsp; \\] Then $g\\in H(D\\setminus \\left\\{ z \\right\\} )$ and continuous at $z$, so \\[&nbsp;&nbsp;&nbsp; 0 = \\int_\\gamma g(w)\\mathop{}\\!\\mathrm{d} w = \\int_\\gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w - f(z) \\underbrace{\\int_\\gamma \\frac{1}{w-z}\\mathop{}\\!\\mathrm{d} w}_{= 2\\pi \\mathrm{i} n(\\gamma,z)}.&nbsp;&nbsp;&nbsp; \\] </i> </p> </div><div></div>"
  },
  {
    "front": "Infinite differentiability and local power series for holomorphic functions.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ is open and $f\\in H(D)$ then $f'\\in H(D)$ (so all derivatives) and for any $z_0\\in D$ and $z\\in B(z_0,r) \\subset D$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\sum_{n=0}^{\\infty} a_n (z-z_0)^n\\\\&nbsp;&nbsp;&nbsp; \\] where $a_n = \\frac{f^{(n)}(z_0)}{n!}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; On any $B(z_0,r)$, by Cauchy's integral formula for $\\gamma = z_0 + r' \\mathrm{e}^{\\mathrm{i} t}$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\frac{1}{2\\pi \\mathrm{i}} \\int_\\gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w,\\\\&nbsp;&nbsp;&nbsp; \\] and the integrand can be expanded as a power series. In particular $f'\\in H(D)$, and the coefficients of the power series have to have this form.</i> </p> </div><div></div>"
  },
  {
    "front": "Morera's theorem (converse of Goursat's lemma)",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ is open and convex, $f\\colon D\\to \\mathbb{C}$ is continuous, and $\\int_{\\partial \\Delta} f(w) \\mathop{}\\!\\mathrm{d} w = 0$ for all triangles, then $f\\in H(D)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; $F(z) = \\int_{[z_0,z]}f(w)\\mathop{}\\!\\mathrm{d} w$ is an antiderivative as in the proof of the local Cauchy theorem, so $f = F' \\in H(D)$.</i> </p> </div><div></div>"
  },
  {
    "front": "\"Identit\\\"\"atssatz (complex analysis)\"",
    "back": "\"<div><p><b>Theorem.</b> <i>[Identit\\\"\"atssatz]<br>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and connected, $f,g\\in H(D)$ such that $\\left\\{ f=g \\right\\} $ has a limit point in $D$. Then $f = g$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; It is sufficient to show that the set of zeros of a non-constant $f\\in H(D)$ is discrete. Let $f(z_0) = 0$, and let $m = \\min \\left\\{ n\\in \\mathbb{N}\\colon a_n(z_0) \\neq 0 \\right\\} $ where $(a_n(z_0))$ are the coefficients of the power series of $f$ in $B(z_0,r_0)$. This minimum exists: if $a_n(z_0) = 0$ for all $n\\in \\mathbb{N}$, then the set $\\left\\{ z\\in D\\colon a_n(z) = 0 \\forall n\\in \\mathbb{N} \\right\\} $ would be non-empty and open(!), and so would its complement, so $f \\equiv 0$ in $D$ (it is connected). This means that $f(z) = (z-z_0)^m g(z)$ for some $g\\in H(D)$ with $g(z_0) \\neq 0$, in particular $g(z) \\neq 0$ in a neighbourhood of $z_0$, so $f(z) \\neq 0$ in a neighbourhood of $z_0$ (with $z_0$ excluded).</i> </p> </div><div></div>\""
  },
  {
    "front": "Riemannscher Hebbarkeitssatz (classification of removable singularities).",
    "back": "<div><p><b>Theorem.</b> <i>[Riemann'scher Hebbarkeitssatz]<br>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and $f\\in H(D\\setminus \\left\\{ a \\right\\} )$. Then $f$ has a removable singularity at $a$ iff $f$ is bounded in a neighbourhood of $a$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $f$ is bounded in $\\dot{B}(a,r)$, and put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h(z) := &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\begin{cases}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (z-a)^2 f(z), &amp; z \\neq a,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0, &amp;z = a,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{cases}\\qquad z\\in B(a,r).&nbsp;&nbsp;&nbsp; \\] Then $h\\in H(D\\setminus \\left\\{ a \\right\\} )$ and \\[&nbsp;&nbsp;&nbsp; \\frac{h(z) - h(a)}{z-a} = (z-a) f(z) \\to 0,\\quad z \\to a,\\\\\\] so actually $h\\in H(D)$ with $h(a) = h'(a) = 0$, so $h(z) = a_2 (z-a)^2 + a_3(z-a)^3+\\ldots $, so $f(z) = a_2 + a_3(z-a) + \\ldots $</i> </p> </div><div></div>"
  },
  {
    "front": "Definitions of pole and essential singularity.",
    "back": "<div><p><b>Definition.</b>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ is open and $f\\in H(D\\setminus \\left\\{ a \\right\\} )$, then $a$ is called a <i>pole of order $m\\in \\mathbb{N}_0$</i> if there is $g\\in H(D)$ with $g(a) \\neq 0$ and \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\frac{g(z)}{(z-a)^m},\\quad z\\in D\\setminus \\left\\{ a \\right\\} .&nbsp;&nbsp;&nbsp; \\] (A pole of order zero is a removable singularity.) If $f$ does not have a pole at $a$ then it is called an <i>essential singularity</i>.</p> </div><div></div>"
  },
  {
    "front": "Großer Satz von Picard (and proof of simpler version Casorati-Weierstraß).",
    "back": "<div><p><b>Theorem.</b> <i>[Großer Satz von Picard]<br>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ is open and $f\\in H(D\\setminus \\left\\{ a \\right\\} )$ has an essential singularity at $a$, then there is $c\\in \\mathbb{C}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\mathbb{C}\\setminus \\left\\{ c \\right\\} \\subset f(B(a,r))&nbsp;&nbsp;&nbsp; \\] for all $r &gt; 0$ for which $B(a,r) \\subset D$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; We proof instead that $\\overline{f(B(a,r))}= \\mathbb{C}$, known as Casorati-Weierstraß. Let $r &gt; 0$ and suppose there was $w_0\\in \\mathbb{C}$ and $\\varepsilon &gt; 0$ with $|f(z) - w_0| \\ge \\varepsilon$ for all $z\\in B(a,r)$. WLOG $w_0 = 0$ (consider $f - w_0$), then define $g(z) = \\frac{1}{f(z)}$ which has a removable singularity at $a$ because $|g(z)| \\le 1 / \\varepsilon$. Also $g\\not\\equiv 0$ so there is $m\\in \\mathbb{N}_0$ and $h\\in H(D)$ with $h \\neq 0$ in $B(a,r')$ with $g(z) = (z-a)^m h(z)$, so \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\frac{h(z)^{-1}}{(z-a)^m},\\quad z\\in B(a,r')&nbsp;&nbsp;&nbsp; \\] has a pole at $a$, a contradiction.</i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of poles (in general, and of specific order).",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ open and $f\\in H(D\\setminus \\left\\{ a \\right\\} )$. <br>&nbsp;&nbsp;&nbsp; <ol>  <li>$f$ has a pole (of any positive order) at $a$ iff $\\lim_{z\\to a}|f(z)| = \\infty$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f$ has a pole of order $m\\in \\mathbb{N}_0$ at $a$ iff $\\lim_{z\\to a}(z-a)^m f(z) \\neq 0$ exists.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f$ has a removable singularity then $|f(z)| \\not\\to \\infty$, and if it has an essential singularity then $\\overline{f(B(a,r))} = \\mathbb{C}$, and if it has a pole then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |f(z)| = \\left|\\frac{g(z)}{(z-a)^m}\\right| \\to \\infty.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] &nbsp;&nbsp;&nbsp;</li>  <li>Clearly if $f$ has a pole of order $m$ this is true by definition. Suppose $L := \\lim_{z\\to a} (z-a)^m f(z) \\neq 0$ and $m &gt; 0$ (we know this for $m = 0$ already). Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |f(z)| = \\underbrace{\\left| z-a \\right| ^{-m}}_{\\to \\infty} \\underbrace{\\left| (z-a)^m f(z) \\right| }_{\\to L} \\to \\infty,\\\\&nbsp;&nbsp;&nbsp; \\] so $f$ has a pole, and then it is easy to show that the order must be $m$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Cauchy integral formula for derivatives.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and convex, $f\\in H(D)$, $\\gamma$ an ssd path in $D$. Then for $z\\in D \\setminus |\\gamma|$ and $n\\in \\mathbb{N}_0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f^{(n)}(z) = \\frac{n!}{2\\pi \\mathrm{i} n(\\gamma,z)} \\int_\\gamma \\frac{f(w)}{(w-z)^{n+1}}\\mathop{}\\!\\mathrm{d} w.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; WLOG $z = 0$. We know this for $n = 0$, suppose it is true for $n\\in \\mathbb{N}_0$. Then for $h \\in \\dot{B}(0,r)\\subset D\\setminus |\\gamma|$ and $n_\\gamma := n(\\gamma,0) = n(\\gamma,h)$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{f^{(n)}(h) - f^{(n)}(0)}{h} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{n!}{2\\pi\\mathrm{i} n_\\gamma h} \\int_\\gamma \\left(\\frac{f(w)}{(w-h)^{n+1}} - \\frac{f(w)}{w^{n+1}}\\right)\\mathop{}\\!\\mathrm{d} s\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;\\to \\frac{(n+1)!}{2\\pi \\mathrm{i} n_\\gamma} \\int_\\gamma \\frac{f(w)}{w^{n+2}}\\mathop{}\\!\\mathrm{d} w,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used <br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{(w-h)^{n+1}} - \\frac{1}{w^{n+1}} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{w^{n+1}(1 - (1 - h / w)^{(n+1))}}{w^{n+1}(w-h)^{n+1}}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= (1+o(1)) \\frac{w^{n+1}(n+1) h / w}{w^{2n+2}} \\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= (1+o(1)) h \\frac{n+1}{w^{n+2}}&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Cauchy's derivative bounds for holomorphic functions.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\in H(B(z_0,R))$, then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| f^{(n)}(z_0) \\right| \\le \\left\\|f\\right\\|_\\infty \\frac{n!}{R^n},\\quad n\\in \\mathbb{N}.&nbsp;&nbsp;&nbsp; \\] </i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $\\gamma(t) = z_0 + r \\mathrm{e}^{\\mathrm{i} t}$ with $r \\in (0,R)$, so by Cauchy's integral formula,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| f^{(n)}(z_0) \\right| = \\left| \\frac{n!}{2\\pi \\mathrm{i}} \\int_\\gamma \\frac{f(w)}{(w-z_0)^{n+1}}\\mathop{}\\!\\mathrm{d} w \\right| \\le \\frac{n!}{2\\pi}&nbsp; \\frac{\\left\\|f\\right\\|_\\infty}{r^{n+1}}L(\\gamma) = \\left\\|f\\right\\|_\\infty \\frac{n!}{r^n},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; because $L(\\gamma) = 2\\pi r$.</i> </p> </div><div></div>"
  },
  {
    "front": "Liouville's theorem (on bounded whole functions).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\in H(\\mathbb{C})$ is bounded, then $f$ is constant.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $M := \\left\\|f\\right\\|_\\infty &lt; \\infty$, then for any $z\\in \\mathbb{C}$ and $R &gt; 0$, by Cauchy's derivative bounds,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\left| f'(z) \\right| \\le \\frac{M}{R},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $f'\\equiv 0$, so $f$ is constant.</i> </p> </div><div></div>"
  },
  {
    "front": "Kleiner Satz von Picard (without proof).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; If $f\\in H(\\mathbb{C})$ then $\\left| \\mathbb{C}\\setminus f(\\mathbb{C}) \\right| \\le 1 $.</i> </p> </div><div></div>"
  },
  {
    "front": "Fundamentalsatz der Algebra.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Every non-constant polynomial over $\\mathbb{C}$ has a zero.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Suppose $p$ has no zeros. Then $\\frac{1}{p} \\in H(\\mathbb{C})$ is bounded ($1 / |p| \\colon \\mathbb{C} \\to [0,\\infty)$ is a continuous function with $1 / |p(z)| \\to 0$ as $|z| \\to \\infty$), so constant.</i> </p> <br><div></div></div>"
  },
  {
    "front": "Gebietstreue (open mapping theorem for holomorphic functions).",
    "back": "<div><p><b>Theorem.</b> <i>[Gebietstreue]<br>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ is open and $f\\in H(D)$ then $f(D)$ is open. If $D$ is connected then so is $f(D)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Connectedness is just continuity of $f$. It suffices to show that if $f\\in H(D)$ with $0\\in D$ and $f(0) = 0$, then there is $\\varepsilon &gt; 0$ with $B(0,\\varepsilon) \\subset f(D)$. Let $r &gt; 0$ such that $B(0,2r)\\subset D$ and $f \\neq 0$ in $\\dot{B}(0,r)$.<br><br>&nbsp;&nbsp;&nbsp; Let $w \\in B(0,\\varepsilon)$ and suppose $w \\not\\in f(D)$. Then $g := \\frac{1}{f - w} \\in H(D)$, \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |g(0)| \\ge \\frac{1}{\\varepsilon} \\to \\infty,\\qquad \\max_{|z| = r} |g(z)| \\le \\frac{1}{\\min_{|z| = r} |f(z)| - \\varepsilon} \\to \\frac{1}{\\min |f(z)|} &gt; 0,\\\\&nbsp;&nbsp;&nbsp; \\] but also $|g(0)| \\le \\max_{|z| = r} |g(z)|$ by Cauchy's integral formula, a contradiction if $\\varepsilon$ is small enough.</i> </p> </div><div></div>"
  },
  {
    "front": "Maximum principle for holomorphic functions.",
    "back": "<div><p><b>Theorem.</b> <i>[Maximum principle]<br>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ open and $f\\in H(D)$ not constant. <br>&nbsp;&nbsp;&nbsp; <ol>  <li>$|f|$ has no local maxima in $D$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $D$ is bounded and $f$ continuous on $\\overline{D}$, then $|f(z)| &lt; \\max_{\\partial D} |f|$.&nbsp;&nbsp;&nbsp;</li></ol><br>&nbsp;&nbsp;&nbsp; If $f$ has no zeros in $D$, then corresponding statements hold for the minimum.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; (i) follows from the fact that $f(D)$ is open, (ii) follows because the continuous function $|f|$ attains a maximum in the closed set $\\overline{D}$ but it can't be attained in $D$ by (i). If $f$ has no zeros, we can apply this to $1 / f$.</i> </p> </div><div></div>"
  },
  {
    "front": "Schwartz' Lemma (on holomorphic $f\\colon \\mathbb{D} \\to \\mathbb{D}$ where $\\mathbb{D} = B(0,1)$).",
    "back": "<div><p><b>Lemma.</b> <i>[Schwartz]<br>&nbsp;&nbsp;&nbsp; Let $f\\in H(\\mathbb{D})$ with $f(\\mathbb{D}) \\subset \\mathbb{D}$ and $f(0) = 0$. Then<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$|f(z)| \\le |z|$ and $|f'(0)| \\le 1$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If either inequality is attained anywhere in $\\mathbb{D}$, then $f(z) = cz$ for some $c \\in \\mathbb{C}$ with $|c| = 1$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Let $g(z) := \\frac{f(z)}{z}$ for $z\\neq 0$. Then $g(0) = f'(0)$ and $g\\in H(\\mathbb{D})$. By the maximum principle, for $z\\in \\mathbb{D}$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |g(z)| \\le \\max_{|z| = r} \\left| \\frac{f(z)}{z} \\right| \\le \\frac{1}{r} \\to 1,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; so $|g| \\le 1$ on $\\mathbb{D}$. If either inequality in (i) is attained it means $|g|$ attains its maximum in $\\mathbb{D}$ so it must be constant.</i> </p> </div><div></div>"
  },
  {
    "front": "Local invertability of holomorphic functions.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; If $D\\subset \\mathbb{C}$ open and $f\\in H(D)$ and $f'(z_0) \\neq 0$, then there is $r &gt; 0$ such that $f$ is injective on $B(z_0,r)$ and $f^{-1} \\in H(f(B(z_0,r)))$ with $(f^{-1})'(w) = \\frac{1}{f'(f^{-1}(w))}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; By continuity of $(z,w) \\mapsto \\frac{f(z)-f(w)}{z-w}$ (with the obvious extension; continuity is not trivial but straightforward), there is $r &gt; 0$ such that $|f(z_1)-f(z_2)| \\ge \\frac{1}{2}|f'(z_0)| |z_1-z_2|$ for all $z_1,z_2\\in B(x_0,r)$. This implies injectivity and Lipschitz continuity of $f^{-1}$. Then<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\lim_{w' \\to w}\\frac{f^{-1}(w) - f^{-1}(w')}{w-w'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{w' \\to w} \\frac{f^{-1}(w) - f^{-1}(w')}{f(f^{-1}(w)) - f(f^{-1}(w'))}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\lim_{z'\\to z} \\frac{z-z'}{f(z) - f(z')}\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;= \\frac{1}{f'(z)},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; where we used that $z' := f^{-1}(w') \\to f^{-1}(w) =: z$.</i> </p> </div><div></div>"
  },
  {
    "front": "A necessary and a sufficient condition for $f\\in H(D)$ to be injective.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open, and $f\\in H(D)$. <br>&nbsp;&nbsp;&nbsp; <ol>  <li>If $f$ is injective, then $f' \\neq 0$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $\\Im f' \\neq 0$ or $\\Re f' \\neq 0$, then $f$ is injective.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Suppose $f'(z_0) = 0$. Then there is a neighbourhood of $f(z_0)$ in which all valued are attained by $f$ on $D$ at least twice ($f$ locally looks like $f(z_0) + c z^m$ for $m\\ge 2$), so $f$ is not injective.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Say $\\Im f' \\neq 0$ on $D$. Then for any $z,w$, with $\\gamma = z \\to w$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\Im f(z) - \\Im f(w) = \\int_\\gamma \\Im f'(a) \\mathop{}\\!\\mathrm{d} a = \\int_0^1 \\Im f'(\\gamma_s) \\cdot \\gamma_s \\mathop{}\\!\\mathrm{d} s,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and $s \\mapsto \\Im f'(\\gamma_s) \\cdot \\gamma_s$ is either everywhere positive or negative, so the integral is not zero. &nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Konvergenzsatz von Weierstraß (on locally uniform limits of holomorphic functions).",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ open, and suppose that $f_n \\in H(D)$ converge locally uniformly to $f\\colon D\\to \\mathbb{C}$. Then $f\\in H(D)$ and $f_n' \\to f'$ locally uniformly.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Firstly $f$ must be continuous. Now let $\\Delta \\subset D$ be a triangle; then $f_n \\to f$ uniformly on $\\Delta$, so \\[&nbsp;&nbsp;&nbsp; \\int_{\\partial \\Delta} f = \\lim_{n\\to \\infty} \\int_{\\partial \\Delta} f_n = 0.\\] Now let $B(z_0,2r) \\subset D$. Then by Cauchy's inequalities, since $f_n - f\\in H(D)$,<br>[$$]\\begin{align*}&nbsp;&nbsp;&nbsp; \\sup_{|z-z_0| &lt; r} |f_n'(z) - f'(z)| \\le \\frac{1}{r} \\sup_{|z-z_0| &lt; 2r} |f_n(z) - f_n(z_0)| \\to 0.\\end{align*}[/$$]</i> </p> </div><div></div>"
  },
  {
    "front": "Global Cauchy Theorem.",
    "back": "<div><p><b>Theorem.</b> <i>[Global Cauchy Theorem]<br>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and $f\\in H(D)$. Then if $\\Gamma$ is cycle in $D$ homologous to zero (i.e. $n(\\Gamma,z) = 0$ for all $z\\not\\in D$), \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n(\\Gamma,z) f(z) = \\frac{1}{2\\pi \\mathrm{i}} \\int_\\Gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w,\\quad z\\in D\\setminus |\\Gamma|.&nbsp;&nbsp;&nbsp; \\] In particular, $\\int_\\Gamma f(w)\\mathop{}\\!\\mathrm{d} w = 0$ for all $f\\in H(D)$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; For $z\\in D$ we can put \\[&nbsp;&nbsp;&nbsp; h(z) := \\frac{1}{2\\pi \\mathrm{i}} \\int_\\gamma \\underbrace{\\frac{f(w) - f(z)}{w-z}}_{=: h(w,z)} \\mathop{}\\!\\mathrm{d} w,\\\\&nbsp;&nbsp;&nbsp; \\] where we use the continuous continuation on $z\\in |\\Gamma|$. Note that wherever $n(\\gamma,z) = 0$, this is equal to \\[&nbsp;&nbsp;&nbsp; h(z) = \\frac{1}{2\\pi \\mathrm{i}} \\int_\\gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w,\\quad z\\not\\in D,\\\\&nbsp;&nbsp;&nbsp; \\] which serves as a definition in $\\mathbb{C}\\setminus D$. Using continuity and Goursat's lemma we can show that $h\\in H(\\mathbb{C})$, but also $|h(z)| \\to 0 $ as $|z| \\to \\infty$ (because the second formula holds on $D^{c}\\cup \\left\\{ z\\colon n(\\gamma,z) = 0 \\right\\} $ whose complement is bounded), but then $h$ is bounded so it must be constant zero, so for $z\\in D\\setminus |\\Gamma|$, \\[&nbsp;&nbsp;&nbsp; 0 = h(z) = \\frac{1}{2\\pi \\mathrm{i}} \\int_\\gamma \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w - n(\\Gamma,z) f(z).\\] For the final statement, let $F(w) := f(w)(w-z)$, so $F\\in H(D)$ and \\[\\int_\\Gamma f(w) \\mathop{}\\!\\mathrm{d} w = \\int_\\Gamma \\frac{F(w)}{w-z} \\mathop{}\\!\\mathrm{d} w = 2\\pi \\mathrm{i} n(\\Gamma,z) F(z) = 0.\\] </i> </p> </div><div></div>"
  },
  {
    "front": "Existence of holomorphic logarithms and antiderivatives for holomorphic functions on simply connected domains.",
    "back": "<div><p><b>Proposition.</b>&nbsp;&nbsp;&nbsp; Let $D\\subset \\mathbb{C}$ be open and simply connected, and $f\\in H(D)$<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$f$ has an antiderivative on $D$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>If $f \\neq 0$ in $D$ then there is $g\\in H(D)$ with $\\mathrm{e}^{g} = f$.&nbsp;&nbsp;&nbsp;</li></ol></p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fix $a\\in D$, then for any $z\\in D$ let $\\gamma$ be a path from $a$ to $z$ and put \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; F(z) := \\int_\\gamma f(w)\\mathop{}\\!\\mathrm{d} w,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] which does not depend on the choice of $\\gamma$ by the global Cauchy theorem because $D$ is simply connected. Now if $B(z,r) \\subset D$ we show $F'(z) = f(z)$ as before.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f' / f\\in H(D)$ so it has an antiderivative $g$. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (f \\mathrm{e}^{-g})' = \\mathrm{e}^{-g} \\left[ f' -f g' \\right] = 0,\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] so $f = c \\mathrm{e}^g = \\mathrm{e}^{c'+g}$ where $\\mathrm{e}^{c'} = c$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><div></div></div>"
  },
  {
    "front": "Laurent series of a holomorphic function. Definition of residue.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $f\\in H(A(r,R,a))$ for $R &gt; r \\ge 0$ and $a\\in \\mathbb{C}$. Then there exists a unique sequence $(a_n)_{n\\in \\mathbb{Z}}$ such that \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\sum_{n=-\\infty}^{\\infty} a_n (z-a)^n,\\qquad z\\in A(r,R,a),\\\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\] with locally uniform convergence. In fact (for any $r &lt; \\rho &lt; R$), \\[&nbsp;&nbsp;&nbsp; a_n = \\frac{1}{2\\pi \\mathrm{i}} \\int_{\\partial B(0,\\rho)} \\frac{f(w)}{(w-a)^{n+1}} \\mathop{}\\!\\mathrm{d} w.&nbsp;&nbsp;&nbsp; \\] The residue of $f$ at $a$ is $\\text{Res}(f,a) := a_{-1}$.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; First note that $a_n$ does not depend on $\\rho$, because $w \\mapsto \\frac{f(w)}{(w-a)^{n+1}} \\in H(A(r,R,a))$ for any $n\\in \\mathbb{Z}$ by the global Cauchy theorem. Now let $z\\in A(r,R,a)$, $\\gamma_r,\\gamma_R$ cycles around $\\partial B(0,r_1)$ and $\\partial B(0,r_2)$ where $r &lt; r_1 &lt; |z| &lt; r_2 &lt; R$. Then by the global Cauchy theorem,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f(z) = \\frac{1}{2\\pi \\mathrm{i}} \\int_{\\gamma_R} \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w - \\frac{1}{2\\pi \\mathrm{i}} \\int_{\\gamma_r} \\frac{f(w)}{w-z}\\mathop{}\\!\\mathrm{d} w.&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; Then, if $w\\in |\\gamma_R|$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\frac{1}{w-z} = \\frac{1}{w-a} \\frac{1}{1 - \\frac{z-a}{w-a}} = \\sum_{n=0}^\\infty \\frac{(z-a)^n}{(w-a)^{n+1}},\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and if $w \\in |\\gamma_r|$, \\[&nbsp;&nbsp;&nbsp; \\frac{1}{w-z} = - \\frac{1}{w-a} \\frac{\\frac{w-a}{z-a}}{1 - \\frac{w-a}{z-a}} = - \\sum_{n=1}^\\infty \\frac{(w-a)^{n-1}}{(z-a)^n} = - \\sum_{n=-1}^{-\\infty} \\frac{(z-a)^n}{(w-a)^{n + 1}}.\\] Plugging this back in gives $f(z) = \\sum_{n=-\\infty}^\\infty a_n (z-a)^n$ </i> </p> </div><div></div>"
  },
  {
    "front": "Characterisation of singularities in terms of Laurent series.",
    "back": "<div><p><b>Lemma.</b> <i>&nbsp;&nbsp;&nbsp; Let $f\\in H(D\\setminus \\left\\{ a \\right\\} )$, and let $(a_n)$ be the coefficients of the Laurent series of $f$ around $a$.<br>&nbsp;&nbsp;&nbsp; <ol>  <li>$f$ has a removable singularity at $a$ iff $a_{-n} = 0$ for all $n \\in \\mathbb{N}$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f$ has a pole of order $m\\in \\mathbb{N}$ at $a$ iff $a_{-m} \\neq 0$ and $a_{-n} = 0$ for all $n &gt; m$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f$ has an essential singularity at $a$ iff $a_{-n} \\neq 0$ for infinitely many $n\\in \\mathbb{N}$.&nbsp;&nbsp;&nbsp;</li></ol></i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; <ol>  <li>Recall that $f$ has a removable singularity iff it is bounded around $a$ iff $a_{-n} = 0$ for all $n\\in \\mathbb{N}$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>$f$ has a removable singularity iff $(z-a)^m f(z) \\to L \\neq 0$ iff $a_{-m}\\neq 0$ but $a_{-n} = 0$ for all $n &gt; m$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>  <li>Follows from (i) and (ii).&nbsp;&nbsp;&nbsp;</li></ol></i> </p> </div><div></div>"
  },
  {
    "front": "Residue theorem.",
    "back": "<div><p><b>Theorem.</b> <i>&nbsp;&nbsp;&nbsp; Let $f\\in H(D\\setminus A)$ for $D\\subset \\mathbb{C}$ open and $A\\subset D$ discrete, and $\\Gamma$ a cycle in $D\\setminus A$ homologous to zero. Then \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_\\Gamma f(w)\\mathop{}\\!\\mathrm{d} w = 2\\pi \\mathrm{i} \\sum_{a\\in A} n(\\Gamma,a) \\operatorname{Res}(f,a),\\\\&nbsp;&nbsp;&nbsp; \\] and the sum is finite.</i> </p> <br><br><p><i><b>Proof.</b>&nbsp;&nbsp;&nbsp; Assume there were infinitely many $a_k \\in A$ with $n(\\Gamma,a_k) \\neq 0$. Because none of them are in the single unbounded component of $\\mathbb{C}\\setminus |\\Gamma|$ the sequence is bounded say $a_k \\to a$. Since $A$ is discrete in $D$ we must have $a\\in \\mathbb{C}\\setminus D$, so $n(\\Gamma,a) = 0$ because it is homologous to zero. The connected component of $\\mathbb{C}\\setminus |\\Gamma|$ that contains $a$ is open, so almost all $a_k$ are in it, but then $n(\\Gamma,a_k)=0$ for those $k$, a contradiction.<br><br>&nbsp;&nbsp;&nbsp; Let $A_0 \\subset A$ be the finite set of circled singularities, and denote \\[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; L_a(z) := \\sum_{n=-1}^{-\\infty} a_n(a) (z-a)^n\\\\&nbsp;&nbsp;&nbsp; \\] the negative part of the Laurent series of $f$ around $a\\in A_0$. Then $D_0 := D\\setminus A \\cup A_0$ is open, $\\Gamma$ is homologous to zero in $D_0$ and $f - \\sum_{a\\in A_0} L_a \\in H(D_0)$ because all of the singularities are removable. Thus by the global Cauchy integral theorem,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_\\Gamma f(w) \\mathop{}\\!\\mathrm{d} w = \\sum_{a\\in A_0} \\int_{\\Gamma} L_a(w) \\mathop{}\\!\\mathrm{d} w,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; and for any given $a\\in A_0$,<br>&nbsp;&nbsp;&nbsp; [$$]\\begin{align*}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\int_{\\Gamma} L_a(w) \\mathop{}\\!\\mathrm{d} w = \\sum_{n=1}^{\\infty} a_{-n}(a) \\int_\\Gamma \\frac{1}{(w-a)^{n}} \\mathop{}\\!\\mathrm{d} w = a_{-1}(a)\\cdot 2\\pi \\mathrm{i} n(\\Gamma,a) ,\\\\&nbsp;&nbsp;&nbsp; \\end{align*}[/$$]<br>&nbsp;&nbsp;&nbsp; because all the terms with $n\\ge 2$ have antiderivatives.</i> </p> </div><div></div>"
  }
]